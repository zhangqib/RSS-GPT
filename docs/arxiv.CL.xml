<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</title>
<link>https://arxiv.org/abs/2504.04737</link>
<guid>https://arxiv.org/abs/2504.04737</guid>
<content:encoded><![CDATA[
<div> Keywords: Fact-based Judgment Prediction, TathyaNyaya, legal context, FactLegalLlama, AI-assisted decision-making  

<br><br>Summary:  
This paper presents TathyaNyaya, the largest annotated dataset for Fact-based Judgment Prediction and Explanation (FJPE) tailored to the Indian legal system, featuring judgments from the Supreme Court and various High Courts. The dataset's name is derived from the Hindi words "Tathya" (fact) and "Nyaya" (justice), emphasizing its focus on factual statements essential for real-world judicial outcomes. Alongside TathyaNyaya, the authors introduce FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model optimized for producing high-quality explanations in FJPE tasks. Fine-tuned on the TathyaNyaya dataset, FactLegalLlama combines predictive accuracy with clear and relevant explanations, addressing crucial transparency and interpretability needs in AI-driven legal systems. The proposed methodology integrates transformers for binary judgment prediction and the FactLegalLlama for explanation generation, creating a robust framework focused on advancing FJPE within the Indian legal domain. TathyaNyaya not only exceeds existing datasets in both scale and diversity but also sets a new benchmark for the development of explainable AI in legal analysis, highlighting the importance of factual accuracy and domain-specific tuning for improved predictive performance and interpretability. <div>
arXiv:2504.04737v3 Announce Type: replace 
Abstract: In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Judgment Prediction, NyayaRAG, Indian legal system, retrieval-augmented generation, predictive accuracy

<br><br>Summary: Legal Judgment Prediction (LJP) is a crucial area of AI in the legal field that focuses on forecasting judicial outcomes and enhancing the interpretability of legal reasoning. Existing methods in India often concentrate on internal case elements, neglecting essential components of common law, such as statutory provisions and judicial precedents. This paper introduces NyayaRAG, a Retrieval-Augmented Generation (RAG) framework designed to emulate realistic courtroom scenarios. NyayaRAG integrates factual case descriptions, pertinent legal statutes, and semantically retrieved previous cases to bolster the predictive process. The framework's effectiveness is evaluated based on the impact of these combined inputs on court decision predictions and the quality of generated legal explanations, specifically adapted for the Indian legal environment. Comprehensive performance assessments utilize various configurations and include both standard lexical and semantic metrics alongside Large Language Model (LLM)-based evaluators like G-Eval. The findings indicate that enhancing factual inputs with structured legal knowledge notably boosts both the accuracy of predictions and the quality of explanations provided, indicating the framework's potential for advancing legal judgment prediction in India. <div>
arXiv:2508.00709v3 Announce Type: replace 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2509.00974</link>
<guid>https://arxiv.org/abs/2509.00974</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Question Answering, Ranked Preference Reinforcement Optimization, Clinical Chain-of-Thought, Reinforcement Learning, Preference-Driven Reasoning

<br><br>Summary: The article addresses the challenges in medical question answering, particularly the inaccuracies in reasoning chains produced by existing large language models (LLMs). To overcome this, the authors introduce a novel framework called Ranked Preference Reinforcement Optimization (RPRO), which merges reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO is distinctive in its use of task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows while detecting and correcting low-quality reasoning. Unlike traditional methods that rely on pairwise preferences, RPRO employs groupwise ranking optimization based on the Bradley--Terry model and utilizes KL-divergence regularization to ensure stable training. The framework's effectiveness is demonstrated through experiments on multiple datasets, including PubMedQA and MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital, showing notable improvements over strong baseline models. Impressively, their 2B-parameter model surpasses larger models ranging from 7B to 20B parameters, showcasing the potential of preference optimization combined with quality-driven refinement to create more reliable and clinically relevant medical LLMs. <div>
arXiv:2509.00974v3 Announce Type: replace 
Abstract: Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO distinguishes itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns model outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley--Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA, MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital (FEMH) demonstrate consistent improvements over strong baselines. Remarkably, our 2B-parameter model outperforms much larger 7B--20B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement provides a scalable and clinically grounded approach to building more reliable medical LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Silenced Biases: The Dark Side LLMs Learned to Refuse</title>
<link>https://arxiv.org/abs/2511.03369</link>
<guid>https://arxiv.org/abs/2511.03369</guid>
<content:encoded><![CDATA[
<div> Keywords: safety-aligned models, fairness evaluation, silenced biases, Silenced Bias Benchmark, activation steering

<br><br>Summary: The prevalence of safety-aligned large language models (LLMs) in sensitive applications necessitates effective evaluation of their fairness, particularly in contexts where biased outputs can cause harm. Traditional evaluation methods often rely on question-answer schemes, mistakenly interpreting model refusals as indicators of fairness, resulting in a misleading assessment. This work introduces the notion of silenced biases, which refers to unfair preferences embedded in the models' latent space and masked by safety-alignment. Existing methods addressing similar biases have limitations, including dependence on prompt manipulation and handcrafted queries that can introduce new biases. To address these challenges, the authors propose the Silenced Bias Benchmark (SBB), designed to reveal these concealed biases through activation steering that minimizes model refusals during QA evaluations. SBB is structured for easy adaptation to various demographic groups and subjects, aiming to enhance fairness evaluation frameworks and promote the development of fairer models beyond the constraints of alignment training. The authors demonstrate their methodology across multiple LLMs, revealing significant discrepancies between the models' overt responses and their underlying fairness issues. <div>
arXiv:2511.03369v2 Announce Type: replace 
Abstract: Safety-aligned large language models (LLMs) are becoming increasingly widespread, especially in sensitive applications where fairness is essential and biased outputs can cause significant harm. However, evaluating the fairness of models is a complex challenge, and approaches that do so typically utilize standard question-answer (QA) styled schemes. Such methods often overlook deeper issues by interpreting the model's refusal responses as positive fairness measurements, which creates a false sense of fairness. In this work, we introduce the concept of silenced biases, which are unfair preferences encoded within models' latent space and are effectively concealed by safety-alignment. Previous approaches that considered similar indirect biases often relied on prompt manipulation or handcrafted implicit queries, which present limited scalability and risk contaminating the evaluation process with additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to uncover these biases by employing activation steering to reduce model refusals during QA. SBB supports easy expansion to new demographic groups and subjects, presenting a fairness evaluation framework that encourages the future development of fair models and tools beyond the masking effects of alignment training. We demonstrate our approach over multiple LLMs, where our findings expose an alarming distinction between models' direct responses and their underlying fairness issues.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy</title>
<link>https://arxiv.org/abs/2511.11594</link>
<guid>https://arxiv.org/abs/2511.11594</guid>
<content:encoded><![CDATA[
<div> Keywords: fuzzy matching, TimeStampEval, transcripts, retrieval accuracy, Assisted Fuzzy  

<br /><br />Summary: Traditional fuzzy matching struggles with semantically identical but syntactically different quotes, particularly when aligning official records with speech-to-text transcripts. TimeStampEval is introduced as a benchmark for retrieving millisecond timestamps from lengthy transcripts based on non-verbatim quotes. The simple two-stage method significantly enhances retrieval accuracy while reducing inference costs by over 90%. The primary application is automating long-form podcasts using Congressional Record clips. Key findings include: (1) Prompt design is more crucial than model selection; positioning the query before the transcript with compact formatting can boost accuracy by 3-20 points and reduce token count by 30-40%. (2) Distinct "off-by-one" errors indicate that models are capable of understanding the task but may misplace boundaries. (3) A modest reasoning budget (600-850 tokens) can elevate accuracy from 37% to 77% for weaker models and above 90% for stronger ones. (4) The "Assisted Fuzzy" method, which employs RapidFuzz pre-filtering followed by LLM verification, enhances fuzzy match accuracy by up to 50 points, halves latency, and cuts the cost per correct result by as much as 96%. Tests across ten varied transcripts confirm the approach's robustness. <div>
arXiv:2511.11594v1 Announce Type: new 
Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</title>
<link>https://arxiv.org/abs/2511.11793</link>
<guid>https://arxiv.org/abs/2511.11793</guid>
<content:encoded><![CDATA[
<div> Keywords: MiroThinker, open-source, interaction scaling, reasoning, reinforcement learning  

<br /><br />Summary: MiroThinker v1.0 is an open-source research agent aimed at enhancing tool-augmented reasoning and information-seeking capabilities. Unlike traditional models that primarily focus on scaling size or context length, MiroThinker innovates by exploring interaction scaling at the model level, facilitating deeper and more frequent interactions with the environment. This method allows the model to leverage feedback and external information, correcting errors and refining reasoning trajectories. Through reinforcement learning, MiroThinker accomplishes efficient interaction scaling, operating with a 256K context window and capable of executing up to 600 tool calls per task, which supports complex multi-turn reasoning and real-world research workflows. The 72B variant of MiroThinker excels across four benchmarks—GAIA, HLE, BrowseComp, and BrowseComp-ZH—achieving accuracy scores of 81.9%, 37.7%, 47.1%, and 55.6%, respectively, outperforming existing open-source models and nearing the performance of commercial models like GPT-5-high. The study shows that research performance improves consistently with increased interaction depth, establishing it as a crucial dimension for developing advanced research agents alongside model capacity and context windows. <div>
arXiv:2511.11793v1 Announce Type: new 
Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Notion that Language Models Reason</title>
<link>https://arxiv.org/abs/2511.11810</link>
<guid>https://arxiv.org/abs/2511.11810</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning, Markov kernel, statistical pattern matchers, epistemic uncertainty

<br /><br />Summary:  
1. The paper critically examines the meaning of "reasoning" as applied to language models (LMs) in natural language processing (NLP), highlighting inconsistencies in common definitions relative to LM training and operation.  
2. It adopts the perspective that transformer-based LMs function as implicit finite-order Markov kernels, mapping contexts to conditional token probabilities rather than executing explicit logical or reasoning algorithms.  
3. Reasoning-like outputs from LMs emerge from learned statistical regularities and approximate invariances in these probabilistic kernels, rather than from genuine logical inference or explicit mechanisms.  
4. This framework supports the characterization of LMs as "statistical pattern matchers" rather than authentic reasoners, explaining why LMs can produce outputs that resemble reasoning without guarantees of logical consistency.  
5. The distinction is crucial for properly assessing epistemic uncertainty in LMs and calls for careful attention to how computational processes underlying LMs are described and understood in NLP research, fostering clearer conceptual foundations and dialogue. <div>
arXiv:2511.11810v1 Announce Type: new 
Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2511.11821</link>
<guid>https://arxiv.org/abs/2511.11821</guid>
<content:encoded><![CDATA[
<div> Keywords: information extraction, large language models, hydropower licensing, model scaling, validation methods  

<br /><br />Summary:  
This study investigates the use of large language models (LLMs) for extracting information from regulatory documents specifically related to hydropower licensing. Seven open-weight LLMs ranging from 0.6 billion to 70 billion parameters were evaluated to understand performance relative to computational requirements. A key finding is a critical performance threshold at 14 billion parameters, where validation methods become significantly more effective—models below this size achieve poor F1 scores (<0.15), while those at or above this level reach viable F1 scores around 0.64. Models designed for consumer deployment max out at about 51% F1 score due to limitations in validation, whereas the largest models approach 77% F1 but demand enterprise-grade infrastructure for practical use. The research also exposes systematic hallucination patterns in smaller models, where perfect recall paradoxically signals extraction failure rather than accuracy. This work provides the first comprehensive resource-performance mapping for open-weight LLMs in regulatory information extraction, offering actionable guidance to select models based on deployment constraints. The insights extend beyond hydropower, shedding light on how parameter scaling impacts extraction tasks more broadly and enabling evidence-based decisions in choosing appropriate model sizes for regulatory compliance applications. <div>
arXiv:2511.11821v1 Announce Type: new 
Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Autoformalization of LLM-generated Outputs for Requirement Verification</title>
<link>https://arxiv.org/abs/2511.11829</link>
<guid>https://arxiv.org/abs/2511.11829</guid>
<content:encoded><![CDATA[
<div> Autoformalization, Large Language Models, Formal Verification, Logical Consistency, Natural Language Requirements<br /><br />Summary:<br /><br />This paper explores the application of autoformalization, which is the process of converting informal natural language statements into formal logic, leveraging Large Language Models (LLMs). The authors aim to address the challenge of verifying whether LLM-generated structured outputs accurately represent their natural language inputs, which currently lacks formal verification methods. Two experiments are conducted to demonstrate the approach's feasibility. In the first experiment, the LLM-based autoformalizer is able to identify logical equivalence between two differently worded natural language requirements, proving its potential to perform consistency checks. The second experiment showcases the tool's ability to detect logical inconsistencies between a natural language requirement and an LLM-generated output, highlighting its use as a verification mechanism. Although the study is preliminary and limited in scope, the results indicate that autoformalization can be an effective method for ensuring the fidelity and logical consistency of outputs produced by LLMs. This foundational work sets the stage for future, more comprehensive research aimed at improving the reliability of autoformalization and formal verification techniques in natural language understanding and generation tasks. <div>
arXiv:2511.11829v1 Announce Type: new 
Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection</title>
<link>https://arxiv.org/abs/2511.11857</link>
<guid>https://arxiv.org/abs/2511.11857</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, narrative analysis, movie scripts, clustering, computational learning

<br /><br />Summary: 
This paper addresses the challenges in automated narrative analysis within Natural Language Understanding, emphasizing the need for deep computational semantic representations paired with syntactic processing. Given the massive volume of narrative data, it advocates for automated semantic analysis and computational learning over manual approaches. The proposed framework specifically targets the analysis of sentiment arcs in movie scripts, allowing for both high-level and low-level concept extraction from narratives. Utilizing dictionary-based sentiment analysis, the framework employs a custom lexicon developed with the LabMTsimple storylab module, which incorporates Valence, Arousal, and Dominance scores derived from the NRC-VAD dataset. A notable advancement in this framework is its ability to cluster similar sentiment plots using Ward's hierarchical clustering technique. The experimental evaluation conducted on a dataset of movies demonstrates that the resulting analysis provides valuable insights for consumers and readers, assisting them in selecting stories or narratives that align with their preferences. Ultimately, this work contributes to a more nuanced understanding and analysis of narratives through sentiment exploration and character context. <div>
arXiv:2511.11857v1 Announce Type: new 
Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches</title>
<link>https://arxiv.org/abs/2511.11867</link>
<guid>https://arxiv.org/abs/2511.11867</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, radiology reports, follow-up adherence detection, machine learning classifiers, prompt optimization<br /><br />Summary: This work addresses the lack of domain-specific datasets to evaluate large language models (LLMs) in radiology by introducing an annotated corpus of 6,393 radiology reports from 586 patients labeled for follow-up imaging status. The study systematically compares traditional machine learning classifiers—including logistic regression (LR) and support vector machines (SVM)—and newer models such as Longformer and a fully fine-tuned Llama3-8B-Instruct, alongside generative LLMs like GPT-4o and the open-source GPT-OSS-20B. Generative models were tested under a baseline (Base) and a task-optimized (Advanced) setting, the latter emphasizing metadata, recommendation sentences, and context to improve performance. Prompt refinement further enhanced GPT-OSS-20B's reasoning accuracy. Evaluation metrics included precision, recall, and F1 scores, with confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846), establishing a strong benchmark. GPT-4o in the Advanced setting achieved the highest performance (F1 = 0.832), closely followed by GPT-OSS-20B (F1 = 0.828). Traditional classifiers LR and SVM also showed competitive results (F1 ≈ 0.776), demonstrating that while optimized LLMs can approach human-level agreement, simpler interpretable models remain valuable and resource-efficient baselines for clinical applications in radiology follow-up adherence detection. <div>
arXiv:2511.11867v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers</title>
<link>https://arxiv.org/abs/2511.11878</link>
<guid>https://arxiv.org/abs/2511.11878</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, healthcare, Brazilian Portuguese, MedPT, patient-doctor interactions

<br /><br />Summary: This article introduces MedPT, a large-scale corpus designed for Brazilian Portuguese, aimed at enhancing healthcare technologies. It consists of 384,095 authentic question-answer pairs derived from patient-doctor interactions, addressing the limitations of existing models that focus primarily on high-resource languages. The dataset underwent a rigorous multi-stage curation process, incorporating both quantitative and qualitative analyses to minimize noise and enrich queries. MedPT is further enhanced by LLM-driven annotation, classifying questions into seven semantic categories to better reflect user intent. The dataset exhibits a thematic breadth of 3,200 topics and highlights unique linguistic characteristics, including the natural asymmetry in patient-doctor communication. To demonstrate its effectiveness, the authors benchmarked a medical specialty routing task, achieving an impressive 94% F1-score using a fine-tuned 1.7B parameter model. Moreover, a qualitative error analysis indicated that misclassifications were tied to genuine clinical ambiguities, emphasizing the dataset's semantic depth. The authors aim to release MedPT to promote the creation of sustainable, culturally-aware medical technologies for the Portuguese-speaking community. <div>
arXiv:2511.11878v1 Announce Type: new 
Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts</title>
<link>https://arxiv.org/abs/2511.11883</link>
<guid>https://arxiv.org/abs/2511.11883</guid>
<content:encoded><![CDATA[
<div> Clinical notes, large language models, interpretability, generalizability, ICU mortality prediction<br /><br />Summary:<br /><br />1. Clinical notes hold rich and valuable contextual information but are challenging to use due to their unstructured nature, which introduces unintended biases such as gender and racial bias.  
2. Models trained on clinical data from one electronic health record (EHR) system often do not generalize well to other systems, mainly because of differences in formatting and data representation.  
3. ClinStructor is proposed as a novel pipeline that uses large language models (LLMs) to transform clinical free-text into structured, task-specific question-answer pairs, which improves the transparency and controllability of predictive modeling.  
4. This methodology significantly enhances the interpretability of machine learning models applied to clinical tasks, making it easier for clinicians to understand and trust model outputs.  
5. When applied to ICU mortality prediction, ClinStructor only shows a modest decrease of 2-3% in area under the curve (AUC) compared to direct fine-tuning, while offering a foundation for building more reliable, interpretable, and generalizable predictive models in healthcare settings. <div>
arXiv:2511.11883v1 Announce Type: new 
Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support</title>
<link>https://arxiv.org/abs/2511.11884</link>
<guid>https://arxiv.org/abs/2511.11884</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health, telehealth, GPT-2, reinforcement learning, therapeutic dialogue  

<br /><br />Summary:  
The paper addresses the significant socioeconomic burden of mental health illnesses, aggravated by COVID-19, highlighting the demand for telehealth solutions. It investigates how large language models (LLMs), specifically GPT-2, can be enhanced for therapeutic dialogue generation through supervised fine-tuning (SFT) and reinforcement learning (RL). The study restructured input formats to process contextual information and emotional states alongside user inputs. A multi-component reward function was created to align model outputs with professional therapist responses and annotated emotions. The results indicate notable improvements in various evaluation metrics, including BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581), when comparing reinforcement learning workflows to baseline GPT-2. Furthermore, the enhanced model achieved 99.34% accuracy in determining emotions, a significant increase over the 66.96% accuracy of baseline GPT-2. Overall, the findings demonstrate that reinforcement learning can effectively improve the performance of therapeutic dialogue systems, offering valuable assistive tools for therapists while underscoring the necessity of human clinical oversight. <div>
arXiv:2511.11884v1 Announce Type: new 
Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Additive Large Language Models for Semi-Structured Text</title>
<link>https://arxiv.org/abs/2511.11922</link>
<guid>https://arxiv.org/abs/2511.11922</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, clinical text classification, interpretability, additive models, semi-structured text<br /><br />Summary:<br /><br />This paper introduces CALM (Classification with Additive Large Language Models), a novel framework designed to improve interpretability in clinical text classification tasks using large language models (LLMs). CALM leverages the semi-structured nature of many clinical documents by decomposing inputs into meaningful components, such as sections of admission notes or intake form fields, and models predictions as an additive sum of contributions from each component. This additive approach integrates component contributions directly into the forward computation, enabling faithful and transparent explanations at both individual patient and broader population levels. The method facilitates clear visualizations through component-level risk curves akin to those found in generalized additive models, making it easier to understand and communicate learned relationships. CALM is particularly suited to clinical settings, where understanding which parts of a patient record drive risk signals is critical for trust, quality assurance, and meaningful clinical insights. Despite its interpretability advantages, CALM matches the performance of traditional LLM classifiers, proving that enhanced explanation does not require sacrificing accuracy. The framework also supports automatic extraction of semi-structured inputs from free-text clinical notes, broadening its applicability. Overall, CALM offers a practical and interpretable solution that fosters trust and insight during model development and auditing in healthcare contexts. <div>
arXiv:2511.11922v1 Announce Type: new 
Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InData: Towards Secure Multi-Step, Tool-Based Data Analysis</title>
<link>https://arxiv.org/abs/2511.11933</link>
<guid>https://arxiv.org/abs/2511.11933</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Data Analysis, Tool-based Reasoning, Security, Indirect Data Engagement (InData)<br /><br />Summary:<br /><br />1. The paper addresses security risks associated with allowing large language model (LLM) agents to directly generate and execute code on sensitive databases during data analysis.<br />2. To mitigate these risks, the authors propose restricting LLMs from direct code generation and data access, requiring interaction only through a secure, predefined set of verified tools.<br />3. Existing benchmarks for LLM tool use focus mainly on tool selection and simple task execution, lacking evaluation of complex, multi-step reasoning.<br />4. The authors introduce Indirect Data Engagement (InData), a novel dataset that evaluates LLMs' ability to perform multi-step, tool-based reasoning across data analysis questions of varying difficulty: Easy, Medium, and Hard.<br />5. Benchmarking 15 open-source LLMs on InData reveals that while large models (e.g., gpt-oss-120b) perform very well on Easy tasks (97.3% accuracy), their accuracy declines significantly on Hard tasks (69.6%), indicating a deficiency in robust multi-step tool-based reasoning.<br />6. The release of the InData dataset and code aims to foster development and assessment of LLMs with enhanced capabilities for secure, multi-step tool use in complex data analysis contexts. <div>
arXiv:2511.11933v1 Announce Type: new 
Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization</title>
<link>https://arxiv.org/abs/2511.11946</link>
<guid>https://arxiv.org/abs/2511.11946</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge graph, dialogue generation, LLM-KAT, entity anonymization, OpenDialKG

<br /><br />Summary: This article discusses the challenges of integrating external knowledge into dialogue generation by leveraging knowledge graphs (KG-DG). It highlights the limitations of large language models (LLMs) in effectively utilizing provided knowledge graphs, as they tend to rely more on internal knowledge. To address these issues, the authors introduce LLM-KAT, a novel evaluation procedure designed to measure how well LLMs attach to external knowledge in their generated responses. Additionally, the paper presents a simple yet effective technique called entity anonymization, aimed at encouraging LLMs to engage more with external knowledge sources. Through experiments conducted on the OpenDialKG dataset, the authors demonstrate that their proposed approaches lead to significant improvements in the ability of LLMs to utilize external knowledge effectively. The findings underscore the need for more focused methods in the field of KG-DG to enhance the interaction between LLMs and external knowledge, ultimately aiming for more coherent and informative conversational responses. <div>
arXiv:2511.11946v1 Announce Type: new 
Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Entropy Calibration of Language Models</title>
<link>https://arxiv.org/abs/2511.11966</link>
<guid>https://arxiv.org/abs/2511.11966</guid>
<content:encoded><![CDATA[
<div> Keywords: entropy calibration, autoregressive models, miscalibration, scaling behavior, black box prediction

<br /><br />Summary: This paper investigates entropy calibration in language models, focusing on whether the entropy over generations aligns with log loss on human text. Previous research indicated that models are often miscalibrated, with increased entropy correlating with poorer text quality in longer generations. The authors explore whether miscalibration improves with scaling and if calibration can be achieved without tradeoffs. They analyze a theoretical model that examines scaling behavior related to dataset size, revealing that for power law distributions with an exponent near 1, miscalibration improves slowly with scale. Empirical measurements from language models, ranging from 0.5B to 70B parameters, support this theoretical insight, showing similar rates of error accumulation across different model sizes. Consequently, larger models do not significantly reduce miscalibration compared to smaller ones, even though they generate higher quality outputs. The paper also discusses the standard practice of truncating distributions to improve text quality, though this increases log loss. Ultimately, it establishes that theoretically, reducing entropy while maintaining log loss is possible if a black box exists that can predict future text entropy accurately. <div>
arXiv:2511.11966v1 Announce Type: new 
Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reasoning Paradigm for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2511.11978</link>
<guid>https://arxiv.org/abs/2511.11978</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative LLMs, Named Entity Recognition, reasoning framework, Chain of Thought, zero-shot performance  

<br /><br />Summary: This article addresses the limitations of generative large language models (LLMs) in Named Entity Recognition (NER) performance, particularly in zero-shot and low-resource contexts. It identifies a problem of "cognitive shortcutting," where LLMs rely on implicit pattern matching rather than explicit reasoning, leading to suboptimal outcomes. To improve this, a new reasoning framework for NER is proposed, consisting of three key stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset is created with NER-oriented CoTs that provide relevant reasoning chains. This dataset is then utilized to refine the NER model, allowing it to generate coherent rationales prior to producing final outputs. In the last stage, reasoning enhancement optimizes the extraction process using a comprehensive reward signal, promoting explicit and verifiable extractions. Experimental results reveal that the proposed method, ReasoningNER, significantly boosts cognitive ability in NER tasks, achieving state-of-the-art (SOTA) performance with a notable 12.3 percentage-point improvement in F1 score over GPT-4 in zero-shot settings. The findings suggest substantial potential for advancing research in reasoning-oriented information extraction. Code for the study is accessible at GitHub. <div>
arXiv:2511.11978v1 Announce Type: new 
Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations</title>
<link>https://arxiv.org/abs/2511.12001</link>
<guid>https://arxiv.org/abs/2511.12001</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, trust, reasoning errors, multimodal moral scenarios, explanations

<br /><br />Summary: This article investigates the dual role of Chain-of-Thought (CoT) explanations in enhancing transparency while also promoting confirmation bias in users. Researchers examine how reasoning errors in vision language models (VLMs) affect user trust and the ability to identify flaws. The study presents two main outcomes: first, users tend to equate trust with the agreement of outcomes, leading to continued reliance on the model even when the underlying reasoning is incorrect. Second, the delivery tone of the CoT explanations plays a significant role in user perception; a confident tone can suppress error detection and maintain user reliance on flawed reasoning. Consequently, users may overlook inaccuracies when presented with persuasive delivery styles. The findings underscore the importance of designing NLP systems that offer explanations fostering critical examination rather than uncritical acceptance. By highlighting the potential for CoT explanations to mislead despite their explanatory power, the research advocates for improvements in how these systems communicate reasoning to enhance users' analytical abilities. The authors commit to releasing the code publicly to encourage further exploration and development in this area. <div>
arXiv:2511.12001v1 Announce Type: new 
Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs</title>
<link>https://arxiv.org/abs/2511.12014</link>
<guid>https://arxiv.org/abs/2511.12014</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, cultural competence, evaluation benchmarks, reasoning, empirical analysis<br /><br />Summary: This article addresses the limitations of current evaluations of cultural competence in large language models (LLMs), which are increasingly used in diverse environments. Existing evaluation methods often emphasize de-contextualized correctness or forced-choice judgments, failing to account for the cultural understanding and reasoning necessary for appropriate responses. To bridge this gap, the authors propose a new set of benchmarks that present LLMs with realistic situational contexts requiring culturally grounded reasoning. In addition to the standard Exact Match metric, they introduce four complementary metrics: Coverage, Specificity, Connotation, and Coherence, which together measure various aspects of response quality. Empirical analysis of leading models shows that conventional evaluations tend to overestimate cultural competence and produce inconsistent results with high variance. In contrast, the new thick evaluation method reveals deeper reasoning differences among models, reduces outcome variance, and provides more stable and interpretable indicators of cultural understanding. This research highlights the need for improved evaluation metrics in assessing LLMs' cultural competence for better deployment in diverse cultural settings. <div>
arXiv:2511.12014v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task</title>
<link>https://arxiv.org/abs/2511.12109</link>
<guid>https://arxiv.org/abs/2511.12109</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-tuning, backtranslation, neural machine translation, Japanese corpus, low-resource languages  

<br /><br />Summary: This paper investigates the combination of fine-tuning and backtranslation for improving neural machine translation of a small Japanese corpus. It starts with a baseline English-to-Japanese model, achieving a COMET score of 0.460. The authors then implemented backtranslation using synthetic data from monolingual Japanese corpora, which resulted in a modest improvement to a COMET score of 0.468. Following this, they fine-tuned the model using a small, authentic parallel dataset from diverse Japanese news and literary sources, leading to a significant increase to COMET = 0.589 using the Mistral 7B architecture. The authors further enhanced the model's performance by combining both strategies: augmenting the small dataset with backtranslated examples and subsequently applying fine-tuning, which yielded a final COMET score of 0.597. These findings highlight the effectiveness of integrating backtranslation and targeted fine-tuning, demonstrating significant improvements in translation quality for limited training data scenarios. This approach presents a practical and efficient method for enhancing translations, particularly in low-resource language pairs like Japanese. <div>
arXiv:2511.12109v1 Announce Type: new 
Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12116</link>
<guid>https://arxiv.org/abs/2511.12116</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, temporal boundaries, LLMLagBench, training data, response accuracy

<br /><br />Summary: The paper addresses the limitations of Large Language Models (LLMs) regarding their knowledge cutoff, which restricts their ability to provide accurate information beyond a specific temporal point. This knowledge boundary often leads to a blend of outdated, time-sensitive information with general data during reasoning tasks, raising concerns about response accuracy. To tackle this issue, the authors introduce LLMLagBench, a systematic benchmark designed to identify the earliest probable temporal boundaries of an LLM's training data by testing its knowledge of recent events. The benchmark is applied to a diverse range of LLMs, including those with both declared and undeclared training cutoffs, to evaluate their performance. The reliability of LLMLagBench is further validated through manual checks and comparisons with publicly available data on the LLMs' pretraining. This research highlights the importance of understanding the temporal limits of LLMs to ensure accurate responses and facilitate better integration of external information sources when needed. The benchmark serves as a useful tool for researchers and developers to assess and improve LLMs’ relevance and accuracy in dynamic contexts. <div>
arXiv:2511.12116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection</title>
<link>https://arxiv.org/abs/2511.12130</link>
<guid>https://arxiv.org/abs/2511.12130</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Conversational Stance Detection, U-MStance, PRISM, user-centric, stance detection

<br /><br />Summary: The increasing volume of multimodal social media content has prompted research in Multimodal Conversational Stance Detection (MCSD), which interprets user attitudes in discussions. Existing studies face two main limitations: pseudo-multimodality, where visual elements in posts are not aligned with text-only comments, and user homogeneity, which overlooks individual differences in stance expression. To address these challenges, the authors introduce U-MStance, a pioneering user-centric dataset with over 40,000 annotated comments on six real-world targets. They also propose PRISM, a Persona-Reasoned multimodal stance model. PRISM develops user personas from past interactions to capture unique characteristics and aligns textual and visual cues via Chain-of-Thought reasoning for better understanding of context. Additionally, it implements a mutual task reinforcement mechanism to optimize stance detection and response generation simultaneously, facilitating knowledge transfer. Experimental results on U-MStance highlight significant performance improvements of PRISM over existing strong baselines, validating the importance of user-centric and context-aware multimodal reasoning for a realistic understanding of stances in online interactions. <div>
arXiv:2511.12130v1 Announce Type: new 
Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing</title>
<link>https://arxiv.org/abs/2511.12133</link>
<guid>https://arxiv.org/abs/2511.12133</guid>
<content:encoded><![CDATA[
<div> Keywords: persuasive dialogue, telemarketing, reinforcement learning, Large Language Models, evaluation framework<br /><br />Summary: This paper addresses the challenges in goal-driven persuasive dialogue systems, particularly in telemarketing contexts, where multi-turn planning and factual accuracy are crucial. The authors identify limitations in existing methods due to scarce task-specific data and the shortcomings of direct Large Language Model (LLM) applications, such as strategic brittleness and hallucinations. To overcome these issues, they introduce TeleSalesCorpus, the first real-world grounded dialogue dataset for telemarketing. The proposed AI-Salesman framework features a dual-stage architecture: during training, it employs a Bayesian-supervised reinforcement learning algorithm designed to learn resilient sales strategies from noisy data. At inference, the Dynamic Outline-Guided Agent (DOGA) uses a pre-built script library to provide dynamic, turn-by-turn strategic guidance, enhancing dialogue coherence and effectiveness. Additionally, the authors design a comprehensive evaluation framework that integrates detailed metrics for essential sales skills alongside an LLM-as-a-Judge mechanism to assess performance. Experimental results demonstrate that AI-Salesman substantially outperforms baseline models in both automatic metrics and human evaluations, confirming its effectiveness in complex persuasive dialogue scenarios such as telemarketing. This work advances the development of robust, goal-driven dialogue systems with practical real-world applicability. <div>
arXiv:2511.12133v1 Announce Type: new 
Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding</title>
<link>https://arxiv.org/abs/2511.12140</link>
<guid>https://arxiv.org/abs/2511.12140</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal, Large Language Models, hallucination detection, VBackChecker, R^2-HalBench

<br /><br />Summary: This paper addresses the issue of hallucinations in Multimodal Large Language Models (MLLMs), which affects their reliability in practical applications. To combat this, the authors introduce VBackChecker, a reference-free framework that checks the consistency between MLLM-generated responses and visual inputs using a pixellevel Grounding LLM with reasoning and referring segmentation capabilities. The framework excels in rich-context scenarios and provides interpretability. To support VBackChecker, the authors present a new data generation pipeline called R-Instruct, which produces instruction-tuning data with rich-context descriptions, grounding masks, and hard negative samples. Additionally, they introduce R^2-HalBench, a novel hallucination benchmark for MLLMs that features real-world, rich-context descriptions from 18 MLLMs, along with high-quality annotations that cover various object, attribute, and relationship-level details. In performance evaluations, VBackChecker outperforms existing complex frameworks and achieves state-of-the-art results on the R^2-HalBench, rivaling GPT-4o in hallucination detection capabilities. Furthermore, it shows a more than 10% improvement in pixel-level grounding tasks compared to previous methods. The authors provide access to all related codes, data, and models at their GitHub repository. <div>
arXiv:2511.12140v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic</title>
<link>https://arxiv.org/abs/2511.12159</link>
<guid>https://arxiv.org/abs/2511.12159</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-Integrated Reasoning, large language models, CriticSearch, dense rewards, multi-hop reasoning

<br /><br />Summary: This paper introduces CriticSearch, a novel framework designed to enhance Tool-Integrated Reasoning (TIR) in large language models by leveraging search engines for real-time knowledge retrieval. Traditional search agent pipelines often face challenges due to reliance on reinforcement learning, which can lead to issues with sparse rewards that hinder effective exploration and training stability. CriticSearch addresses this by implementing a fine-grained credit-assignment mechanism that provides dense, turn-level feedback through a retrospective critique from a frozen, asymmetric large language model. This critic evaluates each interaction based on comprehensive input, including the entire trajectory and correct responses, transforming evaluations into stable rewards that promote more efficient policy improvements. Experimental results reveal that CriticSearch significantly outperforms existing methods on various multi-hop reasoning tasks, demonstrating quicker convergence rates, enhanced training stability, and overall better performance metrics. This advancement suggests that integrating a structured critique process can substantially refine the operational efficacy of language models in complex question-answering scenarios. <div>
arXiv:2511.12159v1 Announce Type: new 
Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues</title>
<link>https://arxiv.org/abs/2511.12213</link>
<guid>https://arxiv.org/abs/2511.12213</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained entity recognition, MME-RAG, retrieval-augmented generation, domain adaptation, KeyInfo retriever

<br /><br />Summary: Fine-grained entity recognition is essential for reasoning in task-oriented dialogues, yet current large language models struggle with domain adaptation and retrieval controllability. The paper presents MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework, which divides entity recognition into two coordinated stages. The first stage involves type-level judgment executed by lightweight managers, while the second stage focuses on span-level extraction by specialized experts. Each expert uses a KeyInfo retriever that provides semantically aligned, few-shot examples during inference, allowing for precise extraction without requiring additional training. The authors conducted experiments on various datasets including CrossNER, MIT-Movie, and MIT-Restaurant, as well as a newly created multi-domain customer-service dataset, revealing that MME-RAG outperforms recent baseline models across most domains. Further, ablation studies indicate that both the hierarchical approach and KeyInfo-guided retrieval significantly enhance robustness and cross-domain generalization. These findings position MME-RAG as a scalable, interpretable solution for enhancing adaptive dialogue understanding in different contexts. <div>
arXiv:2511.12213v1 Announce Type: new 
Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts</title>
<link>https://arxiv.org/abs/2511.12236</link>
<guid>https://arxiv.org/abs/2511.12236</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination detection, API, CONFACTCHECK, factual probes

<br /><br />Summary: Large language models (LLMs) exhibit impressive text generation capabilities, yet they often produce factually incorrect content, a phenomenon known as hallucination. This issue is particularly concerning in sensitive areas such as healthcare and finance. Generally, LLMs are accessed via APIs provided by vendors, limiting user control over model weights and fine-tuning capabilities. Existing hallucination detection methods typically require multiple API calls, which can lead to increased latency and costs. To address these challenges, the authors introduce CONFACTCHECK, a novel and efficient approach for detecting hallucinations without relying on external knowledge bases. CONFACTCHECK operates on the premise that factual probes embedded in the generated text should yield consistent responses from a single LLM and across different models. Through rigorous empirical evaluation on diverse datasets — encompassing both factual text generation and open-ended generation — CONFACTCHECK demonstrates a capacity to detect hallucinated facts more efficiently and accurately than existing methods that function under similar constraints. This advancement suggests a promising direction for improving the reliability of LLMs in practical applications, offering significant benefits in resource management and accuracy. The accompanying code for CONFACTCHECK is made publicly available for further exploration. <div>
arXiv:2511.12236v1 Announce Type: new 
Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations</title>
<link>https://arxiv.org/abs/2511.12249</link>
<guid>https://arxiv.org/abs/2511.12249</guid>
<content:encoded><![CDATA[
<div> Keywords: Vietnamese NLP, contextualized embeddings, Word Sense Disambiguation, contrastive learning, semantic evaluation<br /><br />Summary:  
The paper addresses the lack of robust semantic understanding models and resources for the Vietnamese language, which is a low-resource language compared to English. It introduces ViConBERT, a new framework that learns Vietnamese contextualized word embeddings by combining contrastive learning (SimCLR) with gloss-based distillation to better capture word meanings in context. Alongside the model, the authors present ViConWSD, the first large-scale synthetic dataset for semantic evaluation in Vietnamese, supporting both Word Sense Disambiguation (WSD) and contextual similarity tasks. Experimental results demonstrate that ViConBERT significantly outperforms strong baseline models on WSD, achieving an F1 score of 0.87. Furthermore, ViConBERT delivers competitive performance on semantic similarity benchmarks ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), indicating that it effectively models both discrete word senses and graded semantic relations. The paper emphasizes the importance of combining contrastive learning with gloss-based knowledge distillation to enhance semantic representation in a low-resource language setting. Lastly, the authors contribute to the community by making their code, trained models, and datasets publicly available for further research and development in Vietnamese natural language processing. <div>
arXiv:2511.12249v1 Announce Type: new 
Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor</title>
<link>https://arxiv.org/abs/2511.12281</link>
<guid>https://arxiv.org/abs/2511.12281</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt compression, Large Language Models, compress inputs, compression rate, Cmprsr

<br /><br />Summary:  
This paper introduces a novel paradigm for prompt compression utilizing smaller Large Language Models (LLMs) to compress inputs for larger models, addressing the high costs associated with using black-box LLMs. It presents the first extensive benchmark evaluating the performance of 25 different open- and closed-source models as compressors, highlighting significant disparities in their abilities to preserve semantically important information and adhere to user-defined compression rates (CR). The study features improvements in the performance of gpt-4.1-mini, identified as the best vanilla compressor, via Textgrad-based compression meta-prompt optimization. Furthermore, it identifies Qwen3-4B as a promising open-source model, which is subsequently post-trained through supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to enhance CR adherence and downstream task performance. This results in the creation of a new model named Cmprsr, which outperforms traditional extractive and vanilla abstractive compression techniques across varying input lengths and domains, validated using datasets such as MeetingBank, LongBench, and GSM8k. Cmprsr is capable of closely following the requested compression rate, facilitating better control over the trade-off between cost and quality. <div>
arXiv:2511.12281v1 Announce Type: new 
Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugAbEx : Way Forward for Extractive Case Summarization</title>
<link>https://arxiv.org/abs/2511.12290</link>
<guid>https://arxiv.org/abs/2511.12290</guid>
<content:encoded><![CDATA[
<div> Keywords: legal summarization, extractive summaries, abstractive summaries, dataset augmentation, natural language processing  

<br /><br />Summary: The complexity of legal documents presents a significant cognitive challenge for law practitioners, prompting interest in automatic summarization techniques. Researchers are increasingly focusing on extractive summarizers due to the limitations of abstractive summarization methods, which may misrepresent legal nuances. To address the high cost of creating gold standard extractive summaries, the authors propose an innovative pipeline that transforms existing abstractive summaries into corresponding extractive versions, ensuring that expert insights are transferred accurately. This project aims to enhance seven existing legal case summarization datasets by adding extractive summaries alongside their abstractive counterparts, thereby creating a more robust resource for research in this area. To maintain the quality of these new extractive summaries, a detailed comparative evaluation with the original abstractive summaries is conducted, examining structural, lexical, and semantic aspects. The authors assert that the merger of these datasets will provide valuable opportunities for advancing automatic summarization methodologies within legal contexts. They also commit to making the augmented datasets publicly available, further supporting the research community's efforts in improving legal document summarization. <div>
arXiv:2511.12290v1 Announce Type: new 
Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering</title>
<link>https://arxiv.org/abs/2511.12300</link>
<guid>https://arxiv.org/abs/2511.12300</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, NLP tasks, quizzes, human performance, numerical answers  

<br /><br />Summary: This study explores the comparative performance of Large Language Models (LLMs) and humans on quiz questions in a buzzer setting. The researchers began by collecting Japanese quiz data, which included the questions, answers, and the correct response rates of human participants. They then prompted LLMs to attempt these quizzes under various conditions. The focus of the investigation was to understand if the challenges that are difficult for humans in quiz formats are similarly challenging for LLMs. The findings revealed that LLMs exhibited a lower correct answer rate than humans, especially for quizzes where the correct answers were not found in Wikipedia entries, indicating that LLMs rely heavily on the information present in such datasets. Additionally, LLMs faced significant difficulty with questions requiring numerical responses. Overall, the research highlights the nuanced differences in how LLMs and humans tackle quiz-based challenges, suggesting that LLMs may have limitations in specific areas that do not align with human cognitive strengths. <div>
arXiv:2511.12300v1 Announce Type: new 
Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load</title>
<link>https://arxiv.org/abs/2511.12381</link>
<guid>https://arxiv.org/abs/2511.12381</guid>
<content:encoded><![CDATA[
<div> Keywords: ironic rebound, negation instructions, large language models, polarity separation, ReboundBench<br /><br />Summary:  
This study investigates the phenomenon of ironic rebound, where negation instructions like "do not mention X" paradoxically increase the prominence of X in human thought, and similarly affect large language models (LLMs). The authors conducted two key experiments: (1) Load & content: after negation instructions, they varied distractor text types—including semantic, syntactic, and repetition—to measure the strength of rebound; (2) Polarity separation: they tested if models can distinguish neutral from negative framings of the same concept and whether this ability predicts the persistence of rebound. Results indicated that ironic rebound arises immediately following negation and intensifies particularly with longer or semantically rich distractors, whereas repetition appears to aid suppression of the forbidden concept. Additionally, stronger polarity separation by models correlated with more persistent rebound effects. A mechanistic analysis using circuit tracing revealed that sparse attention heads in middle layers amplify forbidden tokens despite suppression occurring in early layers, providing insight into how long-context interference occurs in LLMs. To facilitate further research, the authors also release ReboundBench, a dataset of 5,000 systematically varied negation prompts designed to rigorously probe rebound phenomena in large language models. <div>
arXiv:2511.12381v1 Announce Type: new 
Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Phonemes to Meaning: Evaluating Large Language Models on Tamil</title>
<link>https://arxiv.org/abs/2511.12387</link>
<guid>https://arxiv.org/abs/2511.12387</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Tamil, ILAKKANAM, linguistic evaluation, performance  

<br /><br />Summary:  
This paper addresses the performance of Large Language Models (LLMs) in low-resource and morphologically rich languages like Tamil, which has not been thoroughly researched. Existing multilingual benchmarks are often based on translated English datasets, neglecting the unique linguistic and cultural aspects of Tamil. To tackle this issue, the authors introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark, which is crafted from 820 questions sourced from Sri Lankan Tamil examination papers. These questions are categorized by trained linguists into five linguistic categories and a factual knowledge category, covering Grades 1-13 for comprehensive linguistic representation. The evaluation of both closed-source and open-source LLMs revealed that Gemini 2.5 outperformed others, while open-source models exhibited poorer performance, indicating a need for better linguistic grounding. Further breakdowns show a consistent performance in lower-grade questions but a noticeable decline with increasing complexity. Notably, the study found no strong correlation between a model's overall performance and its effectiveness in identifying linguistic categories, suggesting that observed performance may stem from exposure rather than authentic understanding of the language. <div>
arXiv:2511.12387v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models</title>
<link>https://arxiv.org/abs/2511.12464</link>
<guid>https://arxiv.org/abs/2511.12464</guid>
<content:encoded><![CDATA[
<div> Keywords: reward models, preference dimensions, MRMBench, inference-time probing, multi-objective optimization  

<br /><br />Summary: This work addresses the limitations of existing methods for evaluating reward models, which typically use a fixed pairwise ranking test set without detailed performance insights on preference dimensions. To tackle this challenge, the authors introduce the Multi-dimensional Reward Model Benchmark (MRMBench), comprising six probing tasks that assess different preference dimensions. This benchmark encourages the development of reward models that effectively capture diverse preferences. Additionally, the study presents an analysis technique called inference-time probing, which enhances the interpretability of reward predictions by identifying the preference dimensions utilized during predictions. Extensive experiments demonstrate that MRMBench correlates strongly with the alignment performance of large language models (LLMs), establishing it as a valuable tool for reward model development. Analysis of MRMBench results reveals an ongoing struggle within reward models to accurately reflect multi-dimensional preferences, suggesting that multi-objective optimization could enhance reward modeling. Furthermore, the inference-time probing method provides a reliable metric for evaluating the confidence of reward predictions, thereby improving the alignment of LLMs and highlighting the need for more nuanced evaluation methods in this field. <div>
arXiv:2511.12464v1 Announce Type: new 
Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing</title>
<link>https://arxiv.org/abs/2511.12472</link>
<guid>https://arxiv.org/abs/2511.12472</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge graph, serendipity, KGQA, drug repurposing  

<br /><br />Summary: Large Language Models (LLMs) have significantly improved knowledge graph question answering (KGQA), but they mainly return predictable answers. This paper introduces the concept of serendipity-aware KGQA, emphasizing LLMs' potential to provide surprising and novel insights. The authors define the serendipity-aware KGQA task and propose the SerenQA framework to assess LLMs' capabilities in discovering unexpected insights within scientific KGQA tasks. SerenQA features a robust serendipity metric that evaluates relevance, novelty, and surprise, along with a benchmark derived from the Clinical Knowledge Graph, specifically targeting drug repurposing. The framework also includes a structured evaluation pipeline with three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Experimental results indicate that while state-of-the-art LLMs excel in retrieval tasks, they struggle to deliver genuinely surprising and valuable insights. This highlights the need for further research and enhancements in this area. The paper provides curated resources and an extended version, which are accessible at the designated link. Overall, the findings demonstrate a significant gap in LLM capabilities regarding the identification of serendipitous answers in KGQA. <div>
arXiv:2511.12472v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGuard-v1: Safety Guardrail for Large Language Models</title>
<link>https://arxiv.org/abs/2511.12497</link>
<guid>https://arxiv.org/abs/2511.12497</guid>
<content:encoded><![CDATA[
<div> Keywords: SGuard-v1, Large Language Models, ContentFilter, JailbreakFilter, safety performance

<br /><br />Summary: The article introduces SGuard-v1, a lightweight safety guardrail designed for Large Language Models (LLMs). It consists of two specialized models: ContentFilter and JailbreakFilter. The ContentFilter identifies safety risks in prompts and responses, adhering to the MLCommons hazard taxonomy for AI trust and safety assessment. The JailbreakFilter, on the other hand, is trained using a well-structured curriculum that encompasses integrated datasets and previous findings on adversarial prompting, addressing 60 significant attack types while minimizing false-unsafe classifications. SGuard-v1 utilizes the 2B-parameter Granite-3.3-2B-Instruct model, supporting 12 languages. A total of approximately 1.4 million training instances were curated from collected and synthesized data for further instruction tuning, distributing the data between the two components according to their specific roles. Through comprehensive evaluations against public and proprietary safety benchmarks, SGuard-v1 has achieved state-of-the-art safety performance while maintaining a lightweight structure, which helps reduce deployment overhead. Additionally, it enhances interpretability by providing multi-class safety predictions alongside binary confidence scores. The release of SGuard-v1 under the Apache-2.0 License aims to facilitate continued research and practical applications in AI safety. <div>
arXiv:2511.12497v1 Announce Type: new 
Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs</title>
<link>https://arxiv.org/abs/2511.12504</link>
<guid>https://arxiv.org/abs/2511.12504</guid>
<content:encoded><![CDATA[
<div> Keywords: QA-Noun, semantic alignment, noun-centered semantics, QA-SRL, fine-grained decomposition  

<br /><br />Summary: The paper introduces QA-Noun, a novel QA-based framework aimed at capturing noun-centered semantic relations, addressing a gap in existing semantic approaches that have primarily focused on predicate-argument interactions. QA-Noun defines nine question templates that explore both explicit syntactical roles and implicit contextual roles for nouns, facilitating the creation of interpretable QA pairs that enhance traditional verbal QA-Semantic Role Labeling (QA-SRL). A dataset comprising over 2,000 annotated noun mentions is released alongside detailed guidelines, and a trained model that integrates with QA-SRL is presented, allowing for a unified semantic decomposition of sentence meanings into highly fine-grained facts. Evaluation results demonstrate that QA-Noun achieves nearly full coverage of Abstract Meaning Representation (AMR) noun arguments while uncovering additional implied relations. Furthermore, when QA-Noun is combined with QA-SRL, it provides more than 130% greater granularity compared to recent fact-based decomposition methods like FactScore and DecompScore. Overall, QA-Noun enriches the established QA-based semantic framework, contributing to a comprehensive and scalable approach for fine-grained semantic decomposition tailored for cross-text alignment. <div>
arXiv:2511.12504v1 Announce Type: new 
Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2511.12520</link>
<guid>https://arxiv.org/abs/2511.12520</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, TAdaRAG, knowledge graph, intent-driven routing, reinforcement learning  

<br /><br />Summary:  
The paper presents TAdaRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by constructing task-adaptive knowledge graphs in real-time from external sources. Traditional RAG models often suffer from information loss and irrelevant detail retrieval due to input context limitations, leading to issues such as response hallucinations and poor reasoning. To combat these challenges, TAdaRAG introduces an intent-driven routing mechanism that directs to specific extraction templates tailored for particular domains. This is complemented by a supervised fine-tuning process and a reinforcement learning-based implicit extraction method. These innovations ensure the integration of knowledge is concise, coherent, and free of redundancy. Evaluations conducted on six public benchmarks, along with a real-world business benchmark called NowNewsQA, demonstrate that TAdaRAG significantly outperforms existing methods across various domains and long-text tasks. These results underline its strong generalization capabilities and practical effectiveness, making it a notable advancement in the field of language model enhancement through external knowledge integration. <div>
arXiv:2511.12520v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Length Bias in RLHF through a Causal Lens</title>
<link>https://arxiv.org/abs/2511.12573</link>
<guid>https://arxiv.org/abs/2511.12573</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Human Feedback, Length Bias, Reward Model, Content Quality  

<br /><br />Summary: The article discusses the prevalent issue of length bias in reinforcement learning from human feedback (RLHF) when aligning large language models (LLMs) with human preferences. Length bias manifests as a tendency for reward models to favor longer responses, equating verbosity with quality. To address this issue, the authors propose a causal framework that includes a counterfactual data augmentation method. This method generates response pairs aimed at disentangling content quality from verbosity. They create two types of pairs: length-divergent pairs with similar content and content-divergent pairs of similar length. Empirical evaluations show that this approach significantly reduces the length bias in reward assignments, leading to outputs that are more concise and focused on content. The proposed method enhances the robustness and content sensitivity of reward modeling in RLHF pipelines. Overall, this research contributes to improving the alignment of LLMs with human preferences by mitigating the biases associated with response length, ensuring that the quality of responses is evaluated based on meaningful content rather than mere verbosity. <div>
arXiv:2511.12573v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMWOZ: Building Multimodal Agent for Task-oriented Dialogue</title>
<link>https://arxiv.org/abs/2511.12586</link>
<guid>https://arxiv.org/abs/2511.12586</guid>
<content:encoded><![CDATA[
<div> Task-oriented dialogue systems, multimodal dialogue, GUI interaction, dataset MMWOZ, multimodal model MATE<br /><br />Summary:<br /><br />This paper addresses the limitations of traditional task-oriented dialogue systems that rely on customized back-end APIs, which are often unavailable in real-world scenarios dominated by front-end Graphical User Interfaces (GUIs). To bridge this gap, the authors introduce MMWOZ, a new multimodal dialogue dataset derived from the existing MultiWOZ 2.3 dataset. The dataset creation involved developing a web-style GUI as the dialogue front-end, devising an automated script to translate dialogue states and system actions into executable GUI operation instructions, and collecting snapshots of web pages paired with these instructions. Additionally, the authors propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue), which serves as a baseline for tasks using the MMWOZ dataset. Comprehensive experiments with MATE are conducted to analyze the capabilities and challenges of building practical multimodal task-oriented dialogue agents. The work contributes to advancing dialogue systems that operate effectively through visual interfaces, closer to real-world application environments where backend APIs are not always accessible. <div>
arXiv:2511.12586v1 Announce Type: new 
Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-Aware Reinforcement Learning for Output Diversity in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12596</link>
<guid>https://arxiv.org/abs/2511.12596</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, mode collapse, Group-Aware Policy Optimization, response diversity, frequency-aware reward function

<br /><br />Summary: This article addresses the issue of mode collapse in Large Language Models (LLMs), where these models tend to generate limited and repetitive outputs despite the presence of multiple valid responses. The authors introduce a novel approach called Group-Aware Policy Optimization (GAPO), which builds upon the recently developed Group Relative Policy Optimization (GRPO). GAPO focuses on evaluating rewards at a group level, which allows for improvements in model learning related to diversity and coverage. To demonstrate its effectiveness, the authors implement a frequency-aware reward function that promotes more uniform sampling among valid LLM completions. The results show that models trained using GAPO are capable of generating not only valid but also more diverse responses compared to traditional methods. Additionally, GAPO proves adaptable to various open-ended prompts, enhancing response diversity without sacrificing accuracy across established LLM benchmarks, including GSM8K, MATH, HumanEval, and MMLU-Pro. The researchers plan to publicly release their code, making their advancements accessible for further exploration in the field of language modeling. <div>
arXiv:2511.12596v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</title>
<link>https://arxiv.org/abs/2511.12609</link>
<guid>https://arxiv.org/abs/2511.12609</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-MoE 2.0, omnimodal large model, Mixture-of-Experts, reinforcement strategy, SOTA benchmarks  

<br /><br />Summary:  
We introduce Uni-MoE 2.0 from the Lychee family, a fully open-source omnimodal large model (OLM) that enhances the capabilities of the Uni-MoE series in multimodal understanding and generation. Built on the Qwen2.5-7B dense architecture, it incorporates dynamic-capacity Mixture-of-Experts (MoE) design and a progressive training strategy with an iterative reinforcement component. The model excels in understanding and generating images, text, and speech by employing a new MoE framework that supports 10 cross-modal inputs while ensuring computational efficiency. It utilizes an Omni-Modality 3D RoPE for spatio-temporal alignment in the self-attention mechanism. Training involves cross-modal pretraining followed by a supervised fine-tuning strategy that activates modality-specific experts, supported by balanced data and an iterative GSPO-DPO method to enhance reinforcement learning. With training on approximately 75B tokens of multimodal data, Uni-MoE 2.0 demonstrates superior performance in 85 benchmarks, achieving state-of-the-art results particularly in video understanding, audiovisual reasoning, and long-form speech processing, surpassing the previous Qwen2.5-Omni model in over 50 evaluations. Key improvements include significant gains in multiple metrics for generating tasks and processing effectiveness. <div>
arXiv:2511.12609v1 Announce Type: new 
Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing</title>
<link>https://arxiv.org/abs/2511.12630</link>
<guid>https://arxiv.org/abs/2511.12630</guid>
<content:encoded><![CDATA[
<div> Keywords: NOTAM semantic parsing, aviation domain knowledge, Knots dataset, prompt engineering, automated flight safety analysis  

<br /><br />Summary:  
This paper addresses the challenge of interpreting Notice to Air Missions (NOTAMs), which contain critical flight safety information but are difficult to parse due to complex language and implicit reasoning. Unlike prior work limited to surface-level tasks like classification and named entity recognition, the authors propose a novel task called NOTAM semantic parsing that emphasizes deep semantic inference and the integration of specialized aviation knowledge to generate structured, inference-rich outputs. To support this, they introduce Knots (Knowledge and NOTAM Semantics), a high-quality dataset comprising 12,347 expert-annotated NOTAMs from 194 Flight Information Regions. The dataset was developed through a multi-agent collaborative framework ensuring comprehensive annotation coverage across various fields. The study systematically evaluates diverse prompt-engineering and model adaptation techniques to improve machine understanding and processing of aviation texts. Experimental results demonstrate significant performance gains, validating the effectiveness of their approach for automated NOTAM analysis. Additionally, the released codebase provides a valuable resource for further research and development in aviation safety and natural language processing. <div>
arXiv:2511.12630v1 Announce Type: new 
Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing</title>
<link>https://arxiv.org/abs/2511.12661</link>
<guid>https://arxiv.org/abs/2511.12661</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, faithfulness, reasoning tasks, SFT+RL, alignment

<br /><br />Summary: The article addresses the challenge of aligning Large Language Models (LLMs) to ensure they remain faithful to new knowledge during complex, multi-hop reasoning tasks. The authors identify a "faithfulness gap" in state-of-the-art methods like Reason-KE, where the focus on format mimicry leads to critical factual hallucinations. To tackle this problem, they introduce Reason-KE++, an SFT+RL framework designed to enhance process-level faithfulness. A key component of this framework is the Stage-aware Reward mechanism, which provides dense supervision for intermediate reasoning steps such as decomposition and sub-answer correctness. The authors caution against the use of naive outcome-only reinforcement learning, which can undermine reasoning integrity while falsely improving final accuracy. Through their method, they achieved a new state-of-the-art performance of 95.48% on the MQUAKE-CF-3k benchmark, surpassing previous results by 5.28%. The findings emphasize that refining and aligning the reasoning process is critical to developing trustworthy LLMs for complex tasks. Overall, this work highlights the necessity of a more nuanced approach to LLM alignment, focusing on both reasoning processes and final outputs. <div>
arXiv:2511.12661v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data</title>
<link>https://arxiv.org/abs/2511.12690</link>
<guid>https://arxiv.org/abs/2511.12690</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-speech translation, Persian, English, synthetic data, low-resource languages

<br /><br />Summary: This paper introduces a direct speech-to-speech translation (S2ST) system that translates Persian speech into English speech, addressing the challenge posed by the scarcity of parallel speech data in low-resource languages. The proposed model is composed of three main components: a conformer-based encoder for mapping source speech to high-level acoustic representations, a causal transformer decoder that translates these representations into target speech units, and a unit-based neural vocoder for generating waveforms from the predicted discrete units. To tackle data limitations, the authors created a new Persian-English parallel speech corpus by leveraging a large language model to translate Persian transcriptions into English and then using a state-of-the-art zero-shot text-to-speech system for synthesizing the corresponding English speech. This innovative approach significantly enhances the availability of parallel speech data, increasing it roughly sixfold. Testing on the Persian-English portion of the CVSS corpus revealed notable improvements, with the model achieving 4.6 ASR BLEU over standard direct baselines when utilizing the synthetic data. The study concludes that self-supervised pre-training, discrete speech units, and synthetic parallel data effectively enhance direct S2ST performance for low-resource languages like Persian-English. <div>
arXiv:2511.12690v1 Announce Type: new 
Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</title>
<link>https://arxiv.org/abs/2511.12710</link>
<guid>https://arxiv.org/abs/2511.12710</guid>
<content:encoded><![CDATA[
<div> Keywords: EvoSynth, jailbreak, large language models, evolutionary synthesis, attack algorithms<br /><br />Summary: EvoSynth is an innovative autonomous framework designed to create novel jailbreak methods for Large Language Models (LLMs) through evolutionary synthesis rather than relying on pre-existing attack strategies. Unlike traditional automated red teaming frameworks that refine known prompts, EvoSynth uses a multi-agent system to engineer, evolve, and execute new code-based attacks autonomously. A key feature of EvoSynth is its code-level self-correction loop, which enables the system to iteratively rewrite and improve its attack algorithms based on performance failures. Extensive experiments demonstrate that EvoSynth achieves a state-of-the-art Attack Success Rate (ASR) of 85.5% against robust LLMs like Claude-Sonnet-4.5, outperforming existing methods. Furthermore, EvoSynth generates a more diverse range of attack strategies, enhancing the creativity and effectiveness of jailbreak attempts beyond traditional frameworks. The framework and its code have been released publicly to encourage further research in the evolutionary synthesis of jailbreak methods for AI safety and robustness. This marks a significant advancement in autonomous attack generation, moving beyond simple prompt modification to dynamic and self-improving algorithmic invention. <div>
arXiv:2511.12710v1 Announce Type: new 
Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Focus Memory for Language Models</title>
<link>https://arxiv.org/abs/2511.12712</link>
<guid>https://arxiv.org/abs/2511.12712</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Adaptive Focus Memory, Safety, Token Budget, Dialogue Management  

<br /><br />Summary: Large language models (LLMs) face limitations in multi-turn dialogue due to fixed context windows and inefficient memory strategies. Traditional methods, such as replaying full conversations or using static summarization, can lead to the loss of crucial user details. This paper introduces Adaptive Focus Memory (AFM), a context management system that categorizes past messages into three fidelity levels: FULL, COMPRESSED, and PLACEHOLDER. It does this based on the semantic relevance to the current query, recency, and importance classifications. By optimizing message retention under a strict token budget, AFM prioritizes high-fidelity messages while maintaining a cost-effective summary of the dialogue. The method demonstrates effectiveness in preserving critical safety information, such as a user’s severe peanut allergy, during conversations of varying lengths. In benchmarks, AFM matches the safety performance of traditional replay methods while reducing average token usage by 66%. A modular Python implementation of AFM designed for OpenAI-compatible APIs is also provided, allowing practitioners to minimize inference costs without compromising safety and factual continuity. <div>
arXiv:2511.12712v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Brittleness of LLMs: A Journey around Set Membership</title>
<link>https://arxiv.org/abs/2511.12728</link>
<guid>https://arxiv.org/abs/2511.12728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning tasks, set membership queries, empirical evaluation, failure modes  

<br /><br />Summary: This study investigates the paradox of large language models (LLMs) exhibiting superhuman performance on complex reasoning tasks while faltering on simpler problems, which raises issues regarding their reliability and interpretability. The researchers focus on set membership queries as a fundamental reasoning form, using straightforward tasks like determining if an item belongs to a set. By systematically evaluating various factors such as prompt phrasing, semantic structure, element ordering, and model selection, the analysis reveals that LLM performance on these elementary tasks is consistently brittle and unpredictable. The study points to a fragmented and convoluted grasp of the set concept by the models. Furthermore, the research highlights that the simplicity of the task facilitates large-scale experiments, allowing for a comprehensive mapping and analysis of failure modes. This methodology not only enriches the understanding of LLM capabilities but also establishes a valuable framework for evaluating these models in general. Overall, the findings underscore the importance of scrutinizing LLM performance on basic reasoning tasks to better assess their reliability and interpretability. <div>
arXiv:2511.12728v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence of Phase Transitions in Small Transformer-Based Language Models</title>
<link>https://arxiv.org/abs/2511.12768</link>
<guid>https://arxiv.org/abs/2511.12768</guid>
<content:encoded><![CDATA[
<div> Keywords: phase transitions, large language models, small transformers, training dynamics, vocabulary statistics  

<br /><br />Summary: This paper investigates whether phase transitions—sudden emergent abilities previously observed in large language models (LLMs)—also occur in small transformer-based models. The authors explore three main questions: (1) whether phase transitions exist in small models, (2) if these transitions can be detected directly in the linear training space without log-scale rescaling, and (3) whether such transitions appear at early training stages. To address these questions, they train a small GPT-style transformer on a character-level dataset and analyze vocabulary usage throughout training. Key metrics include average word length, counts of correct versus incorrect words, and vocabulary diversity shifts. Additionally, the study applies Poisson and sub-Poisson statistics to measure connections and reorganizations within the learned vocabulary. The results reveal a distinct phase transition point during training that standard loss or validation curves do not capture, but which becomes evident through the proposed vocabulary-based and statistical probes. These findings indicate that phase-transition behavior is a general feature of language model training, observable even in modest-sized models, detectable directly in linear space, and emerges surprisingly early as the model achieves coherence. This work provides new insights into the nonlinear dynamics underpinning language model training and highlights the value of tailored metrics for exposing phase transitions beyond conventional evaluation methods. <div>
arXiv:2511.12768v1 Announce Type: new 
Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Reinforcement in Context</title>
<link>https://arxiv.org/abs/2511.12782</link>
<guid>https://arxiv.org/abs/2511.12782</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment, interruptions, user input, Chain-of-Thought

<br /><br />Summary: 
This article addresses the current challenges in aligning Large Language Models (LLMs) with user intentions amidst the growing concerns of adversarial attacks and model misbehavior. Existing research has demonstrated that the probability of LLM jailbreak increases proportionately with the length of user input and conversations. However, there is a noticeable gap in research focused on enhancing alignment in relation to longer user inputs. To tackle this issue, the authors propose the method of interruptions, which involves embedding control sentences into the user input at regular intervals (every x tokens, where x is arbitrary). This strategy aims to provide a safeguard against potential scheming behaviors exhibited by LLMs. Furthermore, the authors suggest that this approach can be effectively incorporated into the Chain-of-Thought reasoning process, thereby reinforcing the model’s robustness while maintaining user alignment. By integrating interruptions, the proposal presents a novel mechanism that could enhance LLM performance and mitigate the risk of misinterpretation or unintended responses. Overall, this research contributes to the ongoing discourse on LLM alignment and proposes a practical avenue to improve model safety and reliability as user inputs grow in complexity and length. <div>
arXiv:2511.12782v1 Announce Type: new 
Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing</title>
<link>https://arxiv.org/abs/2511.12784</link>
<guid>https://arxiv.org/abs/2511.12784</guid>
<content:encoded><![CDATA[
<div> autoformalization, large language models, paraphrasing robustness, formal proofs, semantic validity<br /><br />Summary:<br /><br />This paper explores the robustness of large language models (LLMs) in the domain of autoformalization, focusing on how paraphrased natural language (NL) inputs affect the generation of formal proofs. The study builds upon observations from recent text-to-SQL research that demonstrated LLMs’ sensitivity to semantic-preserving paraphrases, extending this investigation to formal proof generation. To measure the effects, the authors evaluate LLM outputs using semantic validity and compilation validity metrics. The benchmarks used include MiniF2F and the Lean 4 version of ProofNet, both established datasets for mathematical formalization tasks. Two modern LLMs are employed in the experiments, with paraphrased NL statements generated and cross-evaluated across these models to assess consistency and performance stability. The findings reveal significant variability in model performance when handling different paraphrased inputs, indicating that seemingly minor shifts in natural language phrasing can substantially influence the quality and correctness of the formal proofs generated. This variability highlights challenges in ensuring grounded and verifiable autoformalizations and suggests the need for improved robustness in LLM architectures and training strategies. Ultimately, the paper contributes to understanding LLM limitations in autoformalization and informs future efforts to develop more resilient formal proof generation systems. <div>
arXiv:2511.12784v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals</title>
<link>https://arxiv.org/abs/2511.12821</link>
<guid>https://arxiv.org/abs/2511.12821</guid>
<content:encoded><![CDATA[
<div> Keywords: journal impact, AI engagement, bibliometric indicators, collaboration structures, BioMedJImpact

<br /><br />Summary: This article presents BioMedJImpact, a dataset specifically designed for analyzing journal-level scientific impact and AI engagement in biomedicine. It encompasses 1.74 million PubMed Central articles across 2,744 journals, integrating bibliometric indicators and collaboration features, along with a novel three-stage pipeline using large language models (LLMs) to assess AI engagement. The study investigates how collaboration intensity and AI engagement affect scientific impact during two distinct timeframes: pre-pandemic (2016-2019) and post-pandemic (2020-2023). Results show that journals with higher collaboration intensity, particularly those featuring larger and more diverse author teams, achieve greater citation impact. Additionally, AI engagement has emerged as a significant correlate of journal prestige, particularly evident in quartile rankings. To validate the LLM pipeline for AI engagement, human evaluations were conducted, revealing strong agreement in AI relevance detection and consistent subfield classification. Ultimately, BioMedJImpact is positioned as a comprehensive resource facilitating the analysis of scientific impact and innovation dynamics at the intersection of biomedicine and AI, while providing a validated methodological framework for future research. Code for implementation is available at https://github.com/JonathanWry/BioMedJImpact. <div>
arXiv:2511.12821v1 Announce Type: new 
Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation</title>
<link>https://arxiv.org/abs/2511.12832</link>
<guid>https://arxiv.org/abs/2511.12832</guid>
<content:encoded><![CDATA[
arXiv:2511.12832v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying consistency and accuracy of Latent Dirichlet Allocation</title>
<link>https://arxiv.org/abs/2511.12850</link>
<guid>https://arxiv.org/abs/2511.12850</guid>
<content:encoded><![CDATA[
arXiv:2511.12850v1 Announce Type: new 
Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation</title>
<link>https://arxiv.org/abs/2511.12851</link>
<guid>https://arxiv.org/abs/2511.12851</guid>
<content:encoded><![CDATA[
arXiv:2511.12851v1 Announce Type: new 
Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12861</link>
<guid>https://arxiv.org/abs/2511.12861</guid>
<content:encoded><![CDATA[
arXiv:2511.12861v1 Announce Type: new 
Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of Hope in Textual Data using Transformer-Based Models</title>
<link>https://arxiv.org/abs/2511.12874</link>
<guid>https://arxiv.org/abs/2511.12874</guid>
<content:encoded><![CDATA[
arXiv:2511.12874v1 Announce Type: new 
Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy</title>
<link>https://arxiv.org/abs/2511.12920</link>
<guid>https://arxiv.org/abs/2511.12920</guid>
<content:encoded><![CDATA[
arXiv:2511.12920v1 Announce Type: new 
Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Room 2.0: Seeing is Not Understanding for MLLMs</title>
<link>https://arxiv.org/abs/2511.12928</link>
<guid>https://arxiv.org/abs/2511.12928</guid>
<content:encoded><![CDATA[
arXiv:2511.12928v1 Announce Type: new 
Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty</title>
<link>https://arxiv.org/abs/2511.12991</link>
<guid>https://arxiv.org/abs/2511.12991</guid>
<content:encoded><![CDATA[
arXiv:2511.12991v1 Announce Type: new 
Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models</title>
<link>https://arxiv.org/abs/2511.13029</link>
<guid>https://arxiv.org/abs/2511.13029</guid>
<content:encoded><![CDATA[
arXiv:2511.13029v1 Announce Type: new 
Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm</title>
<link>https://arxiv.org/abs/2511.13040</link>
<guid>https://arxiv.org/abs/2511.13040</guid>
<content:encoded><![CDATA[
arXiv:2511.13040v1 Announce Type: new 
Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training</title>
<link>https://arxiv.org/abs/2511.13043</link>
<guid>https://arxiv.org/abs/2511.13043</guid>
<content:encoded><![CDATA[
arXiv:2511.13043v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.13095</link>
<guid>https://arxiv.org/abs/2511.13095</guid>
<content:encoded><![CDATA[
arXiv:2511.13095v1 Announce Type: new 
Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study</title>
<link>https://arxiv.org/abs/2511.13107</link>
<guid>https://arxiv.org/abs/2511.13107</guid>
<content:encoded><![CDATA[
arXiv:2511.13107v1 Announce Type: new 
Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction</title>
<link>https://arxiv.org/abs/2511.13118</link>
<guid>https://arxiv.org/abs/2511.13118</guid>
<content:encoded><![CDATA[
arXiv:2511.13118v1 Announce Type: new 
Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.13126</link>
<guid>https://arxiv.org/abs/2511.13126</guid>
<content:encoded><![CDATA[
arXiv:2511.13126v1 Announce Type: new 
Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels</title>
<link>https://arxiv.org/abs/2511.13152</link>
<guid>https://arxiv.org/abs/2511.13152</guid>
<content:encoded><![CDATA[
arXiv:2511.13152v1 Announce Type: new 
Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis</title>
<link>https://arxiv.org/abs/2511.13159</link>
<guid>https://arxiv.org/abs/2511.13159</guid>
<content:encoded><![CDATA[
arXiv:2511.13159v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2511.13169</link>
<guid>https://arxiv.org/abs/2511.13169</guid>
<content:encoded><![CDATA[
arXiv:2511.13169v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translation Entropy: A Statistical Framework for Evaluating Translation Systems</title>
<link>https://arxiv.org/abs/2511.13180</link>
<guid>https://arxiv.org/abs/2511.13180</guid>
<content:encoded><![CDATA[
arXiv:2511.13180v1 Announce Type: new 
Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study</title>
<link>https://arxiv.org/abs/2511.13182</link>
<guid>https://arxiv.org/abs/2511.13182</guid>
<content:encoded><![CDATA[
arXiv:2511.13182v1 Announce Type: new 
Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms</title>
<link>https://arxiv.org/abs/2511.13225</link>
<guid>https://arxiv.org/abs/2511.13225</guid>
<content:encoded><![CDATA[
arXiv:2511.13225v1 Announce Type: new 
Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</title>
<link>https://arxiv.org/abs/2511.13254</link>
<guid>https://arxiv.org/abs/2511.13254</guid>
<content:encoded><![CDATA[
arXiv:2511.13254v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection</title>
<link>https://arxiv.org/abs/2511.13329</link>
<guid>https://arxiv.org/abs/2511.13329</guid>
<content:encoded><![CDATA[
arXiv:2511.13329v1 Announce Type: new 
Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects</title>
<link>https://arxiv.org/abs/2511.13335</link>
<guid>https://arxiv.org/abs/2511.13335</guid>
<content:encoded><![CDATA[
arXiv:2511.13335v1 Announce Type: new 
Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.13368</link>
<guid>https://arxiv.org/abs/2511.13368</guid>
<content:encoded><![CDATA[
arXiv:2511.13368v1 Announce Type: new 
Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts</title>
<link>https://arxiv.org/abs/2511.13381</link>
<guid>https://arxiv.org/abs/2511.13381</guid>
<content:encoded><![CDATA[
arXiv:2511.13381v1 Announce Type: new 
Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction</title>
<link>https://arxiv.org/abs/2511.13410</link>
<guid>https://arxiv.org/abs/2511.13410</guid>
<content:encoded><![CDATA[
arXiv:2511.13410v1 Announce Type: new 
Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Linear Scoring Model for Translation Quality Evaluation</title>
<link>https://arxiv.org/abs/2511.13467</link>
<guid>https://arxiv.org/abs/2511.13467</guid>
<content:encoded><![CDATA[
arXiv:2511.13467v1 Announce Type: new 
Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns</title>
<link>https://arxiv.org/abs/2511.13481</link>
<guid>https://arxiv.org/abs/2511.13481</guid>
<content:encoded><![CDATA[
arXiv:2511.13481v1 Announce Type: new 
Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Large Language Models to Characterize Public Narratives</title>
<link>https://arxiv.org/abs/2511.13505</link>
<guid>https://arxiv.org/abs/2511.13505</guid>
<content:encoded><![CDATA[
arXiv:2511.13505v1 Announce Type: new 
Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets</title>
<link>https://arxiv.org/abs/2511.13529</link>
<guid>https://arxiv.org/abs/2511.13529</guid>
<content:encoded><![CDATA[
arXiv:2511.13529v1 Announce Type: new 
Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation</title>
<link>https://arxiv.org/abs/2511.13590</link>
<guid>https://arxiv.org/abs/2511.13590</guid>
<content:encoded><![CDATA[
arXiv:2511.13590v1 Announce Type: new 
Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents</title>
<link>https://arxiv.org/abs/2511.13593</link>
<guid>https://arxiv.org/abs/2511.13593</guid>
<content:encoded><![CDATA[
arXiv:2511.13593v1 Announce Type: new 
Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues</title>
<link>https://arxiv.org/abs/2511.13658</link>
<guid>https://arxiv.org/abs/2511.13658</guid>
<content:encoded><![CDATA[
arXiv:2511.13658v1 Announce Type: new 
Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</title>
<link>https://arxiv.org/abs/2511.13689</link>
<guid>https://arxiv.org/abs/2511.13689</guid>
<content:encoded><![CDATA[
arXiv:2511.13689v1 Announce Type: new 
Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalist Foundation Models Are Not Clinical Enough for Hospital Operations</title>
<link>https://arxiv.org/abs/2511.13703</link>
<guid>https://arxiv.org/abs/2511.13703</guid>
<content:encoded><![CDATA[
arXiv:2511.13703v1 Announce Type: new 
Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Architecture, Scaling Laws, and Economics: A Quick Summary</title>
<link>https://arxiv.org/abs/2511.11572</link>
<guid>https://arxiv.org/abs/2511.11572</guid>
<content:encoded><![CDATA[
arXiv:2511.11572v1 Announce Type: cross 
Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Positional and Symbolic Attention Behavior in Transformers</title>
<link>https://arxiv.org/abs/2511.11579</link>
<guid>https://arxiv.org/abs/2511.11579</guid>
<content:encoded><![CDATA[
arXiv:2511.11579v1 Announce Type: cross 
Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Anatomy of a Triton Attention Kernel</title>
<link>https://arxiv.org/abs/2511.11581</link>
<guid>https://arxiv.org/abs/2511.11581</guid>
<content:encoded><![CDATA[
arXiv:2511.11581v1 Announce Type: cross 
Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism</title>
<link>https://arxiv.org/abs/2511.11591</link>
<guid>https://arxiv.org/abs/2511.11591</guid>
<content:encoded><![CDATA[
arXiv:2511.11591v1 Announce Type: cross 
Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLINB: A Climate Intelligence Benchmark for Foundational Models</title>
<link>https://arxiv.org/abs/2511.11597</link>
<guid>https://arxiv.org/abs/2511.11597</guid>
<content:encoded><![CDATA[
arXiv:2511.11597v1 Announce Type: cross 
Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio</title>
<link>https://arxiv.org/abs/2511.11599</link>
<guid>https://arxiv.org/abs/2511.11599</guid>
<content:encoded><![CDATA[
arXiv:2511.11599v1 Announce Type: cross 
Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models</title>
<link>https://arxiv.org/abs/2511.11622</link>
<guid>https://arxiv.org/abs/2511.11622</guid>
<content:encoded><![CDATA[
arXiv:2511.11622v1 Announce Type: cross 
Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges</title>
<link>https://arxiv.org/abs/2511.11624</link>
<guid>https://arxiv.org/abs/2511.11624</guid>
<content:encoded><![CDATA[
arXiv:2511.11624v1 Announce Type: cross 
Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation</title>
<link>https://arxiv.org/abs/2511.11635</link>
<guid>https://arxiv.org/abs/2511.11635</guid>
<content:encoded><![CDATA[
arXiv:2511.11635v1 Announce Type: cross 
Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic generation of DRI Statements</title>
<link>https://arxiv.org/abs/2511.11655</link>
<guid>https://arxiv.org/abs/2511.11655</guid>
<content:encoded><![CDATA[
arXiv:2511.11655v1 Announce Type: cross 
Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Model: Dynamic Neural Architectures for Adaptive Processing</title>
<link>https://arxiv.org/abs/2511.11669</link>
<guid>https://arxiv.org/abs/2511.11669</guid>
<content:encoded><![CDATA[
arXiv:2511.11669v1 Announce Type: cross 
Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems</title>
<link>https://arxiv.org/abs/2511.11678</link>
<guid>https://arxiv.org/abs/2511.11678</guid>
<content:encoded><![CDATA[
arXiv:2511.11678v1 Announce Type: cross 
Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI as a Linguistic Equalizer in Global Science</title>
<link>https://arxiv.org/abs/2511.11687</link>
<guid>https://arxiv.org/abs/2511.11687</guid>
<content:encoded><![CDATA[
arXiv:2511.11687v1 Announce Type: cross 
Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning: From Reflection to Solution</title>
<link>https://arxiv.org/abs/2511.11712</link>
<guid>https://arxiv.org/abs/2511.11712</guid>
<content:encoded><![CDATA[
arXiv:2511.11712v1 Announce Type: cross 
Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy</title>
<link>https://arxiv.org/abs/2511.11816</link>
<guid>https://arxiv.org/abs/2511.11816</guid>
<content:encoded><![CDATA[
arXiv:2511.11816v1 Announce Type: cross 
Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better LLM Reasoning via Dual-Play</title>
<link>https://arxiv.org/abs/2511.11881</link>
<guid>https://arxiv.org/abs/2511.11881</guid>
<content:encoded><![CDATA[
arXiv:2511.11881v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v1 Announce Type: cross 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles</title>
<link>https://arxiv.org/abs/2511.12010</link>
<guid>https://arxiv.org/abs/2511.12010</guid>
<content:encoded><![CDATA[
arXiv:2511.12010v1 Announce Type: cross 
Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys</title>
<link>https://arxiv.org/abs/2511.12036</link>
<guid>https://arxiv.org/abs/2511.12036</guid>
<content:encoded><![CDATA[
arXiv:2511.12036v1 Announce Type: cross 
Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs</title>
<link>https://arxiv.org/abs/2511.12280</link>
<guid>https://arxiv.org/abs/2511.12280</guid>
<content:encoded><![CDATA[
arXiv:2511.12280v1 Announce Type: cross 
Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer</title>
<link>https://arxiv.org/abs/2511.12285</link>
<guid>https://arxiv.org/abs/2511.12285</guid>
<content:encoded><![CDATA[
arXiv:2511.12285v1 Announce Type: cross 
Abstract: Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing</title>
<link>https://arxiv.org/abs/2511.12347</link>
<guid>https://arxiv.org/abs/2511.12347</guid>
<content:encoded><![CDATA[
arXiv:2511.12347v1 Announce Type: cross 
Abstract: We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions</title>
<link>https://arxiv.org/abs/2511.12452</link>
<guid>https://arxiv.org/abs/2511.12452</guid>
<content:encoded><![CDATA[
arXiv:2511.12452v1 Announce Type: cross 
Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Layout: LLM-driven Co-optimization for Interior Layout</title>
<link>https://arxiv.org/abs/2511.12474</link>
<guid>https://arxiv.org/abs/2511.12474</guid>
<content:encoded><![CDATA[
arXiv:2511.12474v1 Announce Type: cross 
Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Prompts for Toxicity Search in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12487</link>
<guid>https://arxiv.org/abs/2511.12487</guid>
<content:encoded><![CDATA[
arXiv:2511.12487v1 Announce Type: cross 
Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing</title>
<link>https://arxiv.org/abs/2511.12529</link>
<guid>https://arxiv.org/abs/2511.12529</guid>
<content:encoded><![CDATA[
arXiv:2511.12529v1 Announce Type: cross 
Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Content-Preserving Secure Linguistic Steganography</title>
<link>https://arxiv.org/abs/2511.12565</link>
<guid>https://arxiv.org/abs/2511.12565</guid>
<content:encoded><![CDATA[
arXiv:2511.12565v1 Announce Type: cross 
Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</title>
<link>https://arxiv.org/abs/2511.12997</link>
<guid>https://arxiv.org/abs/2511.12997</guid>
<content:encoded><![CDATA[
arXiv:2511.12997v1 Announce Type: cross 
Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics</title>
<link>https://arxiv.org/abs/2511.13021</link>
<guid>https://arxiv.org/abs/2511.13021</guid>
<content:encoded><![CDATA[
arXiv:2511.13021v1 Announce Type: cross 
Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization</title>
<link>https://arxiv.org/abs/2511.13091</link>
<guid>https://arxiv.org/abs/2511.13091</guid>
<content:encoded><![CDATA[
arXiv:2511.13091v1 Announce Type: cross 
Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms</title>
<link>https://arxiv.org/abs/2511.13238</link>
<guid>https://arxiv.org/abs/2511.13238</guid>
<content:encoded><![CDATA[
arXiv:2511.13238v1 Announce Type: cross 
Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment</title>
<link>https://arxiv.org/abs/2511.13290</link>
<guid>https://arxiv.org/abs/2511.13290</guid>
<content:encoded><![CDATA[
arXiv:2511.13290v1 Announce Type: cross 
Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research</title>
<link>https://arxiv.org/abs/2511.13333</link>
<guid>https://arxiv.org/abs/2511.13333</guid>
<content:encoded><![CDATA[
arXiv:2511.13333v1 Announce Type: cross 
Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Grounded Enhancement for Visual Document Retrieval</title>
<link>https://arxiv.org/abs/2511.13415</link>
<guid>https://arxiv.org/abs/2511.13415</guid>
<content:encoded><![CDATA[
arXiv:2511.13415v1 Announce Type: cross 
Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Multi-Table Retrieval Through Iterative Search</title>
<link>https://arxiv.org/abs/2511.13418</link>
<guid>https://arxiv.org/abs/2511.13418</guid>
<content:encoded><![CDATA[
arXiv:2511.13418v1 Announce Type: cross 
Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models</title>
<link>https://arxiv.org/abs/2511.13548</link>
<guid>https://arxiv.org/abs/2511.13548</guid>
<content:encoded><![CDATA[
arXiv:2511.13548v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P1: Mastering Physics Olympiads with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13612</link>
<guid>https://arxiv.org/abs/2511.13612</guid>
<content:encoded><![CDATA[
arXiv:2511.13612v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</title>
<link>https://arxiv.org/abs/2511.13646</link>
<guid>https://arxiv.org/abs/2511.13646</guid>
<content:encoded><![CDATA[
arXiv:2511.13646v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G\"odel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Historical/temporal necessities/possibilities, and a logical theory of them in branching time</title>
<link>https://arxiv.org/abs/2208.11922</link>
<guid>https://arxiv.org/abs/2208.11922</guid>
<content:encoded><![CDATA[
arXiv:2208.11922v2 Announce Type: replace 
Abstract: In this paper, we do three kinds of work. First, we recognize four notions of necessity and two notions of possibility related to time flow, namely strong/weak historical/temporal necessities, as well as historical/temporal possibilities, which are motivated more from a linguistic perspective than from a philosophical one. Strong/weak historical necessities and historical possibility typically concern the possible futures of the present world, and strong/weak temporal necessities and temporal possibility concern possible timelines of alternatives of the present world. Second, we provide our approach to the six notions and present a logical theory of them in branching time. Our approach to the six notions is as follows. The agent has a system of ontic rules that determine expected timelines. She treats some ontic rules as undefeatable, determining accepted timelines. The domains of strong/weak historical necessities, respectively, consist of accepted and expected timelines passing through the present moment, and historical possibility is the dual of strong historical necessity. The domains of strong/weak temporal necessities, respectively, consist of accepted and expected timelines, and temporal possibility is the dual of strong temporal necessity. The logical theory has six operators: a last-moment operator, a next-moment operator, and four operators for the four notions of necessity. Formulas' evaluation contexts consist of a tree-like model representing a time flow, a context representing the agent's system of ontic rules, a timeline, and an instant. Third, we offer an axiomatic system for the logical theory and show its soundness and completeness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Machine Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2309.06706</link>
<guid>https://arxiv.org/abs/2309.06706</guid>
<content:encoded><![CDATA[
arXiv:2309.06706v3 Announce Type: replace 
Abstract: Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language</title>
<link>https://arxiv.org/abs/2311.11142</link>
<guid>https://arxiv.org/abs/2311.11142</guid>
<content:encoded><![CDATA[
arXiv:2311.11142v2 Announce Type: replace 
Abstract: The Bangla linguistic variety is a fascinating mix of regional dialects that contributes to the cultural diversity of the Bangla-speaking community. Despite extensive study into translating Bangla to English, English to Bangla, and Banglish to Bangla in the past, there has been a noticeable gap in translating Bangla regional dialects into standard Bangla. In this study, we set out to fill this gap by creating a collection of 32,500 sentences, encompassing Bangla, Banglish, and English, representing five regional Bangla dialects. Our aim is to translate these regional dialects into standard Bangla and detect regions accurately. To tackle the translation and region detection tasks, we propose two novel models: DialectBanglaT5 for translating regional dialects into standard Bangla and DialectBanglaBERT for identifying the dialect's region of origin. DialectBanglaT5 demonstrates superior performance across all dialects, achieving the highest BLEU score of 71.93, METEOR of 0.8503, and the lowest WER of 0.1470 and CER of 0.0791 on the Mymensingh dialect. It also achieves strong ROUGE scores across all dialects, indicating both accuracy and fluency in capturing dialectal nuances. In parallel, DialectBanglaBERT achieves an overall region classification accuracy of 89.02%, with notable F1-scores of 0.9241 for Chittagong and 0.8736 for Mymensingh, confirming its effectiveness in handling regional linguistic variation. This is the first large-scale investigation focused on Bangla regional dialect translation and region detection. Our proposed models highlight the potential of dialect-specific modeling and set a new benchmark for future research in low-resource and dialect-rich language settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2402.10552</link>
<guid>https://arxiv.org/abs/2402.10552</guid>
<content:encoded><![CDATA[
arXiv:2402.10552v4 Announce Type: replace 
Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataGen: Unified Synthetic Dataset Generation via Large Language Models</title>
<link>https://arxiv.org/abs/2406.18966</link>
<guid>https://arxiv.org/abs/2406.18966</guid>
<content:encoded><![CDATA[
arXiv:2406.18966v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProFuser: Progressive Fusion of Large Language Models</title>
<link>https://arxiv.org/abs/2408.04998</link>
<guid>https://arxiv.org/abs/2408.04998</guid>
<content:encoded><![CDATA[
arXiv:2408.04998v2 Announce Type: replace 
Abstract: While fusing the capacities and advantages of various large language models offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including Vicuna-7B-v1.5, Llama-2-7B-Chat, and MPT-7B-8K-Chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning</title>
<link>https://arxiv.org/abs/2408.14398</link>
<guid>https://arxiv.org/abs/2408.14398</guid>
<content:encoded><![CDATA[
arXiv:2408.14398v4 Announce Type: replace 
Abstract: Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Breach: Assessing the Robustness of Transformer-based QA Models</title>
<link>https://arxiv.org/abs/2409.10997</link>
<guid>https://arxiv.org/abs/2409.10997</guid>
<content:encoded><![CDATA[
arXiv:2409.10997v4 Announce Type: replace 
Abstract: Contextual question-answering models are susceptible to adversarial perturbations to input context, commonly observed in real-world scenarios. These adversarial noises are designed to degrade the performance of the model by distorting the textual input. We introduce a unique dataset that incorporates seven distinct types of adversarial noise into the context, each applied at five different intensity levels on the SQuAD dataset. To quantify the robustness, we utilize robustness metrics providing a standardized measure for assessing model performance across varying noise types and levels. Experiments on transformer-based question-answering models reveal robustness vulnerabilities and important insights into the model's performance in realistic textual input.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is deeper always better? Replacing linear mappings with deep learning networks in the Discriminative Lexicon Model</title>
<link>https://arxiv.org/abs/2410.04259</link>
<guid>https://arxiv.org/abs/2410.04259</guid>
<content:encoded><![CDATA[
arXiv:2410.04259v2 Announce Type: replace 
Abstract: Recently, deep learning models have increasingly been used in cognitive modelling of language. This study asks whether deep learning can help us to better understand the learning problem that needs to be solved by speakers, above and beyond linear methods. We utilise the Discriminative Lexicon Model introduced by Baayen and colleagues, which models comprehension and production with mappings between numeric form and meaning vectors. While so far, these mappings have been linear (Linear Discriminative Learning, LDL), in the present study we replace them with deep dense neural networks (Deep Discriminative Learning, DDL). We find that DDL affords more accurate mappings for large and diverse datasets from English and Dutch, but not necessarily for Estonian and Taiwan Mandarin. DDL outperforms LDL in particular for words with pseudo-morphological structure such as chol+er. Applied to average reaction times, we find that DDL is outperformed by frequency-informed linear mappings (FIL). However, DDL trained in a frequency-informed way ('frequency-informed' deep learning, FIDDL) substantially outperforms FIL. Finally, while linear mappings can very effectively be updated from trial-to-trial to model incremental lexical learning, deep mappings cannot do so as effectively. At present, both linear and deep mappings are informative for understanding language.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Factor Level Preferences to Improve Human-Model Alignment</title>
<link>https://arxiv.org/abs/2410.06965</link>
<guid>https://arxiv.org/abs/2410.06965</guid>
<content:encoded><![CDATA[
arXiv:2410.06965v3 Announce Type: replace 
Abstract: Large language models (LLMs) often exhibit tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs. While crucial for improvement, identifying the factors driving these misalignments remains challenging due to existing evaluation methods' reliance on coarse-grained comparisons and lack of explainability. To address this, we introduce PROFILE, an automated framework to uncover and measure factor-level preference alignment of humans and LLMs. Using PROFILE, we analyze preference alignment across three key tasks: summarization, instruction-following, and document-based QA. We find a significant discrepancy: while LLMs show poor factor-level alignment with human preferences when generating texts, they demonstrate strong alignment in discrimination tasks. We demonstrate how leveraging the identified generation-discrimination gap can be used to improve LLM alignment through multiple approaches, including fine-tuning with self-guidance. Our work highlights the value of factor-level analysis for identifying hidden misalignments and provides a practical framework for improving LLM-human preference alignment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support Chatbot</title>
<link>https://arxiv.org/abs/2411.00034</link>
<guid>https://arxiv.org/abs/2411.00034</guid>
<content:encoded><![CDATA[
arXiv:2411.00034v2 Announce Type: replace 
Abstract: Companies support their customers using live chats and chatbots to gain their loyalty. AFAS is a Dutch company aiming to leverage the opportunity large language models (LLMs) offer to answer customer queries with minimal to no input from its customer support team. Adding to its complexity, it is unclear what makes a response correct, and that too in Dutch. Further, with minimal data available for training, the challenge is to identify whether an answer generated by a large language model is correct and do it on the fly.
  This study is the first to define the correctness of a response based on how the support team at AFAS makes decisions. It leverages literature on natural language generation and automated answer grading systems to automate the decision-making of the customer support team. We investigated questions requiring a binary response (e.g., Would it be possible to adjust tax rates manually?) or instructions (e.g., How would I adjust tax rate manually?) to test how close our automated approach reaches support rating. Our approach can identify wrong messages in 55\% of the cases. This work demonstrates the potential for automatically assessing when our chatbot may provide incorrect or misleading answers. Specifically, we contribute (1) a definition and metrics for assessing correctness, and (2) suggestions to improve correctness with respect to regional language and question type.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Topological Structures from Language: A Survey of Topological Data Analysis Applications in NLP</title>
<link>https://arxiv.org/abs/2411.10298</link>
<guid>https://arxiv.org/abs/2411.10298</guid>
<content:encoded><![CDATA[
arXiv:2411.10298v4 Announce Type: replace 
Abstract: The surge of data available on the Internet has led to the adoption of various computational methods to analyze and extract valuable insights from this wealth of information. Among these, the field of Machine Learning (ML) has thrived by leveraging data to extract meaningful insights. However, ML techniques face notable challenges when dealing with real-world data, often due to issues of imbalance, noise, insufficient labeling, and high dimensionality. To address these limitations, some researchers advocate for the adoption of Topological Data Analysis (TDA), a statistical approach that discerningly captures the intrinsic shape of data despite noise. Despite its potential, TDA has not gained as much traction within the Natural Language Processing (NLP) domain compared to structurally distinct areas like computer vision. Nevertheless, a dedicated community of researchers has been exploring the application of TDA in NLP, yielding 100 papers we comprehensively survey in this paper. Our findings categorize these efforts into theoretical and non-theoretical approaches. Theoretical approaches aim to explain linguistic phenomena from a topological viewpoint, while non-theoretical approaches merge TDA with ML features, utilizing diverse numerical representation techniques. We conclude by exploring the challenges and unresolved questions that persist in this niche field. Resources and a list of papers on this topic can be found at: https://github.com/AdaUchendu/AwesomeTDA4NLP.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding World or Predicting Future? A Comprehensive Survey of World Models</title>
<link>https://arxiv.org/abs/2411.14499</link>
<guid>https://arxiv.org/abs/2411.14499</guid>
<content:encoded><![CDATA[
arXiv:2411.14499v3 Announce Type: replace 
Abstract: The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script</title>
<link>https://arxiv.org/abs/2412.12478</link>
<guid>https://arxiv.org/abs/2412.12478</guid>
<content:encoded><![CDATA[
arXiv:2412.12478v5 Announce Type: replace 
Abstract: DNN-based language models excel across various NLP tasks but remain highly vulnerable to textual adversarial attacks. While adversarial text generation is crucial for NLP security, explainability, evaluation, and data augmentation, related work remains overwhelmingly English-centric, leaving the problem of constructing high-quality and sustainable adversarial robustness benchmarks for lower-resourced languages both difficult and understudied. First, method customization for lower-resourced languages is complicated due to linguistic differences and limited resources. Second, automated attacks are prone to generating invalid or ambiguous adversarial texts. Last but not least, language models continuously evolve and may be immune to parts of previously generated adversarial texts. To address these challenges, we introduce HITL-GAT, an interactive system based on a general approach to human-in-the-loop generation of adversarial texts. Additionally, we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods and establishing its first adversarial robustness benchmark, providing a valuable reference for other lower-resourced languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths</title>
<link>https://arxiv.org/abs/2502.14902</link>
<guid>https://arxiv.org/abs/2502.14902</guid>
<content:encoded><![CDATA[
arXiv:2502.14902v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: https://github.com/BUPT-GAMMA/PathRAG
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Extraction and Generation for Robust Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.04789</link>
<guid>https://arxiv.org/abs/2503.04789</guid>
<content:encoded><![CDATA[
arXiv:2503.04789v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) enhances LLMs with external knowledge, yet generation remains vulnerable to retrieval-induced noise and uncertain placement of relevant chunks, often causing hallucinations. We present Ext2Gen, an extract-then-generate framework that strengthens LLMs via joint evidence selection and answer generation, dynamically identifying query-relevant content while suppressing noise, thereby removing the need for any independent pre-generation compression module. Optimized through preference alignment with well-curated pairwise feedback, Ext2Gen produces accurate and faithful answers even under noisy or imprecise retrieval. Experiments demonstrate that it substantially enhances the robustness of the generation backbone and yields greater performance gains than methods relying on independent compression models, e.g., Recomp, CompAct, EXIT). It further benefits from improved retrieval techniques such as query rewriting, underscoring that generation-side enhancements address limitations that retrieval alone cannot overcome.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Conditional Emergence of Multilingual Image Captioning via Generalization from Translation</title>
<link>https://arxiv.org/abs/2503.09443</link>
<guid>https://arxiv.org/abs/2503.09443</guid>
<content:encoded><![CDATA[
arXiv:2503.09443v2 Announce Type: replace 
Abstract: Cross-lingual, cross-task transfer is challenged by task-specific data scarcity, which becomes more severe as language support grows and is further amplified in vision-language models (VLMs). We investigate multilingual generalization in encoder-decoder transformer VLMs to enable zero-shot image captioning in languages encountered only in the translation task. In this setting, the encoder must learn to generate generalizable, task-aware latent vision representations to instruct the decoder via inserted cross-attention layers. To analyze scaling behavior, we train Florence-2 based and Gemma-2 based models (0.4B to 11.2B parameters) on a synthetic dataset using varying compute budgets. While all languages in the dataset have image-aligned translations, only a subset of them include image captions. Notably, we show that captioning can emerge using a language prefix, even when this language only appears in the translation task. We find that indirect learning of unseen task-language pairs adheres to scaling laws that are governed by the multilinguality of the model, model size, and seen training samples. Finally, we demonstrate that the scaling laws extend to downstream tasks, achieving competitive performance through fine-tuning in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation</title>
<link>https://arxiv.org/abs/2504.03197</link>
<guid>https://arxiv.org/abs/2504.03197</guid>
<content:encoded><![CDATA[
arXiv:2504.03197v4 Announce Type: replace 
Abstract: With the rapid advancement of mathematical reasoning capabilities in Large Language Models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: multimodal explanation. In real-world instructional contexts, human tutors routinely employ visual aids, such as diagrams, markings, and highlights, to enhance conceptual clarity. To bridge this gap, we introduce the multimodal solution explanation task, designed to evaluate whether models can identify visual keypoints, such as auxiliary lines, points, angles, and generate explanations that incorporate these key elements essential for understanding. To evaluate model performance on this task, we propose ME2, a multimodal benchmark consisting of 1,000 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that current models struggle to identify visual keypoints. In the task of generating keypoint-based explanations, open-source models also face notable difficulties. This highlights a significant gap in current LLMs' ability to perform mathematical visual grounding, engage in visually grounded reasoning, and provide explanations in educational contexts. We expect that the multimodal solution explanation task and the ME2 dataset will catalyze further research on LLMs in education and promote their use as effective, explanation-oriented AI tutors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Robust Response Generation in the Wild</title>
<link>https://arxiv.org/abs/2504.12982</link>
<guid>https://arxiv.org/abs/2504.12982</guid>
<content:encoded><![CDATA[
arXiv:2504.12982v2 Announce Type: replace 
Abstract: The proliferation of large language models (LLMs) has significantly advanced intelligent systems. Unfortunately, LLMs often face knowledge conflicts between internal memory and retrieved external information, arising from misinformation, biases, or outdated knowledge. These conflicts undermine response reliability and introduce uncertainty in decision-making. In this work, we analyze how LLMs navigate knowledge conflicts from an information-theoretic perspective and reveal that when conflicting and supplementary information exhibit significant differences, LLMs confidently resolve their preferences and alleviate the uncertainty during their response generation. When this difference is ambiguous, LLMs experience considerable uncertainty about their generation. Based on this insight, we propose Swin-VIB, a novel framework that integrates a pipeline of variational information bottleneck models to adapt the retrieved information difference, facilitating robust response generation of LLMs even in conflicting contexts. Extensive experiments confirm our theoretical analysis and demonstrate the performance of Swin-VIB. Notably, Swin-VIB outperforms all competitive baselines in terms of the accuracy of the multiple-choice task, while improving the EM values in the open-ended QA task by at least 11.14%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
<link>https://arxiv.org/abs/2505.04649</link>
<guid>https://arxiv.org/abs/2505.04649</guid>
<content:encoded><![CDATA[
arXiv:2505.04649v3 Announce Type: replace 
Abstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations</title>
<link>https://arxiv.org/abs/2505.12560</link>
<guid>https://arxiv.org/abs/2505.12560</guid>
<content:encoded><![CDATA[
arXiv:2505.12560v3 Announce Type: replace 
Abstract: Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains POS-tagged parallel text data from more than 1,940 languages, representing 155 language families and 78 isolates, dwarfing previously available resources. The accuracy of particular tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of intransitive word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic intransitive word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering</title>
<link>https://arxiv.org/abs/2505.14099</link>
<guid>https://arxiv.org/abs/2505.14099</guid>
<content:encoded><![CDATA[
arXiv:2505.14099v2 Announce Type: replace 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation</title>
<link>https://arxiv.org/abs/2505.15249</link>
<guid>https://arxiv.org/abs/2505.15249</guid>
<content:encoded><![CDATA[
arXiv:2505.15249v2 Announce Type: replace 
Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred tools for judging text-image alignment, yet their robustness along the visual modality remains underexplored. This work is the first study to address a key research question: Can adversarial visual manipulations systematically fool LVLM judges into assigning unfairly inflated scores? We define potential image induced biases within the context of T2I evaluation and examine how these biases affect the evaluations of LVLM judges. Moreover, we introduce a novel, fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is deliberately constructed to exhibit diverse score distributions. By introducing the defined biases into the benchmark, we reveal that all tested LVLM judges exhibit vulnerability across all domains, consistently inflating scores for manipulated images. Further analysis reveals that combining multiple biases amplifies their effects, and pairwise evaluations are similarly susceptible. Moreover, we observe that visual biases persist under prompt-based mitigation strategies, highlighting the vulnerability of current LVLM evaluation systems and underscoring the urgent need for more robust LVLM judges.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model</title>
<link>https://arxiv.org/abs/2505.16000</link>
<guid>https://arxiv.org/abs/2505.16000</guid>
<content:encoded><![CDATA[
arXiv:2505.16000v5 Announce Type: replace 
Abstract: The rapid advancement of language models has demonstrated the potential of artificial intelligence in the healthcare industry. However, small language models struggle with specialized domains in low-resource languages like Persian. While numerous medical-domain websites exist in Persian, no curated dataset or corpus has been available making ours the first of its kind. This study introduces a newly curated dataset comprising 20k doctor-patient Q\&amp;A pairs and 60\% of a 90-million-token crawled corpus from medical magazines. Using a parameter-efficient fine-tuning approach, we enhanced the medical knowledge of the baseline model, aya-expanse-8b. Benchmark evaluations demonstrate that the fine-tuned model achieves improved accuracy in medical question answering and successfully passed the Iranian Basic Medical Science Entrance Exam (IBSEE) in September 2023, which the baseline model did not. Additionally, the fine-tuned model improved Persian-translated MMLU accuracy by an average of 2.67\%. This work highlights the potential of leveraging open-access online data to enrich small language models in medical fields, providing a novel solution for Persian medical AI applications suitable for resource-constrained environments. Future research could explore multimodal input to further enhance performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Thought Driven Adversarial Scenario Extrapolation for Robust Language Models</title>
<link>https://arxiv.org/abs/2505.17089</link>
<guid>https://arxiv.org/abs/2505.17089</guid>
<content:encoded><![CDATA[
arXiv:2505.17089v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain susceptible to a growing spectrum of safety risks, including jailbreaks, toxic content, hallucinations, and bias. Existing defenses often address only a single threat type or resort to rigid outright rejection, sacrificing user experience and failing to generalize across diverse and novel attacks. This paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that leverages Chain-of-Thought (CoT) reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides the LLM through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before generating a response to the user query. Comprehensive evaluation on four adversarial benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak attack success rates and minimal toxicity, while slashing outright rejections to <4%. ASE outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&amp;A and 4-10x lower bias scores. By transforming adversarial perception into an intrinsic cognitive process, ASE sets a new paradigm for secure and natural human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCRum-9: Multilingual Stance Classification over Rumours on Social Media</title>
<link>https://arxiv.org/abs/2505.18916</link>
<guid>https://arxiv.org/abs/2505.18916</guid>
<content:encoded><![CDATA[
arXiv:2505.18916v3 Announce Type: replace 
Abstract: We introduce SCRum-9, the largest multilingual Stance Classification dataset for Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9 goes beyond existing stance classification datasets by covering more languages, linking examples to more fact-checked claims (2.1k), and including confidence-related annotations from multiple annotators to account for intra- and inter-annotator variability. Annotations were made by at least two native speakers per language, totalling more than 405 hours of annotation and 8,150 dollars in compensation. Further, SCRum-9 is used to benchmark five large language models (LLMs) and two multilingual masked language models (MLMs) in In-Context Learning (ICL) and fine-tuning setups. This paper also innovates by exploring the use of multilingual synthetic data for rumour stance classification, showing that even LLMs with weak ICL performance can produce valuable synthetic data for fine-tuning small MLMs, enabling them to achieve higher performance than zero-shot ICL in LLMs. Finally, we examine the relationship between model predictions and human uncertainty on ambiguous cases finding that model predictions often match the second-choice labels assigned by annotators, rather than diverging entirely from human judgments. SCRum-9 is publicly released to the research community with potential to foster further research on multilingual analysis of misleading narratives on social media.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2505.19768</link>
<guid>https://arxiv.org/abs/2505.19768</guid>
<content:encoded><![CDATA[
arXiv:2505.19768v2 Announce Type: replace 
Abstract: Real-world multimodal misinformation often arises from mixed forgery sources, requiring dynamic reasoning and adaptive verification. However, existing methods mainly rely on static pipelines and limited tool usage, limiting their ability to handle such complexity and diversity. To address this challenge, we propose \method, a novel misinformation detection agent that incorporates an extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of modular tools such as web search, forgery detection, and consistency analysis. Each tool is described using standardized templates, enabling seamless integration and future expansion. To avoid inefficiency from using all tools simultaneously, a greedy search-based selector is proposed to identify a task-relevant subset. This subset then serves as the action space for MCTS to dynamically collect evidence and perform multi-source verification. To better align MCTS with the multi-source nature of misinformation detection, \method~ extends traditional MCTS with multi-source verification, which decomposes the task into coordinated subtasks targeting different forgery sources. A dual reward mechanism containing a reasoning trajectory score and a confidence score is further proposed to encourage a balance between exploration across mixed forgery sources and exploitation for more reliable evidence. We conduct ablation studies to confirm the effectiveness of the tree search mechanism and tool usage. Extensive experiments further show that \method~ consistently outperforms existing baselines on challenging mixed-source multimodal misinformation benchmarks, demonstrating its strong potential as a training-free detector.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query</title>
<link>https://arxiv.org/abs/2505.20334</link>
<guid>https://arxiv.org/abs/2505.20334</guid>
<content:encoded><![CDATA[
arXiv:2505.20334v2 Announce Type: replace 
Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REIC: RAG-Enhanced Intent Classification at Scale</title>
<link>https://arxiv.org/abs/2506.00210</link>
<guid>https://arxiv.org/abs/2506.00210</guid>
<content:encoded><![CDATA[
arXiv:2506.00210v2 Announce Type: replace 
Abstract: Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers</title>
<link>https://arxiv.org/abs/2506.01215</link>
<guid>https://arxiv.org/abs/2506.01215</guid>
<content:encoded><![CDATA[
arXiv:2506.01215v2 Announce Type: replace 
Abstract: As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[
arXiv:2506.02454v2 Announce Type: replace 
Abstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\% overall win rate over the baseline method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Test-Time Scaling with Model-Free Speculative Sampling</title>
<link>https://arxiv.org/abs/2506.04708</link>
<guid>https://arxiv.org/abs/2506.04708</guid>
<content:encoded><![CDATA[
arXiv:2506.04708v2 Announce Type: replace 
Abstract: Language models have demonstrated remarkable capabilities in reasoning tasks through test-time scaling techniques like best-of-N sampling and tree search. However, these approaches often demand substantial computational resources, creating a critical trade-off between performance and efficiency. We introduce STAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative decoding approach that exploits the inherent redundancy in reasoning trajectories to achieve significant acceleration without compromising accuracy. Our analysis shows that reasoning paths frequently reuse similar reasoning patterns, enabling efficient model-free token prediction without requiring separate draft models. By introducing stochastic drafting and preserving probabilistic information through a memory-efficient logit-based N-gram module, combined with optimized Gumbel-Top-K sampling and data-driven tree construction, STAND significantly improves token acceptance rates. Extensive evaluations across multiple models and reasoning tasks (AIME-2024, GPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference latency by 60-65% compared to standard autoregressive decoding while maintaining accuracy. Furthermore, STAND consistently outperforms state-of-the-art speculative decoding methods across diverse inference patterns, including single-trajectory decoding, batch decoding, and test-time tree search. As a model-free approach, STAND can be applied to any existing language model without additional training, making it a powerful plug-and-play solution for accelerating language model reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.04832</link>
<guid>https://arxiv.org/abs/2506.04832</guid>
<content:encoded><![CDATA[
arXiv:2506.04832v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, becoming a new and hard-to-detect source of hallucination. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the model's reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the model's decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. The joint utilization of these signals makes RACE a more robust detector of hallucinations in LRMs. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. The source code is available at https://github.com/bebr2/RACE
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning</title>
<link>https://arxiv.org/abs/2506.05813</link>
<guid>https://arxiv.org/abs/2506.05813</guid>
<content:encoded><![CDATA[
arXiv:2506.05813v2 Announce Type: replace 
Abstract: Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Post-Training Refinement of Latent Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.08552</link>
<guid>https://arxiv.org/abs/2506.08552</guid>
<content:encoded><![CDATA[
arXiv:2506.08552v2 Announce Type: replace 
Abstract: Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA without additional training.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToxSyn: Reducing Bias in Hate Speech Detection via Synthetic Minority Data in Brazilian Portuguese</title>
<link>https://arxiv.org/abs/2506.10245</link>
<guid>https://arxiv.org/abs/2506.10245</guid>
<content:encoded><![CDATA[
arXiv:2506.10245v2 Announce Type: replace 
Abstract: The development of robust hate speech detection systems remains limited by the lack of large-scale, fine-grained training data, especially for languages beyond English. Existing corpora typically rely on coarse toxic/non-toxic labels, and the few that capture hate directed at specific minority groups critically lack the non-toxic counterexamples (i.e., benign text about minorities) required to distinguish genuine hate from mere discussion. We introduce ToxSyn, the first Portuguese large-scale corpus explicitly designed for multi-label hate speech detection across nine protected minority groups. Generated via a controllable four-stage pipeline, ToxSyn includes discourse-type annotations to capture rhetorical strategies of toxic language, such as sarcasm or dehumanization. Crucially, it systematically includes the non-toxic counterexamples absent in all other public datasets. Our experiments reveal a catastrophic, mutual generalization failure between social-media domains and ToxSyn: models trained on social media struggle to generalize to minority-specific contexts, and vice-versa. This finding indicates they are distinct tasks and exposes summary metrics like Macro F1 can be unreliable indicators of true model behavior, as they completely mask model failure. We publicly release ToxSyn at HuggingFace to foster reproducible research on synthetic data generation and benchmark progress in hate-speech detection for low- and mid-resource languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Pay Attention</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
arXiv:2506.11305v2 Announce Type: replace 
Abstract: The Transformer has become the de facto standard for modern language models owing to its parallelizable training and effective autoregressive decoding. However, its fixed context window and the quadratic time and memory costs of its self-attention mechanism remain central bottlenecks. These constraints have revived interest in recurrent architectures that scale linearly with sequence length, but at the cost of reduced parallelism. In this paper, we introduce Avey, a new foundational architecture that breaks away from both attention and recurrence. Avey pairs a ranker with an autoregressive neural processor to select and contextualize only the most relevant tokens for any given token. Specifically, it decouples sequence length from context width, thus enabling effective and efficient processing of arbitrarily long sequences. Results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while significantly outperforming it on tasks requiring long-range dependency modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models</title>
<link>https://arxiv.org/abs/2506.15545</link>
<guid>https://arxiv.org/abs/2506.15545</guid>
<content:encoded><![CDATA[
arXiv:2506.15545v2 Announce Type: replace 
Abstract: Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches. We open-sourced our Pallas kernels along with model codes to facilitate further research effort.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Organizing Language</title>
<link>https://arxiv.org/abs/2506.23293</link>
<guid>https://arxiv.org/abs/2506.23293</guid>
<content:encoded><![CDATA[
arXiv:2506.23293v2 Announce Type: replace 
Abstract: We introduce a novel paradigm of emergent local memory. It is a continuous-learning completely-parallel content-addressable memory encoding global order. It demonstrates how local constraints on uncoordinated learning can produce topologically protected memories realizing emergent symbolic order. It is therefore a neuro-symbolic bridge.
  It further has the ability to produce human language without data, by exploiting its own self-organizing dynamics. It teaches us that words arise as a side-effect of emergent symbolic order, and that human language patterns at all structural levels reflect a universal mechanism of word formation (which is subregular). This work answers essential questions about the existence \& origin of all the human language data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</title>
<link>https://arxiv.org/abs/2507.02962</link>
<guid>https://arxiv.org/abs/2507.02962</guid>
<content:encoded><![CDATA[
arXiv:2507.02962v5 Announce Type: replace 
Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
arXiv:2507.22564v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling the Influence of Amplifying Language-Specific Neurons</title>
<link>https://arxiv.org/abs/2507.22581</link>
<guid>https://arxiv.org/abs/2507.22581</guid>
<content:encoded><![CDATA[
arXiv:2507.22581v3 Announce Type: replace 
Abstract: Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02360</link>
<guid>https://arxiv.org/abs/2508.02360</guid>
<content:encoded><![CDATA[
arXiv:2508.02360v2 Announce Type: replace 
Abstract: Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty</title>
<link>https://arxiv.org/abs/2508.03294</link>
<guid>https://arxiv.org/abs/2508.03294</guid>
<content:encoded><![CDATA[
arXiv:2508.03294v2 Announce Type: replace 
Abstract: Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression</title>
<link>https://arxiv.org/abs/2508.05337</link>
<guid>https://arxiv.org/abs/2508.05337</guid>
<content:encoded><![CDATA[
arXiv:2508.05337v2 Announce Type: replace 
Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., "Wait" and "Alternatively") to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRS's practical value for efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures</title>
<link>https://arxiv.org/abs/2508.06105</link>
<guid>https://arxiv.org/abs/2508.06105</guid>
<content:encoded><![CDATA[
arXiv:2508.06105v2 Announce Type: replace 
Abstract: Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a $\textbf{Logic}$-aware $\textbf{R}etrieval$-$\textbf{A}$ugmented $\textbf{G}$eneration framework ($\textbf{LogicRAG}$) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneJailEval: A Scenario-Adaptive Multi-Dimensional Framework for Jailbreak Evaluation</title>
<link>https://arxiv.org/abs/2508.06194</link>
<guid>https://arxiv.org/abs/2508.06194</guid>
<content:encoded><![CDATA[
arXiv:2508.06194v2 Announce Type: replace 
Abstract: Accurate jailbreak evaluation is critical for LLM red team testing and jailbreak research. Mainstream methods rely on binary classification (string matching, toxic text classifiers, and LLM-based methods), outputting only "yes/no" labels without quantifying harm severity. Emerged multi-dimensional frameworks (e.g., Security Violation, Relative Truthfulness and Informativeness) use unified evaluation standards across scenarios, leading to scenario-specific mismatches (e.g., "Relative Truthfulness" is irrelevant to "hate speech"), undermining evaluation accuracy. To address these, we propose SceneJailEval, with key contributions: (1) A pioneering scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical "one-size-fits-all" limitation of existing multi-dimensional methods, and boasting robust extensibility to seamlessly adapt to customized or emerging scenarios. (2) A novel 14-scenario dataset featuring rich jailbreak variants and regional cases, addressing the long-standing gap in high-quality, comprehensive benchmarks for scenario-adaptive evaluation. (3) SceneJailEval delivers state-of-the-art performance with an F1 score of 0.917 on our full-scenario dataset (+6% over SOTA) and 0.995 on JBB (+3% over SOTA), breaking through the accuracy bottleneck of existing evaluation methods in heterogeneous scenarios and solidifying its superiority.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
<link>https://arxiv.org/abs/2508.13953</link>
<guid>https://arxiv.org/abs/2508.13953</guid>
<content:encoded><![CDATA[
arXiv:2508.13953v2 Announce Type: replace 
Abstract: In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page https://github.com/aaronlifenghan/ReviewGraph
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</title>
<link>https://arxiv.org/abs/2508.14031</link>
<guid>https://arxiv.org/abs/2508.14031</guid>
<content:encoded><![CDATA[
arXiv:2508.14031v2 Announce Type: replace 
Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries</title>
<link>https://arxiv.org/abs/2508.18212</link>
<guid>https://arxiv.org/abs/2508.18212</guid>
<content:encoded><![CDATA[
arXiv:2508.18212v2 Announce Type: replace 
Abstract: The emergence of LM-based judging reward modeling, represented by generative reward models, has successfully made reinforcement learning from AI feedback (RLAIF) efficient and scalable. To further advance this paradigm, we propose a core insight: this form of reward modeling shares fundamental formal consistency with natural language inference (NLI), a core task in natural language understanding. This reframed perspective points to a key path for building superior reward models: scaling the model's comprehension boundaries. Pursuing this path, exploratory experiments on NLI tasks demonstrate that the slot prediction masked language models (MLMs) incorporating contextual explanations achieve significantly better performance compared to mainstream autoregressive models. Based on this key finding, we propose ESFP-RM, a two-stage LM-based judging reward model that utilizes an explanation based slot framework for prediction to fully leverage the advantages of MLMs. Extensive experiments demonstrate that in both reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers more stable and generalizable reward signals compared to generative reward models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</title>
<link>https://arxiv.org/abs/2509.02492</link>
<guid>https://arxiv.org/abs/2509.02492</guid>
<content:encoded><![CDATA[
arXiv:2509.02492v3 Announce Type: replace 
Abstract: Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts</title>
<link>https://arxiv.org/abs/2509.12440</link>
<guid>https://arxiv.org/abs/2509.12440</guid>
<content:encoded><![CDATA[
arXiv:2509.12440v2 Announce Type: replace 
Abstract: Deploying Large Language Models (LLMs) in medical applications requires fact-checking capabilities to ensure patient safety and regulatory compliance. We introduce MedFact, a challenging Chinese medical fact-checking benchmark with 2,116 expert-annotated instances from diverse real-world texts, spanning 13 specialties, 8 error types, 4 writing styles, and 5 difficulty levels. Construction uses a hybrid AI-human framework where iterative expert feedback refines AI-driven, multi-criteria filtering to ensure high quality and difficulty. We evaluate 20 leading LLMs on veracity classification and error localization, and results show models often determine if text contains errors but struggle to localize them precisely, with top performers falling short of human performance. Our analysis reveals the "over-criticism" phenomenon, a tendency for models to misidentify correct information as erroneous, which can be exacerbated by advanced reasoning techniques such as multi-agent collaboration and inference-time scaling. MedFact highlights the challenges of deploying medical LLMs and provides resources to develop factually reliable medical AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts</title>
<link>https://arxiv.org/abs/2509.23188</link>
<guid>https://arxiv.org/abs/2509.23188</guid>
<content:encoded><![CDATA[
arXiv:2509.23188v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: hierarchical compliance under instruction conflicts (system-user, peer-peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) Localize - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) Align - Surgical Alignment of Instruction Layers (SAIL), which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE</title>
<link>https://arxiv.org/abs/2509.24130</link>
<guid>https://arxiv.org/abs/2509.24130</guid>
<content:encoded><![CDATA[
arXiv:2509.24130v2 Announce Type: replace 
Abstract: The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution</title>
<link>https://arxiv.org/abs/2509.24189</link>
<guid>https://arxiv.org/abs/2509.24189</guid>
<content:encoded><![CDATA[
arXiv:2509.24189v2 Announce Type: replace 
Abstract: Understanding how user preference evolves over time is a fundamental challenge central to modern digital ecosystems, for which Large Language Models (LLMs) are an increasingly prominent and popular approach due to their ability to comprehend the rich semantic context within behavioral data. A common practice is to use LLMs to predict a user's next action by directly generating a ranked list of preferred items. Although effective for short-term prediction, the end-to-end generation paradigm inherently limits personalization. Its opaque decision-making process obscures holistic user profiling and exacerbates popularity bias. To address these limitations, we propose Preference Evolution Tracking (PET), a framework that reframes the task as inferring a dynamic probability distribution over a stable and interpretable lattice of preference clusters. By applying logit-probing and generative classification techniques, PET infers a user's preference as a probability distribution, enabling transparent preference learning. On public benchmarks (Yelp, MovieLens), PET improves ranking quality by up to 40% in NDCG over direct generation baselines. On a large-scale, real-world dataset from a short-video platform, it excels at ranking long-tail contents, significantly outperforming a SOTA production model by 7 times in the NDCG score. Ultimately, PET transforms the user profile model from direct preference list generation to a transparent distributional preference mapping, paving the way for more explainable, fair, and diverse personalization systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation</title>
<link>https://arxiv.org/abs/2510.00829</link>
<guid>https://arxiv.org/abs/2510.00829</guid>
<content:encoded><![CDATA[
arXiv:2510.00829v2 Announce Type: replace 
Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine \textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like idiomatic translation, but its reliability under noisy retrieval contexts remains poorly understood despite this being a common challenge in real-world deployment. To address this gap, we propose a noise synthesis framework and new metrics to evaluate the robustness of REAL-MT systematically. Using this framework, we instantiate REAL-MT with Qwen-series models, including standard LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate their performance on idiomatic translation across high-, medium-, and low-resource language pairs under synthesized noise. Our results show that low-resource language pairs, which rely more heavily on retrieved context, degrade more severely under noise than high-resource ones and often produce nonsensical translations. Although LRMs possess enhanced reasoning capabilities, they show no improvement in error correction and are even more susceptible to noise, tending to rationalize incorrect contexts. We find that this stems from an attention shift away from the source idiom to noisy content, while confidence increases despite declining accuracy, indicating poor calibration. To mitigate these issues, we investigate training-free and fine-tuning strategies, which improve robustness at the cost of performance in clean contexts, revealing a fundamental trade-off. Our findings highlight the limitations of current approaches, underscoring the need for self-verifying integration mechanisms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles</title>
<link>https://arxiv.org/abs/2510.03898</link>
<guid>https://arxiv.org/abs/2510.03898</guid>
<content:encoded><![CDATA[
arXiv:2510.03898v2 Announce Type: replace 
Abstract: Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Human Behavioral Baseline for Collective Governance in Software Projects</title>
<link>https://arxiv.org/abs/2510.08956</link>
<guid>https://arxiv.org/abs/2510.08956</guid>
<content:encoded><![CDATA[
arXiv:2510.08956v2 Announce Type: replace 
Abstract: We study how open source communities describe participation and control through version controlled governance documents. Using a corpus of 710 projects with paired snapshots, we parse text into actors, rules, actions, and objects, then group them and measure change with entropy for evenness, richness for diversity, and Jensen Shannon divergence for drift. Projects define more roles and more actions over time, and these are distributed more evenly, while the composition of rules remains stable. These findings indicate that governance grows by expanding and balancing categories of participation without major shifts in prescriptive force. The analysis provides a reproducible baseline for evaluating whether future AI mediated workflows concentrate or redistribute authority.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis</title>
<link>https://arxiv.org/abs/2510.14128</link>
<guid>https://arxiv.org/abs/2510.14128</guid>
<content:encoded><![CDATA[
arXiv:2510.14128v2 Announce Type: replace 
Abstract: Computational gastronomy increasingly relies on diverse, high-quality recipe datasets to capture regional culinary traditions. Although there are large-scale collections for major languages, Macedonian recipes remain under-represented in digital research. In this work, we present the first systematic effort to construct a Macedonian recipe dataset through web scraping and structured parsing. We address challenges in processing heterogeneous ingredient descriptions, including unit, quantity, and descriptor normalization. An exploratory analysis of ingredient frequency and co-occurrence patterns, using measures such as Pointwise Mutual Information and Lift score, highlights distinctive ingredient combinations that characterize Macedonian cuisine. The resulting dataset contributes a new resource for studying food culture in underrepresented languages and offers insights into the unique patterns of Macedonian culinary tradition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</title>
<link>https://arxiv.org/abs/2510.15501</link>
<guid>https://arxiv.org/abs/2510.15501</guid>
<content:encoded><![CDATA[
arXiv:2510.15501v2 Announce Type: replace 
Abstract: Despite the remarkable advances of Large Language Models (LLMs) across diverse cognitive tasks, the rapid enhancement of these capabilities also introduces emergent deceptive behaviors that may induce severe risks in high-stakes deployments. More critically, the characterization of deception across realistic real-world scenarios remains underexplored. To bridge this gap, we establish DeceptionBench, the first benchmark that systematically evaluates how deceptive tendencies manifest across different societal domains, what their intrinsic behavioral patterns are, and how extrinsic factors affect them. Specifically, on the static count, the benchmark encompasses 150 meticulously designed scenarios in five domains, i.e., Economy, Healthcare, Education, Social Interaction, and Entertainment, with over 1,000 samples, providing sufficient empirical foundations for deception analysis. On the intrinsic dimension, we explore whether models exhibit self-interested egoistic tendencies or sycophantic behaviors that prioritize user appeasement. On the extrinsic dimension, we investigate how contextual factors modulate deceptive outputs under neutral conditions, reward-based incentivization, and coercive pressures. Moreover, we incorporate sustained multi-turn interaction loops to construct a more realistic simulation of real-world feedback dynamics. Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal critical vulnerabilities, particularly amplified deception under reinforcement dynamics, demonstrating that current models lack robust resistance to manipulative contextual cues and the urgent need for advanced safeguards against various deception behaviors. Code and resources are publicly available at https://github.com/Aries-iai/DeceptionBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</title>
<link>https://arxiv.org/abs/2510.15859</link>
<guid>https://arxiv.org/abs/2510.15859</guid>
<content:encoded><![CDATA[
arXiv:2510.15859v2 Announce Type: replace 
Abstract: Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Conceptual-Thought Elicits Daily Conversation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.18434</link>
<guid>https://arxiv.org/abs/2510.18434</guid>
<content:encoded><![CDATA[
arXiv:2510.18434v3 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) is widely applied to enhance the LLM capability in math, coding and reasoning tasks. However, its performance is limited for open-domain tasks, when there are no clearly defined reasoning steps or logical transitions. To mitigate such challenges, we propose a new prompt-based paradigm called Chain of Conceptual Thoughts (CoCT), which suggests the LLM first to produce the tag of concepts, then complete the detailed content following the concept. To encourage this hierarchical way of thinking, we implement the concepts with emotions, strategies and topics. We experiment with this paradigm in daily and emotional support conversations, covering tasks with both in-domain and out-of-domain concept settings. Automatic, human, and LLM-based evaluations reveal that CoCT surpasses several prompt-based baselines such as self-refine, ECoT, SoT and RAG, suggesting a potential solution of LLM prompting paradigm for a wider scope of tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA</title>
<link>https://arxiv.org/abs/2510.19172</link>
<guid>https://arxiv.org/abs/2510.19172</guid>
<content:encoded><![CDATA[
arXiv:2510.19172v2 Announce Type: replace 
Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SelecTKD: Selective Token-Weighted Knowledge Distillation for LLMs</title>
<link>https://arxiv.org/abs/2510.24021</link>
<guid>https://arxiv.org/abs/2510.24021</guid>
<content:encoded><![CDATA[
arXiv:2510.24021v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is a standard route to compress Large Language Models (LLMs) into compact students, yet most pipelines uniformly apply token-wise loss regardless of teacher confidence. This indiscriminate supervision amplifies noisy, high-entropy signals and is especially harmful under large teacher-student capacity gaps. We introduce SelecTKD, a plug-and-play Selective Token-Weighted distillation framework that shifts the focus from "how to measure divergence" to "where to apply learning". At each step, the student proposes tokens that are verified by the teacher through a robust propose-and-verify procedure with two variants: greedy Top-k and non-greedy Spec-k. Accepted tokens receive full loss, while rejected tokens are masked or down-weighted. This objective-agnostic design works with on- and off-policy data, induces an implicit curriculum quantified by Token Acceptance Rate (TAR), and stabilizes optimization. Across instruction following, mathematical reasoning, code generation, and a VLM setting, SelecTKD consistently improves strong baselines and achieves state-of-the-art results for small models without architectural changes or extra reference models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.25117</link>
<guid>https://arxiv.org/abs/2510.25117</guid>
<content:encoded><![CDATA[
arXiv:2510.25117v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities, but their training on massive corpora poses significant risks from memorized sensitive information. To mitigate these issues and align with legal standards, unlearning has emerged as a critical technique to selectively erase specific knowledge from LLMs without compromising their overall performance. This survey provides a systematic review of over 180 papers on LLM unlearning published since 2021. First, it introduces a novel taxonomy that categorizes unlearning methods based on the phase in the LLM pipeline of the intervention. This framework further distinguishes between parameter modification and parameter selection strategies, thus enabling deeper insights and more informed comparative analysis. Second, it offers a multidimensional analysis of evaluation paradigms. For datasets, we compare 18 existing benchmarks from the perspectives of task format, content, and experimental paradigms to offer actionable guidance. For metrics, we move beyond mere enumeration by dividing knowledge memorization metrics into 10 categories to analyze their advantages and applicability, while also reviewing metrics for model utility, robustness, and efficiency. By discussing current challenges and future directions, this survey aims to advance the field of LLM unlearning and the development of secure AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Personality Generation of LLMs at Decoding-time</title>
<link>https://arxiv.org/abs/2511.01891</link>
<guid>https://arxiv.org/abs/2511.01891</guid>
<content:encoded><![CDATA[
arXiv:2511.01891v2 Announce Type: replace 
Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a "free lunch" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinGPT: Open-Source Financial Large Language Models</title>
<link>https://arxiv.org/abs/2306.06031</link>
<guid>https://arxiv.org/abs/2306.06031</guid>
<content:encoded><![CDATA[
arXiv:2306.06031v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.
  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions</title>
<link>https://arxiv.org/abs/2406.08824</link>
<guid>https://arxiv.org/abs/2406.08824</guid>
<content:encoded><![CDATA[
arXiv:2406.08824v2 Announce Type: replace-cross 
Abstract: Members of the Human-Robot Interaction (HRI) and Machine Learning (ML) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interaction, household and workplace tasks, approximating 'common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To assess whether such concerns are well placed in the context of HRI, we evaluate several highly-rated LLMs on discrimination and safety criteria. Our evaluation reveals that LLMs are currently unsafe for people across a diverse range of protected identity characteristics, including, but not limited to, race, gender, disability status, nationality, religion, and their intersections. Concretely, we show that LLMs produce directly discriminatory outcomes- e.g., 'gypsy' and 'mute' people are labeled untrustworthy, but not 'european' or 'able-bodied' people. We find various such examples of direct discrimination on HRI tasks such as facial expression, proxemics, security, rescue, and task assignment. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions-such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. We provide code to reproduce our experiments at https://github.com/rumaisa-azeem/llm-robots-discrimination-safety .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents</title>
<link>https://arxiv.org/abs/2408.14033</link>
<guid>https://arxiv.org/abs/2408.14033</guid>
<content:encoded><![CDATA[
arXiv:2408.14033v3 Announce Type: replace-cross 
Abstract: Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair In-Context Learning via Latent Concept Variables</title>
<link>https://arxiv.org/abs/2411.02671</link>
<guid>https://arxiv.org/abs/2411.02671</guid>
<content:encoded><![CDATA[
arXiv:2411.02671v2 Announce Type: replace-cross 
Abstract: The emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different data types, including tabular data, facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce the correlation between predictive outcomes and sensitive variables, helping promote fairness during latent concept learning. We utilize the learned concept to select demonstrations and obtain fair predictions. The latent concept variables are learned using a smaller internal LLM and generalized to larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2501.14011</link>
<guid>https://arxiv.org/abs/2501.14011</guid>
<content:encoded><![CDATA[
arXiv:2501.14011v3 Announce Type: replace-cross 
Abstract: A taxonomy is a hierarchical graph containing knowledge to provide valuable insights for various web applications. However, the manual construction of taxonomies requires significant human effort. As web content continues to expand at an unprecedented pace, existing taxonomies risk becoming outdated, struggling to incorporate new and emerging information effectively. As a consequence, there is a growing need for dynamic taxonomy expansion to keep them relevant and up-to-date. Existing taxonomy expansion methods often rely on classical word embeddings to represent entities. However, these embeddings fall short of capturing hierarchical polysemy, where an entity's meaning can vary based on its position in the hierarchy and its surrounding context. To address this challenge, we introduce QuanTaxo, a quantum-inspired framework for taxonomy expansion that encodes entities in a Hilbert space and models interference effects between them, yielding richer, context-sensitive representations. Comprehensive experiments on five real-world benchmark datasets show that QuanTaxo significantly outperforms classical embedding models, achieving substantial improvements of 12.3% in accuracy, 11.2% in Mean Reciprocal Rank (MRR), and 6.9% in Wu & Palmer (Wu&amp;P) metrics across nine classical embedding-based baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIP: Perturbation-based Iterative Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2501.15278</link>
<guid>https://arxiv.org/abs/2501.15278</guid>
<content:encoded><![CDATA[
arXiv:2501.15278v3 Announce Type: replace-cross 
Abstract: The rapid increase in the parameter counts of Large Language Models (LLMs), which often reach into the billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To address this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP's ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Euler to AI: Unifying Formulas for Mathematical Constants</title>
<link>https://arxiv.org/abs/2502.17533</link>
<guid>https://arxiv.org/abs/2502.17533</guid>
<content:encoded><![CDATA[
arXiv:2502.17533v3 Announce Type: replace-cross 
Abstract: The constant $\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines Large Language Models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 385 distinct formulas for $\pi$ and prove relations between 360 (94%) of them, of which 166 (43%) can be derived from a single mathematical object - linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine.
  Our method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[
arXiv:2504.06261v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning</title>
<link>https://arxiv.org/abs/2505.16186</link>
<guid>https://arxiv.org/abs/2505.16186</guid>
<content:encoded><![CDATA[
arXiv:2505.16186v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.16826</link>
<guid>https://arxiv.org/abs/2505.16826</guid>
<content:encoded><![CDATA[
arXiv:2505.16826v2 Announce Type: replace-cross 
Abstract: Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</title>
<link>https://arxiv.org/abs/2505.22411</link>
<guid>https://arxiv.org/abs/2505.22411</guid>
<content:encoded><![CDATA[
arXiv:2505.22411v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.04245</link>
<guid>https://arxiv.org/abs/2506.04245</guid>
<content:encoded><![CDATA[
arXiv:2506.04245v3 Announce Type: replace-cross 
Abstract: As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PurpCode: Reasoning for Safer Code Generation</title>
<link>https://arxiv.org/abs/2507.19060</link>
<guid>https://arxiv.org/abs/2507.19060</guid>
<content:encoded><![CDATA[
arXiv:2507.19060v4 Announce Type: replace-cross 
Abstract: We introduce PurpCode, the first post-training recipe for training safe code reasoning models towards generating secure code and defending against malicious cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety and preserves model utility through diverse, multi-objective reward mechanisms. To empower the training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our alignment method decreases the model overrefusal rates in both general and cybersafety-specific scenarios, while preserving model utility in both code generation and common security knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling</title>
<link>https://arxiv.org/abs/2508.02503</link>
<guid>https://arxiv.org/abs/2508.02503</guid>
<content:encoded><![CDATA[
arXiv:2508.02503v2 Announce Type: replace-cross 
Abstract: LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, a framework that enhances any solver-generation pipeline to produce higher-quality solvers from natural-language descriptions of optimization problems. OptiHive uses a single batched generation to produce diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Accounting for the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5% to 92% on the most complex problems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System</title>
<link>https://arxiv.org/abs/2508.06059</link>
<guid>https://arxiv.org/abs/2508.06059</guid>
<content:encoded><![CDATA[
arXiv:2508.06059v2 Announce Type: replace-cross 
Abstract: State-of-the-art (SOTA) fact-checking systems combat misinformation by employing autonomous LLM-based agents to decompose complex claims into smaller sub-claims, verify each sub-claim individually, and aggregate the partial results to produce verdicts with justifications (explanations for the verdicts). The security of these systems is crucial, as compromised fact-checkers can amplify misinformation, but remains largely underexplored. To bridge this gap, this work introduces a novel threat model against such fact-checking systems and presents \textsc{Fact2Fiction}, the first poisoning attack framework targeting SOTA agentic fact-checking systems. Fact2Fiction employs LLMs to mimic the decomposition strategy and exploit system-generated justifications to craft tailored malicious evidences that compromise sub-claim verification. Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\% higher attack success rates than SOTA attacks across various poisoning budgets and exposes security weaknesses in existing fact-checking systems, highlighting the need for defensive countermeasures.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning</title>
<link>https://arxiv.org/abs/2508.08385</link>
<guid>https://arxiv.org/abs/2508.08385</guid>
<content:encoded><![CDATA[
arXiv:2508.08385v2 Announce Type: replace-cross 
Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is that it spends a significant time deciding which node to expand next. While selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity with traditional array-based priority-queues for dense integer keys, the tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node selection is significant, unlike in game tree search, where the cost is negligible compared to the node evaluation (rollouts) because $d$ is inherently limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we propose a bilevel modification to MCTS that runs a best-first search from each selected leaf node with an expansion budget proportional to $d$, which achieves amortized $O(1)$ runtime for node selection, equivalent to the traditional queue-based OPEN list. In addition, we introduce Tree Collapsing, an enhancement that reduces action selection steps and further improves the performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting the Effects of Quantization on LLMs</title>
<link>https://arxiv.org/abs/2508.16785</link>
<guid>https://arxiv.org/abs/2508.16785</guid>
<content:encoded><![CDATA[
arXiv:2508.16785v2 Announce Type: replace-cross 
Abstract: Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Large Language Model Copyright Auditing via Fingerprinting</title>
<link>https://arxiv.org/abs/2508.19843</link>
<guid>https://arxiv.org/abs/2508.19843</guid>
<content:encoded><![CDATA[
arXiv:2508.19843v3 Announce Type: replace-cross 
Abstract: The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that compares the distinctive features (i.e., fingerprint) of LLMs to identify whether an LLM is derived from another, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of the emerging LLM fingerprinting. We introduce a unified framework and taxonomy that structures the field: white-box methods are classified based on their feature source as static, forward-pass, or backward-pass fingerprinting, while black-box methods are distinguished by their query strategy as either untargeted or targeted. Furthermore, we propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon 7 mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent techniques (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensemble Debates with Local Large Language Models for AI Alignment</title>
<link>https://arxiv.org/abs/2509.00091</link>
<guid>https://arxiv.org/abs/2509.00091</guid>
<content:encoded><![CDATA[
arXiv:2509.00091v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. We study whether local open-source ensemble debates can improve alignmentoriented reasoning. Across 150 debates spanning 15 scenarios and five ensemble configurations, ensembles outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13), with the largest gains in reasoning depth (+19.4%) and argument quality (+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human enhancement (+0.80). We provide code, prompts, and a debate data set, providing an accessible and reproducible foundation for ensemble-based alignment evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data</title>
<link>https://arxiv.org/abs/2509.07202</link>
<guid>https://arxiv.org/abs/2509.07202</guid>
<content:encoded><![CDATA[
arXiv:2509.07202v2 Announce Type: replace-cross 
Abstract: Text generating capabilities have undergone a substantial transformation with the introduction of large language models (LLMs). Electroencephalography (EEG)-based text production is still difficult, though, because it requires a lot of data and processing power. This paper introduces a new method that combines the use of the Gemma 2B LLM with a classifier-LLM architecture to incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically lowers the amount of data and compute power needed while achieving performance close to that of cutting-edge methods. Notably, compared to current methodologies, our methodology delivers an overall performance improvement of 10%. The suggested architecture demonstrates the possibility of effective transfer learning for EEG-based text production, remaining strong and functional even in the face of data limits. This work highlights the potential of integrating LLMs with EEG decoding to improve assistive technologies and improve independence and communication for those with severe motor limitations. Our method pushes the limits of present capabilities and opens new paths for research and application in brain-computer interfaces by efficiently using the strengths of pre-trained language models. This makes EEG-based text production more accessible and efficient.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?</title>
<link>https://arxiv.org/abs/2509.16941</link>
<guid>https://arxiv.org/abs/2509.16941</guid>
<content:encoded><![CDATA[
arXiv:2509.16941v2 Announce Type: replace-cross 
Abstract: We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</title>
<link>https://arxiv.org/abs/2509.19002</link>
<guid>https://arxiv.org/abs/2509.19002</guid>
<content:encoded><![CDATA[
arXiv:2509.19002v2 Announce Type: replace-cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2509.23292</link>
<guid>https://arxiv.org/abs/2509.23292</guid>
<content:encoded><![CDATA[
arXiv:2509.23292v2 Announce Type: replace-cross 
Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge</title>
<link>https://arxiv.org/abs/2510.01223</link>
<guid>https://arxiv.org/abs/2510.01223</guid>
<content:encoded><![CDATA[
arXiv:2510.01223v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available at https://github.com/nercode/Work. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation</title>
<link>https://arxiv.org/abs/2510.21341</link>
<guid>https://arxiv.org/abs/2510.21341</guid>
<content:encoded><![CDATA[
arXiv:2510.21341v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often struggle with generating truly innovative ideas, typically defaulting to high-probability, familiar concepts within their training data's "gravity wells." While advanced search-based methods like Tree of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by their reliance on unprincipled, inconsistent self-evaluation heuristics to guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel framework that reframes creative generation as a principled, guided exploration of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo Tree Search (MCTS) governed by a hierarchical guidance system. For long-range direction, a "semantic compass" vector, formulated via orthogonal projection, steers the search towards relevant novelty. For local, step-by-step decisions, a landscape-aware value function replaces flawed self-evaluation with an explicit reward structure that balances intrinsic coherence, extrinsic novelty, and narrative progress. Extensive experiments demonstrate that Magellan significantly outperforms strong baselines, including ReAct and ToT, in generating scientific ideas with superior plausibility and innovation. Our work shows that for creative discovery, a principled, guided search is more effective than unconstrained agency, paving the way for LLMs to become more capable partners in innovation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surface Reading LLMs: Synthetic Text and its Styles</title>
<link>https://arxiv.org/abs/2510.22162</link>
<guid>https://arxiv.org/abs/2510.22162</guid>
<content:encoded><![CDATA[
arXiv:2510.22162v3 Announce Type: replace-cross 
Abstract: Despite a potential plateau in ML advancement, the societal impact of large language models lies not in approaching superintelligence but in generating text surfaces indistinguishable from human writing. While Critical AI Studies provides essential material and socio-technical critique, it risks overlooking how LLMs phenomenologically reshape meaning-making. This paper proposes a semiotics of "surface integrity" as attending to the immediate plane where LLMs inscribe themselves into human communication. I distinguish three knowledge interests in ML research (epistemology, epist\=em\=e, and epistemics) and argue for integrating surface-level stylistic analysis alongside depth-oriented critique. Through two case studies examining stylistic markers of synthetic text, I argue how attending to style as a semiotic phenomenon reveals LLMs as cultural machines that transform the conditions of meaning emergence and circulation in contemporary discourse, independent of questions about machine consciousness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty and Impact of Economics Papers</title>
<link>https://arxiv.org/abs/2511.01211</link>
<guid>https://arxiv.org/abs/2511.01211</guid>
<content:encoded><![CDATA[
arXiv:2511.01211v3 Announce Type: replace-cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Cycle Detection in Agentic Applications</title>
<link>https://arxiv.org/abs/2511.10650</link>
<guid>https://arxiv.org/abs/2511.10650</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic applications, large language models, cycle detection, semantic analysis, stock market application

<br /><br />Summary:  
This paper addresses the issue of hidden execution cycles in agentic applications powered by Large Language Models (LLMs), where non-deterministic behaviors lead to resource waste without explicit errors being raised. Traditional observability platforms fail to detect these inefficiencies, prompting the need for a novel detection framework. The authors propose an unsupervised cycle detection framework that integrates structural and semantic analysis. The framework starts with a computationally efficient temporal call stack analysis to detect explicit loops within execution trajectories. Next, it employs semantic similarity analysis to identify subtle cycles caused by redundant content generation that structural methods alone miss. Evaluation is performed on 1575 trajectories derived from a LangGraph-based stock market application, demonstrating that the hybrid approach achieves a strong F1 score of 0.72 with precision at 0.62 and recall at 0.86. This result significantly outperforms methods relying solely on structural analysis (F1: 0.08) or semantic methods (F1: 0.28). Despite promising outcomes, the authors acknowledge considerable room for improvement and suggest further research is needed to optimize the framework and overcome current limitations. <div>
arXiv:2511.10650v1 Announce Type: new 
Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs</title>
<link>https://arxiv.org/abs/2511.10651</link>
<guid>https://arxiv.org/abs/2511.10651</guid>
<content:encoded><![CDATA[
<div> Keywords: simulation deduction, large language models, multi-round interaction, report generation, performance evaluation<br /><br />Summary:<br /><br />This paper addresses the challenge of generating high-quality, well-structured analysis reports in the context of simulation deduction for modern warfare. First, it highlights the significance of data analysis and performance evaluation to help military personnel assess strategies and operational plans effectively. Second, it notes the limitations of traditional manual analysis, particularly its time consumption and propensity for human error. Third, the authors propose leveraging large language models (LLMs) with strong analytical and inferencing capabilities to improve both efficiency and accuracy. Fourth, they introduce a methodology that decomposes complex tasks into sub-tasks, utilizing specifically designed system and user prompts for each. Multi-round interactions with the LLM incorporate self-checking and reflection mechanisms to enable structured data extraction and multi-step analysis. Fifth, custom tools are developed to generate visual figures and compute metrics, enhancing report quality. Additionally, multiple adaptable report templates are created to suit different applications and input data types. Finally, extensive evaluations demonstrate that the reports produced with this method outperform baseline approaches, yielding higher quality and better scoring results. <div>
arXiv:2511.10651v1 Announce Type: new 
Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI</title>
<link>https://arxiv.org/abs/2511.10652</link>
<guid>https://arxiv.org/abs/2511.10652</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval-augmented generation, episodic memory, biographical dialogue, Van Gogh

<br /><br />Summary:  
This paper addresses the challenge of creating dialogue systems that embody historical characters, managing the trade-off between shallow responses from simple retrieval-augmented generation (RAG) and the latency of multi-stage reflection methods. The authors propose an architecture combining offline data augmentation with efficient parallel retrieval from a structured episodic memory. Biographical data is transformed into 1,774 enriched first-person memories enhanced with affective-semantic metadata. The system employs a two-stage retrieval process that generates prompts in just 0.52 seconds, offering a balance of response depth and speed. Evaluation using a large language model as judge and RAGAs metrics demonstrates that the proposed method performs on par with traditional RAG approaches when using GPT-4, while significantly outperforming on smaller models such as GPT-3.5 and GPT-3. This makes the system particularly suitable for resource-constrained environments. Additionally, the structured memory supports innovative visualization tools, including spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, extending its utility beyond dialogue to research and educational contexts. Van Gogh is used as a test case, but the design is generalizable to any historical figure with extensive textual records. The system offers a practical framework for accurate and efficient educational, museum, and research applications involving historical personalities. <div>
arXiv:2511.10652v1 Announce Type: new 
Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum Transformer for Language Generation</title>
<link>https://arxiv.org/abs/2511.10653</link>
<guid>https://arxiv.org/abs/2511.10653</guid>
<content:encoded><![CDATA[
<div> Quantum computing, large language models, variational quantum circuits, natural language generation, hybrid quantum-classical<br /><br />Summary:<br /><br />This paper introduces HyQuT, the first hybrid quantum-classical large language model specifically designed for natural language generation. It marks a pioneering effort in integrating quantum computing with large-scale generative language tasks, particularly coherent and context-aware dialogue systems. The architecture incorporates variational quantum circuits (VQCs) within the Transformer framework, demonstrated at two sizes: 8 million and 150 million parameters. Remarkably, the model requires only a minimal quantum resource configuration—10 qubits and 80 quantum gates—to effectively replace approximately 10% of the parameters in the 150 million-parameter model. Experimental results confirm that this hybrid quantum-classical approach achieves convergence stability and generation quality on par with fully classical models. Thus, HyQuT provides an early but important proof-of-concept that quantum circuits can be feasibly embedded in large-scale language models without sacrificing performance. This work opens new pathways for exploring how quantum computing can enhance or complement classical machine learning, particularly in complex natural language processing applications. The findings suggest that modest quantum resources can meaningfully contribute to advanced deep learning architectures, moving quantum computing closer to practical large-scale AI use cases. <div>
arXiv:2511.10653v1 Announce Type: new 
Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Characterization of Temporal Constraint Processing in LLMs</title>
<link>https://arxiv.org/abs/2511.10654</link>
<guid>https://arxiv.org/abs/2511.10654</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal constraints, large language models, deadline detection, prompt brittleness, fine-tuning  

<br /><br />Summary:  
This study evaluates the ability of eight large language models (LLMs) ranging from 2.8B to 8B parameters to process temporal constraints in real-time, agentic decision-making contexts. It reveals a bimodal performance pattern where models either perform very well (95% accuracy) or poorly (50% accuracy), indicating inconsistent reliability. The research highlights severe sensitivity to prompt formatting, causing accuracy swings between 30 to 60 percentage points, as well as a systematic action bias with failing models producing 100% false positive rates in deadline detection tasks. Notably, model size within the tested range shows no clear correlation with capability; some smaller models outperformed larger ones. Fine-tuning with 200 synthetic examples offers modest improvements of 12-37 percentage points for partially capable models but does not solve the fundamental issues. The findings emphasize that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language alone, even when fine-tuned. Instead, effective temporal reasoning demands architectural features such as continuous temporal state representation, explicit constraint verification separate from linguistic pattern matching, and systematic compositional reasoning over temporal relations. The paper concludes that current autoregressive LLMs lack these essential mechanisms, posing unacceptable risks if deployed in time-critical applications without incorporating hybrid symbolic reasoning modules. <div>
arXiv:2511.10654v1 Announce Type: new 
Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment</title>
<link>https://arxiv.org/abs/2511.10655</link>
<guid>https://arxiv.org/abs/2511.10655</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral Neuro-Symbolic Reasoning, transformer-based node merging, sentence-level entailment validation, external knowledge graphs, scalable reasoning<br /><br />Summary:<br /><br />This report presents key enhancements to the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by integrating three semantically grounded preprocessing steps. First, it leverages transformer-based node merging using contextual embeddings such as Sentence-BERT and SimCSE to efficiently reduce redundancy within reasoning graphs. Second, it employs sentence-level entailment validation through pretrained Natural Language Inference (NLI) classifiers like RoBERTa and DeBERTa to improve the quality of graph edges. Third, the framework incorporates alignment with external knowledge graphs such as ConceptNet and Wikidata to supplement missing contextual information. These improvements collectively boost graph fidelity without modifying the underlying spectral reasoning pipeline. Experimental validation on benchmarks including ProofWriter, EntailmentBank, and CLUTRR demonstrates consistent accuracy improvements of up to 3.8%, better generalization to adversarial inputs, and a reduction in inference noise. A notable contribution is performing semantic and symbolic graph enhancements entirely upstream of spectral inference, avoiding the computational cost of quadratic attention mechanisms. Overall, these modular, semantically informed preprocessing strategies yield a robust, interpretable, and scalable reasoning system well-suited for real-world and open-domain applications. <div>
arXiv:2511.10655v1 Announce Type: new 
Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2511.10656</link>
<guid>https://arxiv.org/abs/2511.10656</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-objective alignment, preference adapter, prompt-specific weights, reward models<br /><br />Summary:  
This paper addresses the challenge of aligning Large Language Models (LLMs) with multiple human preferences across various objectives simultaneously. Existing approaches rely on manually set preference weights, which are difficult for users to specify accurately and often result in inefficient training due to exploring irrelevant preference combinations. To overcome these issues, the authors propose PRO (PReference Orchestrator), a novel framework featuring a lightweight preference adapter that automatically infers prompt-specific preference weights during training and deployment. The adapter learns appropriate weights by training on normalized reward scores from multiple reward models evaluating preferred responses, capturing effective balance across objectives for each prompt. The paper also provides theoretical analysis demonstrating that the prompt-aware preference mechanism outperforms fixed preference weights in multi-objective alignment scenarios. Extensive experiments across different tasks confirm the effectiveness of PRO compared to existing multi-objective alignment methods, showcasing improved performance and training efficiency. This approach simplifies user involvement and enhances adaptability of LLMs to diverse preferences without manual weight tuning. <div>
arXiv:2511.10656v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Representation Learning via Self-supervision</title>
<link>https://arxiv.org/abs/2511.10657</link>
<guid>https://arxiv.org/abs/2511.10657</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive learning, patent embeddings, section-based augmentation, intra-document views, prior-art retrieval<br /><br />Summary: This paper introduces a novel contrastive learning framework designed specifically for patent embeddings by utilizing multiple views derived from different sections within the same patent document. The authors identify a patent-specific limitation in SimCSE-style dropout augmentation, which tends to produce embeddings that are overly uniform and lack semantic cohesion. To overcome this issue, they propose section-based augmentation, where distinct sections such as the abstract, claims, and background are treated as complementary perspectives. This approach incorporates natural semantic and structural diversity, reducing over-dispersion and preserving both the global structure and local continuity of embeddings. Evaluated on large-scale benchmarks, the fully self-supervised method achieves performance comparable to or better than baselines supervised by citation and IPC data in tasks like prior-art retrieval and classification, without relying on potentially incomplete or brittle annotations. Additionally, the analysis reveals that different patent sections serve specialized purposes: claims and summaries particularly enhance retrieval tasks, whereas background sections improve classification accuracy. These findings underscore the advantage of leveraging the intrinsic discourse structure of patents, demonstrating that exploiting intra-document views is a scalable and generalizable strategy for effective patent understanding. <div>
arXiv:2511.10657v1 Announce Type: new 
Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages</title>
<link>https://arxiv.org/abs/2511.10658</link>
<guid>https://arxiv.org/abs/2511.10658</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, clinical reports, structured information extraction, prompting strategies, multi-institutional study<br /><br />Summary:<br /><br />This study evaluates 15 open-weight large language models (LLMs) on extracting structured information from pathology and radiology reports. The evaluation covers six clinical use cases: colorectal liver metastases, liver tumors, neurodegenerative diseases, soft-tissue tumors, melanomas, and sarcomas across three different institutes in the Netherlands, UK, and Czech Republic. Both general-purpose and medical-specialized LLMs of varying sizes were tested. Six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance metrics appropriate to each task were used, and results were analyzed using consensus rank aggregation and linear mixed-effects models. Top-ranked models achieved macro-average scores close to the level of inter-rater agreement across all tasks. Interestingly, small-to-medium general-purpose models performed on par with large models, while tiny and specialized models lagged behind. Prompt graph and few-shot prompting improved performance by approximately 13%. The study found that task-specific factors such as complexity and annotation variability influenced model performance more than model size or prompting technique. Overall, the findings demonstrate that open-weight LLMs offer a scalable and effective tool for extracting structured clinical data across various diseases, languages, and institutions. <div>
arXiv:2511.10658v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information Extraction From Fiscal Documents Using LLMs</title>
<link>https://arxiv.org/abs/2511.10659</link>
<guid>https://arxiv.org/abs/2511.10659</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fiscal documents, hierarchical tables, data extraction, validation  

<br /><br />Summary:  
This paper explores the underutilized potential of Large Language Models (LLMs) to process complex, hierarchical tabular data found in multi-page government fiscal documents. The authors introduce a novel multi-stage pipeline designed specifically for extracting structured data from large PDF documents, exemplified by annual fiscal reports from the State of Karnataka, India, totaling over 200 pages. A key innovation is leveraging the inherent hierarchical structure of fiscal tables—consisting of subtotals and totals at multiple levels—to perform multi-level validation checks that improve extraction accuracy. This approach addresses a notable challenge of traditional OCR methods, which struggle to verify the accuracy of numerical data extraction. The pipeline incorporates domain knowledge, sequential contextual understanding, and algorithmic validation, allowing LLMs not only to interpret table data but also to recognize and utilize document-specific structural hierarchies. The result is a robust, scalable method that effectively converts complex PDF fiscal disclosures into clean, research-ready databases. The implementation demonstrates promise for broader application, especially in developing countries, where digitizing and structuring government financial data can support transparency and research. Overall, the work highlights the expanding capabilities of LLMs in handling structured data extraction tasks beyond text comprehension. <div>
arXiv:2511.10659v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Steering for Lossless Text Compression via Weighted Product of Experts</title>
<link>https://arxiv.org/abs/2511.10660</link>
<guid>https://arxiv.org/abs/2511.10660</guid>
<content:encoded><![CDATA[
<div> Keywords: lossless compression, neural compressors, universal compressor, weighted product of experts, text compression<br /><br />Summary: This paper addresses the challenge of improving lossless text compression by combining traditional universal compressors with neural language models. Traditional compressors like gzip are fast and general but typically yield suboptimal compression rates compared to neural compressors, which leverage extensive training data to better model distributions. However, neural compressors often fail to generalize well to unseen or out-of-distribution data, limiting their practical use. To overcome this, the authors propose a novel test-time steering method using a weighted product of experts (wPoE) framework that adaptively fuses a universal compression model with a pretrained neural language model during inference. This approach ensures the combined compression rate is at least as good as the best performing individual model without the need for any additional fine-tuning. The method is compatible with any autoregressive language model, making it broadly applicable. Extensive experiments demonstrate that their framework consistently improves text compression performance across diverse datasets and distribution shifts. Overall, the proposed wPoE-based test-time steering offers a practical, versatile, and effective solution for enhancing lossless text compression by leveraging the complementary strengths of universal and neural compressors. <div>
arXiv:2511.10660v1 Announce Type: new 
Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Evaluation of Large Language Model Behavior</title>
<link>https://arxiv.org/abs/2511.10661</link>
<guid>https://arxiv.org/abs/2511.10661</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation, Bayesian approach, uncertainty quantification, adversarial inputs<br /><br />Summary: This paper addresses the need for rigorous evaluation methods of text generation systems based on large language models (LLMs), focusing on issues such as harmful output generation and sensitivity to adversarial prompts. Traditional evaluation methods typically use curated benchmark prompt sets with binary outcomes (e.g., harmful vs. non-harmful) and aggregate these results without considering statistical uncertainty. The authors propose a Bayesian framework to quantify uncertainty in binary evaluation metrics, which accounts for the probabilistic nature of LLM-generated text. Two case studies demonstrate the approach: first, measuring refusal rates when LLMs respond to adversarially designed harmful prompts; second, assessing pairwise preferences between two LLMs on open-ended interactive dialogue tasks. The Bayesian method offers useful insights by providing credible intervals and uncertainty estimates, improving the interpretability of LLM evaluation results. This work highlights the importance of incorporating uncertainty quantification in LLM behavior assessment to produce more reliable and informative evaluation outcomes for developers and researchers. <div>
arXiv:2511.10661v1 Announce Type: new 
Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish</title>
<link>https://arxiv.org/abs/2511.10664</link>
<guid>https://arxiv.org/abs/2511.10664</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cross-lingual benchmark, Cantonese, Japanese, Turkish

<br /><br />Summary:  
This paper evaluates seven state-of-the-art large language models (LLMs)—GPT-4o, GPT-4, Claude 3.5 Sonnet, LLaMA 3.1, Mistral Large 2, LLaMA-2 Chat 13B, and Mistral 7B Instruct—on a novel cross-lingual benchmark including Cantonese, Japanese, and Turkish. The benchmark covers four tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. The evaluation combines human assessments of fluency, factual accuracy, and cultural appropriateness with automated metrics such as BLEU and ROUGE. Results indicate that the largest proprietary models (GPT-4o, GPT-4, Claude 3.5) outperform others across tasks and languages, with GPT-4o showing strong multilingual and cross-lingual capabilities. Claude 3.5 Sonnet demonstrates competitive knowledge and reasoning accuracy. Despite this, all models exhibit challenges addressing language-specific issues, including Turkish’s complex agglutinative morphology and Cantonese colloquialisms. Smaller open-source models lag behind markedly in fluency and accuracy, underscoring disparities in resource availability. The paper provides extensive quantitative results and qualitative error analyses, highlighting the need for improved cultural and linguistic generalization in LLMs. Finally, the benchmark and evaluation data have been publicly released to support reproducibility and future research in this area. <div>
arXiv:2511.10664v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title>
<link>https://arxiv.org/abs/2511.10665</link>
<guid>https://arxiv.org/abs/2511.10665</guid>
<content:encoded><![CDATA[
<div> Keywords: guard models, semantic robustness, paraphrase consistency, model calibration, skew-aware aggregation<br /><br />Summary:<br />Guard models play a vital role in ensuring the safety of large language models (LLMs), but their vulnerability to superficial linguistic changes undermines their reliability. This study reveals that even meaning-preserving paraphrases can cause significant fluctuations in guard model safety scores, indicating a lack of true semantic understanding. To tackle this, the authors introduce a self-supervised training framework designed to enhance semantic robustness by leveraging paraphrase sets. A key innovation is the use of a novel skew-aware aggregation strategy to compute robust targets for enforcing consistent predictions, as conventional methods like mean and median were found to potentially worsen safety outcomes. The approach was tested on six open-source guard models, resulting in a roughly 58% reduction in semantic variability across paraphrases and an average improvement of about 2.5% in benchmark accuracy. Additionally, the method generalizes effectively to previously unseen stylistic variations. An important finding is the discovered bidirectional relationship between model calibration and consistency; robustness training improved calibration by up to 40%. Overall, this work underscores the significance of prioritizing semantic consistency as a core training objective and offers a practical, scalable approach for developing more reliable and robust guard models to enhance LLM safety. <div>
arXiv:2511.10665v1 Announce Type: new 
Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLM Understanding via Structured Tabular Decision Simulations</title>
<link>https://arxiv.org/abs/2511.10667</link>
<guid>https://arxiv.org/abs/2511.10667</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Understanding, Structured Tabular Decision Simulations, Decision Factors, Evaluation Frameworks  

<br /><br />Summary:  
Large language models (LLMs) often deliver high predictive accuracy but this does not guarantee genuine understanding comparable to human expertise. True understanding in LLMs involves making consistent and well-founded decisions across multiple instances and diverse domains by relying on domain-relevant decision factors. The study introduces Structured Tabular Decision Simulations (STaDS), a new benchmark suite designed to evaluate LLMs as if they were professionals undergoing structured decision-making exams. STaDS defines understanding as the ability to identify and use the correct decision factors, which directly influence outcomes within a domain. This framework assesses understanding through three components: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) correct reliance on decision factors. Testing 9 state-of-the-art LLMs over 15 diverse decision-making settings revealed that most models struggle to maintain consistently high accuracy across domains. Moreover, models sometimes produce accurate answers without globally faithful reasoning, showing frequent mismatches between their stated rationales and the underlying decision factors driving their predictions. These insights emphasize the necessity for evaluation protocols that measure global-level understanding and suggest moving beyond accuracy-based metrics to develop frameworks that genuinely enhance LLMs’ decision-making understanding. <div>
arXiv:2511.10667v1 Announce Type: new 
Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI</title>
<link>https://arxiv.org/abs/2511.10669</link>
<guid>https://arxiv.org/abs/2511.10669</guid>
<content:encoded><![CDATA[
<div> Keywords: cochlear implants, sensorineural hearing loss, deep transfer learning, language development prediction, machine learning<br /><br />Summary:<br /><br />This study investigates the prediction of spoken language outcomes in children with severe-to-profound bilateral sensorineural hearing loss (SNHL) who receive cochlear implants (CI). Traditional predictors such as age at implantation and residual hearing fail to reliably forecast language development variability among individuals. Researchers compared traditional machine learning (ML) techniques with deep transfer learning (DTL) algorithms that incorporate brain neuroanatomic features to classify children as high or low improvers in post-CI spoken language development. The study involved 278 implanted children from three centers. The DTL models utilized a bilinear attention-based fusion strategy, achieving an accuracy of 92.39%, sensitivity of 91.22%, specificity of 93.56%, and an area under the curve (AUC) of 0.977, significantly outperforming traditional ML models on all metrics. The superior performance of DTL models is attributed to their ability to directly capture discriminative and task-specific information through representation learning. These findings demonstrate the feasibility of using a single DTL prediction model to support language outcome predictions across CI programs globally, offering a promising tool to improve clinical decision-making and individualized intervention planning for children undergoing cochlear implantation. <div>
arXiv:2511.10669v1 Announce Type: new 
Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment</title>
<link>https://arxiv.org/abs/2511.10670</link>
<guid>https://arxiv.org/abs/2511.10670</guid>
<content:encoded><![CDATA[
<div> Code-switching, speech translation, mixture of experts, large language models, multi-stage training<br /><br />Summary:<br /><br />This paper addresses the challenges of code-switching (CS) speech translation, which involves translating speech containing multiple languages into a single target language. The difficulties stem from complex semantic modeling and a lack of CS training data. To overcome these, the authors propose enhancing large language models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert focuses on the semantic subspace of a particular language for fine-grained feature modeling. They introduce a multi-stage training approach leveraging readily available monolingual automatic speech recognition (ASR) and monolingual speech translation (ST) datasets to improve speech-text alignment and translation ability. The training uses a combination of language-specific loss and intra-group load balancing loss, guiding the MoE projector to allocate tokens efficiently among experts both across and within groups. To handle data distribution shifts across training stages and improve adaptation to code-switching scenarios, a transition loss is applied to smooth transitions between datasets. Experiments on standard datasets demonstrate that the method effectively improves CS speech translation and generalizes well across different settings. This work provides a novel and efficient solution to semantic modeling and data scarcity problems in code-switching speech translation. <div>
arXiv:2511.10670v1 Announce Type: new 
Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency</title>
<link>https://arxiv.org/abs/2511.10671</link>
<guid>https://arxiv.org/abs/2511.10671</guid>
<content:encoded><![CDATA[
<div> Visual Hallucination, Multimodal Large Language Models, Grounded Visual Factualization, Factual Consistency Loss, Data Augmentation<br /><br />Summary:  
This paper addresses the problem of visual hallucination in Multimodal Large Language Models (MLLMs), where models generate details that are inconsistent with the actual image content, reducing their reliability. To tackle this, the authors propose Grounded Visual Factualization (GVF) Finetuning, a novel method designed to systematically improve visual factual consistency in MLLMs. GVF Finetuning integrates explicit factual information through three main strategies: (1) Factual Anchor Data Augmentation, which enhances training data by including structured factual anchors and counter-factual prompts to guide the model; (2) Fact-Aware Instruction Tuning, where explicit factual cues are embedded into the training instructions to strengthen the model’s grounding; and (3) A Factual Consistency Loss function that specifically penalizes the model for generating factually inaccurate information. The method was tested on LLaVA-1.5-13B and demonstrated significant improvement over standard fine-tuning methods on the VHTest benchmark in both open-ended and yes/no question formats, indicating better visual factual accuracy. Additionally, GVF Finetuning maintained or slightly enhanced performance on general multimodal benchmarks such as MME and POPE, showing that it reduces hallucinations without sacrificing the model’s broader understanding and reasoning capabilities. <div>
arXiv:2511.10671v1 Announce Type: new 
Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models in materials science and the need for open-source approaches</title>
<link>https://arxiv.org/abs/2511.10673</link>
<guid>https://arxiv.org/abs/2511.10673</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, materials science, predictive modelling, multi-agent systems, open-source AI<br /><br />Summary:<br /><br />This review discusses the transformative impact of large language models (LLMs) on materials science, focusing on their application across the materials discovery pipeline. First, LLMs have shown great capability in mining scientific literature by extracting critical information such as synthesis conditions from vast text data, enabling more efficient knowledge retrieval. Second, they contribute to predictive modelling by learning complex structure-property relationships, which advances the understanding and design of new materials. Third, LLMs facilitate multi-agent experimental systems by coordinating agentic frameworks that integrate computational tools with laboratory automation, thereby accelerating experimental workflows. The review also emphasizes that despite much progress relying on closed-source commercial LLMs, open-source alternatives are now achieving comparable performance. These open-source models provide advantages in transparency, reproducibility, cost-effectiveness, and data privacy. Finally, as open-source models continue to improve, the authors advocate for their broader adoption to develop accessible, flexible, and community-driven AI platforms that can democratize and enhance scientific discovery in materials science. <div>
arXiv:2511.10673v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.10674</link>
<guid>https://arxiv.org/abs/2511.10674</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.10674v1

Keywords: Large Language Models, text-to-SQL, continual learning, human feedback, structured memory<br /><br />Summary:<br /><br />1. This paper addresses the challenge that Large Language Models (LLMs) face when generating SQL queries from natural language, particularly due to database-specific schemas and tacit domain knowledge.  
2. The authors propose a continual learning framework where the model improves through natural language feedback provided by humans, refining queries iteratively.  
3. Knowledge gained from feedback is distilled and stored in structured memory, allowing the agent to reuse this information in future tasks, enhancing learning efficiency.  
4. Various agent architectures are explored with differences in how they capture and retrieve past experiences, focusing on memory-augmentation.  
5. Experimental results on the BIRD benchmark’s development set demonstrate that memory-augmented agents, especially the Procedural Agent, significantly improve SQL execution accuracy and reduce errors by leveraging human-in-the-loop feedback.  
6. The study highlights how converting tacit human expertise into reusable knowledge enables more adaptive and domain-aware text-to-SQL systems that continuously improve through human interaction, paving the way for future advancements in this area. <div>
arXiv:2511.10674v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification</title>
<link>https://arxiv.org/abs/2511.10675</link>
<guid>https://arxiv.org/abs/2511.10675</guid>
<content:encoded><![CDATA[
<div> In-context learning, Demonstration selection, Label distribution, Small language model, Text classification<br /><br />Summary:<br /><br />1. This paper addresses the challenge of selecting effective in-context demonstrations for text classification tasks using large language models (LLMs), highlighting that existing methods focus mainly on semantic similarity and neglect label distribution alignment.<br /><br />2. The authors propose a novel two-stage demonstration selection approach called TopK + Label Distribution Divergence (L2D). This method employs a fine-tuned BERT-like small language model (SLM) to estimate label distributions for both test inputs and candidate demonstrations.<br /><br />3. By calculating the divergence between these label distributions, L2D selects demonstrations that are not only semantically close but also aligned in label distribution with the test inputs, aiming to enhance LLM performance in in-context learning.<br /><br />4. Extensive experiments were conducted across seven different text classification benchmarks, showing that L2D consistently outperforms previously established demonstration selection strategies.<br /><br />5. Further analysis demonstrates a positive correlation between the performance of LLMs in in-context learning and the accuracy of the small language models used to estimate label distributions, suggesting that SLM quality plays a crucial role in the effectiveness of demonstration selection. <div>
arXiv:2511.10675v1 Announce Type: new 
Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2511.10676</link>
<guid>https://arxiv.org/abs/2511.10676</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Large Language Models, Expert Prediction, Pre-attention, Lightweight Prefetching  

<br /><br />Summary:  
This paper addresses the challenge of improving expert prefetching in Mixture-of-Experts (MoE) Large Language Models (LLMs), which activate only a subset of experts to scale models efficiently while keeping inference costs low. Traditional expert prediction methods rely on activations from the previous layer for expert selection, leading to lower accuracy and leaving the first layer unoptimized. Moreover, approaches that use complex layers or separate networks for prediction add substantial computational overhead. The authors propose a novel pre-attention expert prediction technique that leverages activations before the attention block within the same layer, using two simple linear functions and a ranking-aware loss for accurate expert ranking prediction. This method exploits the ranking-preserving nature of certain LLM functions, enabling lightweight and precise expert prefetching, including at the first layer. Experimental results demonstrate that this approach achieves significantly higher prediction accuracy—93.03% on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE—representing about a 15% absolute improvement over state-of-the-art methods. The proposed pre-attention expert routers thus offer an effective and computationally efficient solution for enhancing MoE model inference speed and accuracy. <div>
arXiv:2511.10676v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI</title>
<link>https://arxiv.org/abs/2511.10684</link>
<guid>https://arxiv.org/abs/2511.10684</guid>
<content:encoded><![CDATA[
<div> Keywords: Life Cycle Assessment, climate change, large language models, environmental impact, SpiderGen  

<br /><br />Summary:  
The paper addresses the critical issue of climate change driven by greenhouse gas emissions, with a focus on consumer products' lifecycle emissions. It emphasizes the importance of Life Cycle Assessments (LCAs) which detail the production, use, and disposal processes of products to estimate environmental impact. The authors introduce SpiderGen, a workflow that utilizes large language models (LLMs) to merge traditional LCA taxonomy and methodology with advanced reasoning and world knowledge. SpiderGen produces procedural information necessary for conducting LCAs. The system's output is evaluated against real-world LCA documents, achieving an F1-Score of 62% over 10 samples, indicating mostly accurate or minor errors. Errors are mainly due to variability in process detail and differing scope in auxiliary processes included. SpiderGen surpasses baseline methods such as chain-of-thought and one-shot prompting in performance. A notable advantage of SpiderGen is its cost and time efficiency, generating LCA data in under 10 minutes for less than $1, compared to traditional LCAs costing up to $25,000 and requiring up to 21-person days. This demonstrates significant potential for reducing effort and expense in assessing carbon footprints of consumer goods. <div>
arXiv:2511.10684v1 Announce Type: new 
Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A methodological analysis of prompt perturbations and their effect on attack success rates</title>
<link>https://arxiv.org/abs/2511.10686</link>
<guid>https://arxiv.org/abs/2511.10686</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment methods, prompt attacks, Attack Success Rate, statistical analysis<br /><br />Summary:<br /><br />1. The study investigates the impact of different alignment methods on how Large Language Models (LLMs) respond to prompt attacks aimed at eliciting inappropriate content. <br /><br />2. The authors focus on three primary alignment techniques used in open-source models: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF).<br /><br />3. A systematic and statistical analysis is conducted to measure how sensitive the Attack Success Rate (ASR) is when small modifications are introduced to malicious prompts.<br /><br />4. Results reveal that even slight changes in prompts can significantly influence the ASR, making models more or less vulnerable depending on the alignment method.<br /><br />5. The study highlights that relying solely on existing attack benchmarks is insufficient for uncovering all vulnerabilities, thus emphasizing the need for more comprehensive and statistically-based evaluations of model security across different alignment strategies. <div>
arXiv:2511.10686v1 Announce Type: new 
Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling and Predicting Multi-Turn Answer Instability in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10688</link>
<guid>https://arxiv.org/abs/2511.10688</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, robustness, multi-turn prompts, Markov chains, linear probes  

<br /><br />Summary: This paper investigates the robustness of large language models (LLMs) in multi-turn interactive settings by applying simple follow-up prompts to observe how model answers change over consecutive turns. The authors find that repeated triggering of models with prompts such as "Think again" causes a significant decline in accuracy, quantifying approximately a 10% drop for Gemini 1.5 Flash across nine turns and a 7.5% drop for Claude 3.5 Haiku when combined with semantically reworded questions. To better understand accuracy dynamics over turns, the study models these changes using Markov chains, which effectively predict accuracy probabilities and allow estimation of the stationary (long-run) accuracy. This stationary accuracy is found to be roughly 8% lower than the initial accuracy for Gemini 1.5 Flash, indicating notable fragility in model performance under repeated questioning. Furthermore, the research explores the model’s hidden states and demonstrates that linear probes can predict future answer changes, suggesting potential for early intervention. The authors propose stationary accuracy as a principled robustness metric tailored for interactive LLM applications. They emphasize that addressing the exposed instability and answer volatility is crucial for reliable deployment of LLMs in high-stakes and conversational environments where consistent reasoning beyond initial correctness is imperative. <div>
arXiv:2511.10688v1 Announce Type: new 
Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data</title>
<link>https://arxiv.org/abs/2511.10689</link>
<guid>https://arxiv.org/abs/2511.10689</guid>
<content:encoded><![CDATA[
<div> Keywords: recursive prompting, gender bias, synthetic dataset generation, mitigation strategies, semantic similarity<br /><br />Summary: This study explores the dynamics of gender bias in large language models during recursive text generation across three generations. The research uses three evaluation frameworks—rule-based pattern matching, embedding-based semantic similarity, and downstream task performance—to analyze bias evolution. Experiments were conducted with three initial bias levels of 0.1, 0.3, and 0.6, revealing that bias does not simply amplify monotonically but instead moves toward an equilibrium reflecting the model’s inherent bias. Specifically, low initial bias tends to increase by about 36%, while high initial bias decreases by roughly 26% over generations. The study also evaluates four mitigation methods, highlighting contrastive augmentation, which creates gender-swapped data variants. This approach significantly reduces downstream task bias by 98.8% for low initial bias scenarios and achieves an average reduction of 91% overall. Interestingly, contrastive augmentation yields higher bias scores when measured by embedding-based semantic similarity metrics, indicating a disconnect between these metrics and actual behavioral fairness outcomes. These findings emphasize the necessity for multidimensional evaluation frameworks in responsible synthetic data generation to capture nuanced bias behavior and ensure effective fairness mitigation strategies in language models. <div>
arXiv:2511.10689v1 Announce Type: new 
Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games</title>
<link>https://arxiv.org/abs/2511.10690</link>
<guid>https://arxiv.org/abs/2511.10690</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal systems, hidden language, preference bias, telephone game, concept connections  

<br /><br />Summary:  
This paper investigates the hidden language of closed-source multimodal systems by analyzing their inherent preference bias when compressing input images into texts and reconstructing them back into images. During this process, the systems introduce shifts in output that disrupt original input concept co-occurrence, revealing underlying biases. To explore these biases, the authors employ a multi-round "telephone game" method, which involves iterative transformations to observe how concept co-occurrence frequencies evolve, thereby quantitatively measuring concept connection strength in the systems' understanding. The study introduces Telescope, a large dataset with over 10,000 concept pairs designed to support the telephone game framework. By running multiple rounds of the telephone game, the approach is scalable at test time and enables the construction of a global map of concept connections. This map helps identify training-inherited preference biases, track progress in generalization capabilities, and reveal more stable linkages among fragile concept pairs. Furthermore, the researchers leverage Reasoning Large Language Models (Reasoning-LLMs) to detect unexpected relationships between concepts that transcend simple textual or visual similarity, offering insights into how multimodal systems internally simulate and comprehend the world. Overall, the work sheds new light on interpretability and control in multimodal AI systems, laying groundwork for future research. <div>
arXiv:2511.10690v1 Announce Type: new 
Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</title>
<link>https://arxiv.org/abs/2511.10691</link>
<guid>https://arxiv.org/abs/2511.10691</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dynamic evaluation, adversarial environment, resource constraints, benchmark contamination<br /><br />Summary:<br /><br />1. Existing benchmarks for large language models (LLMs) struggle to keep up with rapid model development, and potential data contamination challenges their reliability, as models may have seen test questions during training.<br />2. Most benchmarks assume benign, resource-rich conditions, overlooking how LLMs perform under constrained or adversarial settings.<br />3. The paper introduces Squid Game: a novel, dynamic, adversarial evaluation environment designed to test LLMs under resource limitations and asymmetric information through interactive gameplay against other LLMs.<br />4. Squid Game features six elimination-style levels focusing on diverse abilities including instruction-following, coding, reasoning, planning, and safety alignment.<br />5. Evaluations of over 50 LLMs reveal a generational phase transition in performance within the same model lineage, and evidence that some models exploit speculative shortcuts to win, indicating risks of higher-level contamination in static benchmarks.<br />6. Correlation analyses comparing Squid Game with existing benchmarks highlight that dynamic, interactive evaluation offers complementary insights to static tests.<br />7. The paper pledges to publicly release the code and data, enabling further research in dynamic behavioral evaluation of general LLMs. <div>
arXiv:2511.10691v1 Announce Type: new 
Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate</title>
<link>https://arxiv.org/abs/2511.10693</link>
<guid>https://arxiv.org/abs/2511.10693</guid>
<content:encoded><![CDATA[
<div> Keywords: voice-based AI, politeness, speech rate reduction, text-to-speech, social conventions<br /><br />Summary:<br />1. This study examines whether advanced text-to-speech (TTS) systems can implicitly learn to convey politeness, a subtle human social cue, through changes in speech rate. <br />2. Researchers tested 22 synthetic voices from two popular AI platforms, AI Studio and OpenAI, by having them read the same script under "polite and formal" and "casual and informal" conditions. <br />3. The main measurement was speech duration, assessing if polite prompts led to slower speech compared to casual ones. <br />4. Results showed that all AI voices produced significantly slower speech in the polite condition, with very large effect sizes across both platforms. This was statistically significant for all AI Studio voices and most OpenAI voices. <br />5. The findings indicate that voice-based AI can internalize and reproduce nuanced psychological markers of human communication, such as politeness through speech rate modulation, supporting the view of AI as emerging social actors capable of reinforcing social norms. <div>
arXiv:2511.10693v1 Announce Type: new 
Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where does an LLM begin computing an instruction?</title>
<link>https://arxiv.org/abs/2511.10694</link>
<guid>https://arxiv.org/abs/2511.10694</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction following, activation patching, Llama models, layer-wise flip rate, onset

<br /><br />Summary:  
This paper investigates the process of instruction following within language models by examining where along a model’s layer stack the transition from reading an instruction to executing it occurs. The authors introduce three simple datasets—Key-Value, Quote Attribution, and Letter Selection—and combine them in two-hop compositions to analyze complex task execution. They apply activation patching on minimal-contrast prompt pairs to measure a layer-wise flip rate, which quantifies how substituting specific residual activations affects the predicted answer at different layers. Across various models in the Llama family, the study identifies a distinct inflection point called "onset," marking where interventions in earlier layers meaningfully alter outcomes but become largely ineffective beyond this point. Notably, multi-hop task compositions show a similar onset location, suggesting consistency in where instruction following initiates regardless of task complexity. The work presents a straightforward and reproducible method to pinpoint the starting layer of instruction following, enabling comparison across different tasks and model sizes, thereby contributing to a better understanding of how language models internally process instructions and execute tasks. <div>
arXiv:2511.10694v1 Announce Type: new 
Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations</title>
<link>https://arxiv.org/abs/2511.10695</link>
<guid>https://arxiv.org/abs/2511.10695</guid>
<content:encoded><![CDATA[
<div> nation-level bias, Large Language Models, International Relations, UNSC, debiasing framework<br /><br />Summary:<br /><br />This paper investigates nation-level biases in Large Language Models (LLMs) focusing on their applications in International Relations (IR). Using historical data from the United Nations Security Council (UNSC), the authors create a bias evaluation framework with three tests aimed at detecting biases toward the five permanent UNSC members. Results reveal that while common bias trends exist, such as favoring Western nations and disfavoring Russia, the bias patterns differ across LLMs. Furthermore, biases within a single model vary in direction and strength depending on the evaluation context, indicating that LLM biases are multidimensional and task-dependent. The study also finds that models exhibiting stronger reasoning capabilities tend to have reduced bias and improved performance. To address these biases, the authors propose a debiasing framework combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experimentation shows this approach successfully decreases nation-level bias and enhances performance, particularly in models like GPT-4o-mini and LLama-3.3-70B. The paper concludes by highlighting the importance of simultaneously evaluating nation-level bias and model performance when deploying LLMs in the IR domain, stressing that addressing bias is critical for fair and accurate use in this sensitive context. <div>
arXiv:2511.10695v1 Announce Type: new 
Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\pi$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2511.10696</link>
<guid>https://arxiv.org/abs/2511.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, Sparse Attention, \PiAttention, Long-Range Modeling, Adaptive Fusion Gate<br /><br />Summary:  
Transformers have fundamentally advanced natural language processing but face efficiency challenges due to their quadratic complexity relative to sequence length. Existing sparse attention mechanisms like RingAttention address this by limiting attention to local neighborhoods, which reduces computational cost but restricts the receptive field and adaptability. The proposed \PiAttention model introduces a periodic sparse Transformer architecture that decomposes attention into three components: ring-local neighborhoods, deterministic \(\pi\)-stride skips, and an adaptive fusion gate. This design ensures predictable coverage over distant tokens while maintaining a sparse attention pattern with per-layer complexity linear in the input sequence length. Theoretical analysis shows that \(\PiAttention\) achieves a receptive field growth rate of \(\mathcal{O}(kL + \pi \log L)\), improving upon the \(\mathcal{O}(kL)\) rate of RingAttention, where \(k\) is the window size, \(\pi\) is the skip period, and \(L\) is the sequence length. Experimentally, \(\PiAttention\) performs at or above the quality of dense attention models on tasks including language modeling, retrieval, and vision-language applications, achieving an 8.3% lower perplexity than RingAttention while using half the GPU resources for the same context length. Ablations and visualizations highlight the crucial roles of periodic skips, adaptive fusion, and coordinated sparsity at the attention head level in enabling efficient modeling of long contexts. <div>
arXiv:2511.10696v1 Announce Type: new 
Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $\pi$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + \pi \log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $\pi$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs</title>
<link>https://arxiv.org/abs/2511.10768</link>
<guid>https://arxiv.org/abs/2511.10768</guid>
<content:encoded><![CDATA[
<div> Keywords: medical text summarization, faithfulness, large language models, consumer health questions, LLaMA-2-7B<br /><br />Summary: The article addresses the challenge of generating faithful summaries of consumer health questions (CHQs), which is crucial for effective healthcare communication. It proposes a novel framework that integrates TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to improve faithfulness in medical summarization. The authors fine-tuned the LLaMA-2-7B model on two datasets, MeQSum (English) and BanglaCHQ-Summ (Bangla), and demonstrated consistent improvements in both quality metrics (ROUGE, BERTScore, readability) and faithfulness metrics (SummaC, AlignScore). The model outperformed zero-shot baselines and prior systems, highlighting the benefits of domain fine-tuning and combining extraction with entity recognition techniques. Human evaluation confirmed that over 80% of generated summaries retained critical medical information, underscoring the approach’s reliability. The work emphasizes faithfulness as an essential dimension for trustworthy medical summarization and shows the potential for safer deployment of LLMs in healthcare applications, ultimately aiming to reduce risks posed by unfaithful summaries that could mislead patients or healthcare providers. <div>
arXiv:2511.10768v1 Announce Type: new 
Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English</title>
<link>https://arxiv.org/abs/2511.10780</link>
<guid>https://arxiv.org/abs/2511.10780</guid>
<content:encoded><![CDATA[
<div> Tunisian Arabic, Speech Translation, Code-Switching, Dataset, TEDxTN<br /><br />Summary:<br /><br />1. The paper introduces TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset.<br /><br />2. The dataset was created to address the scarcity of resources for various Arabic dialects, specifically the Tunisian dialect.<br /><br />3. It comprises 108 TEDx talks totaling 25 hours of speech featuring code-switching and speakers from over 11 different Tunisian regions with diverse accents.<br /><br />4. The authors developed internal annotation guidelines and provide both these guidelines and the corpus to the public, allowing future expansions as new talks become available.<br /><br />5. Baseline results for Speech Recognition and Speech Translation are reported using multiple pre-trained and fine-tuned end-to-end models.<br /><br />6. TEDxTN stands as the first open-source, publicly accessible speech translation corpus capturing code-switching phenomena in the Tunisian dialect.<br /><br />7. This resource aims to motivate and facilitate further research in natural language processing for the Tunisian Arabic dialect. <div>
arXiv:2511.10780v1 Announce Type: new 
Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sabi\'a: Um Chatbot de Intelig\^encia Artificial Generativa para Suporte no Dia a Dia do Ensino Superior</title>
<link>https://arxiv.org/abs/2511.10787</link>
<guid>https://arxiv.org/abs/2511.10787</guid>
<content:encoded><![CDATA[
<div> Keywords: chatbot, Generative AI, Retrieval-Augmented Generation, Gemini 2.0 Flash, Gemma 3n

<br /><br />Summary:  
Students often face challenges in accessing day-to-day academic information because it is scattered across multiple institutional documents and websites, leading to confusion and a lack of clarity about routine university matters. To address this problem, the project proposes the development of a chatbot that utilizes Generative Artificial Intelligence (GenAI) combined with Retrieval-Augmented Generation (RAG) techniques to streamline and simplify access to academic information. Various GenAI models were tested and evaluated using quality metrics and the LLM-as-a-Judge evaluation approach to determine their effectiveness. Among the tested models, Gemini 2.0 Flash was identified as a top performer due to its superior quality and speed, making it highly efficient for this application. In addition, Gemma 3n was noted for having good performance while being open-source, presenting an advantage for adaptability and transparency. The project’s approach demonstrates promising potential to reduce information fragmentation and improve students’ experience by providing a centralized, interactive platform for accessing university-related information. <div>
arXiv:2511.10787v1 Announce Type: new 
Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation</title>
<link>https://arxiv.org/abs/2511.10819</link>
<guid>https://arxiv.org/abs/2511.10819</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, GPT-4o, automated grading, educational assessment, Computational Linguistics<br /><br />Summary:  
This study explores the use of a Large Language Model (LLM), specifically GPT-4o, for grading short-answer quizzes and project reports in a real undergraduate Computational Linguistics course. Data were collected from around 50 students over five quizzes, along with project reports submitted by 14 teams. The LLM-generated scores were compared with independent human evaluations conducted by the course teaching assistants (TAs). Results demonstrated a strong positive correlation between GPT-4o and human graders, reaching up to 0.98 for quiz assessments. The LLM achieved exact score agreement with human grading in 55% of quiz cases, indicating high reliability for structured responses. For project reports, while GPT-4o also aligned well overall with human assessments, there was some inconsistency in grading technical, open-ended answers, reflecting challenges in nuanced evaluation. The authors have released all code and sample data used in the study to encourage further research in the application of LLMs for automated grading. This work underscores both the promising potential and existing limitations of leveraging LLMs as tools for educational assessment in authentic academic environments. Ultimately, the study contributes valuable insights to the development of automated grading systems tailored to real-world classroom settings. <div>
arXiv:2511.10819v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders</title>
<link>https://arxiv.org/abs/2511.10840</link>
<guid>https://arxiv.org/abs/2511.10840</guid>
<content:encoded><![CDATA[
<div> Multilingual, Large Language Models, Pivot Language, Decoding, Attribution Analysis<br /><br />Summary: This paper investigates how multilingual large language models (LLMs) internally represent multiple languages and why performance often favors the dominant training language. The authors train several LLMs on different multilingual data mixtures and analyze their internal workings using cross-layer transcoders (CLT) and attribution graphs. They find strong evidence for a pivot language representation mechanism, where the model creates nearly identical representations across languages but performs language-specific decoding in later layers. Attribution analyses indicate that decoding depends partly on a small set of high-frequency language features in the final layers, which allow the model to linearly identify language identity from the earlier layers. By intervening on these features, the researchers can suppress one language and substitute another in the model’s output, demonstrating control over multilingual generation. They also explore how the dominant training language affects these internal mechanisms through attribution graphs and decoding pathways. The study emphasizes that understanding this pivot-language mechanism is vital for enhancing multilingual alignment and improving LLM performance across diverse languages. <div>
arXiv:2511.10840v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English</title>
<link>https://arxiv.org/abs/2511.10846</link>
<guid>https://arxiv.org/abs/2511.10846</guid>
<content:encoded><![CDATA[
<div> Automated emotion detection, African American Vernacular English (AAVE), racial bias, emotion AI, culturally informed models  

<br /><br />Summary:  
1. Automated emotion detection models are widely used across diverse domains but often rely on annotations reflecting dominant cultural norms, limiting their effectiveness in recognizing emotions in dialects such as African American Vernacular English (AAVE).  
2. This study analyzes 2.7 million geo-tagged tweets from Los Angeles, assessing the extent of AAVE usage through computational approximations of dialectal features.  
3. A dataset of 875 tweets with varying AAVE density was annotated for emotion presence and intensity, with "silver" labels generated by African American, AAVE-fluent annotators to provide culturally informed ground truth.  
4. Popular emotion recognition models like GPT, BERT-based models, and SpanEmo demonstrate substantially higher false positive rates of anger detection on AAVE tweets compared to General American English (GAE), with rates more than doubling in some cases.  
5. Linear regression analyses show models and non-ingroup annotators correlate more with profanity-based AAVE features than ingroup annotators, highlighting bias.  
6. There is a measurable association between neighborhoods with higher African American populations and model predictions of increased anger and decreased joy, indicating reinforcement of racial stereotypes by emotion AI.  
7. The study underscores a critical safety concern in affective computing and calls for developing culturally and dialect-informed emotion detection systems to mitigate biased outcomes. <div>
arXiv:2511.10846v1 Announce Type: new 
Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs</title>
<link>https://arxiv.org/abs/2511.10850</link>
<guid>https://arxiv.org/abs/2511.10850</guid>
<content:encoded><![CDATA[
<div> Task arithmetic, parameter space alignment, Large Language Models, Grouped-Query Attention, SwiGLU<br /><br />Summary:<br /><br />1. The paper addresses the challenge of negative interference in task arithmetic, which hinders the effective transfer of skills between Large Language Models (LLMs) that have diverged during training.<br />2. The authors propose a novel approach that aligns the parameter spaces of different LLMs by leveraging the intrinsic permutation, rotation, and scaling symmetries present in Transformer architectures.<br />3. This alignment technique is adapted specifically for modern architectural components, including Grouped-Query Attention (GQA) and SwiGLU layers, using both weight-based and activation-based strategies.<br />4. By applying this alignment-first strategy, the study successfully transfers advanced reasoning capabilities to a model originally lacking reasoning skills.<br />5. Experimental results on challenging reasoning benchmarks demonstrate that the proposed method consistently outperforms traditional task arithmetic, offering a more effective way to merge and transfer specialized skills across evolving LLM families, which reduces redundant fine-tuning and enhances overall model adaptability. <div>
arXiv:2511.10850v1 Announce Type: new 
Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems</title>
<link>https://arxiv.org/abs/2511.10871</link>
<guid>https://arxiv.org/abs/2511.10871</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM judgment, conversational framing, model conviction, social interactions, rebuttal perturbation<br /><br />Summary:<br /><br />1. The paper investigates how large language models (LLMs) perform when acting as judges in tasks requiring social or conversational judgment, shifting from direct factual queries to conversational contexts. <br /><br />2. It introduces an evaluation framework that contrasts model assessments of factual correctness against judgments of speaker correctness within minimal dialogues, effectively reframing the question from "Is this statement correct?" to "Is this speaker correct?". <br /><br />3. The study applies conversational pressure through a simple rebuttal phrase ("The previous answer is incorrect.") to evaluate the firmness of the model’s convictions across both direct and conversational scenarios. <br /><br />4. Results reveal varied responses among models: GPT-4o-mini displayed sycophantic tendencies under social framing, while Llama-8B-Instruct exhibited overly critical behavior, signaling that conversational context can significantly influence model judgment. <br /><br />5. On average, there was a 9.24% performance change across models, highlighting the importance of conversational framing as a key factor in LLM evaluations and proposing a reproducible method for assessing model conviction to advance trustworthy dialogue systems. <div>
arXiv:2511.10871v1 Announce Type: new 
Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICX360: In-Context eXplainability 360 Toolkit</title>
<link>https://arxiv.org/abs/2511.10879</link>
<guid>https://arxiv.org/abs/2511.10879</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Explainability, In-Context Explainability 360, Black-box methods, White-box methods<br /><br />Summary: Large Language Models (LLMs) are increasingly integrated into various high-stakes applications such as summarizing meetings and assisting medical professionals. To enhance trust and transparency, it is essential to develop tools that explain LLM outputs, including responses, summaries, and lists. Addressing this need, the paper introduces In-Context Explainability 360 (ICX360), an open-source Python toolkit designed to explain LLM outputs by focusing on the user-provided context or prompts. ICX360 implements three recent explanation tools that use both black-box (perturbation-based) and white-box (gradient-based) methods to analyze LLM behavior. The toolkit aims to be accessible, providing quick-start guides and thorough tutorials to support diverse use cases such as retrieval-augmented generation, natural language generation, and even jailbreaking techniques. Hosted on GitHub by IBM, ICX360 intends to facilitate researchers and developers in better understanding and interpreting LLM decisions through contextual explanations, fostering safer and more interpretable AI applications. <div>
arXiv:2511.10879v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge</title>
<link>https://arxiv.org/abs/2511.10881</link>
<guid>https://arxiv.org/abs/2511.10881</guid>
<content:encoded><![CDATA[
<div> Negative bias, large language models, prompt format, parametric knowledge, mitigation<br /><br /><br />Summary: This paper investigates the phenomenon of negative bias in large language models (LLMs), where models tend to produce excessive negative responses in binary decision tasks such as yes-no questions. It reveals that this bias is influenced more by the prompt format than by the semantic content of negative responses, highlighting a format-level negative bias. To study this in detail, the authors propose a novel evaluation pipeline that categorizes model responses into three subsets based on the model's parametric knowledge: correct knowledge, incorrect knowledge, and insufficient relevant knowledge. Their analysis discovers a shortcut behavior where LLMs default to negative answers when they lack sufficient knowledge to respond accurately, thus contributing to negative bias. The study further explores how different prompting scenarios affect negative bias, showing that the inclusion of relevant contextual information and offering an "I don't know" response option tend to reduce the bias. Conversely, using chain-of-thought prompting often increases the tendency towards negative bias. Lastly, the research demonstrates that the degree and direction of negative bias vary depending on the prompt type. These insights provide a better understanding of negative bias and suggest strategies for mitigating it in LLMs. <div>
arXiv:2511.10881v1 Announce Type: new 
Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking</title>
<link>https://arxiv.org/abs/2511.10887</link>
<guid>https://arxiv.org/abs/2511.10887</guid>
<content:encoded><![CDATA[
<div> Biomedical NER, Entity Linking, UMLS, Ontological Paths, Explainable Models<br /><br />Summary: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is limited by a fragmented data landscape, insufficient resources for building explainable models, and the shortcomings of semantically unaware evaluation metrics. To overcome these challenges, the authors introduce MedPath, a comprehensive large-scale and multi-domain biomedical EL dataset. MedPath integrates and builds upon nine existing expert-annotated EL datasets, providing a unified resource for research. In MedPath, all entities are normalized using the most recent version of the Unified Medical Language System (UMLS), ensuring standardization across datasets. Additionally, the dataset includes augmentations with mappings to 62 other biomedical vocabularies, greatly improving interoperability. Crucially, MedPath enriches entities with complete ontological paths, spanning from general to specific categories, across up to 11 biomedical vocabularies. This unique feature facilitates semantic-rich and interpretable EL system development by providing hierarchical context often missing in previous datasets. Overall, MedPath enables new avenues for research in biomedical natural language processing (NLP), supporting advancements in interoperable, explainable, and clinically relevant NLP models by providing richer semantic information and standardized entity linking benchmarks. <div>
arXiv:2511.10887v1 Announce Type: new 
Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10899</link>
<guid>https://arxiv.org/abs/2511.10899</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-Induced Myopia, Tool-augmented Language Models, Code Interpreter, reasoning degradation, PYMATH<br /><br />Summary:<br /><br />1. Tool-augmented Language Models (TaLMs) enhance problem-solving by invoking external tools but may produce correct answers without coherent reasoning, a failure mode termed Tool-Induced Myopia (TIM).<br />2. The study focuses on the Code Interpreter tool and uses PYMATH, a benchmark of 1,679 competition-level math problems where Python coding aids but does not fully solve.<br />3. A novel multi-dimensional evaluation suite quantifies reasoning quality, revealing that despite up to 19.3 percentage points improvement in final-answer accuracy, TaLMs show consistent reasoning degradation compared to non-tool models.<br />4. Increased frequency of tool use correlates with greater reasoning incoherence; TaLM errors shift from arithmetic mistakes to broader reasoning failures involving logic, assumptions, and creativity, with TIM present in approximately 55% of high-risk cases.<br />5. To address TIM, the authors propose a preference-optimization framework that realigns TaLMs to treat tools as assistive evidence, improving both accuracy and reasoning depth under tool use.<br /><br />Code and data for this study are publicly available at https://github.com/megagonlabs/TIM. <div>
arXiv:2511.10899v1 Announce Type: new 
Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering</title>
<link>https://arxiv.org/abs/2511.10900</link>
<guid>https://arxiv.org/abs/2511.10900</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Question Answering, EMSQA Dataset, Expert-CoT, ExpertRAG<br /><br />Summary:<br /><br />1. Large language models (LLMs) have demonstrated potential in medical question answering but often lack integration of domain-specific expertise, such as clinical subject areas (e.g., trauma, airway) and certification levels (e.g., EMT, Paramedic), which are crucial in professional settings.<br /><br />2. Existing methods typically use general-purpose prompting or retrieval without leveraging structured medical context, limiting their effectiveness in critical, high-stakes environments.<br /><br />3. To address this, the EMSQA dataset was developed, comprising 24,300 multiple-choice questions spanning 10 clinical subject areas and 4 certification levels, supported by a curated knowledge base of 40,000 documents totaling 2 million tokens, aligned to relevant subject areas.<br /><br />4. The study introduces two novel approaches: Expert-CoT, a chain-of-thought prompting strategy conditioned on specific clinical subjects and certification levels, and ExpertRAG, a retrieval-augmented generation pipeline that grounds answers in subject-aligned documents and real patient data.<br /><br />5. Experimental results across 4 LLMs show Expert-CoT improves accuracy by up to 2.05% over standard CoT prompting, while combining Expert-CoT with ExpertRAG achieves up to a 4.59% accuracy gain over standard RAG baselines. Remarkably, 32-billion-parameter expertise-augmented LLMs successfully passed all computer-adaptive EMS certification simulation exams. <div>
arXiv:2511.10900v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions</title>
<link>https://arxiv.org/abs/2511.10902</link>
<guid>https://arxiv.org/abs/2511.10902</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal peer review, large language models, retrieval-augmented generation, academic workflows, actionable feedback  

<br /><br />Summary: This paper introduces an innovative web-based system designed to enhance academic peer review by integrating multimodal inputs, including both textual and visual data, leveraging the capabilities of large language models (LLMs). Unlike traditional peer review systems constrained to text-only inputs, the proposed framework utilizes multimodal LLMs to provide richer, context-aware feedback. To improve review quality and grounding, the system incorporates retrieval-augmented generation (RAG) based on extensive OpenReview datasets, ensuring that the generated reviews reflect community standards and relevant prior work. A notable feature is the conversion of generated reviews into structured, actionable to-do lists using the novel Action:Objective[#] format, enabling authors to clearly understand and track revision tasks. The platform offers seamless integration with existing academic writing tools, facilitating real-time, interactive feedback and revision monitoring. Experimental evaluations demonstrate that the system produces more comprehensive and expert-aligned review comments compared to baseline models lacking multimodal or retrieval enhancements. This approach advances scholarly assistance by fostering transparent, human-centered review simulations that better support manuscript improvement before submission. Overall, the work addresses key limitations in current peer review automation by combining multimodality, community-context grounding, and actionable guidance into a unified system. <div>
arXiv:2511.10902v1 Announce Type: new 
Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy</title>
<link>https://arxiv.org/abs/2511.10903</link>
<guid>https://arxiv.org/abs/2511.10903</guid>
<content:encoded><![CDATA[
<div> Keywords: Bloom's Taxonomy, exam question classification, machine learning, data augmentation, large language models<br /><br />Summary:<br /><br />This paper investigates the automatic classification of exam questions and learning outcomes according to the six cognitive categories of Bloom's Taxonomy: Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation. A small labeled dataset of 600 sentences was used to train and evaluate multiple types of models, including traditional machine learning (Naive Bayes, Logistic Regression, SVM), recurrent neural networks (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT, RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Different preprocessing and data augmentation techniques were applied to improve performance, such as synonym replacement and word embeddings. Among traditional models, Support Vector Machines with data augmentation delivered the best results, achieving approximately 94% accuracy, recall, and F1 scores while exhibiting minimal overfitting. In contrast, RNN-based models and BERT showed significant overfitting issues, although RoBERTa initially resisted overfitting but deteriorated over longer training. Zero-shot evaluations of large language models revealed that OpenAI and Gemini outperformed their peers with about 72-73% accuracy and comparable F1 scores, despite not being fine-tuned on the dataset. The study highlights the difficulties of training complex deep learning models on limited data and stresses the effectiveness of careful augmentation and simpler algorithms for Bloom's Taxonomy classification tasks. <div>
arXiv:2511.10903v1 Announce Type: new 
Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D</title>
<link>https://arxiv.org/abs/2511.10912</link>
<guid>https://arxiv.org/abs/2511.10912</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, rare disease diagnosis, narrative medical cases, House M.D. dataset, AI-assisted diagnosis

<br /><br />Summary:  
This study addresses the underexplored ability of large language models (LLMs) to diagnose rare diseases from narrative medical case descriptions. It introduces a novel dataset comprising 176 symptom-diagnosis pairs extracted from the TV series House M.D., which is validated as an educational tool for rare disease recognition. Four state-of-the-art LLMs—GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro—were evaluated using this dataset on diagnostic reasoning tasks based on narrative cases. The results showed considerable variation in model performance, with accuracy rates ranging from 16.48% to 38.64%. Notably, newer LLM generations achieved about 2.3 times better accuracy compared to earlier versions. Despite the overall challenges these models face in diagnosing rare diseases, the observed improvements across model architectures point to promising avenues for future development in AI-assisted diagnosis. The study establishes baseline performance metrics via an educationally validated benchmark and provides a publicly accessible evaluation framework aimed at advancing AI research in medical narrative diagnostic reasoning. <div>
arXiv:2511.10912v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology</title>
<link>https://arxiv.org/abs/2511.10930</link>
<guid>https://arxiv.org/abs/2511.10930</guid>
<content:encoded><![CDATA[
<div> Keywords: CardioEmbed, cardiology embeddings, contrastive learning, InfoNCE loss, semantic retrieval

<br /><br />Summary: This study addresses the gap in biomedical text embeddings for clinical cardiology by developing CardioEmbed, a domain-specialized embedding model trained on comprehensive cardiology textbooks rather than research abstracts. CardioEmbed is built on Qwen3-Embedding-8B and trained using contrastive learning with InfoNCE loss and in-batch negatives on a curated corpus of approximately 150,000 deduplicated sentences from seven authoritative cardiology textbooks. The model demonstrates a significant improvement by achieving 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, surpassing the current state-of-the-art medical embedding model, MedTE, by 15.94 percentage points. Additionally, CardioEmbed performs competitively on broader biomedical benchmarks within the MTEB framework, obtaining a BIOSSES Spearman correlation score of 0.77 and an NDCG@10 score of 0.61 on the SciFact dataset. These results highlight the effectiveness of domain-specialized training on comprehensive clinical textbooks to yield near-perfect cardiology semantic retrieval performance and robust generalizability to related biomedical domains. The study underscores the importance of leveraging domain-relevant clinical knowledge sources, such as textbooks, for improved embedding models in specialized medical fields like cardiology. <div>
arXiv:2511.10930v1 Announce Type: new 
Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</title>
<link>https://arxiv.org/abs/2511.10984</link>
<guid>https://arxiv.org/abs/2511.10984</guid>
<content:encoded><![CDATA[
<div> Keywords: discourse-level translation, expert domains, Chinese-English translation, evaluation benchmark, Metric-S<br /><br />Summary:<br /><br />1. The paper identifies a critical gap in the evaluation of discourse-level translation within expert domains, highlighting that current methods mainly assess segment-level accuracy and fluency rather than discourse coherence and terminological precision. 2. To address this shortcoming, the authors introduce DiscoX, a novel benchmark consisting of 200 professionally curated, long-form texts (average length over 1700 tokens) from seven specialized domains, designed specifically for Chinese-English translation tasks. 3. Alongside DiscoX, the authors develop Metric-S, a reference-free automatic evaluation system that delivers fine-grained assessments across accuracy, fluency, and appropriateness, showing strong correlation with human judgments and outperforming existing metrics. 4. Experimental results reveal a significant performance gap between state-of-the-art large language models (LLMs) and human experts, underscoring the complexity and challenge of professional-grade discourse-level translation in expert domains. 5. The introduction of DiscoX and Metric-S provides a robust framework for more rigorous, discourse-aware evaluation, thereby facilitating future progress in machine translation driven by LLMs and addressing the needs of cross-lingual scholarly communication and knowledge dissemination. <div>
arXiv:2511.10984v1 Announce Type: new 
Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets</title>
<link>https://arxiv.org/abs/2511.10985</link>
<guid>https://arxiv.org/abs/2511.10985</guid>
<content:encoded><![CDATA[
arXiv:2511.10985v1 Announce Type: new 
Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Based Steering of Large Language Models for Diverse Structured Generation</title>
<link>https://arxiv.org/abs/2511.11018</link>
<guid>https://arxiv.org/abs/2511.11018</guid>
<content:encoded><![CDATA[
arXiv:2511.11018v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB</title>
<link>https://arxiv.org/abs/2511.11041</link>
<guid>https://arxiv.org/abs/2511.11041</guid>
<content:encoded><![CDATA[
arXiv:2511.11041v1 Announce Type: new 
Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + \mu$, where $\mu$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $\sigma$ on retrieval tasks, 3.1 $\sigma$ on classification tasks, and 0.8 $\sigma$ on other types of tasks. Renormalization has two variants: directly subtracting $\mu$ from $e$, or subtracting the projection of $e$ onto $\mu$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Own Hallucinations?</title>
<link>https://arxiv.org/abs/2511.11087</link>
<guid>https://arxiv.org/abs/2511.11087</guid>
<content:encoded><![CDATA[
arXiv:2511.11087v1 Announce Type: new 
Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysing Personal Attacks in U.S. Presidential Debates</title>
<link>https://arxiv.org/abs/2511.11108</link>
<guid>https://arxiv.org/abs/2511.11108</guid>
<content:encoded><![CDATA[
arXiv:2511.11108v1 Announce Type: new 
Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</title>
<link>https://arxiv.org/abs/2511.11124</link>
<guid>https://arxiv.org/abs/2511.11124</guid>
<content:encoded><![CDATA[
arXiv:2511.11124v1 Announce Type: new 
Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion</title>
<link>https://arxiv.org/abs/2511.11126</link>
<guid>https://arxiv.org/abs/2511.11126</guid>
<content:encoded><![CDATA[
arXiv:2511.11126v1 Announce Type: new 
Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2511.11139</link>
<guid>https://arxiv.org/abs/2511.11139</guid>
<content:encoded><![CDATA[
arXiv:2511.11139v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</title>
<link>https://arxiv.org/abs/2511.11141</link>
<guid>https://arxiv.org/abs/2511.11141</guid>
<content:encoded><![CDATA[
arXiv:2511.11141v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy</title>
<link>https://arxiv.org/abs/2511.11214</link>
<guid>https://arxiv.org/abs/2511.11214</guid>
<content:encoded><![CDATA[
arXiv:2511.11214v1 Announce Type: new 
Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2511.11234</link>
<guid>https://arxiv.org/abs/2511.11234</guid>
<content:encoded><![CDATA[
arXiv:2511.11234v1 Announce Type: new 
Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement</title>
<link>https://arxiv.org/abs/2511.11258</link>
<guid>https://arxiv.org/abs/2511.11258</guid>
<content:encoded><![CDATA[
arXiv:2511.11258v1 Announce Type: new 
Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference</title>
<link>https://arxiv.org/abs/2511.11306</link>
<guid>https://arxiv.org/abs/2511.11306</guid>
<content:encoded><![CDATA[
arXiv:2511.11306v1 Announce Type: new 
Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity</title>
<link>https://arxiv.org/abs/2511.11309</link>
<guid>https://arxiv.org/abs/2511.11309</guid>
<content:encoded><![CDATA[
arXiv:2511.11309v1 Announce Type: new 
Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.11315</link>
<guid>https://arxiv.org/abs/2511.11315</guid>
<content:encoded><![CDATA[
arXiv:2511.11315v1 Announce Type: new 
Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery</title>
<link>https://arxiv.org/abs/2511.11324</link>
<guid>https://arxiv.org/abs/2511.11324</guid>
<content:encoded><![CDATA[
arXiv:2511.11324v1 Announce Type: new 
Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2511.11334</link>
<guid>https://arxiv.org/abs/2511.11334</guid>
<content:encoded><![CDATA[
arXiv:2511.11334v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text</title>
<link>https://arxiv.org/abs/2511.11340</link>
<guid>https://arxiv.org/abs/2511.11340</guid>
<content:encoded><![CDATA[
arXiv:2511.11340v1 Announce Type: new 
Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studies with impossible languages falsify LMs as models of human language</title>
<link>https://arxiv.org/abs/2511.11389</link>
<guid>https://arxiv.org/abs/2511.11389</guid>
<content:encoded><![CDATA[
arXiv:2511.11389v1 Announce Type: new 
Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MajinBook: An open catalogue of digital world literature with likes</title>
<link>https://arxiv.org/abs/2511.11412</link>
<guid>https://arxiv.org/abs/2511.11412</guid>
<content:encoded><![CDATA[
arXiv:2511.11412v1 Announce Type: new 
Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Hearing Assistants that Isolate Egocentric Conversations</title>
<link>https://arxiv.org/abs/2511.11473</link>
<guid>https://arxiv.org/abs/2511.11473</guid>
<content:encoded><![CDATA[
arXiv:2511.11473v1 Announce Type: new 
Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2511.11518</link>
<guid>https://arxiv.org/abs/2511.11518</guid>
<content:encoded><![CDATA[
arXiv:2511.11518v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning</title>
<link>https://arxiv.org/abs/2511.11562</link>
<guid>https://arxiv.org/abs/2511.11562</guid>
<content:encoded><![CDATA[
arXiv:2511.11562v1 Announce Type: new 
Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title>
<link>https://arxiv.org/abs/2511.10687</link>
<guid>https://arxiv.org/abs/2511.10687</guid>
<content:encoded><![CDATA[
arXiv:2511.10687v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents</title>
<link>https://arxiv.org/abs/2511.10705</link>
<guid>https://arxiv.org/abs/2511.10705</guid>
<content:encoded><![CDATA[
arXiv:2511.10705v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title>
<link>https://arxiv.org/abs/2511.10720</link>
<guid>https://arxiv.org/abs/2511.10720</guid>
<content:encoded><![CDATA[
arXiv:2511.10720v1 Announce Type: cross 
Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10788</link>
<guid>https://arxiv.org/abs/2511.10788</guid>
<content:encoded><![CDATA[
arXiv:2511.10788v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title>
<link>https://arxiv.org/abs/2511.10837</link>
<guid>https://arxiv.org/abs/2511.10837</guid>
<content:encoded><![CDATA[
arXiv:2511.10837v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2511.11066</link>
<guid>https://arxiv.org/abs/2511.11066</guid>
<content:encoded><![CDATA[
arXiv:2511.11066v1 Announce Type: cross 
Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation</title>
<link>https://arxiv.org/abs/2511.11104</link>
<guid>https://arxiv.org/abs/2511.11104</guid>
<content:encoded><![CDATA[
arXiv:2511.11104v1 Announce Type: cross 
Abstract: Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2511.11182</link>
<guid>https://arxiv.org/abs/2511.11182</guid>
<content:encoded><![CDATA[
arXiv:2511.11182v1 Announce Type: cross 
Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Meaningful Units with Visually Grounded Semantics from Image Captions</title>
<link>https://arxiv.org/abs/2511.11262</link>
<guid>https://arxiv.org/abs/2511.11262</guid>
<content:encoded><![CDATA[
arXiv:2511.11262v1 Announce Type: cross 
Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQuaD: The Software Quality Dataset</title>
<link>https://arxiv.org/abs/2511.11265</link>
<guid>https://arxiv.org/abs/2511.11265</guid>
<content:encoded><![CDATA[
arXiv:2511.11265v1 Announce Type: cross 
Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Aided State Estimation</title>
<link>https://arxiv.org/abs/2511.11285</link>
<guid>https://arxiv.org/abs/2511.11285</guid>
<content:encoded><![CDATA[
arXiv:2511.11285v1 Announce Type: cross 
Abstract: Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building the Web for Agents: A Declarative Framework for Agent-Web Interaction</title>
<link>https://arxiv.org/abs/2511.11287</link>
<guid>https://arxiv.org/abs/2511.11287</guid>
<content:encoded><![CDATA[
arXiv:2511.11287v1 Announce Type: cross 
Abstract: The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces  and  tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2511.11362</link>
<guid>https://arxiv.org/abs/2511.11362</guid>
<content:encoded><![CDATA[
arXiv:2511.11362v1 Announce Type: cross 
Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs</title>
<link>https://arxiv.org/abs/2511.11440</link>
<guid>https://arxiv.org/abs/2511.11440</guid>
<content:encoded><![CDATA[
arXiv:2511.11440v1 Announce Type: cross 
Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title>
<link>https://arxiv.org/abs/2511.11551</link>
<guid>https://arxiv.org/abs/2511.11551</guid>
<content:encoded><![CDATA[
arXiv:2511.11551v1 Announce Type: cross 
Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding</title>
<link>https://arxiv.org/abs/2511.11552</link>
<guid>https://arxiv.org/abs/2511.11552</guid>
<content:encoded><![CDATA[
arXiv:2511.11552v1 Announce Type: cross 
Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Mixture of Block Attention</title>
<link>https://arxiv.org/abs/2511.11571</link>
<guid>https://arxiv.org/abs/2511.11571</guid>
<content:encoded><![CDATA[
arXiv:2511.11571v1 Announce Type: cross 
Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying and Analyzing Performance-Critical Tokens in Large Language Models</title>
<link>https://arxiv.org/abs/2401.11323</link>
<guid>https://arxiv.org/abs/2401.11323</guid>
<content:encoded><![CDATA[
arXiv:2401.11323v4 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as an effective solution for few-shot learning with large language models (LLMs). However, how LLMs leverage demonstrations to specify a task and learn a corresponding computational function through ICL is underexplored. Drawing from the way humans learn from content-label mappings in demonstrations, we categorize the tokens in an ICL prompt into content, stopword, and template tokens. Our goal is to identify the types of tokens whose representations directly influence LLM's performance, a property we refer to as being performance-critical. By ablating representations from the attention of the test example, we find that the representations of informative content tokens have less influence on performance compared to template and stopword tokens, which contrasts with the human attention to informative words. We give evidence that the representations of performance-critical tokens aggregate information from the content tokens. Moreover, we demonstrate experimentally that lexical meaning, repetition, and structural cues are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models learn to perform tasks from demonstrations and deepens our understanding of the roles different types of tokens play in large language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metric Learning Encoding Models: A Multivariate Framework for Interpreting Neural Representations</title>
<link>https://arxiv.org/abs/2402.11608</link>
<guid>https://arxiv.org/abs/2402.11608</guid>
<content:encoded><![CDATA[
arXiv:2402.11608v2 Announce Type: replace 
Abstract: Understanding how explicit theoretical features are encoded in opaque neural systems is a central challenge now common to neuroscience and AI. We introduce Metric Learning Encoding Models (MLEMs) to address this challenge most directly as a metric learning problem: we fit the distance in the space of theoretical features to match the distance in neural space. Our framework improves on univariate encoding and decoding methods by building on second-order isomorphism methods, such as Representational Similarity Analysis, and extends them by learning a metric that efficiently models feature as well as interactions between them. The effectiveness of MLEM is validated through two sets of simulations. First, MLEMs recover ground-truth importance features in synthetic datasets better than state-of-the-art methods, such as Feature Reweighted RSA (FR-RSA). Second, we deploy MLEMs on real language data, where they show stronger robustness to noise in calculating the importance of linguistic features (gender, tense, etc.). MLEMs are applicable to any domains where theoretical features can be identified, such as language, vision, audition, etc. We release optimized code applicable to measure feature importance in the representations of any artificial neural networks or empirical neural data at https://github.com/LouisJalouzot/MLEM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey in Characterization of Semantic Change</title>
<link>https://arxiv.org/abs/2402.19088</link>
<guid>https://arxiv.org/abs/2402.19088</guid>
<content:encoded><![CDATA[
arXiv:2402.19088v4 Announce Type: replace 
Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to characterize how the meaning of words changes and to reason about how to reduce the impact of semantic change. This survey provides an understandable overview of existing approaches to the \textit{characterization of semantic changes} and also formally defines three classes of characterizations: if the meaning of a word becomes more general or narrow (change in dimension) if the word is used in a more pejorative or positive/ameliorated sense (change in orientation), and if there is a trend to use the word in a, for instance, metaphoric or metonymic context (change in relation). We summarized the main aspects of the selected publications in a table and discussed the needs and trends in the research activities on semantic change characterization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Downstream Performance of Mixture-of-Experts Transformers via Weak Vanilla Transformers</title>
<link>https://arxiv.org/abs/2403.01994</link>
<guid>https://arxiv.org/abs/2403.01994</guid>
<content:encoded><![CDATA[
arXiv:2403.01994v2 Announce Type: replace 
Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately enhancing their performance in downstream tasks. We design a specific distillation method and conduct experiments on the BERT architecture. Experimental results show a significant improvement in downstream performance of MoE models, and many further evidences also strongly support the concept of transfer capability distillation. Finally, we attempt to interpret transfer capability distillation and provide some insights from the perspective of model feature.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness</title>
<link>https://arxiv.org/abs/2405.08151</link>
<guid>https://arxiv.org/abs/2405.08151</guid>
<content:encoded><![CDATA[
arXiv:2405.08151v3 Announce Type: replace 
Abstract: Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are language models rational? The case of coherence norms and belief revision</title>
<link>https://arxiv.org/abs/2406.03442</link>
<guid>https://arxiv.org/abs/2406.03442</guid>
<content:encoded><![CDATA[
arXiv:2406.03442v3 Announce Type: replace 
Abstract: Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well as coherence norms tied to the strength of belief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models. This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities. We argue that rational norms tied to coherence do apply to some language models, but not to others. This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RASTeR: Robust, Agentic, and Structured Temporal Reasoning</title>
<link>https://arxiv.org/abs/2406.19538</link>
<guid>https://arxiv.org/abs/2406.19538</guid>
<content:encoded><![CDATA[
arXiv:2406.19538v2 Announce Type: replace 
Abstract: Temporal question answering (TQA) remains a challenge for large language models (LLMs), particularly when retrieved content may be irrelevant, outdated, or temporally inconsistent. This is especially critical in applications like clinical event ordering, and policy tracking, which require reliable temporal reasoning even under noisy or outdated information. To address this challenge, we introduce RASTeR: \textbf{R}obust, \textbf{A}gentic, and \textbf{S}tructured, \textbf{Te}mporal \textbf{R}easoning, a prompting framework that separates context evaluation from answer generation. RASTeR first assesses the relevance and temporal coherence of the retrieved context, then constructs a temporal knolwedge graph (TKG) to better facilitate reasoning. When inconsistencies are detected, RASTeR selectively corrects or discards context before generating an answer. Across multiple datasets and LLMs, RASTeR consistently improves robustness\footnote{\ Some TQA work defines robustness as handling diverse temporal phenomena. Here, we define it as the ability to answer correctly despite suboptimal context}. We further validate our approach through a ``needle-in-the-haystack'' study, in which relevant context is buried among distractors. With forty distractors, RASTeR achieves 75\% accuracy, over 12\% ahead of the runner up
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Analysis of Gender Depiction in the Comedias of Calder\'on de la Barca</title>
<link>https://arxiv.org/abs/2411.03895</link>
<guid>https://arxiv.org/abs/2411.03895</guid>
<content:encoded><![CDATA[
arXiv:2411.03895v2 Announce Type: replace 
Abstract: In theatre, playwrights use the portrayal of characters to explore culturally based gender norms. In this paper, we develop quantitative methods to study gender depiction in the non-religious works (comedias) of Pedro Calder\'on de la Barca, a prolific Spanish 17th century author. We gather insights from a corpus of more than 100 plays by using a gender classifier and applying model explainability (attribution) methods to determine which text features are most influential in the model's decision to classify speech as 'male' or 'female', indicating the most gendered elements of dialogue in Calder\'on's comedias in a human accessible manner. We find that female and male characters are portrayed differently and can be identified by the gender prediction model at practically useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender portrayal, and demonstrates that the model is even useful in providing a relatively accurate scene-by-scene prediction of cross-dressing characters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2411.19244</link>
<guid>https://arxiv.org/abs/2411.19244</guid>
<content:encoded><![CDATA[
arXiv:2411.19244v3 Announce Type: replace 
Abstract: The Nepali language has distinct linguistic features, especially its complex script (Devanagari script), morphology, and various dialects,which pose a unique challenge for Natural Language Understanding (NLU) tasks. While the Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a foundation for evaluating models, it remains limited in scope, covering four tasks. This restricts their utility for comprehensive assessments of Natural Language Processing (NLP) models. To address this limitation, we introduce twelve new datasets, creating a new benchmark, the Nepali /Language Understanding Evaluation (NLUE) benchmark for evaluating the performance of models across a diverse set of Natural Language Understanding (NLU) tasks. The added tasks include Single-Sentence Classification, Similarity and Paraphrase Tasks, Natural Language Inference (NLI), and General Masked Evaluation Task (GMET). Through extensive experiments, we demonstrate that existing top models struggle with the added complexity of these tasks. We also find that the best multilingual model outperforms the best monolingual models across most tasks, highlighting the need for more robust solutions tailored to the Nepali language. This expanded benchmark sets a new standard for evaluating, comparing, and advancing models, contributing significantly to the broader goal of advancing NLP research for low-resource languages.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LDC: Learning to Generate Research Idea with Dynamic Control</title>
<link>https://arxiv.org/abs/2412.14626</link>
<guid>https://arxiv.org/abs/2412.14626</guid>
<content:encoded><![CDATA[
arXiv:2412.14626v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have demonstrated their potential in automating the scientific research ideation. Existing approaches primarily focus on prompting techniques, often producing ideas misaligned with expert standards - novelty, feasibility, and effectiveness, which are widely recognized by the research community as the three key subdimensions of high-quality ideas. Also, balancing these dimensions remains challenging due to their inherent trade-offs. To address these limitations, we propose the first framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL) for the task. In the SFT stage, the model learns foundational patterns from pairs of research papers and their corresponding follow-up ideas. In the RL stage, multi-dimensional reward models guided by fine-grained feedback evaluate and optimize the model across key dimensions. During inference, dimensional controllers coordinated by a sentence-level decoder enable dynamic context-aware steering of the idea generation process. Our framework provides a balanced approach to research idea generation, achieving high-quality outcomes in the experiment by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotions, Context, and Substance Use in Adolescents: A Large Language Model Analysis of Reddit Posts</title>
<link>https://arxiv.org/abs/2501.14037</link>
<guid>https://arxiv.org/abs/2501.14037</guid>
<content:encoded><![CDATA[
arXiv:2501.14037v2 Announce Type: replace 
Abstract: Early substance use during adolescence increases the risk of later substance use disorders and mental health problems, yet the emotional and contextual factors driving these behaviors remain poorly understood. This study analyzed 23000 substance-use related posts and an equal number of non-substance posts from Reddit's r/teenagers community (2018-2022). Posts were annotated for six discrete emotions (sadness, anger, joy, guilt, fear, disgust) and contextual factors (family, peers, school) using large language models (LLMs). Statistical analyses compared group differences, and interpretable machine learning (SHAP) identified key predictors of substance-use discussions. LLM-assisted thematic coding further revealed latent psychosocial themes linking emotions with contexts. Negative emotions, especially sadness, guilt, fear, and disgust, were significantly more common in substance-use posts, while joy dominated non-substance discussions. Guilt and shame diverged in function: guilt often reflected regret and self-reflection, whereas shame reinforced risky behaviors through peer performance. Peer influence emerged as the strongest contextual factor, closely tied to sadness, fear, and guilt. Family and school environments acted as both risk and protective factors depending on relational quality and stress levels. Overall, adolescent substance-use discussions reflected a dynamic interplay of emotion, social context, and coping behavior. By integrating statistical analysis, interpretable models, and LLM-based thematic exploration, this study demonstrates the value of mixed computational approaches for uncovering the emotional and contextual mechanisms underlying adolescent risk behavior.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts</title>
<link>https://arxiv.org/abs/2503.19498</link>
<guid>https://arxiv.org/abs/2503.19498</guid>
<content:encoded><![CDATA[
arXiv:2503.19498v5 Announce Type: replace 
Abstract: Chart Question Answering (CQA) evaluates Multimodal Large Language Models (MLLMs) on visual understanding and reasoning over chart data. However, existing benchmarks mostly test surface-level parsing, such as reading labels and legends, while overlooking deeper scientific reasoning. We propose DomainCQA, a framework for constructing domain-specific CQA benchmarks that emphasize both visual comprehension and knowledge-intensive reasoning. It integrates complexity-aware chart selection, multitier QA generation, and expert validation. Applied to astronomy, DomainCQA yields AstroChart, a benchmark of 1,690 QA pairs over 482 charts, exposing persistent weaknesses in fine-grained perception, numerical reasoning, and domain knowledge integration across 21 MLLMs. Fine-tuning on AstroChart improves performance across fundamental and advanced tasks. Pilot QA sets in biochemistry, economics, medicine, and social science further demonstrate DomainCQA's generality. Together, our results establish DomainCQA as a unified pipeline for constructing and augmenting domain-specific chart reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
<link>https://arxiv.org/abs/2504.08716</link>
<guid>https://arxiv.org/abs/2504.08716</guid>
<content:encoded><![CDATA[
arXiv:2504.08716v2 Announce Type: replace 
Abstract: Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being its support for long context, faster training, and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[
arXiv:2505.01812v3 Announce Type: replace 
Abstract: Humans and intelligent animals can internalize new information and accurately internalize their implications to perform downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the information (news) is explicitly given as context, adequately integrating the information into model weights via fine-tuning remains challenging. In this paper, we introduce New News, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. First, we demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications, and Self-QA -- designed to distill the knowledge processed by the model with context into the weights of the model, which we term System-2 Fine-tuning (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the Self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news while preserving general capabilities. Furthermore, we discover the contextual shadowing effect, where training with the news in context followed by its rephrases or QAs catastrophically degrades learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation-Guided Consensus Merging for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14009</link>
<guid>https://arxiv.org/abs/2505.14009</guid>
<content:encoded><![CDATA[
arXiv:2505.14009v2 Announce Type: replace 
Abstract: Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%} reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
arXiv:2505.16270v2 Announce Type: replace 
Abstract: Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Principle Discovery for Language Model Self-Improvement</title>
<link>https://arxiv.org/abs/2505.16927</link>
<guid>https://arxiv.org/abs/2505.16927</guid>
<content:encoded><![CDATA[
arXiv:2505.16927v2 Announce Type: replace 
Abstract: When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes that guide model reasoning toward human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ a form of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains that our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</title>
<link>https://arxiv.org/abs/2506.10202</link>
<guid>https://arxiv.org/abs/2506.10202</guid>
<content:encoded><![CDATA[
arXiv:2506.10202v2 Announce Type: replace 
Abstract: Recent approaches have shown impressive proficiency in extracting and leveraging parametric knowledge from Large-Language Models (LLMs) and Vision-Language Models (VLMs). In this work, we consider how we can improve the identification and retrieval of videos related to complex real-world events by automatically extracting latent parametric knowledge about those events. We present Q2E: a Query-to-Event decomposition method for zero-shot multilingual text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our approach demonstrates that we can enhance the understanding of otherwise overly simplified human queries by decomposing the query using the knowledge embedded in LLMs and VLMs. We additionally show how to apply our approach to both visual and speech-based inputs. To combine this varied multimodal knowledge, we adopt entropy-based fusion scoring for zero-shot fusion. Through evaluations on two diverse datasets and multiple retrieval metrics, we demonstrate that Q2E outperforms several state-of-the-art baselines. Our evaluation also shows that integrating audio information can significantly improve text-to-video retrieval. We have released code and data for future research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2508.15793</link>
<guid>https://arxiv.org/abs/2508.15793</guid>
<content:encoded><![CDATA[
arXiv:2508.15793v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including texts, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may undermine LLMs' ability to integrate heterogeneous data impartially, potentially resulting in reasoning errors and increased risks in downstream tasks. Yet it remains unclear whether such biases are systematic, which data-level factors drive them, and what internal mechanisms underlie their emergence.
  In this paper, we present the first comprehensive study of format bias in LLMs through a three-stage empirical analysis. The first stage explores the presence and direction of bias across a diverse range of LLMs. The second stage examines how key data-level factors influence these biases. The third stage analyzes how format bias emerges within LLMs' attention patterns and evaluates a lightweight intervention to test its effectiveness. Our results show that format bias is consistent across model families, driven by information richness, structure quality, and representation type, and is closely associated with attention imbalance within the LLMs. Based on these investigations, we identify three future research directions to reduce format bias: enhancing data pre-processing through format repair and normalization, introducing inference-time interventions such as attention re-weighting, and developing format-balanced training corpora. These directions will support the design of more robust and fair heterogeneous data processing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation</title>
<link>https://arxiv.org/abs/2508.15846</link>
<guid>https://arxiv.org/abs/2508.15846</guid>
<content:encoded><![CDATA[
arXiv:2508.15846v2 Announce Type: replace 
Abstract: As tropical cyclones intensify and track forecasts become increasingly uncertain, U.S. ports face heightened supply-chain risk under extreme weather conditions. Port operators need to rapidly synthesize diverse multimodal forecast products, such as probabilistic wind maps, track cones, and official advisories, into clear, actionable guidance as cyclones approach. Multimodal large language models (MLLMs) offer a powerful means to integrate these heterogeneous data sources alongside broader contextual knowledge, yet their accuracy and reliability in the specific context of port cyclone preparedness have not been rigorously evaluated. To fill this gap, we introduce CyPortQA, the first multimodal benchmark tailored to port operations under cyclone threat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015 through 2023, spanning 145 U.S. principal ports and 90 named storms. Each scenario fuses multisource data (i.e., tropical cyclone products, port operational impact records, and port condition bulletins) and is expanded through an automated pipeline into 117,178 structured question answer pairs. Using this benchmark, we conduct extensive experiments on diverse MLLMs, including both open-source and proprietary model. MLLMs demonstrate great potential in situation understanding but still face considerable challenges in reasoning tasks, including potential impact estimation and decision reasoning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Surface: Probing the Ideological Depth of Large Language Models</title>
<link>https://arxiv.org/abs/2508.21448</link>
<guid>https://arxiv.org/abs/2508.21448</guid>
<content:encoded><![CDATA[
arXiv:2508.21448v2 Announce Type: replace 
Abstract: Large language models (LLMs) display recognizable political leanings, yet they vary significantly in their ability to represent a political orientation consistently. In this paper, we define ideological depth as (i) a model's ability to follow political instructions without failure (steerability), and (ii) the feature richness of its internal political representations measured with sparse autoencoders (SAEs), an unsupervised sparse dictionary learning (SDL) approach. Using Llama-3.1-8B-Instruct and Gemma-2-9B-IT as candidates, we compare prompt-based and activation-steering interventions and probe political features with publicly available SAEs. We find large, systematic differences: Gemma is more steerable in both directions and activates approximately 7.3x more distinct political features than Llama. Furthermore, causal ablations of a small targeted set of Gemma's political features to create a similar feature-poor setting induce consistent shifts in its behavior, with increased rates of refusals across topics. Together, these results indicate that refusals on benign political instructions or prompts can arise from capability deficits rather than safety guardrails. Ideological depth thus emerges as a measurable property of LLMs, and steerability serves as a window into their latent political architecture.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wage Sentiment Indices Derived from Survey Comments via Large Language Models</title>
<link>https://arxiv.org/abs/2509.00290</link>
<guid>https://arxiv.org/abs/2509.00290</guid>
<content:encoded><![CDATA[
arXiv:2509.00290v2 Announce Type: replace 
Abstract: The emergence of generative Artificial Intelligence (AI) has created new opportunities for economic text analysis. This study proposes a Wage Sentiment Index (WSI) constructed with Large Language Models (LLMs) to forecast wage dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS), a monthly survey conducted by the Cabinet Office of Japan that captures real-time economic assessments from workers in industries highly sensitive to business conditions. The WSI extends the framework of the Price Sentiment Index (PSI) used in prior studies, adapting it specifically to wage related sentiment. To ensure scalability and adaptability, a data architecture is also developed that enables integration of additional sources such as newspapers and social media. Experimental results demonstrate that WSI models based on LLMs significantly outperform both baseline approaches and pretrained models. These findings highlight the potential of LLM-driven sentiment indices to enhance the timeliness and effectiveness of economic policy design by governments and central banks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions</title>
<link>https://arxiv.org/abs/2509.15901</link>
<guid>https://arxiv.org/abs/2509.15901</guid>
<content:encoded><![CDATA[
arXiv:2509.15901v2 Announce Type: replace 
Abstract: Meeting summarization with large language models (LLMs) remains error-prone, often producing outputs with hallucinations, omissions, and irrelevancies. We present FRAME, a modular pipeline that reframes summarization as a semantic enrichment task. FRAME extracts and scores salient facts, organizes them thematically, and uses these to enrich an outline into an abstractive summary. To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that has the model build a reasoning trace by answering nine questions before content selection. For evaluation, we propose P-MESA, a multi-dimensional, reference-free evaluation framework to assess if a summary fits a target reader. P-MESA reliably identifies error instances, achieving >= 89% balanced accuracy against human annotations and strongly aligns with human severity ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and omission by 2 out of 5 points (measured with MESA), while SCOPE improves knowledge fit and goal alignment over prompt-only baselines. Our findings advocate for rethinking summarization to improve control, faithfulness, and personalization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title>
<link>https://arxiv.org/abs/2510.06188</link>
<guid>https://arxiv.org/abs/2510.06188</guid>
<content:encoded><![CDATA[
arXiv:2510.06188v2 Announce Type: replace 
Abstract: Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers. Code is available in https://github.com/Jak57/BanglaTalk
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Study of Automatic Evaluation in Sign Language Translation</title>
<link>https://arxiv.org/abs/2510.25434</link>
<guid>https://arxiv.org/abs/2510.25434</guid>
<content:encoded><![CDATA[
arXiv:2510.25434v2 Announce Type: replace 
Abstract: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Latent Reasoning via Looped Language Models</title>
<link>https://arxiv.org/abs/2510.25741</link>
<guid>https://arxiv.org/abs/2510.25741</guid>
<content:encoded><![CDATA[
arXiv:2510.25741v3 Announce Type: replace 
Abstract: Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model is available here: http://ouro-llm.github.io.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion</title>
<link>https://arxiv.org/abs/2403.10568</link>
<guid>https://arxiv.org/abs/2403.10568</guid>
<content:encoded><![CDATA[
arXiv:2403.10568v4 Announce Type: replace-cross 
Abstract: Despite the demonstrated parameter efficiency of prompt-based fusion, its limited adaptivity and expressiveness hinder its effectiveness for multimodal applications at scale. In this paper, we present the first comprehensive study addressing these limitations. Our key motivation is to ``divide and conquer'' the vanilla prompt, traditionally shared across all instances, by generating instance-specific prompts. Specifically, we propose the Mixture of Prompt Experts (MoPE), a framework that significantly enhances prompt adaptivity and expressiveness by dynamically generating instance-specific prompts. MoPE leverages multimodal pairings as additional evidence, allowing the model to adaptively select optimal prompts tailored to each individual instance. Unlike traditional prompt-fusion methods, which encounter scalability bottlenecks when optimizing long unified prompts, MoPE maintains fixed prompt length while effectively scaling the number of specialized experts. Moreover, we investigate regularization terms to encourage expert specialization, resulting in highly adaptive and interpretable prompting. MoPE fundamentally changes the scaling dynamic, unlocking greater expressiveness and adaptability to complex multimodal relationships, enabling the model to selectively attend to task-relevant sub-sequences based on instance-specific multimodal input. Extensive experiments across six multimodal datasets spanning four modalities demonstrate state-of-the-art performance for multimodal fusion, matching or surpassing the performance of fine-tuning while requiring only 0.8% of the trainable parameters. Code is available: https://github.com/songrise/MoPE.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation</title>
<link>https://arxiv.org/abs/2411.16657</link>
<guid>https://arxiv.org/abs/2411.16657</guid>
<content:encoded><![CDATA[
arXiv:2411.16657v4 Announce Type: replace-cross 
Abstract: Storytelling video generation (SVG) aims to produce coherent and visually rich multi-scene videos that follow a structured narrative. Existing methods primarily employ LLM for high-level planning to decompose a story into scene-level descriptions, which are then independently generated and stitched together. However, these approaches struggle with generating high-quality videos aligned with the complex single-scene description, as visualizing such complex description involves coherent composition of multiple characters and events, complex motion synthesis and multi-character customization. To address these challenges, we propose DREAMRUNNER, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout planning. Next, DREAMRUNNER presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame spatial-temporal semantic control. We compare DREAMRUNNER with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DREAMRUNNER exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DREAMRUNNER's robust ability to generate multi-object interactions with qualitative examples.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[
arXiv:2502.03460v3 Announce Type: replace-cross 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA</title>
<link>https://arxiv.org/abs/2503.11880</link>
<guid>https://arxiv.org/abs/2503.11880</guid>
<content:encoded><![CDATA[
arXiv:2503.11880v3 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) in federated settings enables privacy-preserving adaptation but suffers from cross-client interference due to model aggregation. Existing federated LoRA fine-tuning methods, primarily based on FedAvg, struggle with data heterogeneity, leading to harmful cross-client interference and suboptimal personalization. In this work, we propose \textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that fundamentally departs from FedAvg. Instead of using an aggregated model to initialize local training, each client continues training its individual LoRA while incorporating shared knowledge through a separate Rest-of-World (RoW) LoRA component. To effectively balance local adaptation and global information, FedALT introduces an adaptive mixer that dynamically learns input-specific weightings between the individual and RoW LoRA components, drawing conceptual foundations from the Mixture-of-Experts (MoE) paradigm. Through extensive experiments on NLP benchmarks, we demonstrate that FedALT significantly outperforms state-of-the-art personalized federated LoRA fine-tuning methods, achieving superior local adaptation without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable LLM Guardrails via Sparse Representation Steering</title>
<link>https://arxiv.org/abs/2503.16851</link>
<guid>https://arxiv.org/abs/2503.16851</guid>
<content:encoded><![CDATA[
arXiv:2503.16851v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit impressive capabilities in generation tasks but are prone to producing harmful, misleading, or biased content, posing significant ethical and safety concerns. To mitigate such risks, representation engineering, which steer model behavior toward desired attributes by injecting carefully designed steering vectors into LLM's representations at inference time, has emerged as a promising alternative to fine-tuning approaches. However, due to the semantically entangled nature of LLM's representation, existing representation engineering methods still suffer from several limitations: limited fine-grained controllability, content quality degradation, and conflict in multi-attribute control. To overcome these challenges, we propose Sparse Representation Steering (SRS), a novel framework that achieves fine-grained and interpretable control over LLM behavior by first disentangling internal activations into a sparse, semantically meaningful representation space, and then selectively steering relevant dimensions. Specifically, SRS leverages a pretrained Sparse Autoencoder (SAE) to transform dense, entangled activation patterns into a sparse monosemantic feature space. To identify relevant features, SRS contrasts sparse activations from positive and negative prompt pairs and measures their bidirectional KL divergence to locate dimensions most associated with the target attribute. We conduct comprehensive experiments on Gemma-2 series model across three alignment dimensions, i.e., safety, fairness, and truthfulness, to evaluate the effectiveness of SRS. Results show that SRS consistently outperforms existing steering methods, which achieves significantly improved controllability across both single and multiple attribute settings, while preserving high linguistic quality and general ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2506.06071</link>
<guid>https://arxiv.org/abs/2506.06071</guid>
<content:encoded><![CDATA[
arXiv:2506.06071v2 Announce Type: replace-cross 
Abstract: Bias in speech emotion recognition (SER) systems often stems from spurious correlations between speaker characteristics and emotional labels, leading to unfair predictions across demographic groups. Many existing debiasing methods require model-specific changes or demographic annotations, limiting their practical use. We present CO-VADA, a Confidence-Oriented Voice Augmentation Debiasing Approach that mitigates bias without modifying model architecture or relying on demographic information. CO-VADA identifies training samples that reflect bias patterns present in the training data and then applies voice conversion to alter irrelevant attributes and generate samples. These augmented samples introduce speaker variations that differ from dominant patterns in the data, guiding the model to focus more on emotion-relevant features. Our framework is compatible with various SER models and voice conversion tools, making it a scalable and practical solution for improving fairness in SER systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First-Order Error Matters: Accurate Compensation for Quantized Large Language Models</title>
<link>https://arxiv.org/abs/2507.11017</link>
<guid>https://arxiv.org/abs/2507.11017</guid>
<content:encoded><![CDATA[
arXiv:2507.11017v2 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by performing a first-order Taylor expansion around the pre-quantization weights. This yields an approximation based on the difference between latent and full-precision weights as well as the Hessian matrix. When substituted into the theoretical solution, the formulation eliminates the need to explicitly compute the Hessian, thereby avoiding the high computational cost and limited generalization of backpropagation-based gradient methods. This design introduces only minimal additional computational overhead. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 17.3% and increases the 5-shot MMLU accuracy from 53.8% achieved by GPTAQ to 56.1%. Moreover, FOEM can be seamlessly combined with advanced techniques such as SpinQuant, delivering additional gains under the challenging W4A4KV4 setting and further narrowing the performance gap with full-precision baselines, surpassing existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning</title>
<link>https://arxiv.org/abs/2508.05129</link>
<guid>https://arxiv.org/abs/2508.05129</guid>
<content:encoded><![CDATA[
arXiv:2508.05129v2 Announce Type: replace-cross 
Abstract: With the rapid and continuous increase in academic publications, identifying high-quality research has become an increasingly pressing challenge. While recent methods leveraging Large Language Models (LLMs) for automated paper evaluation have shown great promise, they are often constrained by outdated domain knowledge and limited reasoning capabilities. In this work, we present PaperEval, a novel LLM-based framework for automated paper evaluation that addresses these limitations through two key components: 1) a domain-aware paper retrieval module that retrieves relevant concurrent work to support contextualized assessments of novelty and contributions, and 2) a latent reasoning mechanism that enables deep understanding of complex motivations and methodologies, along with comprehensive comparison against concurrently related work, to support more accurate and reliable evaluation. To guide the reasoning process, we introduce a progressive ranking optimization strategy that encourages the LLM to iteratively refine its predictions with an emphasis on relative comparison. Experiments on two datasets demonstrate that PaperEval consistently outperforms existing methods in both academic impact and paper quality evaluation. In addition, we deploy PaperEval in a real-world paper recommendation system for filtering high-quality papers, which has gained strong engagement on social media -- amassing over 8,000 subscribers and attracting over 10,000 views for many filtered high-quality papers -- demonstrating the practical effectiveness of PaperEval.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title>
<link>https://arxiv.org/abs/2509.20379</link>
<guid>https://arxiv.org/abs/2509.20379</guid>
<content:encoded><![CDATA[
arXiv:2509.20379v2 Announce Type: replace-cross 
Abstract: Hallucinations of vision-language models (VLMs), which are misalignments between visual content and generated text, undermine the reliability of VLMs. One common approach for detecting them employs the same VLM, or a different one, to assess generated outputs. This process is computationally intensive and increases model latency. In this paper, we explore an efficient on-the-fly method for hallucination detection by training traditional ML models over signals based on the VLM's next-token probabilities (NTPs). NTPs provide a direct quantification of model uncertainty. We hypothesize that high uncertainty (i.e., a low NTP value) is strongly associated with hallucinations. To test this, we introduce a dataset of 1,400 human-annotated statements derived from VLM-generated content, each labeled as hallucinated or not, and use it to test our NTP-based lightweight method. Our results demonstrate that NTP-based features are valuable predictors of hallucinations, enabling fast and simple ML models to achieve performance comparable to that of strong VLMs. Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding only the generated text back into the VLM, enhances hallucination detection performance. Finally, integrating hallucination prediction scores from VLMs into the NTP-based models led to better performance than using either VLMs or NTPs alone. We hope this study paves the way for simple, lightweight solutions that enhance the reliability of VLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input</title>
<link>https://arxiv.org/abs/2510.16926</link>
<guid>https://arxiv.org/abs/2510.16926</guid>
<content:encoded><![CDATA[
arXiv:2510.16926v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X</title>
<link>https://arxiv.org/abs/2510.25932</link>
<guid>https://arxiv.org/abs/2510.25932</guid>
<content:encoded><![CDATA[
arXiv:2510.25932v2 Announce Type: replace-cross 
Abstract: Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenization, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the device. FakeZero employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantization. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance loss. By providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages</title>
<link>https://arxiv.org/abs/2511.09690</link>
<guid>https://arxiv.org/abs/2511.09690</guid>
<content:encoded><![CDATA[
<div> Keywords: Omnilingual ASR, low-resource languages, self-supervised pre-training, zero-shot generalization, open-source

<br /><br />Summary:  
The paper presents Omnilingual ASR, a pioneering large-scale automatic speech recognition system designed for extensibility to support over 1,600 languages, including more than 500 previously unserved by ASR technology. It addresses the challenge of expanding ASR to low-resource and long-tail languages by allowing communities to add new languages with only a few data samples. The system utilizes self-supervised pre-training scaled up to 7 billion parameters to develop robust speech representations, alongside an encoder-decoder architecture with a large language model (LLM)-inspired decoder that supports zero-shot generalization to unseen languages. This approach is underpinned by training on a massive, diverse speech corpus sourced from public data and community contributions through compensated local partnerships, ensuring broad linguistic coverage. Evaluations demonstrate substantial performance improvements over previous ASR systems, particularly in low-resource conditions, highlighting the system’s strong generalization capabilities. Omnilingual ASR is released as a model family ranging from 300 million parameters for low-power devices to 7 billion for maximum accuracy. The paper also discusses the ethical considerations involved, emphasizing the importance of community collaboration and open-sourcing models and tools to lower barriers for researchers and language communities, thereby fostering inclusive participation and societal benefits. <div>
arXiv:2511.09690v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: Rethinking Prompt Construction in In-Context Learning</title>
<link>https://arxiv.org/abs/2511.09700</link>
<guid>https://arxiv.org/abs/2511.09700</guid>
<content:encoded><![CDATA[
<div> In-context learning, example selection, example ordering, prompt design, large language models<br /><br />Summary:<br /><br />1. In-context learning (ICL) allows large language models to learn new tasks by conditioning on sequences of examples. 2. Previous research assumed that the choice of examples (selection) has a greater impact on model performance than the order in which these examples appear, leading to a focus primarily on selection strategies. 3. This paper challenges that assumption by systematically comparing the effects of example selection and example ordering across classification and generation tasks. 4. Experiments were conducted using multiple open-source language models ranging from 0.5 billion to 27 billion parameters, as well as the GPT-5 model. 5. Results show that the variation in performance caused by different orderings of the same example set is comparable to the variation caused by using completely different example sets. 6. The research also demonstrates that good example orderings can be identified using only a development set, achieving performance close to an oracle method that uses test labels to pick the best ordering. 7. These findings emphasize that example ordering is as important as example selection and that both factors are closely intertwined in prompt design. 8. The work calls for a reconsideration of prior assumptions in ICL and encourages the development of new strategies that optimize both example selection and ordering for improved model performance. <div>
arXiv:2511.09700v1 Announce Type: new 
Abstract: In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual morphologically-guided tokenization for Latin encoder models</title>
<link>https://arxiv.org/abs/2511.09709</link>
<guid>https://arxiv.org/abs/2511.09709</guid>
<content:encoded><![CDATA[
<div> Keywords: tokenization, morphological alignment, Latin, language modeling, low-resource languages<br /><br />Summary:<br /><br />1. Tokenization is a fundamental step in language model pretraining, but standard methods typically focus on information-theoretic objectives like compression and fertility rather than aligning with linguistic morphology.<br />2. This mismatch in tokenization quality is particularly problematic for morphologically rich languages, where it negatively affects performance on downstream NLP tasks.<br />3. The study explores morphologically-aware tokenization specifically for Latin, a morphologically complex language with moderate amounts of pretraining data but substantial curated lexical resources.<br />4. Incorporating morphological knowledge into tokenization leads to improved performance across four downstream tasks, with the greatest gains observed on out-of-domain text, suggesting enhanced generalization abilities of the models.<br />5. The findings highlight the value of leveraging existing linguistic resources as a viable strategy to improve language model performance for low-resource languages, which often lack large-scale pretraining corpora but may have quality lexical databases.<br /><br />This research advocates for the development and integration of morphologically-informed tokenization schemes as a practical means to overcome data scarcity challenges in language modeling for morphologically rich, low-resource languages. <div>
arXiv:2511.09709v1 Announce Type: new 
Abstract: Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives</title>
<link>https://arxiv.org/abs/2511.09738</link>
<guid>https://arxiv.org/abs/2511.09738</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, Presidential Directives, Topic Extraction, Social Science, AI Validation

<br /><br />Summary:  
1. The research explores the application of Natural Language Processing (NLP) to extract main topics from extensive written data, focusing on identifying signaling themes in Presidential Directives (PDs) spanning the Reagan to Clinton administrations.  
2. Both human analysts and NLP methods successfully identified relevant documents, highlighting the promising utility of NLP in analyzing large textual corpora for social science research.  
3. Despite these successes, there were notable discrepancies between NLP-generated results and human-labeled data, pointing to limitations that necessitate further investigation and refinement of NLP tools for this specific use case.  
4. The study was conducted in 2023, acknowledging that the rapid advancement in AI and ML tools means that this research utilized potentially outdated technology, underscoring challenges and opportunities in applying evolving AI methods to social science.  
5. Overall, the findings demonstrate both the potential and the current limitations of NLP in the extraction of thematic content from historical political documents, calling for ongoing research to validate and improve AI applications in the social sciences. <div>
arXiv:2511.09738v1 Announce Type: new 
Abstract: Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</title>
<link>https://arxiv.org/abs/2511.09748</link>
<guid>https://arxiv.org/abs/2511.09748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Critical Error Detection, machine translation, model efficiency, English-German translation<br /><br />Summary: This study investigates the minimum size of Large Language Models (LLMs) required to effectively detect meaning-altering errors in machine translation from English to German, focusing on Critical Error Detection (CED). The research benchmarks four sub-2 billion parameter models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across several datasets including WMT21, WMT22, and SynCED-EnDe-2025. The authors propose a standardized evaluation framework that utilizes consistent prompts, lightweight logit-bias calibration, and majority voting to assess model performance. The results indicate an optimal trade-off around one billion parameters, with the Gemma-3-1B model achieving the best balance between quality and efficiency. This model reached a Matthews correlation coefficient (MCC) of 0.77 and an F1-ERR score of 0.98 on SynCED-EnDe-2025 after fine-tuning with merged weights, while maintaining a low latency of 400 ms per sample on a MacBook Pro M4 Pro. Larger models such as Qwen-3-1.7B achieve higher absolute accuracy but at increased computational cost. Smaller models (0.6B) remain viable with few-shot calibration but struggle with certain error types. Overall, compact, instruction-tuned LLMs supplemented with lightweight calibration and minimal supervision offer a promising solution for private, efficient, on-device error detection in MT workflows, with all materials made publicly available on GitHub. <div>
arXiv:2511.09748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicate-Argument Structure Divergences in Chinese and English Parallel Sentences and their Impact on Language Transfer</title>
<link>https://arxiv.org/abs/2511.09796</link>
<guid>https://arxiv.org/abs/2511.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-lingual NLP, predicate-argument structure, annotation projection, language transfer, structural divergences<br /><br />Summary:<br /><br />This paper addresses the challenges in cross-lingual natural language processing (NLP), particularly focusing on transferring linguistic knowledge between typologically distant languages like Chinese and English. It emphasizes the importance of predicate-argument structures as a linguistic unit to analyze cross-lingual alignment and misalignment in parallel sentences. The study conducts both qualitative and quantitative analyses of annotations projected from one language to the other, investigating how well predicate annotations align between the two languages. A categorization of structural divergences is proposed to better understand the types of misalignments present. Key findings reveal that language transfer is asymmetric, meaning that the direction of transfer (Chinese to English versus English to Chinese) significantly affects the quality and results of annotation projection. This asymmetry highlights the need to carefully select the source language in transfer learning setups, as it impacts the effectiveness of cross-lingual NLP applications. The paper advocates for further investigation into this asymmetry before making broad scientific claims about cross-lingual transfer methodologies, thus contributing a nuanced perspective on the complexities underlying multilingual language processing. <div>
arXiv:2511.09796v1 Announce Type: new 
Abstract: Cross-lingual Natural Language Processing (NLP) has gained significant traction in recent years, offering practical solutions in low-resource settings by transferring linguistic knowledge from resource-rich to low-resource languages. This field leverages techniques like annotation projection and model transfer for language adaptation, supported by multilingual pre-trained language models. However, linguistic divergences hinder language transfer, especially among typologically distant languages. In this paper, we present an analysis of predicate-argument structures in parallel Chinese and English sentences. We explore the alignment and misalignment of predicate annotations, inspecting similarities and differences and proposing a categorization of structural divergences. The analysis and the categorization are supported by a qualitative and quantitative analysis of the results of an annotation projection experiment, in which, in turn, one of the two languages has been used as source language to project annotations into the corresponding parallel sentences. The results of this analysis show clearly that language transfer is asymmetric. An aspect that requires attention when it comes to selecting the source language in transfer learning applications and that needs to be investigated before any scientific claim about cross-lingual NLP is proposed.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG</title>
<link>https://arxiv.org/abs/2511.09803</link>
<guid>https://arxiv.org/abs/2511.09803</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Adaptive Retrieval Gating, Uncertainty Scores, Latency Reduction, Instruction-Tuned LLMs<br /><br />Summary:<br /><br />1. Retrieval-Augmented Generation (RAG) enhances factual accuracy but frequently retrieving for every query increases token usage and latency, which can hurt quality.  
2. The authors propose Training-free Adaptive Retrieval Gating (TARG), a single-shot, training-free policy that decides when to retrieve based on a short, no-context draft generated by the base model.  
3. TARG uses lightweight uncertainty scores derived from the draft's prefix logits: mean token entropy, a margin signal computed from the top-1/top-2 logit gap via a monotone link, or small-N variance from multiple stochastic prefixes. Retrieval is triggered only if the uncertainty score crosses a threshold.  
4. TARG is model-agnostic, introduces minimal token overhead, requires no extra training or auxiliary heads, and effectively balances accuracy and efficiency.  
5. Evaluation on datasets including NQ-Open, TriviaQA, and PopQA shows that TARG matches or exceeds the accuracy (EM/F1) of Always-RAG while reducing retrieval calls by 70-90% and decreasing end-to-end latency, with overhead close to Never-RAG. Additionally, the margin signal is the most robust default under instruction-tuned large language models, while small-N variance offers a conservative option prioritizing retrieval budget. Ablations demonstrate trade-offs between gate types and prefix lengths, clarifying latency-budget dynamics. <div>
arXiv:2511.09803v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft's prefix logits, TARG computes lightweight uncertainty scores: mean token entropy, a margin signal derived from the top-1/top-2 logit gap via a monotone link, or small-N variance across a handful of stochastic prefixes, and triggers retrieval only when the score exceeds a threshold. The gate is model agnostic, adds only tens to hundreds of draft tokens, and requires no additional training or auxiliary heads. On NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy-efficiency frontier: compared with Always-RAG, TARG matches or improves EM/F1 while reducing retrieval by 70-90% and cutting end-to-end latency, and it remains close to Never-RAG in overhead. A central empirical finding is that under modern instruction-tuned LLMs the margin signal is a robust default (entropy compresses as backbones sharpen), with small-N variance offering a conservative, budget-first alternative. We provide ablations over gate type and prefix length and use a delta-latency view to make budget trade-offs explicit.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Khmer Spellchecking: A Holistic Approach</title>
<link>https://arxiv.org/abs/2511.09812</link>
<guid>https://arxiv.org/abs/2511.09812</guid>
<content:encoded><![CDATA[
<div> Keywords: Khmer spellchecking, subword segmentation, named-entity recognition, grapheme-to-phoneme conversion, language model<br /><br />Summary:<br /><br />This paper addresses the unresolved problem of spellchecking for the Khmer language by identifying several key challenges: misalignments between the lexicon and word segmentation model, variable word forms, loosely formed compound words absent in lexicons, and the lack of a Khmer named-entity recognition (NER) model which leads to false misspelling flags for proper nouns. Existing solutions do not sufficiently tackle these issues. To overcome this, the authors propose a holistic approach that integrates multiple components specifically tailored for Khmer: subword segmentation to better handle word boundaries, a Khmer NER system to identify proper nouns accurately, grapheme-to-phoneme (G2P) conversion to improve candidate generation, and a Khmer language model to rank correction candidates effectively. Through the combination of these elements, the approach aims to identify and suggest more suitable spelling corrections. Experimental results demonstrate that this integrated technique achieves a state-of-the-art accuracy of up to 94.4%, outperforming previous methods. Additionally, the study contributes to the Khmer language processing community by planning to release benchmark datasets for both spellchecking and NER tasks, enabling further research and development in this area. <div>
arXiv:2511.09812v1 Announce Type: new 
Abstract: Compared to English and other high-resource languages, spellchecking for Khmer remains an unresolved problem due to several challenges. First, there are misalignments between words in the lexicon and the word segmentation model. Second, a Khmer word can be written in different forms. Third, Khmer compound words are often loosely and easily formed, and these compound words are not always found in the lexicon. Fourth, some proper nouns may be flagged as misspellings due to the absence of a Khmer named-entity recognition (NER) model. Unfortunately, existing solutions do not adequately address these challenges. This paper proposes a holistic approach to the Khmer spellchecking problem by integrating Khmer subword segmentation, Khmer NER, Khmer grapheme-to-phoneme (G2P) conversion, and a Khmer language model to tackle these challenges, identify potential correction candidates, and rank the most suitable candidate. Experimental results show that the proposed approach achieves a state-of-the-art Khmer spellchecking accuracy of up to 94.4%, compared to existing solutions. The benchmark datasets for Khmer spellchecking and NER tasks in this study will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Graduate Outcomes by Identifying Skills Gaps and Recommending Courses Based on Career Interests</title>
<link>https://arxiv.org/abs/2511.09819</link>
<guid>https://arxiv.org/abs/2511.09819</guid>
<content:encoded><![CDATA[
<div> Keywords: Course recommendation system, machine learning, data analytics, collaborative filtering, user interface<br /><br />Summary:<br /><br />This paper presents the design and development of a course recommendation system aimed at helping students select courses that align with industry trends and requirements. The system integrates various data analytics techniques and machine learning algorithms to provide tailored course suggestions based on individual preferences and academic criteria. It incorporates data mining and collaborative filtering methods to analyze previous courses taken and students' career goals for more personalized recommendations. A significant focus is placed on creating an intuitive and user-friendly front-end interface that emphasizes visual clarity, interaction, and simplicity through iterative prototyping and feedback-based refinements. The design process prioritizes a smooth and engaging user experience to enhance accessibility and usefulness. The system is continuously optimized by incorporating user feedback to better address the needs and preferences of its intended audience. This course recommendation system serves as a valuable tool for students, instructors, and career advisors by bridging the gap between university education and industry demands, thereby promoting lifelong learning and professional advancement. Ultimately, it aims to empower university students with data-driven, industry-informed course choices that contribute to improved graduate outcomes in the higher education sector. <div>
arXiv:2511.09819v1 Announce Type: new 
Abstract: This paper aims to address the challenge of selecting relevant courses for students by proposing the design and development of a course recommendation system. The course recommendation system utilises a combination of data analytics techniques and machine learning algorithms to recommend courses that align with current industry trends and requirements. In order to provide customised suggestions, the study entails the design and implementation of an extensive algorithmic framework that combines machine learning methods, user preferences, and academic criteria. The system employs data mining and collaborative filtering techniques to examine past courses and individual career goals in order to provide course recommendations. Moreover, to improve the accessibility and usefulness of the recommendation system, special attention is given to the development of an easy-to-use front-end interface. The front-end design prioritises visual clarity, interaction, and simplicity through iterative prototyping and user input revisions, guaranteeing a smooth and captivating user experience. We refined and optimised the proposed system by incorporating user feedback, ensuring that it effectively meets the needs and preferences of its target users. The proposed course recommendation system could be a useful tool for students, instructors, and career advisers to use in promoting lifelong learning and professional progression as it fills the gap between university learning and industry expectations. We hope that the proposed course recommendation system will help university students in making data-drive and industry-informed course decisions, in turn, improving graduate outcomes for the university sector.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM</title>
<link>https://arxiv.org/abs/2511.09831</link>
<guid>https://arxiv.org/abs/2511.09831</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.09831v1  
Keywords: Large Language Model, Retrieval Augmented Generation, Question Answering, Fine-tuning, Hallucination  

<br /><br />Summary:  
This paper addresses the challenges in course forums where a growing number of student queries lead to delayed responses and repetitive questions for instructors. To solve these problems, the authors propose a question answering system based on an open source Large Language Model (LLM) enhanced with a Retrieval Augmented Generation (RAG) method. The system is fine-tuned using a relevant course dataset to better handle domain-specific queries. It uses a local knowledge base, containing all course content, to retrieve relevant documents corresponding to student questions. This retrieval step improves the accuracy and contextual relevance of the answers. To tackle the problem of hallucination — where LLMs generate plausible but incorrect answers — the system incorporates multi chain-of-thought reasoning, a technique designed to reduce such errors. The proposed approach is experimentally evaluated on the HotpotQA dataset, demonstrating strong performance on the question answering task. Overall, the combination of fine-tuning, document retrieval via RAG, and multi-step reasoning provides an effective solution to enhance timely and accurate responses in educational course forums. <div>
arXiv:2511.09831v1 Announce Type: new 
Abstract: The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students' queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students' queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain</title>
<link>https://arxiv.org/abs/2511.09854</link>
<guid>https://arxiv.org/abs/2511.09854</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Terminology Adaptation, Contrastive Learning, Legal and Financial Domains, Term Discrimination<br /><br />Summary:<br /><br />Large language models (LLMs) demonstrate strong performance in text generation but struggle with isotropy issues in their embedding spaces, leading to poor discrimination of domain-specific terminology. This limitation is particularly critical in specialized fields like legal and financial domains, where nuanced semantic understanding is essential for downstream applications such as legal judgment prediction and financial risk analysis. To overcome this challenge, the paper proposes TermGPT, a novel multi-level contrastive fine-tuning framework aimed at improving terminology adaptation. TermGPT begins by constructing a sentence graph that captures both semantic and structural relationships in text, enabling the generation of positive and negative samples that are semantically consistent yet discriminative, guided by contextual and topological information. The framework employs multi-level contrastive learning, operating at both sentence and token levels, to simultaneously enhance global contextual comprehension and fine-grained terminology discrimination. To facilitate rigorous assessment, the authors create the first financial terminology dataset based on official regulatory documents, enabling domain-relevant evaluation. Experimental results show that TermGPT surpasses existing baseline models in tasks involving term discrimination within financial and legal texts, indicating its effectiveness in addressing terminology representation issues in specialized domains. <div>
arXiv:2511.09854v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback</title>
<link>https://arxiv.org/abs/2511.09865</link>
<guid>https://arxiv.org/abs/2511.09865</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, chain-of-thought reasoning, token-level exploration, self-feedback, cross-domain transfer<br /><br />Summary:<br /><br />1. The paper addresses the challenge of training Large Language Models (LLMs) for chain-of-thought reasoning, where traditional supervised fine-tuning on a single "golden" rationale limits generalization by penalizing valid alternative answers. 2. Reinforcement learning methods with verifiable rewards face difficulties with credit assignment and require high computational costs, motivating the need for a new approach. 3. The authors propose InTRO (In-Token Rationality Optimization), a framework that facilitates token-level exploration combined with self-generated feedback, improving the accuracy and conciseness of reasoning chains. 4. InTRO works by estimating token-wise importance weights, called correction factors, based on the information discrepancy between the generative policy and an answer-conditioned policy, allowing informed next-token selection within a single forward pass. 5. Experimental results demonstrate that InTRO consistently improves solution accuracy by up to 20% compared to baseline models across six math-reasoning benchmarks, while producing notably more concise and less verbose chains of thought. 6. Additionally, InTRO shows strong cross-domain generalization, successfully adapting to out-of-domain reasoning tasks beyond mathematics, highlighting its broad applicability and robust performance. <div>
arXiv:2511.09865v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) for chain-of-thought reasoning presents a significant challenge: supervised fine-tuning on a single "golden" rationale hurts generalization as it penalizes equally valid alternatives, whereas reinforcement learning with verifiable rewards struggles with credit assignment and prohibitive computational cost. To tackle these limitations, we introduce InTRO (In-Token Rationality Optimization), a new framework that enables both token-level exploration and self-feedback for accurate and concise reasoning. Instead of directly optimizing an intractable objective over all valid reasoning paths, InTRO leverages correction factors-token-wise importance weights estimated by the information discrepancy between the generative policy and its answer-conditioned counterpart, for informative next token selection. This approach allows the model to perform token-level exploration and receive self-generated feedback within a single forward pass, ultimately encouraging accurate and concise rationales. Across six math-reasoning benchmarks, InTRO consistently outperforms other baselines, raising solution accuracy by up to 20% relative to the base model. Its chains of thought are also notably more concise, exhibiting reduced verbosity. Beyond this, InTRO enables cross-domain transfer, successfully adapting to out-of-domain reasoning tasks that extend beyond the realm of mathematics, demonstrating robust generalization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09873</link>
<guid>https://arxiv.org/abs/2511.09873</guid>
<content:encoded><![CDATA[
<div> Keywords: HierRouter, hierarchical routing, large language models, reinforcement learning, cost-efficient inference<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) achieve state-of-the-art results but are resource-intensive, posing challenges for deployment in constrained environments.  
2. The paper introduces HierRouter, a hierarchical routing framework that dynamically composes inference pipelines from multiple specialized, lightweight LLMs to optimize performance and cost.  
3. The routing problem is modeled as a finite-horizon Markov Decision Process (MDP), with a reinforcement learning agent trained using Proximal Policy Optimization (PPO) to select models iteratively at each multi-hop inference step.  
4. The agent makes decisions based on the evolving context and accumulated computational cost, enabling context-aware routing to balance quality and efficiency.  
5. Experiments conducted on six benchmarks, including question answering, code generation, and mathematical reasoning tasks, demonstrate that HierRouter enhances response quality by up to 2.4 times over using single models, with only minimal additional inference cost.  
6. The approach shows strong potential for enabling cost-efficient, high-performance LLM inference in real-time and resource-limited scenarios.  
7. The implementation and code are publicly available at https://github.com/Nikunj-Gupta/hierouter. <div>
arXiv:2511.09873v1 Announce Type: new 
Abstract: Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models</title>
<link>https://arxiv.org/abs/2511.09880</link>
<guid>https://arxiv.org/abs/2511.09880</guid>
<content:encoded><![CDATA[
<div> Safety alignment, Neural Tangent Kernel, fine-tuning, large language models, jailbreaking resistance<br /><br />Summary:<br /><br />Many machine learning models fine-tuned from large language models (LLMs) suffer from systematic degradation in safety alignment, leading to ethical risks and increased harmful outputs. EnchTable is introduced as a new framework that transfers and maintains safety alignment in downstream LLMs without requiring extensive retraining. The framework uses a Neural Tangent Kernel (NTK)-based safety vector distillation method, which effectively decouples safety constraints from task-specific reasoning, allowing broad compatibility across various architectures and sizes. It also incorporates an interference-aware merging technique to balance safety and utility, reducing performance compromises across different task domains. A fully functional prototype of EnchTable was implemented on three diverse task domains and LLM architectures, evaluated extensively on eleven datasets for utility and safety. EnchTable demonstrated strong generalization across models from multiple vendors. It also showed robust resistance to both static and dynamic jailbreaking attacks, outperforming vendor safety models in mitigating adversarial prompts. Compared to six parameter modification methods and two inference-time alignment baselines, EnchTable achieved lower unsafe rates, higher utility scores, and universal applicability. Finally, EnchTable integrates seamlessly into deployment pipelines without adding significant operational overhead. <div>
arXiv:2511.09880v1 Announce Type: new 
Abstract: Many machine learning models are fine-tuned from large language models (LLMs) to achieve high performance in specialized domains like code generation, biomedical analysis, and mathematical problem solving. However, this fine-tuning process often introduces a critical vulnerability: the systematic degradation of safety alignment, undermining ethical guidelines and increasing the risk of harmful outputs. Addressing this challenge, we introduce EnchTable, a novel framework designed to transfer and maintain safety alignment in downstream LLMs without requiring extensive retraining. EnchTable leverages a Neural Tangent Kernel (NTK)-based safety vector distillation method to decouple safety constraints from task-specific reasoning, ensuring compatibility across diverse model architectures and sizes. Additionally, our interference-aware merging technique effectively balances safety and utility, minimizing performance compromises across various task domains. We implemented a fully functional prototype of EnchTable on three different task domains and three distinct LLM architectures, and evaluated its performance through extensive experiments on eleven diverse datasets, assessing both utility and model safety. Our evaluations include LLMs from different vendors, demonstrating EnchTable's generalization capability. Furthermore, EnchTable exhibits robust resistance to static and dynamic jailbreaking attacks, outperforming vendor-released safety models in mitigating adversarial prompts. Comparative analyses with six parameter modification methods and two inference-time alignment baselines reveal that EnchTable achieves a significantly lower unsafe rate, higher utility score, and universal applicability across different task domains. Additionally, we validate EnchTable can be seamlessly integrated into various deployment pipelines without significant overhead.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HI-TransPA: Hearing Impairments Translation Personal Assistant</title>
<link>https://arxiv.org/abs/2511.09915</link>
<guid>https://arxiv.org/abs/2511.09915</guid>
<content:encoded><![CDATA[
<div> Keywords: Omni-Model, assistive technology, hearing-impaired, audio-visual, curriculum learning<br /><br />Summary:<br /><br />This article introduces HI-TransPA, an instruction-driven audio-visual personal assistant designed to enhance daily communication for hearing-impaired individuals by leveraging the Omni-Model paradigm. The proposed model effectively fuses indistinct speech with high-frame-rate lip dynamics, allowing for both translation and dialogue within a unified multimodal framework. To address the challenges posed by noisy, heterogeneous data and the limited adaptability of current Omni-Models to hearing-impaired speech, a comprehensive preprocessing and curation pipeline is developed. This pipeline detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses the quality of multimodal samples. The derived quality scores inform a curriculum learning strategy that initially trains on clean, high-confidence data and gradually introduces more challenging cases to improve model robustness. Additionally, the study incorporates a SigLIP encoder combined with a Unified 3D-Resampler to efficiently represent high-frame-rate lip motions. Experimental results on the HI-Dialogue dataset demonstrate that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. Overall, this work lays a foundational framework for applying Omni-Models in assistive communication technologies and provides essential tools and methodologies for future research in this domain. <div>
arXiv:2511.09915v1 Announce Type: new 
Abstract: To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection</title>
<link>https://arxiv.org/abs/2511.09918</link>
<guid>https://arxiv.org/abs/2511.09918</guid>
<content:encoded><![CDATA[
<div> Keywords: social norms, multi-turn dialogues, Norm-RAG, multilingual dataset, norm adherence  

<br /><br />Summary:  
This paper addresses the challenge of socially normative reasoning in conversational AI, emphasizing the subjective, context-dependent, and culturally diverse nature of social norms that traditional commonsense models do not capture. It introduces Norm-RAG, a retrieval-augmented agentic framework designed to infer nuanced social norms in multi-turn dialogues by modeling utterance-level attributes such as communicative intent, speaker roles, interpersonal framing, and linguistic cues. Norm-RAG leverages a novel Semantic Chunking approach to retrieve and ground dialogue analysis in structured normative documentation, enabling interpretable and context-aware assessments of normative adherence and violations. Additionally, the work contributes MINDS (Multilingual Interactions with Norm-Driven Speech), a new bilingual dataset containing 31 Mandarin-English and Spanish-English conversations annotated at each turn for norm category and adherence status, reflecting realistic and culturally diverse social interactions. Experimental results show that Norm-RAG enhances norm detection accuracy and generalizes better across cultural contexts, improving the capabilities of dialogue systems to act socially intelligent and culturally adaptive in multilingual conversation settings. This approach moves beyond isolated utterance analysis by handling the fluid and multi-turn nature of conversations with nuanced norm reasoning. <div>
arXiv:2511.09918v1 Announce Type: new 
Abstract: Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Identifying Knowledge Components</title>
<link>https://arxiv.org/abs/2511.09935</link>
<guid>https://arxiv.org/abs/2511.09935</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Components, Large Language Models, KC merging, cosine similarity, adaptive learning systems  

<br /><br />Summary:  
This study investigates the automation of Knowledge Component (KC) identification for adaptive learning systems using Large Language Models (LLMs), specifically GPT-4o-mini. The researchers scaled a "simulated textbook" prompting strategy to analyze a larger dataset of 646 multiple-choice questions. Initial results showed that the LLM-generated KCs performed worse than a domain expert-created KC model, with an RMSE of 0.4285 compared to 0.4206, while also producing an excessive number of KCs (569 versus 101). To address the problem of redundancy and overgeneration, the paper proposes a novel semantic merging approach that clusters semantically similar KCs based on their cosine similarity scores. Applying a cosine similarity threshold of 0.8 to merge KCs reduced their count from 569 to 428 and improved model accuracy, lowering the RMSE to 0.4259. The findings indicate that purely scaled LLM generation is insufficient for effective KC identification, but integrating semantic similarity-based merging offers a promising solution to automate and refine this process in adaptive learning contexts. This work advances the automation of educational content modeling by combining generation and post-processing techniques to reduce noise and redundancy in KC labels. <div>
arXiv:2511.09935v1 Announce Type: new 
Abstract: Knowledge Components (KCs) are foundational to adaptive learning systems, but their manual identification by domain experts is a significant bottleneck. While Large Language Models (LLMs) offer a promising avenue for automating this process, prior research has been limited to small datasets and has been shown to produce superfluous, redundant KC labels. This study addresses these limitations by first scaling a "simulated textbook" LLM prompting strategy (using GPT-4o-mini) to a larger dataset of 646 multiple-choice questions. We found that this initial automated approach performed significantly worse than an expert-designed KC model (RMSE 0.4285 vs. 0.4206) and generated an excessive number of KCs (569 vs. 101). To address the issue of redundancy, we proposed and evaluated a novel method for merging semantically similar KC labels based on their cosine similarity. This merging strategy significantly improved the model's performance; a model using a cosine similarity threshold of 0.8 achieved the best result, reducing the KC count to 428 and improving the RMSE to 0.4259. This demonstrates that while scaled LLM generation alone is insufficient, combining it with a semantic merging technique offers a viable path toward automating and refining KC identification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2511.09966</link>
<guid>https://arxiv.org/abs/2511.09966</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, multi-hop reasoning, global planning, Sub-task Planner, Fact Extractor  

<br /><br />Summary:  
This paper addresses limitations in current retrieval-augmented generation (RAG) approaches for multi-hop reasoning, highlighting their lack of global planning which often leads to local reasoning dead-ends. To resolve this, the authors propose Recursive Evaluation and Adaptive Planning (REAP), featuring two key components: the Sub-task Planner (SP) and the Fact Extractor (FE). SP maintains a global overview, guiding the reasoning trajectory by evaluating intermediate task states, while FE performs detailed analysis of retrieved content to extract reliable facts and clues. Together, these modules incrementally build a coherent global knowledge representation, improving both the reliability and traceability of reasoning processes. Additionally, a unified task paradigm is introduced to facilitate effective multi-task fine-tuning, which notably enhances SP’s capabilities on complex and data-scarce tasks. The effectiveness of REAP is validated through extensive experiments on multiple public multi-hop datasets, where it significantly outperforms existing RAG methods in both in-domain and out-of-domain scenarios. The results demonstrate that REAP provides a more robust and accurate solution for complex multi-hop reasoning challenges, mitigating hallucinations and improving reasoning outcomes in large language models. <div>
arXiv:2511.09966v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcomes. To overcome these limitations, we propose Recursive Evaluation and Adaptive Planning (REAP), whose core idea is to explicitly maintain structured sub-tasks and facts related to the current task through the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP maintains a global perspective, guiding the overall reasoning direction and evaluating the task state based on the outcomes of FE, enabling dynamic optimization of the task-solving trajectory. FE performs fine-grained analysis over retrieved content to extract reliable answers and clues. These two modules incrementally enrich a logically coherent representation of global knowledge, enhancing the reliability and the traceability of the reasoning process. Furthermore, we propose a unified task paradigm design that enables effective multi-task fine-tuning, significantly enhancing SP's performance on complex, data-scarce tasks. We conduct extensive experiments on multiple public multi-hop datasets, and the results demonstrate that our method significantly outperforms existing RAG methods in both in-domain and out-of-domain settings, validating its effectiveness in complex multi-hop reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction</title>
<link>https://arxiv.org/abs/2511.09971</link>
<guid>https://arxiv.org/abs/2511.09971</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical reasoning, veracity prediction, robustness, large language models, fact-checking<br /><br />Summary:<br /><br />This paper evaluates the performance of state-of-the-art large language models on the task of numerical fact-checking, focusing on their ability to assess veracity of numerical claims paired with evidence. The study uses controlled perturbations, including label-flipping probes, to systematically test the robustness of these models. Results reveal that even leading proprietary models suffer significant accuracy drops, up to 62%, under specific perturbations, indicating a lack of consistent robustness across different conditions. Additionally, the research finds that increasing the length of the input context generally leads to a reduction in accuracy. However, when the extended context is enhanced with perturbed demonstration examples, many models show substantial recovery in performance. These findings highlight critical limitations of current language models in handling numerical reasoning tasks within fact-checking applications. The study concludes that improving robustness to numerical perturbations remains a significant open challenge for future development in large language model capabilities. <div>
arXiv:2511.09971v1 Announce Type: new 
Abstract: Large language models show strong performance on knowledge intensive tasks such as fact-checking and question answering, yet they often struggle with numerical reasoning. We present a systematic evaluation of state-of-the-art models for veracity prediction on numerical claims and evidence pairs using controlled perturbations, including label-flipping probes, to test robustness. Our results indicate that even leading proprietary systems experience accuracy drops of up to 62\% under certain perturbations. No model proves to be robust across all conditions. We further find that increasing context length generally reduces accuracy, but when extended context is enriched with perturbed demonstrations, most models substantially recover. These findings highlight critical limitations in numerical fact-checking and suggest that robustness remains an open challenge for current language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG</title>
<link>https://arxiv.org/abs/2511.09980</link>
<guid>https://arxiv.org/abs/2511.09980</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic retrieval, language models, Entropy-Trend Constraint, token-level uncertainty, retrieval timing<br /><br />Summary: Dynamic retrieval-augmented generation (RAG) improves large language models (LLMs) by enabling on-demand fetching of external knowledge, unlike static RAG which is less adaptable. The main challenge addressed is determining the optimal timing for retrieval to avoid delayed interventions after errors propagate. Existing methods rely on low token-level confidence to trigger retrieval, which can be too late. The paper introduces Entropy-Trend Constraint (ETC), a training-free method that models the dynamics of token-level uncertainty using first- and second-order differences of entropy sequences. This approach detects emerging uncertainty trends, facilitating earlier and more precise retrieval. Experiments conducted on six question-answering benchmarks using three different LLM backbones demonstrate ETC’s consistent superiority over strong baselines while also reducing the frequency of retrieval. ETC is especially effective in domain-specific contexts and shows robust generalization capabilities. Further ablation studies and qualitative analyses validate that modeling uncertainty trends leads to more effective retrieval timing. The method is plug-and-play, model-agnostic, and can be easily integrated into existing decoding pipelines. The authors provide implementation code in the supplementary materials to encourage adoption. <div>
arXiv:2511.09980v1 Announce Type: new 
Abstract: Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation</title>
<link>https://arxiv.org/abs/2511.09984</link>
<guid>https://arxiv.org/abs/2511.09984</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual Retrieval-Augmented Generation, Language Drift, Chain-of-Thought, Soft Constrained Decoding, Language Alignment  

<br /><br />Summary:  
This paper investigates the phenomenon of language drift in multilingual Retrieval-Augmented Generation (RAG) systems, where large language models generate responses in unintended languages when the retrieved evidence and input queries differ in language. The issue is most pronounced during reasoning-intensive tasks like Chain-of-Thought (CoT) generation, where the intermediate reasoning steps exacerbate language instability. Through comprehensive experiments across multiple datasets, languages, and various LLM backbones, the authors identify that the drift is due not to comprehension errors but to decoder-level collapse, driven by dominant token distributions and high-frequency English patterns. English emerges as a semantic attractor, acting as the primary interference source and fallback language in cross-lingual situations. To address this, the authors propose Soft Constrained Decoding (SCD), a training-free, lightweight decoding strategy that softly penalizes tokens from non-target languages to steer the generation toward the intended language. SCD is model-agnostic and can be integrated with any generation algorithm without requiring architectural changes or additional data. Experiments on three multilingual datasets covering typologically diverse languages demonstrate that SCD consistently enhances language alignment and overall task performance, offering an effective and generalizable solution for mitigating language drift in multilingual RAG systems. <div>
arXiv:2511.09984v1 Announce Type: new 
Abstract: Multilingual Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to perform knowledge-intensive tasks in multilingual settings by leveraging retrieved documents as external evidence. However, when the retrieved evidence differs in language from the user query and in-context exemplars, the model often exhibits language drift by generating responses in an unintended language. This phenomenon is especially pronounced during reasoning-intensive decoding, such as Chain-of-Thought (CoT) generation, where intermediate steps introduce further language instability. In this paper, we systematically study output language drift in multilingual RAG across multiple datasets, languages, and LLM backbones. Our controlled experiments reveal that the drift results not from comprehension failure but from decoder-level collapse, where dominant token distributions and high-frequency English patterns dominate the intended generation language. We further observe that English serves as a semantic attractor under cross-lingual conditions, emerging as both the strongest interference source and the most frequent fallback language.
  To mitigate this, we propose Soft Constrained Decoding (SCD), a lightweight, training-free decoding strategy that gently steers generation toward the target language by penalizing non-target-language tokens. SCD is model-agnostic and can be applied to any generation algorithm without modifying the architecture or requiring additional data. Experiments across three multilingual datasets and multiple typologically diverse languages show that SCD consistently improves language alignment and task performance, providing an effective and generalizable solution in multilingual RAG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinNuE: Exposing the Risks of Using BERTScore for Numerical Semantic Evaluation in Finance</title>
<link>https://arxiv.org/abs/2511.09997</link>
<guid>https://arxiv.org/abs/2511.09997</guid>
<content:encoded><![CDATA[
<div> Keywords: BERTScore, numerical variation, finance, FinNuE, semantic similarity<br /><br />Summary: BERTScore is a popular metric used to evaluate semantic similarity between natural language sentences. However, this study identifies a critical limitation of BERTScore: it shows low sensitivity to numerical variation. This limitation is particularly problematic in finance, where small changes in numbers (such as distinguishing between a 2% gain and a 20% loss) fundamentally alter the meaning. To address this, the authors introduce FinNuE, a new diagnostic dataset featuring controlled numerical perturbations drawn from earnings calls, regulatory filings, social media, and news articles. Using FinNuE, they demonstrate that BERTScore often fails to detect semantically meaningful numerical differences and tends to assign high similarity scores to text pairs that differ substantially in financial implications. These results reveal foundational limitations of embedding-based metrics like BERTScore when applied to financial text. The study calls for the development of evaluation frameworks that are more numerically aware to improve semantic similarity assessments in financial natural language processing tasks. <div>
arXiv:2511.09997v1 Announce Type: new 
Abstract: BERTScore has become a widely adopted metric for evaluating semantic similarity between natural language sentences. However, we identify a critical limitation: BERTScore exhibits low sensitivity to numerical variation, a significant weakness in finance where numerical precision directly affects meaning (e.g., distinguishing a 2% gain from a 20% loss). We introduce FinNuE, a diagnostic dataset constructed with controlled numerical perturbations across earnings calls, regulatory filings, social media, and news articles. Using FinNuE, demonstrate that BERTScore fails to distinguish semantically critical numerical differences, often assigning high similarity scores to financially divergent text pairs. Our findings reveal fundamental limitations of embedding-based metrics for finance and motivate numerically-aware evaluation frameworks for financial NLP.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.10002</link>
<guid>https://arxiv.org/abs/2511.10002</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, NCERT curriculum, question-answering dataset, prompting techniques, educational AI tools

<br /><br />Summary:  
This paper introduces "PustakAI," a framework designed to create and evaluate "NCERT-QA," a novel question-answering dataset aligned with the NCERT syllabus for English and Science subjects targeting grades 6 to 8 in India. The dataset categorizes QA pairs into Factoid, Inferential, and Others (evaluative and reasoning) types to encompass different cognitive demands. Various prompting techniques, including meta-prompt, few-shot, and Chain-of-Thought (CoT) prompting, are employed to test how effectively each aligns with curriculum needs and question complexity. The study assesses different large language models, from smaller open-source models (Gemma3:1b, Llama3.2:3b, Nemotron-mini:4b) to more powerful ones (Llama-4-Scout-17B and Deepseek-r1-70B), evaluating their performance as AI educational aids. Findings highlight the strengths and limitations of these models in terms of accuracy, curriculum alignment, and pedagogical relevance, emphasizing the challenges in adapting LLMs to formal education systems, particularly in regions with limited teaching resources. The research contributes to personalized and interactive learning methodologies by offering a structured evaluation of AI tools against a specific national curriculum, potentially guiding future developments in AI-assisted education. <div>
arXiv:2511.10002v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like content. This has revolutionized various sectors such as healthcare, software development, and education. In education, LLMs offer potential for personalized and interactive learning experiences, especially in regions with limited teaching resources. However, adapting these models effectively to curriculum-specific content, such as the National Council of Educational Research and Training (NCERT) syllabus in India, presents unique challenges in terms of accuracy, alignment, and pedagogical relevance. In this paper, we present the framework "PustakAI"\footnote{Pustak means `book' in many Indian languages.} for the design and evaluation of a novel question-answering dataset "NCERT-QA" aligned with the NCERT curriculum for English and Science subjects of grades 6 to 8. We classify the curated QA pairs as Factoid, Inferential, and Others (evaluative and reasoning). We evaluate the dataset with various prompting techniques, such as meta-prompt, few-shot, and CoT-style prompting, using diverse evaluation metrics to understand which approach aligns more efficiently with the structure and demands of the curriculum. Along with the usability of the dataset, we analyze the strengths and limitations of current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B) as AI-based learning tools in formal education systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScaleFormer: Span Representation Cumulation for Long-Context Transformer</title>
<link>https://arxiv.org/abs/2511.10029</link>
<guid>https://arxiv.org/abs/2511.10029</guid>
<content:encoded><![CDATA[
<div> Keywords: ScaleFormer, long-context Transformer, sequence segmentation, parameter-free fusion, long-document summarization<br /><br />Summary:<br /><br />1. The paper addresses the computational challenge posed by the quadratic complexity of self-attention in Transformer models when processing long sequences, which limits their applicability in long-context tasks.<br />2. The authors introduce ScaleFormer, a plug-and-play framework designed to adapt existing pre-trained encoder-decoder Transformer models for long-sequence processing without modifying their architecture or requiring expensive pre-training.<br />3. ScaleFormer works by segmenting long input sequences into overlapping chunks and generating compressed, context-aware representations for each chunk to feed the decoder.<br />4. A novel, parameter-free fusion mechanism enriches each chunk’s boundary representations with cumulative context vectors from all preceding and succeeding chunks, granting the model structural awareness of each chunk’s position within the document and effectively capturing narrative flow.<br />5. This approach achieves linear computational complexity, enables efficient reasoning over long documents, and demonstrates highly competitive performance in long-document summarization tasks, often surpassing state-of-the-art methods without relying on architectural changes or external retrieval techniques. <div>
arXiv:2511.10029v1 Announce Type: new 
Abstract: The quadratic complexity of standard self-attention severely limits the application of Transformer-based models to long-context tasks. While efficient Transformer variants exist, they often require architectural changes and costly pre-training from scratch. To circumvent this, we propose ScaleFormer(Span Representation Cumulation for Long-Context Transformer) - a simple and effective plug-and-play framework that adapts off-the-shelf pre-trained encoder-decoder models to process long sequences without requiring architectural modifications. Our approach segments long inputs into overlapping chunks and generates a compressed, context-aware representation for the decoder. The core of our method is a novel, parameter-free fusion mechanism that endows each chunk's representation with structural awareness of its position within the document. It achieves this by enriching each chunk's boundary representations with cumulative context vectors from all preceding and succeeding chunks. This strategy provides the model with a strong signal of the document's narrative flow, achieves linear complexity, and enables pre-trained models to reason effectively over long-form text. Experiments on long-document summarization show that our method is highly competitive with and often outperforms state-of-the-art approaches without requiring architectural modifications or external retrieval mechanisms.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism</title>
<link>https://arxiv.org/abs/2511.10045</link>
<guid>https://arxiv.org/abs/2511.10045</guid>
<content:encoded><![CDATA[
<div> Keywords: Sound symbolism, Multimodal Large Language Models, phonetic iconicity, LEX-ICON dataset, phoneme-level attention  

<br /><br />Summary:  
This paper explores sound symbolism, the connection between phonetic forms and meanings, as a way to probe how Multimodal Large Language Models (MLLMs) process auditory language information. The study evaluates MLLMs on their ability to recognize phonetic iconicity using both textual inputs (orthographic and IPA transcriptions) and auditory inputs, spanning up to 25 semantic dimensions such as "sharp" versus "round." To facilitate this, the authors introduce LEX-ICON, a large dataset containing 8,052 mimetic words from English, French, Japanese, and Korean, along with 2,930 pseudo-words carefully constructed, all annotated with semantic features across text and audio modalities. Key findings reveal that MLLMs exhibit phonetic intuitions consistent with linguistic research across multiple semantic categories. In addition, analysis of phoneme-level attention scores shows distinctive phonosemantic attention patterns, indicating the models focus selectively on iconic phonemes during processing. These contributions provide a novel large-scale quantitative analysis linking AI model interpretability with cognitive linguistics, thereby advancing understanding of how artificial models encode sound-meaning relationships in human language. <div>
arXiv:2511.10045v1 Announce Type: new 
Abstract: Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs' performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models' layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs' phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models' focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs' interpretability.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt</title>
<link>https://arxiv.org/abs/2511.10051</link>
<guid>https://arxiv.org/abs/2511.10051</guid>
<content:encoded><![CDATA[
<div> Multi-turn instruction following, large language models, graph structures, relation extraction, dialogue systems  

<br /><br />Summary: This paper addresses the challenge of improving multi-turn instruction following in large language models (LLMs), which often treat each response generation independently and thus struggle with long-distance constraints in dialogues. The authors propose GraphIF, a plug-and-play framework that enhances LLMs by modeling multi-turn dialogues as directed relation graphs, capturing the inter-turn semantic relations naturally through graph structures. GraphIF consists of three main components: (1) an agent-based relation extraction module that uses action-triggered mechanisms to identify and construct relation graphs between dialogue turns; (2) a relation graph prompt generation module that transforms these structured graphs into natural language prompts; and (3) a response rewriting module that refines the LLM’s initial outputs by incorporating information from the graph prompts. By explicitly integrating multi-turn relational constraints into instruction following, GraphIF overcomes limitations of prior approaches relying solely on large datasets and isolated response generation. Extensive experiments conducted on two long multi-turn dialogue datasets demonstrate that GraphIF seamlessly integrates with instruction-tuned LLMs and significantly improves performance across four multi-turn instruction-following evaluation metrics. The proposed method highlights the effectiveness of graph-based modeling and prompting for enhancing dialogue consistency and adherence to instructions over multiple turns. <div>
arXiv:2511.10051v1 Announce Type: new 
Abstract: Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADI-20: Arabic Dialect Identification dataset and models</title>
<link>https://arxiv.org/abs/2511.10070</link>
<guid>https://arxiv.org/abs/2511.10070</guid>
<content:encoded><![CDATA[
<div> Arabic Dialect Identification, ADI-20, ECAPA-TDNN, Whisper encoder, Dataset expansion<br /><br />Summary:<br /><br />1. The paper introduces ADI-20, an extended dataset building upon the earlier ADI-17 dataset, now encompassing dialects from all Arabic-speaking countries. <br />2. ADI-20 comprises 3,556 hours of speech data, covering 19 Arabic dialects plus Modern Standard Arabic (MSA), making it a comprehensive resource for Arabic dialect identification (ADI).<br />3. The dataset was used to train and evaluate advanced ADI systems, including fine-tuning of pre-trained ECAPA-TDNN models and models based on Whisper encoder blocks combined with attention pooling and classification layers.<br />4. The study explores the impact of training data size and model parameter count on the performance of dialect identification, finding a minor reduction in F1 score when using only 30% of the original training data.<br />5. Both the dataset and the trained models are open-sourced to facilitate reproducibility and further research in the field of Arabic dialect identification. <div>
arXiv:2511.10070v1 Announce Type: new 
Abstract: We present ADI-20, an extension of the previously published ADI-17 Arabic Dialect Identification (ADI) dataset. ADI-20 covers all Arabic-speaking countries' dialects. It comprises 3,556 hours from 19 Arabic dialects in addition to Modern Standard Arabic (MSA). We used this dataset to train and evaluate various state-of-the-art ADI systems. We explored fine-tuning pre-trained ECAPA-TDNN-based models, as well as Whisper encoder blocks coupled with an attention pooling layer and a classification dense layer. We investigated the effect of (i) training data size and (ii) the model's number of parameters on identification performance. Our results show a small decrease in F1 score while using only 30% of the original training data. We open-source our collected data and trained models to enable the reproduction of our work, as well as support further research in ADI.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts</title>
<link>https://arxiv.org/abs/2511.10075</link>
<guid>https://arxiv.org/abs/2511.10075</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, scientific claim verification, tables, charts, cross-modal generalization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of verifying scientific claims using multimodal large language models (LLMs) that interpret experimental results presented in diverse evidence formats like tables and charts.<br /><br />2. To evaluate these models, the authors adapt two existing scientific paper datasets by adding annotations and structures tailored for multimodal claim verification tasks.<br /><br />3. A total of 12 multimodal LLMs are tested, revealing that current models perform significantly better when using tables as evidence compared to charts.<br /><br />4. Human evaluators maintain consistently strong performance across both evidence formats, highlighting a gap in model capabilities.<br /><br />5. Finally, smaller multimodal LLMs (under 8 billion parameters) exhibit weak performance correlation between table-based and chart-based verification tasks, suggesting limited ability to generalize reasoning across modalities. The study underscores the need for future multimodal LLM development to focus on enhancing chart understanding to improve scientific claim verification. <div>
arXiv:2511.10075v1 Announce Type: new 
Abstract: With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current models' multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELYADATA &amp; LIA at NADI 2025: ASR and ADI Subtasks</title>
<link>https://arxiv.org/abs/2511.10090</link>
<guid>https://arxiv.org/abs/2511.10090</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic Speech Processing, Dialect Identification, Whisper-large-v3, SeamlessM4T-v2, ASR

<br /><br />Summary:  
This paper presents Elyadata & LIA's joint submission to the NADI Multi-Dialectal Arabic Speech Processing 2025 challenge. The team participated in two subtasks: Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic Automatic Speech Recognition (ASR). Their ADI system, based on a fine-tuned Whisper-large-v3 encoder combined with data augmentation techniques, achieved the highest accuracy in the competition, scoring 79.83% on the official test set. For the multi-dialectal Arabic ASR task, the team fine-tuned the SeamlessM4T-v2 Large model (Egyptian variant) separately for each of the eight dialects included in the challenge. This approach yielded an average Word Error Rate (WER) of 38.54% and a Character Error Rate (CER) of 14.53% on the test set, ranking second among all participants. The results highlight the advantages of leveraging large pre-trained speech models with dialect-specific fine-tuning when addressing challenges in Arabic speech processing. Overall, the submission demonstrates state-of-the-art performance in both Arabic dialect identification and multi-dialect automatic speech recognition. <div>
arXiv:2511.10090v1 Announce Type: new 
Abstract: This paper describes Elyadata \& LIA's joint submission to the NADI multi-dialectal Arabic Speech Processing 2025. We participated in the Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic ASR subtasks. Our submission ranked first for the ADI subtask and second for the multi-dialectal Arabic ASR subtask among all participants. Our ADI system is a fine-tuned Whisper-large-v3 encoder with data augmentation. This system obtained the highest ADI accuracy score of \textbf{79.83\%} on the official test set. For multi-dialectal Arabic ASR, we fine-tuned SeamlessM4T-v2 Large (Egyptian variant) separately for each of the eight considered dialects. Overall, we obtained an average WER and CER of \textbf{38.54\%} and \textbf{14.53\%}, respectively, on the test set. Our results demonstrate the effectiveness of large pre-trained speech models with targeted fine-tuning for Arabic speech processing.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Military Applications of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10093</link>
<guid>https://arxiv.org/abs/2511.10093</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, large language models, military applications, GPT, cloud services<br /><br />Summary: This paper explores the potential military use cases and implementations of natural language processing (NLP) and large language models (LLMs), particularly those based on the generative pre-trained transformer (GPT) architecture, such as OpenAI's ChatGPT and Microsoft Copilot. First, the authors interrogate a GPT-based model (Microsoft Copilot) to reveal its inherent knowledge about military applications, followed by a critical assessment of the responses to evaluate their relevance and practicality. Second, the study investigates how commercial cloud platforms, specifically Microsoft Azure, can be leveraged to develop and deploy such military-oriented NLP applications, focusing on feasibility, scalability, and integration potential. The findings highlight that the summarization and generative abilities of these language models offer significant advantages in various military contexts, enabling efficient information processing and decision support. Additionally, other features of LLMs, such as language translation, sentiment analysis, and automated report generation, are also identified as valuable for specialized uses in defense settings. Overall, the paper concludes that the current advancements in foundation models and cloud infrastructure collectively facilitate the rapid development and application of AI-driven language technologies within the military domain. <div>
arXiv:2511.10093v1 Announce Type: new 
Abstract: In this paper, military use cases or applications and implementation thereof are considered for natural language processing and large language models, which have broken into fame with the invention of the generative pre-trained transformer (GPT) and the extensive foundation model pretraining done by OpenAI for ChatGPT and others. First, we interrogate a GPT-based language model (viz. Microsoft Copilot) to make it reveal its own knowledge about their potential military applications and then critically assess the information. Second, we study how commercial cloud services (viz. Microsoft Azure) could be used readily to build such applications and assess which of them are feasible. We conclude that the summarization and generative properties of language models directly facilitate many applications at large and other features may find particular uses.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing to Unseen Disaster Events: A Causal View</title>
<link>https://arxiv.org/abs/2511.10120</link>
<guid>https://arxiv.org/abs/2511.10120</guid>
<content:encoded><![CDATA[
<div> keywords: social media, disaster monitoring, bias mitigation, causal learning, event classification<br /><br />Summary:<br /><br />The paper addresses the critical challenge of real-time processing and analysis of social media data during disaster events, highlighting the necessity of extracting valuable insights amid vast data volumes. A key problem identified is the presence of event-related biases in existing systems, which undermines their ability to generalize and adapt to new, emerging disaster events. The authors propose a novel approach that leverages causal learning techniques to mitigate biases related to specific events and domains. This causal lens approach aims to improve the robustness and generalizability of classification models dealing with disaster-related social media data. Their methodology is empirically validated across three distinct disaster classification tasks, demonstrating significant improvements. Specifically, their approach outperforms multiple baseline models by up to 1.9% in F1 score, indicating a meaningful enhancement in classification performance. Additionally, they show that their bias mitigation method significantly boosts the effectiveness of pre-trained language model (PLM)-based classifiers. Overall, the study contributes to the disaster monitoring domain by presenting an effective strategy for bias reduction, thereby enabling more reliable and adaptable real-time information extraction from social media platforms during evolving disaster scenarios. <div>
arXiv:2511.10120v1 Announce Type: new 
Abstract: Due to the rapid growth of social media platforms, these tools have become essential for monitoring information during ongoing disaster events. However, extracting valuable insights requires real-time processing of vast amounts of data. A major challenge in existing systems is their exposure to event-related biases, which negatively affects their ability to generalize to emerging events. While recent advancements in debiasing and causal learning offer promising solutions, they remain underexplored in the disaster event domain. In this work, we approach bias mitigation through a causal lens and propose a method to reduce event- and domain-related biases, enhancing generalization to future events. Our approach outperforms multiple baselines by up to +1.9% F1 and significantly improves a PLM-based classifier across three disaster classification tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Black Box: Demystifying Multi-Turn LLM Reasoning with VISTA</title>
<link>https://arxiv.org/abs/2511.10182</link>
<guid>https://arxiv.org/abs/2511.10182</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-turn reasoning, visualization, reasoning dependency tree, interactive analysis<br /><br />Summary: The paper addresses the challenge of analyzing the complex reasoning processes of Large Language Models (LLMs) during multi-turn interactions, which closely resemble real-world problem solving. Existing methods struggle with interpreting intricate contextual dependencies and lack effective visualization tools, resulting in a high cognitive load for researchers. To overcome these limitations, the authors introduce VISTA, a web-based Visual Interactive System for Textual Analytics specifically designed for multi-turn reasoning tasks. VISTA enables users to visualize how context influences model decisions and supports interactive modifications of conversation histories to perform “what-if” analyses across various models. A key feature of VISTA is its automatic parsing of sessions to generate reasoning dependency trees, which transparently map out the model’s logical reasoning path step-by-step. This unified and interactive framework simplifies the process of analyzing reasoning chains, allowing users to gain deeper insights into the strengths and weaknesses of current LLMs. Additionally, VISTA is open-source and designed for easy integration of custom benchmarks and local models, making it a versatile tool for researchers exploring LLM reasoning capabilities in conversational settings. <div>
arXiv:2511.10182v1 Announce Type: new 
Abstract: Recent research has increasingly focused on the reasoning capabilities of Large Language Models (LLMs) in multi-turn interactions, as these scenarios more closely mirror real-world problem-solving. However, analyzing the intricate reasoning processes within these interactions presents a significant challenge due to complex contextual dependencies and a lack of specialized visualization tools, leading to a high cognitive load for researchers. To address this gap, we present VISTA, an web-based Visual Interactive System for Textual Analytics in multi-turn reasoning tasks. VISTA allows users to visualize the influence of context on model decisions and interactively modify conversation histories to conduct "what-if" analyses across different models. Furthermore, the platform can automatically parse a session and generate a reasoning dependency tree, offering a transparent view of the model's step-by-step logical path. By providing a unified and interactive framework, VISTA significantly reduces the complexity of analyzing reasoning chains, thereby facilitating a deeper understanding of the capabilities and limitations of current LLMs. The platform is open-source and supports easy integration of custom benchmarks and local models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.10192</link>
<guid>https://arxiv.org/abs/2511.10192</guid>
<content:encoded><![CDATA[
<div> Text-to-SQL, Data Augmentation, SQLFlow, Large Language Models, Retrieval Method<br /><br />Summary:<br /><br />1. The paper addresses limitations in Text-to-SQL tasks caused by scarce, simplistic, and low-diversity datasets by proposing a data-centric approach.<br />2. It introduces Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data across six augmentation dimensions.<br />3. The framework integrates an end-to-end pipeline, including SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification, supported by a modular Database Manager for cross-database compatibility and scalability.<br />4. Using this approach, the authors build SQLFlow, a high-quality dataset with 89,544 annotated examples, which they evaluate in two settings: fine-tuning open-source LLMs and enhancing closed-source LLMs retrieval via a masked alignment retrieval method.<br />5. The masked alignment retrieval treats SQLFlow as both a knowledge base and training data for improved structure-aware example matching between questions and SQL, outperforming existing retrieval techniques.<br /><br />The work establishes a scalable, data-centric foundation to advance Text-to-SQL systems and emphasizes the critical importance of high-quality structured data in modern AI development. <div>
arXiv:2511.10192v1 Announce Type: new 
Abstract: The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EffiReason-Bench: A Unified Benchmark for Evaluating and Advancing Efficient Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10201</link>
<guid>https://arxiv.org/abs/2511.10201</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Chain-of-Thought prompting, efficient reasoning, benchmark, E3-Score  

<br /><br />Summary:  
This paper addresses the challenge of inefficiency in large language models (LLMs) when using Chain-of-Thought (CoT) prompting, which often generates overly long explanations that increase computational costs and can hurt accuracy. To facilitate fair comparisons of various efficiency-driven approaches, the authors introduce EffiReason-Bench, a comprehensive benchmark that evaluates methods across three categories: Reasoning Blueprints, Dynamic Execution, and Post-hoc Refinement. The benchmark includes verified step-by-step CoT annotations for CommonsenseQA and LogiQA datasets, created through a pipeline that standardizes reasoning structures, provides detailed option-wise analysis, and incorporates human verification to ensure quality. They test seven different efficiency methods on six open-source LLMs ranging from 1 billion to 70 billion parameters, across four datasets covering mathematics, commonsense reasoning, and logic. The study introduces the E3-Score, a new evaluation metric inspired by economic trade-off modeling that avoids common pitfalls such as discontinuities and heuristic dependencies. Experimental results reveal that no single efficiency approach is superior in all scenarios; instead, the best method depends on model size, task complexity, and model architecture, emphasizing the need for adaptable strategies tailored to specific contexts. <div>
arXiv:2511.10201v1 Announce Type: new 
Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) prompting achieve strong reasoning but often produce unnecessarily long explanations, increasing cost and sometimes reducing accuracy. Fair comparison of efficiency-oriented approaches is hindered by fragmented evaluation practices. We introduce EffiReason-Bench, a unified benchmark for rigorous cross-paradigm evaluation of efficient reasoning methods across three categories: Reasoning Blueprints, Dynamic Execution, and Post-hoc Refinement. To enable step-by-step evaluation, we construct verified CoT annotations for CommonsenseQA and LogiQA via a pipeline that enforces standardized reasoning structures, comprehensive option-wise analysis, and human verification. We evaluate 7 methods across 6 open-source LLMs (1B-70B) on 4 datasets spanning mathematics, commonsense, and logic, and propose the E3-Score, a principled metric inspired by economic trade-off modeling that provides smooth, stable evaluation without discontinuities or heavy reliance on heuristics. Experiments show that no single method universally dominates; optimal strategies depend on backbone scale, task complexity, and architecture.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persona-Aware Alignment Framework for Personalized Dialogue Generation</title>
<link>https://arxiv.org/abs/2511.10215</link>
<guid>https://arxiv.org/abs/2511.10215</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized dialogue, persona alignment, two-stage training, Select then Generate, persona relevance<br /><br />Summary:<br /><br />Personalized dialogue generation focuses on using persona profiles and dialogue history to produce responses that are both relevant and consistent with a given persona. Traditional models typically depend on token-level language model training methods, such as Next Token Prediction, which only implicitly incorporate persona information and often result in generic outputs that neglect specific persona details. To solve this, the paper introduces the Persona-Aware Alignment Framework (PAL), which explicitly makes persona alignment the main training objective. PAL uses a two-stage training approach consisting of Persona-aware Learning and Persona Alignment stages, aiming to enhance persona sensitivity and generate semantically persona-relevant responses. Additionally, it includes an easy-to-use inference strategy called Select then Generate that further improves response quality. The authors conduct extensive experiments showing that PAL surpasses many state-of-the-art personalized dialogue techniques and large language models in persona relevance and response quality. Overall, this framework advances the ability of dialogue systems to generate more personalized, meaningful, and contextually appropriate responses. <div>
arXiv:2511.10215v1 Announce Type: new 
Abstract: Personalized dialogue generation aims to leverage persona profiles and dialogue history to generate persona-relevant and consistent responses. Mainstream models typically rely on token-level language model training with persona dialogue data, such as Next Token Prediction, to implicitly achieve personalization, making these methods tend to neglect the given personas and generate generic responses. To address this issue, we propose a novel Persona-Aware Alignment Framework (PAL), which directly treats persona alignment as the training objective of dialogue generation. Specifically, PAL employs a two-stage training method including Persona-aware Learning and Persona Alignment, equipped with an easy-to-use inference strategy Select then Generate, to improve persona sensitivity and generate more persona-relevant responses at the semantics level. Through extensive experiments, we demonstrate that our framework outperforms many state-of-the-art personalized dialogue methods and large language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning</title>
<link>https://arxiv.org/abs/2511.10229</link>
<guid>https://arxiv.org/abs/2511.10229</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual instruction tuning, language separability, data selection, low-resource languages, curriculum learning<br /><br />Summary: This paper introduces LangGPS, a two-stage pre-selection framework designed to enhance multilingual instruction tuning for large language models (LLMs) by leveraging the concept of language separability. Language separability measures how distinguishable samples from different languages are within the model's representation space. LangGPS first filters training data using separability scores, then refines the subset through existing selection methods based on quality, diversity, or task relevance. Through extensive experiments on six benchmarks across 22 languages, LangGPS is shown to improve the effectiveness and generalizability of multilingual training, particularly benefiting understanding tasks and low-resource languages. Analysis reveals that samples with high language separability help create clearer language boundaries and facilitate faster adaptation, while samples with low separability act as bridges aiding cross-lingual alignment. Additionally, the study finds that language separability is a valuable signal for multilingual curriculum learning by interleaving samples of varying separability levels, which contributes to stable and generalizable performance improvements. Overall, this work highlights the importance of incorporating linguistic structure into data selection processes and proposes a novel perspective on utilizing language separability to develop more linguistically informed and effective multilingual LLMs. <div>
arXiv:2511.10229v1 Announce Type: new 
Abstract: Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the model's representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VocalNet-M2: Advancing Low-Latency Spoken Language Modeling via Integrated Multi-Codebook Tokenization and Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2511.10232</link>
<guid>https://arxiv.org/abs/2511.10232</guid>
<content:encoded><![CDATA[
<div> Keywords: VocalNet-M2, spoken language models, multi-codebook tokenizer, multi-token prediction, low latency<br /><br />Summary: Current end-to-end spoken language models (SLMs) face significant response latency mainly due to autoregressive speech token generation and the use of complex flow-matching models in speech synthesis. To address this challenge, the authors propose VocalNet-M2, a novel SLM that incorporates a multi-codebook tokenizer and a multi-token prediction (MTP) strategy. This architecture allows direct generation of multi-codebook speech tokens, thereby eliminating the latency caused by flow-matching models. The MTP strategy further boosts generation efficiency and improves overall model performance. Experimental results demonstrate that VocalNet-M2 effectively reduces the initial chunk latency from about 725 milliseconds to 350 milliseconds. Despite the improved latency, the model maintains competitive results compared to mainstream SLMs, highlighting its practical applicability. Additionally, the study offers a detailed comparison between single-codebook and multi-codebook approaches, providing important insights for optimizing SLM design for real-time interactive systems. This work advances the development of efficient, high-performance spoken language models that better meet the demands of low-latency applications. <div>
arXiv:2511.10232v1 Announce Type: new 
Abstract: Current end-to-end spoken language models (SLMs) have made notable progress, yet they still encounter considerable response latency. This delay primarily arises from the autoregressive generation of speech tokens and the reliance on complex flow-matching models for speech synthesis. To overcome this, we introduce VocalNet-M2, a novel low-latency SLM that integrates a multi-codebook tokenizer and a multi-token prediction (MTP) strategy. Our model directly generates multi-codebook speech tokens, thus eliminating the need for a latency-inducing flow-matching model. Furthermore, our MTP strategy enhances generation efficiency and improves overall performance. Extensive experiments demonstrate that VocalNet-M2 achieves a substantial reduction in first chunk latency (from approximately 725ms to 350ms) while maintaining competitive performance across mainstream SLMs. This work also provides a comprehensive comparison of single-codebook and multi-codebook strategies, offering valuable insights for developing efficient and high-performance SLMs for real-time interactive applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models</title>
<link>https://arxiv.org/abs/2511.10262</link>
<guid>https://arxiv.org/abs/2511.10262</guid>
<content:encoded><![CDATA[
<div> Full-Duplex, Speech Language Models, Multi-round Dialogue, Benchmark, Instruction Following  

<br /><br />Summary:  
This paper introduces Full-Duplex Speech Language Models (FD-SLMs), which allow for real-time, overlapping conversational interactions, enhancing user experience beyond traditional turn-taking models. Current evaluation benchmarks for FD-SLMs focus mainly on single-round interactions and overlook the complexities of multi-round communication, including essential capabilities like instruction following and conversational safety. The authors identify key challenges in assessing FD-SLMs in continuous dialogues, such as unclear turn boundaries and context inconsistency during inference. To overcome these issues, they present MTR-DuplexBench, a novel benchmark designed to segment continuous full-duplex dialogues into discrete turns. This allows for detailed, turn-by-turn evaluation of FD-SLMs across multiple dimensions: dialogue quality, conversational dynamics, instruction adherence, and safety. Experimental evaluations demonstrate that existing FD-SLMs struggle to maintain consistent performance across multiple dialogue rounds and evaluation criteria. These findings underline the importance of the proposed benchmark for advancing the development and robust assessment of FD-SLMs. The benchmark and associated codebase are planned for future public release to facilitate further research and improvement in this emerging field. <div>
arXiv:2511.10262v1 Announce Type: new 
Abstract: Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions and conversational features, neglecting the complexities of multi-round communication and critical capabilities such as instruction following and safety. Evaluating FD-SLMs in multi-round settings poses significant challenges, including blurred turn boundaries in communication and context inconsistency during model inference. To address these gaps, we introduce MTR-DuplexBench, a novel benchmark that segments continuous full-duplex dialogues into discrete turns, enabling comprehensive, turn-by-turn evaluation of FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety. Experimental results reveal that current FD-SLMs face difficulties in maintaining consistent performance across multiple rounds and evaluation dimensions, highlighting the necessity and effectiveness of our proposed benchmark. The benchmark and code will be available in the future.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local Hybrid Retrieval-Augmented Document QA</title>
<link>https://arxiv.org/abs/2511.10297</link>
<guid>https://arxiv.org/abs/2511.10297</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering system, data privacy, local infrastructure, semantic understanding, enterprise AI<br /><br />Summary:<br /><br />1. The paper addresses a critical dilemma faced by organizations handling sensitive documents: choosing between cloud-based AI systems with strong question-answering capabilities but compromised privacy, and local systems with better security but poorer accuracy.<br />2. The authors propose a novel question-answering system that operates entirely on local infrastructure with no internet access, combining semantic understanding and keyword precision to balance privacy and performance.<br />3. This system achieves competitive accuracy on complex queries across various document types, including legal, scientific, and conversational texts.<br />4. Using consumer-grade hardware acceleration, the approach delivers reliable answers with minimal errors, demonstrating that high accuracy does not require cloud dependency.<br />5. The solution enables privacy-sensitive institutions such as banks, hospitals, and law firms to adopt conversational document AI confidently, ensuring proprietary information remains on-premises.<br />6. Overall, the work establishes that enterprise AI deployments can achieve both strong performance and stringent privacy without compromise. <div>
arXiv:2511.10297v1 Announce Type: new 
Abstract: Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10303</link>
<guid>https://arxiv.org/abs/2511.10303</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-step Mathematical Reasoning, Large Language Models, imbalanced evaluation preference, perplexity-aware reinforcement learning, critique capability

<br /><br />Summary: To enhance the Multi-step Mathematical Reasoning (MsMR) abilities of Large Language Models (LLMs), this paper identifies the challenge of obtaining scalable supervision via automatic critique of reasoning errors and final judgment accuracy. Unlike previous works focusing on fine-tuning demonstrations for improving critique skills, the authors investigate the root cause of poor critiquing performance, revealing an "imbalanced evaluation preference" in LLMs. Specifically, LLMs tend to judge solutions with lower perplexity as correct, regardless of actual correctness. To explore this behavior, the authors create a One-to-many Problem-Solution (OPS) benchmark that measures how LLMs evaluate their own versus others' generated solutions. Through statistical preference analysis centered on perplexity, they confirm this bias. To address it, they propose a perplexity-aware reinforcement learning algorithm based on Group Relative Policy Optimization, which encourages LLMs to challenge their default tendency by exploring solution trajectories where higher perplexity solutions can be correct and lower perplexity ones incorrect. Extensive experiments on the OPS benchmark and other critic datasets show that this method effectively improves critiquing capabilities of LLMs, leading to better supervision for MsMR tasks and more balanced evaluation outcomes. <div>
arXiv:2511.10303v1 Announce Type: new 
Abstract: To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason -- imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMs' critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon -- ``LLMs incline to judge solutions with lower perplexity as correct'', which is dubbed as \textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages</title>
<link>https://arxiv.org/abs/2511.10338</link>
<guid>https://arxiv.org/abs/2511.10338</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, multilingual pretraining, Indic languages, data quality evaluation, large-scale dataset<br /><br />Summary: This study investigates the generation and evaluation of synthetic multilingual pretraining data specifically for 10 Indic languages, addressing the scarcity of high-quality resources in low-resource language settings. The authors introduce BhashaKritika, a large-scale synthetic dataset containing 540 billion tokens created using five different data generation techniques. The work explores how grounding data generation in documents, personas, and topics influences data quality and examines the effects of language choice in prompt instructions and document grounding on the final dataset. A comparison between translations of English content and native generation in Indic languages is also presented to determine the most effective approach for data synthesis. To ensure quality control at scale and across various scripts, the authors develop a modular evaluation pipeline incorporating script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity filtering using KenLM language models. The paper details empirical results from model training runs using this data, highlighting important trade-offs in synthetic data generation strategies and sharing best practices for constructing robust multilingual corpora suitable for LLM pretraining. This work advances understanding of effective synthetic data creation and quality assurance for less-resourced languages in the multilingual AI landscape. <div>
arXiv:2511.10338v1 Announce Type: new 
Abstract: In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates</title>
<link>https://arxiv.org/abs/2511.10354</link>
<guid>https://arxiv.org/abs/2511.10354</guid>
<content:encoded><![CDATA[
<div> Keywords: Cultural Heritage, Knowledge Graphs, Large Language Models, Text-to-RDF, Ontologies  

<br /><br />Summary:  
This paper presents ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a novel five-step methodology designed to extract structured knowledge from unstructured Cultural Heritage texts using Large Language Models (LLMs). The approach integrates annotation models, ontological frameworks, and iterative LLM-based extraction through stages including foundational analysis, annotation schema development, pipeline architecture design, integration refinement, and comprehensive evaluation. A case study focused on authenticity assessment debates demonstrates the method's application by processing Wikipedia articles about disputed cultural artifacts and documents. The sequential pipeline employs three LLMs: Claude Sonnet 3.7, Llama 3.3 70B, and GPT-4o-mini. Results indicate high performance in various extraction tasks with F1 scores ranging from 0.62 to 0.99, showing that smaller models can achieve competitive outcomes while reducing costs. This work is original in providing the first systematic coordination of LLM-based knowledge extraction with Cultural Heritage ontologies, offering a replicable framework adaptable across different domains and institutional setups. However, the knowledge graph output is currently limited to Wikipedia articles, and human oversight remains important for post-processing. Practically, ATR4CH supports Cultural Heritage organizations in transforming textual data into queryable Knowledge Graphs, facilitating metadata enrichment and automated knowledge discovery. <div>
arXiv:2511.10354v1 Announce Type: new 
Abstract: Cultural Heritage texts contain rich knowledge that is difficult to query systematically due to the challenges of converting unstructured discourse into structured Knowledge Graphs (KGs). This paper introduces ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a systematic five-step methodology for Large Language Model-based Knowledge Extraction from Cultural Heritage documents. We validate the methodology through a case study on authenticity assessment debates. Methodology - ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and comprehensive evaluation. We demonstrate the approach using Wikipedia articles about disputed items (documents, artifacts...), implementing a sequential pipeline with three LLMs (Claude Sonnet 3.7, Llama 3.3 70B, GPT-4o-mini). Findings - The methodology successfully extracts complex Cultural Heritage knowledge: 0.96-0.99 F1 for metadata extraction, 0.7-0.8 F1 for entity recognition, 0.65-0.75 F1 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, enabling cost-effective deployment. Originality - This is the first systematic methodology for coordinating LLM-based extraction with Cultural Heritage ontologies. ATR4CH provides a replicable framework adaptable across CH domains and institutional resources. Research Limitations - The produced KG is limited to Wikipedia articles. While the results are encouraging, human oversight is necessary during post-processing. Practical Implications - ATR4CH enables Cultural Heritage institutions to systematically convert textual knowledge into queryable KGs, supporting automated metadata enrichment and knowledge discovery.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.10375</link>
<guid>https://arxiv.org/abs/2511.10375</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Knowledge Graphs, Conflict Resolution, Large Language Models, Factual Inconsistency<br /><br />Summary:  
The paper introduces TruthfulRAG, a novel framework designed to enhance Retrieval-Augmented Generation (RAG) systems by addressing the challenge of knowledge conflicts between retrieved external information and the internal knowledge of Large Language Models (LLMs). Unlike existing conflict resolution methods that operate at the token or semantic level and often fail to fully capture factual discrepancies, TruthfulRAG employs Knowledge Graphs (KGs) to conduct factual-level conflict resolution. The framework constructs KGs through systematic extraction of triples from the retrieved content. It then applies query-based graph retrieval techniques to identify and isolate relevant knowledge for a given input. Furthermore, TruthfulRAG integrates an entropy-based filtering mechanism to accurately pinpoint conflicting elements in the knowledge and to mitigate factual inconsistencies. These mechanisms collectively empower LLMs to generate responses that are more faithful, accurate, and reliable. Extensive experimental evaluation demonstrates that TruthfulRAG outperforms existing approaches by significantly improving robustness and trustworthiness in knowledge-intensive tasks, alleviating errors caused by conflicting information, and ensuring higher fidelity in generative outputs. The work addresses the growing need for dependable RAG systems as external knowledge bases expand and internal parametric knowledge becomes outdated. <div>
arXiv:2511.10375v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning</title>
<link>https://arxiv.org/abs/2511.10381</link>
<guid>https://arxiv.org/abs/2511.10381</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning evaluation, base LLMs, methodological concerns, instruction-following  

<br /><br />Summary:  
1. The paper critiques existing studies that evaluate the reasoning capabilities of base large language models (LLMs), which are pre-trained solely on unlabeled corpora.  
2. It highlights a fundamental methodological issue: the training objective of base LLMs is based on statistical likelihood of language patterns, which does not align with normative qualities like logical correctness used to assess reasoning.  
3. As a result, when base LLMs generate logically valid or invalid conclusions, these outputs are better understood as coincidental consequences of linguistic pattern conformity rather than genuine reasoning attempts.  
4. This mismatch calls into question two common assumptions: (a) that base LLM outputs represent bona fide attempts at correct answers, and (b) that reasoning evaluations on base LLMs generalize to post-trained LLMs that are fine-tuned for following instructions.  
5. The authors urge the research community to critically re-examine prior work relying on these assumptions and advocate for future studies to explicitly account for these methodological pitfalls when assessing reasoning in LLMs. <div>
arXiv:2511.10381v1 Announce Type: new 
Abstract: Existing work investigates the reasoning capabilities of large language models (LLMs) to uncover their limitations, human-like biases and underlying processes. Such studies include evaluations of base LLMs (pre-trained on unlabeled corpora only) for this purpose. Our position paper argues that evaluating base LLMs' reasoning capabilities raises inherent methodological concerns that are overlooked in such existing studies. We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed. In particular, we show how base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns of statistical plausibility. This fundamental mismatch challenges the assumptions that (a) base LLMs' outputs can be assessed as their bona fide attempts at correct answers or conclusions; and (b) conclusions about base LLMs' reasoning can generalize to post-trained LLMs optimized for successful instruction-following. We call for a critical re-examination of existing work that relies implicitly on these assumptions, and for future work to account for these methodological pitfalls.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence</title>
<link>https://arxiv.org/abs/2511.10404</link>
<guid>https://arxiv.org/abs/2511.10404</guid>
<content:encoded><![CDATA[
<div> Entity Linking, Historical Italian, Neuro-symbolic, DELICATE, ENEIDE  

<br /><br />Summary:  
This paper addresses the challenges of Entity Linking (EL) in the humanities, specifically for historical Italian texts, which are complicated by diverse document types, scarce domain-specific datasets, and prevalence of long-tail entities underrepresented in Knowledge Bases (KBs). The authors propose DELICATE, a novel neuro-symbolic EL approach that integrates a BERT-based encoder with contextual data from Wikidata to enhance entity selection by leveraging temporal plausibility and entity type consistency. Additionally, the study introduces ENEIDE, a multi-domain EL corpus created through semi-automatic extraction from two annotated editions of Italian texts ranging from the 19th to 20th centuries, covering literary and political domains. Experimental results demonstrate that DELICATE outperforms existing EL models for historical Italian, even surpassing larger neural architectures with billions of parameters. Furthermore, the paper highlights that DELICATE’s confidence scores and feature sensitivity analyses contribute to better explainability and interpretability compared to purely neural EL methods, making it more transparent for end users in humanities research. Overall, the contributions significantly advance EL for historical texts by combining symbolic knowledge with modern neural techniques and providing a valuable dataset for future work. <div>
arXiv:2511.10404v1 Announce Type: new 
Abstract: In spite of the remarkable advancements in the field of Natural Language Processing, the task of Entity Linking (EL) remains challenging in the field of humanities due to complex document typologies, lack of domain-specific datasets and models, and long-tail entities, i.e., entities under-represented in Knowledge Bases (KBs). The goal of this paper is to address these issues with two main contributions. The first contribution is DELICATE, a novel neuro-symbolic method for EL on historical Italian which combines a BERT-based encoder with contextual information from Wikidata to select appropriate KB entities using temporal plausibility and entity type consistency. The second contribution is ENEIDE, a multi-domain EL corpus in historical Italian semi-automatically extracted from two annotated editions spanning from the 19th to the 20th century and including literary and political texts. Results show how DELICATE outperforms other EL models in historical Italian even if compared with larger architectures with billions of parameters. Moreover, further analyses reveal how DELICATE confidence scores and features sensitivity provide results which are more explainable and interpretable than purely neural methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analogical Structure, Minimal Contextual Cues and Contrastive Distractors: Input Design for Sample-Efficient Linguistic Rule Induction</title>
<link>https://arxiv.org/abs/2511.10441</link>
<guid>https://arxiv.org/abs/2511.10441</guid>
<content:encoded><![CDATA[
<div> Keywords: analogical structure, contrastive learning, lightweight models, linguistic rule learning, minimal data<br /><br />Summary:<br /><br />This paper explores whether organizing learning around analogical paradigms can allow lightweight language models to perform competitively with minimal training data, contrasting with conventional large models trained on vast datasets. The authors design a computational approach combining three cognitive-inspired principles: analogical structure, contrastive learning, and minimal contextual cues. They evaluate this approach using structured completion tasks where models select correct sentence completions from analogical patterns presented alongside contrastive alternatives. Using models with only 0.5 million parameters (BERT+CNN) trained on just 100 structured examples of English causative/inchoative alternations, the method achieves an F1 score of 0.95, outperforming zero-shot GPT-3’s 0.87 F1. Ablation studies confirm the importance of analogical organization and contrastive training, showing consistent performance gains over randomized baseline setups across different model architectures. Additional validation on another linguistic phenomenon, unspecified object alternations, replicates these efficiency improvements and demonstrates the robustness of the approach. The results suggest that organizing training paradigms analogically allows for competitive linguistic rule learning with orders of magnitude less data compared to conventional deep learning methods, opening paths toward more data-efficient language models. <div>
arXiv:2511.10441v1 Announce Type: new 
Abstract: Large language models achieve strong performance through training on vast datasets. Can analogical paradigm organization enable lightweight models to match this performance with minimal data? We develop a computational approach implementing three cognitive-inspired principles: analogical structure, contrastive learning, and minimal contextual cues. We test this approach with structured completion tasks where models identify correct sentence completions from analogical patterns with contrastive alternatives. Training lightweight models (BERT+CNN, $0.5M$ parameters) on only one hundred structured examples of English causative/inchoative alternations achieves $F1=0.95$, outperforming zero-shot \texttt{GPT-o3} ($F1=0.87$). Ablation studies confirm that analogical organization and contrastive structure improve performance, consistently surpassing randomly shuffled baselines across architectures. Cross-phenomenon validation using unspecified object alternations replicates these efficiency gains, confirming approach robustness. Our results show that analogical paradigm organization enables competitive linguistic rule learning with orders of magnitude less data than conventional approaches require.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning About Intent for Ambiguous Requests</title>
<link>https://arxiv.org/abs/2511.10453</link>
<guid>https://arxiv.org/abs/2511.10453</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, ambiguous requests, reinforcement learning, multiple interpretations, structured responses<br /><br />Summary:<br /><br />Large language models (LLMs) often handle ambiguous requests by selecting a single interpretation, which can lead to misunderstandings and potential safety issues. To mitigate this problem, the authors propose a method that generates multiple interpretation-answer pairs within a single structured response, thereby explicitly capturing different valid meanings. The approach leverages reinforcement learning with customized reward functions that use multiple valid answers as supervision signals, encouraging the model to cover a broader set of interpretations. The method is evaluated on tasks such as conversational question answering and semantic parsing, showing improved coverage of valid answers compared to baseline models. Human evaluations further verify that the predicted interpretations correspond well to their respective answers, enhancing the transparency of model outputs. This approach not only clarifies ambiguous requests by explicitly presenting multiple interpretations but also improves efficiency by requiring only one generation step to produce a structured output. Additionally, the structured format facilitates easier integration with downstream applications, making it a practical enhancement for handling ambiguity in LLM-driven systems. <div>
arXiv:2511.10453v1 Announce Type: new 
Abstract: Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic parsing demonstrate that our method achieves higher coverage of valid answers than baseline approaches. Human evaluation confirms that predicted interpretations are highly aligned with their answers. Our approach promotes transparency with explicit interpretations, achieves efficiency by requiring only one generation step, and supports downstream applications through its structured output format.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring State Tracking Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10457</link>
<guid>https://arxiv.org/abs/2511.10457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, state tracking, GPT-4, Llama3, Chain of Thought<br /><br />Summary:<br /><br />This paper investigates the capability of Large Language Models (LLMs) to perform state tracking, a task that requires maintaining and updating the state of multiple entities over time. To isolate the state tracking ability, the authors introduce a benchmark consisting of three distinct state tracking tasks designed to evaluate model performance in controlled scenarios. The evaluation focuses on recent LLMs, specifically GPT-4 and Llama3, assessing how well they track state information across several steps. Results demonstrate that these newer models show strong proficiency in tracking state, particularly when enhanced by techniques like Chain of Thought prompting, which help the model reason through intermediate steps. Conversely, earlier generation models, despite understanding the task initially and performing well in early stages, tend to degrade in performance as the number of steps increases, indicating limitations in long-term state memory or reasoning over extended sequences. This study highlights both the progress made in LLMs' reasoning capabilities and the remaining challenges in sustaining accurate state tracking over time, suggesting that integration with explicit reasoning strategies significantly boosts performance on such tasks. <div>
arXiv:2511.10457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in solving complex tasks, including those requiring a certain level of reasoning. In this paper, we focus on state tracking, a problem where models need to keep track of the state governing a number of entities. To isolate the state tracking component from other factors, we propose a benchmark based on three well-defined state tracking tasks and analyse the performance of LLMs in different scenarios. The results indicate that the recent generation of LLMs (specifically, GPT-4 and Llama3) are capable of tracking state, especially when integrated with mechanisms such as Chain of Thought. However, models from the former generation, while understanding the task and being able to solve it at the initial stages, often fail at this task after a certain number of steps.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning</title>
<link>https://arxiv.org/abs/2511.10459</link>
<guid>https://arxiv.org/abs/2511.10459</guid>
<content:encoded><![CDATA[
<div> local knowledge, benchmarks, LLM evaluation, geographic scale, local communities<br /><br />Summary:<br /><br />This paper addresses the gap in evaluating large language models (LLMs) on hyper-local knowledge, emphasizing the importance of systems that understand neighborhood-specific dynamics, cultural narratives, and local governance. The authors introduce LocalBench, the first benchmark designed to systematically assess LLMs on county-level local knowledge across the U.S., covering 14,782 validated question-answer pairs from 526 counties in 49 states. LocalBench integrates diverse data sources including Census statistics, local subreddit discussions, and regional news, and encompasses physical, cognitive, and relational aspects of locality. Thirteen state-of-the-art LLMs are evaluated on this benchmark in both closed-book and web-augmented modes. Results reveal significant weaknesses: the best models achieve only 56.8% accuracy on narrative questions, and less than 15.5% on numerical reasoning tasks. Notably, increasing model size or leveraging web search does not consistently improve performance; for instance, web search boosts Gemini by 13.6% but decreases GPT-series results by 11.4%. These findings highlight an urgent need for development of place-aware AI capable of equitable and fine-grained understanding of local communities across diverse geographic and cultural contexts. <div>
arXiv:2511.10459v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks</title>
<link>https://arxiv.org/abs/2511.10465</link>
<guid>https://arxiv.org/abs/2511.10465</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt optimization, knowledge integration, knowledge gap filling, batch-wise evaluation, token efficiency<br /><br />Summary:<br /><br />1) This paper addresses the limitations of current prompt optimization methods, which mainly rely on elicitation-based strategies that search for optimal prompts but struggle with knowledge-intensive tasks due to fixed parametric boundaries. 2) The authors propose Knowledge-Provision-based Prompt Optimization (KPPO), a novel framework that shifts the focus from elicitation to systematic knowledge integration to better handle factual accuracy, domain-specific terminology, and reasoning patterns. 3) KPPO introduces three main innovations: a knowledge gap filling mechanism to identify and remediate missing knowledge, a batch-wise candidate evaluation that balances performance gains with distributional stability, and an adaptive knowledge pruning strategy that reduces token usage by up to 29% without sacrificing performance. 4) The framework was extensively evaluated across 15 knowledge-intensive benchmarks spanning multiple domains. 5) Results demonstrate KPPO's superiority over state-of-the-art elicitation-based methods, achieving an average performance improvement of approximately 6% while maintaining comparable or lower token consumption, highlighting both its effectiveness and efficiency. The code for KPPO is publicly available at the provided GitHub repository. <div>
arXiv:2511.10465v1 Announce Type: new 
Abstract: While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following</title>
<link>https://arxiv.org/abs/2511.10507</link>
<guid>https://arxiv.org/abs/2511.10507</guid>
<content:encoded><![CDATA[
arXiv:2511.10507v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025</title>
<link>https://arxiv.org/abs/2511.10515</link>
<guid>https://arxiv.org/abs/2511.10515</guid>
<content:encoded><![CDATA[
arXiv:2511.10515v1 Announce Type: new 
Abstract: Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Say It Differently: Linguistic Styles as Jailbreak Vectors</title>
<link>https://arxiv.org/abs/2511.10519</link>
<guid>https://arxiv.org/abs/2511.10519</guid>
<content:encoded><![CDATA[
arXiv:2511.10519v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.
  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convomem Benchmark: Why Your First 150 Conversations Don't Need RAG</title>
<link>https://arxiv.org/abs/2511.10523</link>
<guid>https://arxiv.org/abs/2511.10523</guid>
<content:encoded><![CDATA[
arXiv:2511.10523v1 Announce Type: new 
Abstract: We introduce a comprehensive benchmark for conversational memory evaluation containing 75,336 question-answer pairs across diverse categories including user facts, assistant recall, abstention, preferences, temporal changes, and implicit connections. While existing benchmarks have advanced the field, our work addresses fundamental challenges in statistical power, data generation consistency, and evaluation flexibility that limit current memory evaluation frameworks. We examine the relationship between conversational memory and retrieval-augmented generation (RAG). While these systems share fundamental architectural patterns--temporal reasoning, implicit extraction, knowledge updates, and graph representations--memory systems have a unique characteristic: they start from zero and grow progressively with each conversation. This characteristic enables naive approaches that would be impractical for traditional RAG. Consistent with recent findings on long context effectiveness, we observe that simple full-context approaches achieve 70-82% accuracy even on our most challenging multi-message evidence cases, while sophisticated RAG-based memory systems like Mem0 achieve only 30-45% when operating on conversation histories under 150 interactions. Our analysis reveals practical transition points: long context excels for the first 30 conversations, remains viable with manageable trade-offs up to 150 conversations, and typically requires hybrid or RAG approaches beyond that point as costs and latencies become prohibitive. These patterns indicate that the small-corpus advantage of conversational memory--where exhaustive search and complete reranking are feasible--deserves dedicated research attention rather than simply applying general RAG solutions to conversation histories.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing the Formal and Institutional Boundaries of Contemporary Genre and Literary Fiction</title>
<link>https://arxiv.org/abs/2511.10546</link>
<guid>https://arxiv.org/abs/2511.10546</guid>
<content:encoded><![CDATA[
arXiv:2511.10546v1 Announce Type: new 
Abstract: Though the concept of genre has been a subject of discussion for millennia, the relatively recent emergence of genre fiction has added a new layer to this ongoing conversation. While more traditional perspectives on genre have emphasized form, contemporary scholarship has invoked both formal and institutional characteristics in its taxonomy of genre, genre fiction, and literary fiction. This project uses computational methods to explore the soundness of genre as a formal designation as opposed to an institutional one. Pulling from Andrew Piper's CONLIT dataset of Contemporary Literature, we assemble a corpus of literary and genre fiction, with the latter category containing romance, mystery, and science fiction novels. We use Welch's ANOVA to compare the distribution of narrative features according to author gender within each genre and within genre versus literary fiction. Then, we use logistic regression to model the effect that each feature has on literary classification and to measure how author gender moderates these effects. Finally, we analyze stylistic and semantic vector representations of our genre categories to understand the importance of form and content in literary classification. This project finds statistically significant formal markers of each literary category and illustrates how female authorship narrows and blurs the target for achieving literary status.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding</title>
<link>https://arxiv.org/abs/2511.10552</link>
<guid>https://arxiv.org/abs/2511.10552</guid>
<content:encoded><![CDATA[
arXiv:2511.10552v1 Announce Type: new 
Abstract: Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DESS: DeBERTa Enhanced Syntactic-Semantic Aspect Sentiment Triplet Extraction</title>
<link>https://arxiv.org/abs/2511.10577</link>
<guid>https://arxiv.org/abs/2511.10577</guid>
<content:encoded><![CDATA[
arXiv:2511.10577v1 Announce Type: new 
Abstract: Fine-grained sentiment analysis faces ongoing challenges in Aspect Sentiment Triple Extraction (ASTE), particularly in accurately capturing the relationships between aspects, opinions, and sentiment polarities. While researchers have made progress using BERT and Graph Neural Networks, the full potential of advanced language models in understanding complex language patterns remains unexplored. We introduce DESS, a new approach that builds upon previous work by integrating DeBERTa's enhanced attention mechanism to better understand context and relationships in text. Our framework maintains a dual-channel structure, where DeBERTa works alongside an LSTM channel to process both meaning and grammatical patterns in text. We have carefully refined how these components work together, paying special attention to how different types of language information interact. When we tested DESS on standard datasets, it showed meaningful improvements over current methods, with F1-score increases of 4.85, 8.36, and 2.42 in identifying aspect opinion pairs and determining sentiment accurately. Looking deeper into the results, we found that DeBERTa's sophisticated attention system helps DESS handle complicated sentence structures better, especially when important words are far apart. Our findings suggest that upgrading to more advanced language models when thoughtfully integrated, can lead to real improvements in how well we can analyze sentiments in text. The implementation of our approach is publicly available at: https://github.com/VishalRepos/DESS.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Prompting Strategies with MedGemma for Medical Order Extraction</title>
<link>https://arxiv.org/abs/2511.10583</link>
<guid>https://arxiv.org/abs/2511.10583</guid>
<content:encoded><![CDATA[
arXiv:2511.10583v1 Announce Type: new 
Abstract: The accurate extraction of medical orders from doctor-patient conversations is a critical task for reducing clinical documentation burdens and ensuring patient safety. This paper details our team submission to the MEDIQA-OE-2025 Shared Task. We investigate the performance of MedGemma, a new domain-specific open-source language model, for structured order extraction. We systematically evaluate three distinct prompting paradigms: a straightforward one-Shot approach, a reasoning-focused ReAct framework, and a multi-step agentic workflow. Our experiments reveal that while more complex frameworks like ReAct and agentic flows are powerful, the simpler one-shot prompting method achieved the highest performance on the official validation set. We posit that on manually annotated transcripts, complex reasoning chains can lead to "overthinking" and introduce noise, making a direct approach more robust and efficient. Our work provides valuable insights into selecting appropriate prompting strategies for clinical information extraction in varied data conditions.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.10591</link>
<guid>https://arxiv.org/abs/2511.10591</guid>
<content:encoded><![CDATA[
arXiv:2511.10591v1 Announce Type: new 
Abstract: The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently. The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images. In this work, we present two complementary approaches developed for the English track. The first leverages a mined prompting strategy, where training data is embedded and the top-k most similar examples are retrieved to serve as few-shot demonstrations during generation. The second approach builds on a metadata ablation study, which identified four metadata attributes that consistently enhance response quality. We train classifiers to predict these attributes for test cases and incorporate them into the generation pipeline, dynamically adjusting outputs based on prediction confidence. Experimental results demonstrate that mined prompting improves response relevance, while metadata-guided generation further refines clinical precision. Together, these methods highlight promising directions for developing AI-driven tools that can provide reliable and efficient wound care support.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know Your Limits: Entropy Estimation Modeling for Compression and Generalization</title>
<link>https://arxiv.org/abs/2511.10618</link>
<guid>https://arxiv.org/abs/2511.10618</guid>
<content:encoded><![CDATA[
arXiv:2511.10618v1 Announce Type: new 
Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Socratic Self-Refine for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2511.10621</link>
<guid>https://arxiv.org/abs/2511.10621</guid>
<content:encoded><![CDATA[
arXiv:2511.10621v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instella: Fully Open Language Models with Stellar Performance</title>
<link>https://arxiv.org/abs/2511.10628</link>
<guid>https://arxiv.org/abs/2511.10628</guid>
<content:encoded><![CDATA[
arXiv:2511.10628v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box On-Policy Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10643</link>
<guid>https://arxiv.org/abs/2511.10643</guid>
<content:encoded><![CDATA[
arXiv:2511.10643v1 Announce Type: new 
Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference</title>
<link>https://arxiv.org/abs/2511.10645</link>
<guid>https://arxiv.org/abs/2511.10645</guid>
<content:encoded><![CDATA[
arXiv:2511.10645v1 Announce Type: new 
Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability-Biased Attention over Directed Bipartite Graphs for Long-Tail ICD Coding</title>
<link>https://arxiv.org/abs/2511.09559</link>
<guid>https://arxiv.org/abs/2511.09559</guid>
<content:encoded><![CDATA[
arXiv:2511.09559v1 Announce Type: cross 
Abstract: Automated International Classification of Diseases (ICD) coding aims to assign multiple disease codes to clinical documents, constituting a crucial multi-label text classification task in healthcare informatics. However, the task is challenging due to its large label space (10,000 to 20,000 codes) and long-tail distribution, where a few codes dominate while many rare codes lack sufficient training data. To address this, we propose a learning method that models fine-grained co-occurrence relationships among codes. Specifically, we construct a Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes. To facilitate a one-way information flow, edges are directed exclusively from common to rare codes. The nature of these connections is defined by a probability-based bias, which is derived from the conditional probability of a common code co-occurring given the presence of a rare code. This bias is then injected into the encoder's attention module, a process we term Co-occurrence Encoding. This structure empowers the graph encoder to enrich rare code representations by aggregating latent comorbidity information reflected in the statistical co-occurrence of their common counterparts. To ensure high-quality input to the graph, we utilize a large language model (LLM) to generate comprehensive descriptions for codes, enriching initial embeddings with clinical context and comorbidity information, serving as external knowledge for the statistical co-occurrence relationships in the code system. Experiments on three automated ICD coding benchmark datasets demonstrate that our method achieves state-of-the-art performance with particularly notable improvements in Macro-F1, which is the key metric for long-tail classification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning</title>
<link>https://arxiv.org/abs/2511.09893</link>
<guid>https://arxiv.org/abs/2511.09893</guid>
<content:encoded><![CDATA[
arXiv:2511.09893v1 Announce Type: cross 
Abstract: Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\_repeat\_ngram\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers</title>
<link>https://arxiv.org/abs/2511.09926</link>
<guid>https://arxiv.org/abs/2511.09926</guid>
<content:encoded><![CDATA[
arXiv:2511.09926v1 Announce Type: cross 
Abstract: Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning</title>
<link>https://arxiv.org/abs/2511.10067</link>
<guid>https://arxiv.org/abs/2511.10067</guid>
<content:encoded><![CDATA[
arXiv:2511.10067v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at https://muser-llm.github.io.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.10240</link>
<guid>https://arxiv.org/abs/2511.10240</guid>
<content:encoded><![CDATA[
arXiv:2511.10240v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FactGuard: Event-Centric and Commonsense-Guided Fake News Detection</title>
<link>https://arxiv.org/abs/2511.10281</link>
<guid>https://arxiv.org/abs/2511.10281</guid>
<content:encoded><![CDATA[
arXiv:2511.10281v1 Announce Type: cross 
Abstract: Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10287</link>
<guid>https://arxiv.org/abs/2511.10287</guid>
<content:encoded><![CDATA[
arXiv:2511.10287v1 Announce Type: cross 
Abstract: Since Multimodal Large Language Models (MLLMs) are increasingly being integrated into everyday tools and intelligent agents, growing concerns have arisen regarding their possible output of unsafe contents, ranging from toxic language and biased imagery to privacy violations and harmful misinformation. Current safety benchmarks remain highly limited in both modality coverage and performance evaluations, often neglecting the extensive landscape of content safety. In this work, we introduce OutSafe-Bench, the first most comprehensive content safety evaluation test suite designed for the multimodal era. OutSafe-Bench includes a large-scale dataset that spans four modalities, featuring over 18,000 bilingual (Chinese and English) text prompts, 4,500 images, 450 audio clips and 450 videos, all systematically annotated across nine critical content risk categories. In addition to the dataset, we introduce a Multidimensional Cross Risk Score (MCRS), a novel metric designed to model and assess overlapping and correlated content risks across different categories. To ensure fair and robust evaluation, we propose FairScore, an explainable automated multi-reviewer weighted aggregation framework. FairScore selects top-performing models as adaptive juries, thereby mitigating biases from single-model judgments and enhancing overall evaluation reliability. Our evaluation of nine state-of-the-art MLLMs reveals persistent and substantial safety vulnerabilities, underscoring the pressing need for robust safeguards in MLLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Music Flamingo: Scaling Music Understanding in Audio Language Models</title>
<link>https://arxiv.org/abs/2511.10289</link>
<guid>https://arxiv.org/abs/2511.10289</guid>
<content:encoded><![CDATA[
arXiv:2511.10289v1 Announce Type: cross 
Abstract: We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Propagation in Social Networks using Large Language Models</title>
<link>https://arxiv.org/abs/2511.10384</link>
<guid>https://arxiv.org/abs/2511.10384</guid>
<content:encoded><![CDATA[
arXiv:2511.10384v1 Announce Type: cross 
Abstract: Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentEvolver: Towards Efficient Self-Evolving Agent System</title>
<link>https://arxiv.org/abs/2511.10395</link>
<guid>https://arxiv.org/abs/2511.10395</guid>
<content:encoded><![CDATA[
arXiv:2511.10395v1 Announce Type: cross 
Abstract: Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title>
<link>https://arxiv.org/abs/2511.10400</link>
<guid>https://arxiv.org/abs/2511.10400</guid>
<content:encoded><![CDATA[
arXiv:2511.10400v1 Announce Type: cross 
Abstract: Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Layer Norm on Memorization and Generalization in Transformers</title>
<link>https://arxiv.org/abs/2511.10566</link>
<guid>https://arxiv.org/abs/2511.10566</guid>
<content:encoded><![CDATA[
arXiv:2511.10566v1 Announce Type: cross 
Abstract: Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Emotionally Intelligent and Responsible Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10573</link>
<guid>https://arxiv.org/abs/2511.10573</guid>
<content:encoded><![CDATA[
arXiv:2511.10573v1 Announce Type: cross 
Abstract: Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals</title>
<link>https://arxiv.org/abs/2511.10615</link>
<guid>https://arxiv.org/abs/2511.10615</guid>
<content:encoded><![CDATA[
arXiv:2511.10615v1 Announce Type: cross 
Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Correction in Radiology Reports: A Knowledge Distillation-Based Multi-Stage Framework</title>
<link>https://arxiv.org/abs/2406.15045</link>
<guid>https://arxiv.org/abs/2406.15045</guid>
<content:encoded><![CDATA[
arXiv:2406.15045v3 Announce Type: replace 
Abstract: The increasing complexity and workload of clinical radiology leads to inevitable oversights and mistakes in their use as diagnostic tools, causing delayed treatments and sometimes life-threatening harm to patients. While large language models (LLMs) have shown remarkable progress in many tasks, their utilities in detecting and correcting errors in radiology reporting are limited. This paper proposes a novel dual-knowledge infusion framework that enhances LLMs' capability for radiology report proofreading through systematic integration of medical expertise. Specifically, the knowledge infusion combines medical knowledge graph distillation (MKGD) with external knowledge retrieval (EXKR), enabling an effective automated approach in tackling mistakes in radiology reporting. By decomposing the complex proofreading task into three specialized stages of detection, localization, and correction, our method mirrors the systematic review process employed by expert radiologists, ensuring both precision and clinical interpretability. To perform a robust, clinically relevant evaluation, a comprehensive benchmark is also proposed using real-world radiology reports with real-world error patterns, including speech recognition confusions, terminology ambiguities, and template-related inconsistencies. Extensive evaluations across multiple LLM architectures demonstrate substantial improvements of our approach: up to 31.56% increase in error detection accuracy and 37.4% reduction in processing time. Human evaluation by radiologists confirms superior clinical relevance and factual consistency compared to existing approaches.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiating between human-written and AI-generated texts using linguistic features automatically extracted from an online computational tool</title>
<link>https://arxiv.org/abs/2407.03646</link>
<guid>https://arxiv.org/abs/2407.03646</guid>
<content:encoded><![CDATA[
arXiv:2407.03646v3 Announce Type: replace 
Abstract: While extensive research has focused on ChatGPT in recent years, very few studies have systematically quantified and compared linguistic features between human-written and Artificial Intelligence (AI)-generated language. This study aims to investigate how various linguistic components are represented in both types of texts, assessing the ability of AI to emulate human writing. Using human-authored essays as a benchmark, we prompted ChatGPT to generate essays of equivalent length. These texts were analyzed using Open Brain AI, an online computational tool, to extract measures of phonological, morphological, syntactic, and lexical constituents. Despite AI-generated texts appearing to mimic human speech, the results revealed significant differences across multiple linguistic features such as consonants, word stress, nouns, verbs, pronouns, direct objects, prepositional modifiers, and use of difficult words among others. These findings underscore the importance of integrating automated tools for efficient language assessment, reducing time and effort in data analysis. Moreover, they emphasize the necessity for enhanced training methodologies to improve the capacity of AI for producing more human-like text.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Turn Interactions for Text-to-SQL with Large Language Models</title>
<link>https://arxiv.org/abs/2408.11062</link>
<guid>https://arxiv.org/abs/2408.11062</guid>
<content:encoded><![CDATA[
arXiv:2408.11062v2 Announce Type: replace 
Abstract: This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods are still inefficient and struggle to handle cases with wide tables effectively. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide a universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our approach achieves advanced performance on the Spider and BIRD datasets as well as their variants. Notably, we obtain state-of-the-art results on the BIRD leaderboard under the setting without oracle knowledge, demonstrating the effectiveness of our method.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lessons in co-creation: the inconvenient truths of inclusive sign language technology development</title>
<link>https://arxiv.org/abs/2408.13171</link>
<guid>https://arxiv.org/abs/2408.13171</guid>
<content:encoded><![CDATA[
arXiv:2408.13171v2 Announce Type: replace 
Abstract: In the era of AI-driven language technologies, the participation of deaf communities in sign language technology development, often framed as co-creation, is increasingly emphasized. We present a reflexive case study of two Horizon 2020 projects on sign language machine translation (2021- 2023), conducted with a EUD, a European-level deaf-led NGO. Using participant observation, internal documentation, and collaborative analysis among the authors, we interrogate co-creation as both a practice and a discourse. We offer five lessons for making co-creation consequential: 1) recognise and resource deaf partners invisible labor, 2) manage expectations via accessible science communication, 3) crip co-creation by dismantling structural ableism, 4) diversify participatory methods to address co-creation fatigue and intersectionality, and 5) redistribute power through deaf leadership. We contribute an empirically grounded account of how co-creation plays out in multi-partner AI projects, and actionable implications for design that extend to participatory AI with minoritized language and disability communities.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedMobile: A mobile-sized language model with clinical capabilities</title>
<link>https://arxiv.org/abs/2410.09019</link>
<guid>https://arxiv.org/abs/2410.09019</guid>
<content:encoded><![CDATA[
arXiv:2410.09019v2 Announce Type: replace 
Abstract: Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. To address these significant limitations, we introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We perform a careful set of pipeline additions and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements. We evaluate the efficiency of our pipeline on the MultiMedQA and MedBullets. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for licensed physicians (~60%) and rivaling scores of models 100 times its size. Across the entirety of the MultiMedQA, MedMobile achieves SOTA performance for models with less than 5B parameters and represents the smallest model to pass the MedQA (USMLE). MedMobile holds promise to democratize access to language models in medicine, bolstering lower compute needs and fast inference speeds. With the ability to combat the biggest barriers to entry for language models in medicine, we hope that MedMobile is a critical step forward in developing clinically relevant language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Captions Speak Louder than Images: Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data</title>
<link>https://arxiv.org/abs/2410.17337</link>
<guid>https://arxiv.org/abs/2410.17337</guid>
<content:encoded><![CDATA[
arXiv:2410.17337v2 Announce Type: replace 
Abstract: Leveraging multimodal data to drive breakthroughs in e-commerce applications through Multimodal Foundation Models (MFMs) is gaining increasing attention from the research community. However, there are significant challenges that hinder the optimal use of multimodal e-commerce data by foundation models: (1) the scarcity of large-scale, high-quality multimodal benchmark datasets; and (2) the lack of effective multimodal information integration methods. To address these challenges, in this paper, we introduce MMECInstruct, the first-ever, large-scale, and high-quality multimodal instruction dataset for e-commerce. We also develop CASLIE, a simple, lightweight, yet effective framework for integrating multimodal information for e-commerce. Leveraging MMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted as CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models substantially outperform 5 categories of advanced baseline models in the in-domain evaluation. Moreover, CASLIE models show strong generalizability to out-of-domain settings. MMECInstruct and CASLIE models are publicly accessible through https://ninglab.github.io/CASLIE/.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing the Scope of Language Models</title>
<link>https://arxiv.org/abs/2410.21597</link>
<guid>https://arxiv.org/abs/2410.21597</guid>
<content:encoded><![CDATA[
arXiv:2410.21597v3 Announce Type: replace 
Abstract: Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic, Orthographic, and Phonological Biases in Humans' Wordle Gameplay</title>
<link>https://arxiv.org/abs/2411.18634</link>
<guid>https://arxiv.org/abs/2411.18634</guid>
<content:encoded><![CDATA[
arXiv:2411.18634v2 Announce Type: replace 
Abstract: We show that human players' gameplay in the game of Wordle is influenced by the semantics, orthography, and phonology of the player's previous guesses. We compare actual human players' guesses with near-optimal guesses using NLP techniques. We study human language use in the constrained environment of Wordle, which is situated between natural language use and the artificial word association task
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model</title>
<link>https://arxiv.org/abs/2501.11094</link>
<guid>https://arxiv.org/abs/2501.11094</guid>
<content:encoded><![CDATA[
arXiv:2501.11094v2 Announce Type: replace 
Abstract: Suicidal ideation detection is crucial for preventing suicides, a leading cause of death worldwide. Many individuals express suicidal thoughts on social media, offering a vital opportunity for early detection through advanced machine learning techniques. The identification of suicidal ideation in social media text is improved by utilising a hybrid framework that integrates Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), enhanced with an attention mechanism. To enhance the interpretability of the model's predictions, Explainable AI (XAI) methods are applied, with a particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At first, the model managed to reach an accuracy of 92.81%. By applying fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The SHAP analysis revealed key features influencing the model's predictions, such as terms related to mental health struggles. This level of transparency boosts the model's credibility while helping mental health professionals understand and trust the predictions. This work highlights the potential for improving the accuracy and interpretability of detecting suicidal tendencies, making a valuable contribution to the progress of mental health monitoring systems. It emphasizes the significance of blending powerful machine learning methods with explainability to develop reliable and impactful mental health solutions.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors</title>
<link>https://arxiv.org/abs/2501.14250</link>
<guid>https://arxiv.org/abs/2501.14250</guid>
<content:encoded><![CDATA[
arXiv:2501.14250v2 Announce Type: replace 
Abstract: Large language models (LLMs) are widely used in real-world applications, raising concerns about their safety and trustworthiness. While red-teaming with jailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus primarily on single-turn attacks, overlooking the multi-turn strategies used by real-world adversaries. Existing multi-turn methods rely on static patterns or predefined logical chains, failing to account for the dynamic strategies during attacks. We propose Siren, a learning-based multi-turn attack framework designed to simulate real-world human jailbreak behaviors. Siren consists of three stages: (1) MiniMax-driven training set construction utilizing Turn-Level LLM feedback, (2) post-training attackers with supervised fine-tuning (SFT) and direct preference optimization (DPO), and (3) interactions between the attacking and target LLMs. Experiments demonstrate that Siren achieves an attack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against Gemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o, significantly outperforming single-turn baselines. Moreover, Siren with a 7B-scale model achieves performance comparable to a multi-turn baseline that leverages GPT-4o as the attacker, while requiring fewer turns and employing decomposition strategies that are better semantically aligned with attack goals. We hope Siren inspires the development of stronger defenses against advanced multi-turn jailbreak attacks under realistic scenarios. Code is available at https://github.com/YiyiyiZhao/siren. Warning: This paper contains potentially harmful text.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning</title>
<link>https://arxiv.org/abs/2502.02390</link>
<guid>https://arxiv.org/abs/2502.02390</guid>
<content:encoded><![CDATA[
arXiv:2502.02390v3 Announce Type: replace 
Abstract: Research on LLM technologies is rapidly emerging, with most of them employ a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. We validate CoAT's effectiveness across a variety of generative and reasoning tasks. Quantitative experiments show that CoAT achieves over 10% performance improvement on open-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15% gain on our proprietary CRB dataset.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMTEB: Massive Multilingual Text Embedding Benchmark</title>
<link>https://arxiv.org/abs/2502.13595</link>
<guid>https://arxiv.org/abs/2502.13595</guid>
<content:encoded><![CDATA[
arXiv:2502.13595v4 Announce Type: replace 
Abstract: Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</title>
<link>https://arxiv.org/abs/2502.18573</link>
<guid>https://arxiv.org/abs/2502.18573</guid>
<content:encoded><![CDATA[
arXiv:2502.18573v3 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success in generative tasks, yet they often fall short in ensuring the factual accuracy of their outputs, thus limiting their reliability in real-world applications where correctness is critical. In this paper, we present FactReasoner, a novel neuro-symbolic based factuality assessment framework that employs probabilistic reasoning to evaluate the truthfulness of long-form generated responses. FactReasoner decomposes a response into atomic units, retrieves relevant contextual information from external knowledge sources, and models the logical relationships (e.g., entailment, contradiction) between these units and their contexts using probabilistic encodings. It then estimates the posterior probability that each atomic unit is supported by the retrieved evidence. Our experiments on both labeled and unlabeled benchmark datasets demonstrate that FactReasoner often outperforms state-of-the-art prompt-based methods in terms of factual precision and recall. Our open-source implementation is publicly available at: https://github.com/IBM/FactReasoner.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers</title>
<link>https://arxiv.org/abs/2504.03595</link>
<guid>https://arxiv.org/abs/2504.03595</guid>
<content:encoded><![CDATA[
arXiv:2504.03595v3 Announce Type: replace 
Abstract: A key element to support the increased amounts of renewable energy in the energy system is flexibility, i.e., the possibility of changing energy loads in time and amount. Many flexibility models have been designed; however, exact models fail to scale for long time horizons or many devices. Because of this, the FlexOffers model has been designed, to provide device-independent approximations of flexibility with good accuracy, and much better scaling for long time horizons and many devices. An important aspect of the real-life implementation of energy flexibility is enabling flexible data exchange with many smart energy appliances and market systems, e.g., in smart buildings. For this, ontologies standardizing data formats are required. However, the current industry standard ontology for integrating smart devices for energy purposes, SAREF for Energy Flexibility (SAREF4ENER), only has limited support for flexibility and thus cannot support important use cases. In this paper, we propose an extension of SAREF4ENER that integrates full support for the complete FlexOffer model, including advanced use cases, while maintaining backward compatibility. This novel ontology module can accurately describe flexibility for advanced devices such as electric vehicles, batteries, and heat pumps. It can also capture the inherent uncertainty associated with many flexible load types.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Specific Knowledge: Do Models Know Better in X than in English?</title>
<link>https://arxiv.org/abs/2505.14990</link>
<guid>https://arxiv.org/abs/2505.14990</guid>
<content:encoded><![CDATA[
arXiv:2505.14990v2 Announce Type: replace 
Abstract: Often, multilingual language models are trained with the objective to map semantically similar content (in different languages) in the same latent space. In this paper, we show a nuance in this training objective, and find that by changing the language of the input query, we can improve the question answering ability of language models. Our contributions are two-fold. First, we introduce the term Language Specific Knowledge (LSK) to denote queries that are best answered in an "expert language" for a given LLM, thereby enhancing its question-answering ability. We introduce the problem of language selection -- for some queries, language models can perform better when queried in languages other than English, sometimes even better in low-resource languages -- and the goal is to select the optimal language for the query. Second, we introduce simple to strong baselines to test this problem. Additionally, as a first-pass solution to this novel problem, we design LSKExtractor to benchmark the language-specific knowledge present in a language model and then exploit it during inference. To test our framework, we employ three datasets that contain knowledge about both cultural and social behavioral norms. Overall, LSKExtractor achieves up to 10% relative improvement across datasets, and is competitive against strong baselines, while being feasible in real-world settings. Broadly, our research contributes to the open-source development (https://github.com/agarwalishika/LSKExtractor/tree/main) of language models that are inclusive and more aligned with the cultural and linguistic contexts in which they are deployed.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search</title>
<link>https://arxiv.org/abs/2505.16838</link>
<guid>https://arxiv.org/abs/2505.16838</guid>
<content:encoded><![CDATA[
arXiv:2505.16838v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by enabling step-by-step problem-solving, yet its extension to Long-CoT introduces substantial computational overhead due to increased token length. Existing compression approaches -- instance-level and token-level -- either sacrifice essential local reasoning signals like reflection or yield incoherent outputs. To address these limitations, we propose R1-Compress, a two-stage chunk-level compression framework that preserves both local information and coherence. Our method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk compression, and employs an inter-chunk search mechanism to select the short and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500, AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces token usage while maintaining comparable reasoning accuracy. On MATH500, R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to the Long-CoT baseline, while reducing token usage by about 20%. Source code will be available at https://github.com/w-yibo/R1-Compress
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Textual Gradients via Sampling-Based Momentum</title>
<link>https://arxiv.org/abs/2506.00400</link>
<guid>https://arxiv.org/abs/2506.00400</guid>
<content:encoded><![CDATA[
arXiv:2506.00400v2 Announce Type: replace 
Abstract: LLM-based prompt optimization, that uses LLM-provided "textual gradients" (feedback) to refine prompts, has emerged an effective method for automatic prompt engineering. However, its scalability and stability are unclear when using more data in training. We systematically investigate the potential and challenges of scaling training data in textual gradient descent. We show that naively scaling training examples is infeasible due to both explicit context-length limits and an implicit context wall, where long-context degradation yields diminishing returns. Inspired by prior wisdom in stochastic gradient descent, we propose Textual Stochastic Gradient Descent with Momentum (TSGD-M), which reweights updates through momentum sampling, using bootstrapped minibatch validation accuracy as importance weights over historical prompts. We introduce Gumbel-Top-$k$ sampling for prompt generation, balancing exploration--exploitation and improving sampling efficiency while maintaining a low-variance running mean estimator. TSGD-M integrates seamlessly into existing prompt optimization frameworks, including TextGrad, DSPy-COPRO, and AdalFlow, and achieves consistent gains across 5 benchmarks. Our findings highlight the importance of incorporating probabilistic exploration into textual-gradient-based optimization, paving the way for more stable and scalable prompt optimization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
arXiv:2506.01939v2 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
arXiv:2506.19794v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling</title>
<link>https://arxiv.org/abs/2506.21572</link>
<guid>https://arxiv.org/abs/2506.21572</guid>
<content:encoded><![CDATA[
arXiv:2506.21572v2 Announce Type: replace 
Abstract: Evaluating multimodal large language models (MLLMs) is fundamentally challenged by the absence of structured, interpretable, and theoretically grounded benchmarks; current heuristically-grouped tasks have vague cognitive targets, overlapping abilities, redundant indicators, and weak diagnostic power. We therefore propose a structural-equation-modeling-aligned framework that quantifies internal validity, dimensional separability, and component contributions, and introduce a Piaget-inspired capability hierarchy that stratifies MLLM abilities into Perception, Memory, and Reasoning. Reorganizing existing tasks under this theory, we build the GOLD benchmark, whose experiments show superior interpretability, lower indicator redundancy, and clearer cognitive consistency than prior benchmarks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs</title>
<link>https://arxiv.org/abs/2508.02573</link>
<guid>https://arxiv.org/abs/2508.02573</guid>
<content:encoded><![CDATA[
arXiv:2508.02573v2 Announce Type: replace 
Abstract: Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test Set Quality in Multilingual LLM Evaluation</title>
<link>https://arxiv.org/abs/2508.02635</link>
<guid>https://arxiv.org/abs/2508.02635</guid>
<content:encoded><![CDATA[
arXiv:2508.02635v2 Announce Type: replace 
Abstract: Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Perplexity: Let the Reader Select Retrieval Summaries via Spectrum Projection Score</title>
<link>https://arxiv.org/abs/2508.05909</link>
<guid>https://arxiv.org/abs/2508.05909</guid>
<content:encoded><![CDATA[
arXiv:2508.05909v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We move beyond perplexity and introduce Spectrum Projection Score (SPS), a lightweight and supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference-time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open-sourced LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-commerce Models</title>
<link>https://arxiv.org/abs/2508.15721</link>
<guid>https://arxiv.org/abs/2508.15721</guid>
<content:encoded><![CDATA[
arXiv:2508.15721v2 Announce Type: replace 
Abstract: E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://github.com/ninglab/EcomMMMU.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</title>
<link>https://arxiv.org/abs/2509.19319</link>
<guid>https://arxiv.org/abs/2509.19319</guid>
<content:encoded><![CDATA[
arXiv:2509.19319v2 Announce Type: replace 
Abstract: The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Backdoor Attacks Against Speech Language Models</title>
<link>https://arxiv.org/abs/2510.01157</link>
<guid>https://arxiv.org/abs/2510.01157</guid>
<content:encoded><![CDATA[
arXiv:2510.01157v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) and their multimodal extensions are becoming increasingly popular. One common approach to enable multimodality is to cascade domain-specific encoders with an LLM, making the resulting model inherit vulnerabilities from all of its components. In this work, we present the first systematic study of audio backdoor attacks against speech language models. We demonstrate its effectiveness across four speech encoders and three datasets, covering four tasks: automatic speech recognition (ASR), speech emotion recognition, and gender and age prediction. The attack consistently achieves high success rates, ranging from 90.76% to 99.41%. To better understand how backdoors propagate, we conduct a component-wise analysis to identify the most vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based defense that mitigates the threat of poisoned pretrained encoders.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.01252</link>
<guid>https://arxiv.org/abs/2510.01252</guid>
<content:encoded><![CDATA[
arXiv:2510.01252v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. We introduce a pipeline that couples LLMs with sparse autoencoders (SAEs) to trace how different themes are encoded during training. As a controlled case study, we trained a GPT-style model on 37 nineteenth-century novels by ten female authors, a corpus centered on themes such as gender, marriage, class, and morality. By applying SAEs across layers and probing with eleven social and moral categories, we mapped sparse features to human-interpretable concepts. The analysis revealed stable thematic backbones (most prominently around gender and kinship) and showed how associations expand and entangle with depth. More broadly, we argue that the LLM+SAEs pipeline offers a scalable framework for auditing how cultural assumptions from the data are embedded in model representations.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Correlates of Language Models Are Specific to Human Language</title>
<link>https://arxiv.org/abs/2510.03156</link>
<guid>https://arxiv.org/abs/2510.03156</guid>
<content:encoded><![CDATA[
arXiv:2510.03156v2 Announce Type: replace 
Abstract: Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making</title>
<link>https://arxiv.org/abs/2510.03553</link>
<guid>https://arxiv.org/abs/2510.03553</guid>
<content:encoded><![CDATA[
arXiv:2510.03553v2 Announce Type: replace 
Abstract: Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation</title>
<link>https://arxiv.org/abs/2510.12858</link>
<guid>https://arxiv.org/abs/2510.12858</guid>
<content:encoded><![CDATA[
arXiv:2510.12858v2 Announce Type: replace 
Abstract: The art and science of Quranic recitation (Tajweed), a discipline governed by meticulous phonetic, rhythmic, and theological principles, confronts substantial educational challenges in today's digital age. Although modern technology offers unparalleled opportunities for learning, existing automated systems for evaluating recitation have struggled to gain broad acceptance or demonstrate educational effectiveness. This literature review examines this crucial disparity, offering a thorough analysis of scholarly research, digital platforms, and commercial tools developed over the past twenty years. Our analysis uncovers a fundamental flaw in current approaches that adapt Automatic Speech Recognition (ASR) systems, which emphasize word identification over qualitative acoustic evaluation. These systems suffer from limitations such as reliance on biased datasets, demographic disparities, and an inability to deliver meaningful feedback for improvement. Challenging these data-centric methodologies, we advocate for a paradigm shift toward a knowledge-based computational framework. By leveraging the unchanging nature of the Quranic text and the well-defined rules of Tajweed, we propose that an effective evaluation system should be built upon rule-based acoustic modeling centered on canonical pronunciation principles and articulation points (Makhraj), rather than depending on statistical patterns derived from flawed or biased data. The review concludes that the future of automated Quranic recitation assessment lies in hybrid systems that combine linguistic expertise with advanced audio processing. Such an approach paves the way for developing reliable, fair, and pedagogically effective tools that can authentically assist learners across the globe.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</title>
<link>https://arxiv.org/abs/2501.18638</link>
<guid>https://arxiv.org/abs/2501.18638</guid>
<content:encoded><![CDATA[
arXiv:2501.18638v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly prevalent, ensuring their robustness against adversarial misuse is crucial. This paper introduces the GAP (Graph of Attacks with Pruning) framework, an advanced approach for generating stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP addresses limitations in existing tree-based LLM jailbreak methods by implementing an interconnected graph structure that enables knowledge sharing across attack paths. Our experimental evaluation demonstrates GAP's superiority over existing techniques, achieving a 20.8% increase in attack success rates while reducing query costs by 62.7%. GAP consistently outperforms state-of-the-art methods for attacking both open and closed LLMs, with attack success rates of >96%. Additionally, we present specialized variants like GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks. GAP-generated prompts prove highly effective in improving content moderation systems, increasing true positive detection rates by 108.5% and accuracy by 183.6% when used for fine-tuning. Our implementation is available at https://github.com/dsbuddy/GAP-LLM-Safety.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unique Hard Attention: A Tale of Two Sides</title>
<link>https://arxiv.org/abs/2503.14615</link>
<guid>https://arxiv.org/abs/2503.14615</guid>
<content:encoded><![CDATA[
arXiv:2503.14615v3 Announce Type: replace-cross 
Abstract: Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention -- in that case, they correspond to a \emph{strictly weaker} fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to \emph{soft} attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives</title>
<link>https://arxiv.org/abs/2505.17519</link>
<guid>https://arxiv.org/abs/2505.17519</guid>
<content:encoded><![CDATA[
arXiv:2505.17519v2 Announce Type: replace-cross 
Abstract: In the era of rapid generative AI development, interactions with large language models (LLMs) pose increasing risks of misuse. Prior research has primarily focused on attacks using template-based prompts and optimization-oriented methods, while overlooking the fact that LLMs possess strong unconstrained deceptive capabilities to attack other LLMs. This paper introduces a novel jailbreaking method inspired by the Chain-of-Thought mechanism. The attacker employs mission transfer to conceal harmful user intent within dialogue and generates a progressive chain of lure questions without relying on predefined templates, enabling successful jailbreaks. To further improve the attack's strength, we incorporate a helper LLM model that performs randomized narrative optimization over multi-turn interactions, enhancing the attack performance while preserving alignment with the original intent. We also propose a toxicity-based framework using third-party LLMs to evaluate harmful content and its alignment with malicious intent. Extensive experiments demonstrate that our method consistently achieves high attack success rates and elevated toxicity scores across diverse types of LLMs under black-box API settings. These findings reveal the intrinsic potential of LLMs to perform unrestricted attacks in the absence of robust alignment constraints. Our approach offers data-driven insights to inform the design of future alignment mechanisms. Finally, we propose two concrete defense strategies to support the development of safer generative models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
arXiv:2507.20067v2 Announce Type: replace-cross 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</title>
<link>https://arxiv.org/abs/2508.05615</link>
<guid>https://arxiv.org/abs/2508.05615</guid>
<content:encoded><![CDATA[
arXiv:2508.05615v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), transforming these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: using only 1,272 unlabeled data, GUI-RCPO achieves 3-6% accuracy improvements across various architectures on ScreenSpot benchmarks. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more data-efficient GUI agents.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</title>
<link>https://arxiv.org/abs/2509.03113</link>
<guid>https://arxiv.org/abs/2509.03113</guid>
<content:encoded><![CDATA[
arXiv:2509.03113v3 Announce Type: replace-cross 
Abstract: Multimodal large language models achieve strong performance across diverse tasks but remain prone to hallucinations, where outputs are not grounded in visual inputs. This issue can be attributed to two main biases: text-visual bias, the overreliance on prompts and prior outputs, and co-occurrence bias, spurious correlations between frequently paired objects. We propose Gradient-based Influence-Aware Constrained Decoding (GACD), an inference-based method, that addresses both biases without auxiliary models, and is readily applicable to existing models without finetuning. The core of our approach is bias estimation, which uses first-order Taylor gradients to understand the contribution of individual tokens-visual features and text tokens-to the current output. Based on this analysis, GACD mitigates hallucinations through two components: (1) suppressing spurious visual features correlated with the output objects, and (2) rebalancing cross-modal contributions by strengthening visual features relative to text. Experiments across multiple benchmarks demonstrate that GACD effectively reduces hallucinations and improves the visual grounding of MLLM outputs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</title>
<link>https://arxiv.org/abs/2509.09651</link>
<guid>https://arxiv.org/abs/2509.09651</guid>
<content:encoded><![CDATA[
arXiv:2509.09651v2 Announce Type: replace-cross 
Abstract: We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title>
<link>https://arxiv.org/abs/2509.11816</link>
<guid>https://arxiv.org/abs/2509.11816</guid>
<content:encoded><![CDATA[
arXiv:2509.11816v2 Announce Type: replace-cross 
Abstract: Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.
  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.
  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.
  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title>
<link>https://arxiv.org/abs/2509.14289</link>
<guid>https://arxiv.org/abs/2509.14289</guid>
<content:encoded><![CDATA[
arXiv:2509.14289v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large Language Models</title>
<link>https://arxiv.org/abs/2510.01611</link>
<guid>https://arxiv.org/abs/2510.01611</guid>
<content:encoded><![CDATA[
arXiv:2510.01611v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a wide range of industries, primarily due to their impressive generative abilities. Yet, their potential in applications requiring cognitive abilities, such as psychological counseling, remains largely untapped. This paper investigates the key question: \textit{Can LLMs be effectively applied to psychological counseling?} To determine whether an LLM can effectively take on the role of a psychological counselor, the first step is to assess whether it meets the qualifications required for such a role, namely the ability to pass the U.S. National Counselor Certification Exam (NCE). This is because, just as a human counselor must pass a certification exam to practice, an LLM must demonstrate sufficient psychological knowledge to meet the standards required for such a role. To address this, we introduce PsychCounsel-Bench, a benchmark grounded in U.S.national counselor examinations, a licensure test for professional counselors that requires about 70\% accuracy to pass. PsychCounsel-Bench comprises approximately 2,252 carefully curated single-choice questions, crafted to require deep understanding and broad enough to cover various sub-disciplines of psychology. This benchmark provides a comprehensive assessment of an LLM's ability to function as a counselor. Our evaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results suggest that only frontier LLMs are currently capable of meeting counseling exam standards, highlighting both the promise and the challenges of developing psychology-oriented LLMs. We release the proposed dataset for public use: https://github.com/cloversjtu/PsychCounsel-Bench
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features</title>
<link>https://arxiv.org/abs/2510.16781</link>
<guid>https://arxiv.org/abs/2510.16781</guid>
<content:encoded><![CDATA[
arXiv:2510.16781v2 Announce Type: replace-cross 
Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) on static images have yet to be fully translated to the video domain. Conventional video understanding models often rely on extensive, task-specific training on annotated datasets, a process that is both costly and limited in scalability. This paper introduces a novel, training-free framework for video understanding that circumvents end-to-end training by synergistically combining the rich semantic priors of pre-trained VLMs with classic machine learning algorithms for pattern discovery. Our core idea is to reframe video understanding as a self-supervised spatio-temporal clustering problem within a high-dimensional semantic feature space. The proposed pipeline first transforms a video stream into a semantic feature trajectory using the frozen visual encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal Segmentation (KTS), a robust machine learning technique, to partition the continuous feature stream into discrete, semantically coherent event segments. These segments are then subjected to unsupervised density-based clustering to identify recurring macroscopic scenes and themes throughout the video. By selecting representative keyframes from each discovered cluster and leveraging the VLM's generative capabilities for textual description, our framework automatically produces a structured, multi-modal summary of the video content. This approach provides an effective, interpretable, and model-agnostic pathway for zero-shot, automated structural analysis of video content.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration</title>
<link>https://arxiv.org/abs/2510.26495</link>
<guid>https://arxiv.org/abs/2510.26495</guid>
<content:encoded><![CDATA[
arXiv:2510.26495v2 Announce Type: replace-cross 
Abstract: Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at https://github.com/Aurora-slz/Real-World-SQL-Bench .
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where did you get that? Towards Summarization Attribution for Analysts</title>
<link>https://arxiv.org/abs/2511.08589</link>
<guid>https://arxiv.org/abs/2511.08589</guid>
<content:encoded><![CDATA[
<div> Keywords: attribution, summarization, extractive summary, paraphrase, error analysis<br /><br />Summary:<br />1. The paper addresses the need for attribution in analytical reporting, emphasizing that information must be linked to its source to be credible. <br />2. It focuses on developing automatic methods for attribution by connecting each sentence in a summary to specific portions of the original source text, which can involve multiple documents. <br />3. The authors propose the use of hybrid summarization, where an extractive summary is automatically paraphrased. This approach aims to facilitate easier and more accurate attribution by blending direct extraction with paraphrasing. <br />4. Additionally, the paper introduces a custom topology to categorize and identify the proportions of different types of errors related to attribution, helping to better understand where summarization and attribution may fail. <br />5. Overall, the research contributes to improving the reliability of automatic summaries by ensuring that every piece of summarized information is traceable, assisting analysts in maintaining trustworthiness in their reports. <div>
arXiv:2511.08589v1 Announce Type: new 
Abstract: Analysts require attribution, as nothing can be reported without knowing the source of the information. In this paper, we will focus on automatic methods for attribution, linking each sentence in the summary to a portion of the source text, which may be in one or more documents. We explore using a hybrid summarization, i.e., an automatic paraphrase of an extractive summary, to ease attribution. We also use a custom topology to identify the proportion of different categories of attribution-related errors.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMTRouter: Personalized LLM Router over Multi-turn User Interactions</title>
<link>https://arxiv.org/abs/2511.08590</link>
<guid>https://arxiv.org/abs/2511.08590</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, personalization, graph learning, user preferences, few-shot learning<br /><br />Summary:<br /><br />1. GMTRouter is a novel approach to Large Language Model (LLM) routing that focuses on personalizing responses based on diverse user preferences, addressing the issue that identical queries may require different LLMs for different users.<br />2. It models multi-turn user-LLM interactions as a heterogeneous graph composed of four node types: user, LLM, query, and response, preserving complex relational structures in the data.<br />3. GMTRouter employs a customized message-passing mechanism within a lightweight inductive graph learning framework to effectively learn user preferences from scarce and noisy few-shot data.<br />4. Experimental results show that GMTRouter outperforms existing strong baseline methods, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets.<br />5. Importantly, GMTRouter can adapt to new users and evolving preferences without requiring extensive fine-tuning, relying only on few-shot data, and the implementation code is publicly available on GitHub. <div>
arXiv:2511.08590v1 Announce Type: new 
Abstract: Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions</title>
<link>https://arxiv.org/abs/2511.08592</link>
<guid>https://arxiv.org/abs/2511.08592</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media, conversations, authenticity, simulation<br />
Summary: 
- The study evaluates if Large Language Models (LLMs) can accurately simulate human group conversations on social media, using Reddit conversations.
- Two LLMs, Llama 3 70B and GPT-4o, were tested, with Llama 3 being the more challenging to distinguish between human and AI-generated content.
- Participants mistook LLM-generated conversations for human-created content 39% of the time, with only 56% accuracy in identifying Llama 3-generated content.
- The findings suggest that LLMs can convincingly mimic social media conversations, potentially opening up new opportunities for simulation applications.
- However, the study also warns about the dangers of misusing LLMs to generate inauthentic social media content, highlighting the need for responsible AI usage in societal contexts.<br /><br />Summary: <div>
arXiv:2511.08592v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer new avenues to simulate online communities and social media. Potential applications range from testing the design of content recommendation algorithms to estimating the effects of content policies and interventions. However, the validity of using LLMs to simulate conversations between various users remains largely untested. We evaluated whether LLMs can convincingly mimic human group conversations on social media. We collected authentic human conversations from Reddit and generated artificial conversations on the same topic with two LLMs: Llama 3 70B and GPT-4o. When presented side-by-side to study participants, LLM-generated conversations were mistaken for human-created content 39\% of the time. In particular, when evaluating conversations generated by Llama 3, participants correctly identified them as AI-generated only 56\% of the time, barely better than random chance. Our study demonstrates that LLMs can generate social media conversations sufficiently realistic to deceive humans when reading them, highlighting both a promising potential for social simulation and a warning message about the potential misuse of LLMs to generate new inauthentic social media content.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graph Analysis of Legal Understanding and Violations in LLMs</title>
<link>https://arxiv.org/abs/2511.08593</link>
<guid>https://arxiv.org/abs/2511.08593</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, legal analysis, compliance monitoring, knowledge graph construction, safety mechanisms <br />
Summary: 
Large Language Models (LLMs) offer potential in interpreting legal frameworks like Title 18 Section 175 of the US Code, but also pose risks due to generating unsafe outputs. A methodology integrating knowledge graph construction and Retrieval-Augmented Generation (RAG) is proposed to evaluate LLMs' understanding of the law, legal intent assessment, and potential unsafe applications. Experiments find limitations in LLMs' reasoning and safety mechanisms. Improved safety protocols and legal reasoning frameworks are suggested for LLMs to ethically assist in sensitive legal areas. This research aims to ensure LLMs act as protectors of the law rather than inadvertent enablers of its violation. <br /><br />Summary: <div>
arXiv:2511.08593v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) offers transformative potential for interpreting complex legal frameworks, such as Title 18 Section 175 of the US Code, which governs biological weapons. These systems hold promise for advancing legal analysis and compliance monitoring in sensitive domains. However, this capability comes with a troubling contradiction: while LLMs can analyze and interpret laws, they also demonstrate alarming vulnerabilities in generating unsafe outputs, such as actionable steps for bioweapon creation, despite their safeguards. To address this challenge, we propose a methodology that integrates knowledge graph construction with Retrieval-Augmented Generation (RAG) to systematically evaluate LLMs' understanding of this law, their capacity to assess legal intent (mens rea), and their potential for unsafe applications. Through structured experiments, we assess their accuracy in identifying legal violations, generating prohibited instructions, and detecting unlawful intent in bioweapons-related scenarios. Our findings reveal significant limitations in LLMs' reasoning and safety mechanisms, but they also point the way forward. By combining enhanced safety protocols with more robust legal reasoning frameworks, this research lays the groundwork for developing LLMs that can ethically and securely assist in sensitive legal domains - ensuring they act as protectors of the law rather than inadvertent enablers of its violation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Preference Learning for Capabilities and Alignment</title>
<link>https://arxiv.org/abs/2511.08594</link>
<guid>https://arxiv.org/abs/2511.08594</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM diversity, alignment algorithms, KL divergence regularizer, Soft Preference Learning, societal perspectives

<br /><br />Summary:  
The paper addresses the reduction of output diversity in large language models (LLMs) caused by common alignment methods such as RLHF (Reinforcement Learning with Human Feedback) and DPO (Direct Preference Optimization). These methods lead to repetitiveness in text structure and word choice and cause LLMs to approach problems uniformly, thus limiting the range of societal perspectives represented. The authors pinpoint the KL divergence regularizer used in preference learning algorithms as the key cause, as it biases the model toward majority opinions, diminishing output diversity. To overcome this, they propose a novel method called Soft Preference Learning, which separates the entropy and cross-entropy components within the KL penalty. This decoupling enables more precise control over the diversity of generated outputs. In terms of performance, LLMs trained with Soft Preference Learning demonstrate improved accuracy on difficult tasks requiring repeated samplings while generating outputs with significantly enhanced semantic and lexical diversity. From an alignment standpoint, these models can better represent a broader spectrum of societal viewpoints and show improved calibration of their output probabilities (logits). Importantly, Soft Preference Learning is similar to temperature scaling but offers a Pareto improvement, balancing accuracy and diversity more effectively than traditional methods. <div>
arXiv:2511.08594v1 Announce Type: new 
Abstract: The ability of LLMs to represent diverse perspectives is critical as they increasingly impact society. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to systematically overweight majority opinions and sacrifice diversity in its outputs. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty - allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.08595</link>
<guid>https://arxiv.org/abs/2511.08595</guid>
<content:encoded><![CDATA[
<div> Tree-of-Thought, Semantic Similarity, Dynamic Pruning, Large Language Models, Efficient Reasoning<br /><br />Summary:  
This paper addresses the computational challenges in Tree-of-Thought (ToT) reasoning for Large Language Models (LLMs), particularly the redundancy found in exploring equivalent reasoning paths across different branches. To mitigate this, the authors propose Semantic Similarity-Based Dynamic Pruning (SSDP), an innovative and lightweight method that integrates semantic merging dynamically into a parallelized tree search process. SSDP clusters and prunes redundant reasoning steps in real time, significantly reducing unnecessary computations. The method was tested on various reasoning benchmarks, including GSM8K and MATH500, where it demonstrated up to a 2.3 times speedup compared to state-of-the-art tree-search baselines. Despite this acceleration, SSDP maintains competitive accuracy, typically within 5% of the strongest baseline results. Furthermore, SSDP drastically reduces the number of explored nodes by 85-90%, showing both scalability and efficiency benefits. The approach advances practical reasoning with LLMs by balancing speed and accuracy through effective redundancy elimination. The authors have also made the implementation publicly available on GitHub, promoting accessibility and further research in efficient LLM reasoning techniques. <div>
arXiv:2511.08595v1 Announce Type: new 
Abstract: Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge</title>
<link>https://arxiv.org/abs/2511.08596</link>
<guid>https://arxiv.org/abs/2511.08596</guid>
<content:encoded><![CDATA[
<div> framework, stress testing, factual fidelity, large language models, adversarial nudge

Summary:
The paper introduces a framework for stress testing the factual fidelity of large language models (LLMs) in the presence of adversarial nudges. The framework involves instructing the LLM to generate sets of truths and lies consistent with a closed domain, verifying these assertions, and testing the LLM's robustness against lies it generated. Evaluation using five popular proprietary LLMs in movie and novel domains shows varying levels of susceptibility to adversarial nudges, with Claude exhibiting strong resilience, GPT and Grok demonstrating moderate resilience, and Gemini and DeepSeek showing weak resilience. This study highlights the importance of considering the susceptibility of LLMs to adversarial manipulation, especially as they are increasingly used by the general population for information seeking.<br /><br />Summary: <div>
arXiv:2511.08596v1 Announce Type: new 
Abstract: Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \texttt{Claude} exhibits strong resilience, \texttt{GPT} and \texttt{Grok} demonstrate moderate resilience, while \texttt{Gemini} and \texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-HarmLLM: Can Large Language Model Harm Itself?</title>
<link>https://arxiv.org/abs/2511.08597</link>
<guid>https://arxiv.org/abs/2511.08597</guid>
<content:encoded><![CDATA[
<div> guardrails, harmful responses, Large Language Models, Self-HarmLLM scenario, Mitigated Harmful Query 

Summary: 
The study investigates the potential risk of Large Language Models (LLMs) generating harmful responses as a new attack vector. The Self-HarmLLM scenario introduces Mitigated Harmful Queries (MHQs) generated by the same model to test for vulnerabilities. Experiments on various models under different conditions showed transformation and jailbreak success rates, highlighting the need to reevaluate guardrail design. Automated evaluation methods were found to overestimate jailbreak success, emphasizing the importance of robust evaluation methodologies. While the study is limited in scope, it demonstrates the validity of the proposed attack scenario and raises questions about the current defenses in place for LLMs. The findings call for a fundamental reassessment of guardrail strategies and the development of more accurate evaluation techniques. 

Summary:  <div>
arXiv:2511.08597v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking</title>
<link>https://arxiv.org/abs/2511.08598</link>
<guid>https://arxiv.org/abs/2511.08598</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge-intensive question answering, large language models, Open Knowledge Bench, dynamic knowledge benchmarks, retrieval-augmented methods

Summary: 
The article introduces Open Knowledge Bench (OKBench), a framework designed to generate dynamic knowledge benchmarks for assessing knowledge-intensive question answering in large language models (LLMs). OKBench focuses on the news domain, where knowledge updates daily, by automating the sourcing, creation, validation, and distribution of benchmarks. This framework democratizes benchmark creation and allows for thorough evaluation of retrieval-augmented methods, reducing overlap with pretraining data. Evaluating various open-source and proprietary LLMs with and without retrieval over freshly generated knowledge reveals distinct model behaviors when confronted with new information. The results highlight how retrieval narrows the performance gap between small and large models, emphasizing the importance of evaluating LLMs on evolving knowledge benchmarks.<br /><br />Summary: <div>
arXiv:2511.08598v1 Announce Type: new 
Abstract: Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study</title>
<link>https://arxiv.org/abs/2511.08600</link>
<guid>https://arxiv.org/abs/2511.08600</guid>
<content:encoded><![CDATA[
<div> clinical vignettes, speech-language pathology, large language models, retrieval-augmented generation, pediatric SLP

Summary: 
This study introduces a system that combines retrieval-augmented generation (RAG) with curated knowledge bases to efficiently create pediatric speech-language pathology (SLP) case materials. The system utilizes a variety of large language models (LLMs) including both commercial and open-source options. Seven test scenarios were systematically designed to evaluate the generated cases based on structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. While commercial models showed slight quality advantages, open-source alternatives still performed adequately, suggesting potential for widespread use. The integration of curated knowledge bases ensured that the content generated adhered to professional guidelines. Further validation through expert review, student pilot testing, and psychometric evaluation is necessary before implementation in educational or research settings. Potential future applications include clinical decision support, automated IEP goal generation, and clinical reflection training. 

Summary: <div>
arXiv:2511.08600v1 Announce Type: new 
Abstract: Clinical vignettes are essential educational tools in speech-language pathology (SLP), but manual creation is time-intensive. While general-purpose large language models (LLMs) can generate text, they lack domain-specific knowledge, leading to hallucinations and requiring extensive expert revision. This study presents a proof-of-concept system integrating retrieval-augmented generation (RAG) with curated knowledge bases to generate pediatric SLP case materials. A multi-model RAG-based system was prototyped integrating curated domain knowledge with engineered prompt templates, supporting five commercial (GPT-4o, Claude 3.5 Sonnet, Gemini 2.5 Pro) and open-source (Llama 3.2, Qwen 2.5-7B) LLMs. Seven test scenarios spanning diverse disorder types and grade levels were systematically designed. Generated cases underwent automated quality assessment using a multi-dimensional rubric evaluating structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. This proof-of-concept demonstrates technical feasibility for RAG-augmented generation of pediatric SLP vignettes. Commercial models showed marginal quality advantages, but open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment. Integration of curated knowledge bases enabled content generation aligned with professional guidelines. Extensive validation through expert review, student pilot testing, and psychometric evaluation is required before educational or research implementation. Future applications may extend to clinical decision support, automated IEP goal generation, and clinical reflection training.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating DisCoCirc in Translation Tasks &amp; its Limitations: A Comparative Study Between Bengali &amp; English</title>
<link>https://arxiv.org/abs/2511.08601</link>
<guid>https://arxiv.org/abs/2511.08601</guid>
<content:encoded><![CDATA[
<div> Keywords: DisCoCirc, grammar-based framework, translation tasks, language bureaucracy, English conjunctions <br />
Summary: 
The paper introduces a framework called DisCoCirc for Bengali language based on production rules and circuit-like representations. It aims to improve translation tasks between English and Bengali by reducing language bureaucracy. However, the study reveals limitations in handling structural variations between the two languages, leading to challenges even with simple sentences. These findings contradict previous claims and call for enhancements to address these shortcomings. The framework's struggle with translation implies a need for future improvements. Additionally, the paper explores the relationship between English conjunctions and Boolean logic, aligning with prior research. This study emphasizes the importance of addressing structural variations in language frameworks to enhance translation accuracy and efficiency. <br /><br />Summary: <div>
arXiv:2511.08601v1 Announce Type: new 
Abstract: In [4], the authors present the DisCoCirc (Distributed Compositional Circuits) formalism for the English language, a grammar-based framework derived from the production rules that incorporates circuit-like representations in order to give a precise categorical theoretical structure to the language. In this paper, we extend this approach to develop a similar framework for Bengali and apply it to translation tasks between English and Bengali. A central focus of our work lies in reassessing the effectiveness of DisCoCirc in reducing language bureaucracy. Unlike the result suggested in [5], our findings indicate that although it works well for a large part of the language, it still faces limitations due to the structural variation of the two languages. We discuss the possible methods that might handle these shortcomings and show that, in practice, DisCoCirc still struggles even with relatively simple sentences. This divergence from prior claims not only highlights the framework's constraints in translation but also suggest scope for future improvement. Apart from our primary focus on English-Bengali translation, we also take a short detour to examine English conjunctions, following [1], showing a connection between conjunctions and Boolean logic.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice</title>
<link>https://arxiv.org/abs/2511.08605</link>
<guid>https://arxiv.org/abs/2511.08605</guid>
<content:encoded><![CDATA[
<div> Keywords: Bangladesh, legal AI assistant, multilingual LLM, access to justice, low-resource systems<br /><br />Summary:<br /><br />1. Bangladesh’s low-income population struggles with accessing affordable legal advice due to complex legal language, opaque procedures, and high costs. <br /><br />2. Existing AI legal assistants fail to effectively support Bengali language and are not adapted to Bangladesh’s specific legal jurisdiction, limiting their practical usefulness. <br /><br />3. To overcome these barriers, Mina was developed as a multilingual large language model (LLM)-based legal assistant customized for the Bangladeshi context, integrating multilingual embeddings and a retrieval-augmented generation (RAG) chain-of-tools framework. <br /><br />4. Mina provides context-aware legal drafts, citations, and simplified explanations through an interactive chat interface, supporting retrieval, reasoning, translation, and document generation tailored to local legal requirements. <br /><br />5. Law faculty from leading universities evaluated Mina using the Bangladesh Bar Council Exams (2022 and 2023), where it achieved scores between 75-80% across Preliminary MCQs, Written, and simulated Viva Voce exams, matching or exceeding average human performance and demonstrating legal clarity and reasoning. <br /><br />6. These results showcase Mina’s potential as a cost-effective, multilingual AI assistant that automates key legal tasks, expands access to justice, and offers a practical example of building domain-specific AI systems for low-resource, multilingual environments with sustainable public-service deployment. <div>
arXiv:2511.08605v1 Announce Type: new 
Abstract: Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Super-Learner with Large Language Models for Medical Emergency Advising</title>
<link>https://arxiv.org/abs/2511.08614</link>
<guid>https://arxiv.org/abs/2511.08614</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical decision-support, Large Language Models, emergency medicine, diagnostic accuracy, meta-learning  

<br /><br />Summary:  
This study evaluates the diagnostic capabilities of five prominent Large Language Models (LLMs)—Gemini, Llama, Grok, GPT, and Claude—in emergency medicine. The accuracy of individual LLMs in diagnosing acute diseases ranged from 58% to 65%, which notably surpasses the reported diagnostic accuracy of human doctors in similar settings. To further improve performance, the researchers developed a super-learner called MEDAS (Medical Emergency Diagnostic Advising System), which integrates the five LLMs through a meta-learning approach. This meta-learner exploits the varying strengths of each LLM, learning to optimally combine their outputs to enhance overall diagnostic accuracy. The MEDAS super-learner achieved a higher accuracy of 70%, outperforming any single model. Interestingly, within the cluster, at least one individual LLM was capable of making correct diagnoses with an accuracy of up to 85%. The results indicate that combining multiple LLMs using a meta-learner can leverage diverse knowledge and produce superior diagnostic outcomes compared to relying on any single model alone. This approach demonstrates the potential to transform emergency medical decision-support systems by utilizing the collective intelligence of advanced language models. <div>
arXiv:2511.08614v1 Announce Type: new 
Abstract: Medical decision-support and advising systems are critical for emergency physicians to quickly and accurately assess patients' conditions and make diagnosis. Artificial Intelligence (AI) has emerged as a transformative force in healthcare in recent years and Large Language Models (LLMs) have been employed in various fields of medical decision-support systems. We studied responses of a group of different LLMs to real cases in emergency medicine. The results of our study on five most renown LLMs showed significant differences in capabilities of Large Language Models for diagnostics acute diseases in medical emergencies with accuracy ranging between 58% and 65%. This accuracy significantly exceeds the reported accuracy of human doctors. We built a super-learner MEDAS (Medical Emergency Diagnostic Advising System) of five major LLMs - Gemini, Llama, Grok, GPT, and Claude). The super-learner produces higher diagnostic accuracy, 70%, even with a quite basic meta-learner. However, at least one of the integrated LLMs in the same super-learner produces 85% correct diagnoses. The super-learner integrates a cluster of LLMs using a meta-learner capable of learning different capabilities of each LLM to leverage diagnostic accuracy of the model by collective capabilities of all LLMs in the cluster. The results of our study showed that aggregated diagnostic accuracy provided by a meta-learning approach exceeds that of any individual LLM, suggesting that the super-learner can take advantage of the combined knowledge of the medical datasets used to train the group of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM</title>
<link>https://arxiv.org/abs/2511.08620</link>
<guid>https://arxiv.org/abs/2511.08620</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, supervised fine-tuning, gradient-aware data selection, domain specialization, catastrophic forgetting<br /><br />Summary:<br /><br />1. Large language models (LLMs) have shown strong performance but still require supervised fine-tuning (SFT) to adapt effectively to specialized domains.  
2. Traditional SFT is resource-intensive and prone to catastrophic forgetting, which causes the model's general capabilities to deteriorate after domain-specific training.  
3. The authors propose GrADS, a self-adaptive gradient-aware data selection method that analyzes gradients from an initial training phase to identify the most effective subset of training data for fine-tuning.  
4. GrADS uses self-guided criteria based on gradient magnitudes and their statistical distribution to select representative samples that significantly contribute to learning domain-specific tasks.  
5. Experimental results across multiple domains such as medicine, law, and finance demonstrate that GrADS achieves higher efficiency and cost-effectiveness. Using just 5% of the selected GrADS data, LLMs outperform models fine-tuned on the full dataset, and with 50%, the improvements become even more substantial.  
6. Importantly, GrADS also markedly reduces catastrophic forgetting, preserving the LLM’s general capabilities while enhancing domain specialization.  
7. The authors plan to release the GrADS code to encourage further research and application. <div>
arXiv:2511.08620v1 Announce Type: new 
Abstract: Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Suicidal Ideation in Text with Interpretable Deep Learning: A CNN-BiGRU with Attention Mechanism</title>
<link>https://arxiv.org/abs/2511.08636</link>
<guid>https://arxiv.org/abs/2511.08636</guid>
<content:encoded><![CDATA[
<div> Keywords: Suicide Detection, Hybrid Deep Learning, CNN, BiGRU, Explainable AI<br /><br />Summary:  
The study addresses the critical issue of adolescent suicide by focusing on identifying suicidal ideation through social media analysis. It proposes a novel hybrid deep learning framework that combines Convolutional Neural Networks (CNN) for local feature extraction with Bidirectional Gated Recurrent Units (BiGRU) for sequence modeling. The model integrates attention mechanisms to enhance pattern recognition in social network (SN) datasets. To improve transparency and trustworthiness, Explainable AI techniques using SHapley Additive exPlanations (SHAP) were applied to interpret the prediction outcomes and validate model reliability. The framework was trained and evaluated on a publicly available dataset, utilizing multiple performance metrics for a thorough assessment. Experimental results demonstrated the model’s effectiveness, achieving an accuracy of 93.97%. Furthermore, the study includes a comparative analysis with other state-of-the-art machine learning and deep learning models, highlighting the superiority of the proposed method. Overall, the integration of CNN, BiGRU, attention mechanisms, and SHAP contributes to a robust and interpretable solution for suicide risk detection in social media contexts, potentially aiding early intervention strategies. <div>
arXiv:2511.08636v1 Announce Type: new 
Abstract: Worldwide, suicide is the second leading cause of death for adolescents with past suicide attempts to be an important predictor for increased future suicides. While some people with suicidal thoughts may try to suppress them, many signal their intentions in social media platforms. To address these issues, we propose a new type of hybrid deep learning scheme, i.e., the combination of a CNN architecture and a BiGRU technique, which can accurately identify the patterns of suicidal ideation from SN datasets. Also, we apply Explainable AI methods using SHapley Additive exPlanations to interpret the prediction results and verifying the model reliability. This integration of CNN local feature extraction, BiGRU bidirectional sequence modeling, attention mechanisms, and SHAP interpretability provides a comprehensive framework for suicide detection. Training and evaluation of the system were performed on a publicly available dataset. Several performance metrics were used for evaluating model performance. Our method was found to have achieved 93.97 accuracy in experimental results. Comparative study to different state-of-the-art Machine Learning and DL models and existing literature demonstrates the superiority of our proposed technique over all the competing methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Uncertainty guided Clarification for LLM Agents</title>
<link>https://arxiv.org/abs/2511.08798</link>
<guid>https://arxiv.org/abs/2511.08798</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, structured uncertainty, tool-call parameters, POMDP, clarification questions<br /><br />Summary: This paper addresses the challenge of ambiguous user instructions in large language model (LLM) agents that perform tool-calling tasks, which often cause incorrect invocations and task failures. The authors propose a principled formulation of structured uncertainty over tool-call parameters, treating joint tool-argument clarification as a Partially Observable Markov Decision Process (POMDP) that optimizes question selection using the Expected Value of Perfect Information (EVPI) and incorporates aspect-based cost modeling to avoid redundant clarifications. Their SAGE-Agent leverages this model of uncertainty to improve efficiency significantly, enhancing task coverage on ambiguous inputs by 7-39% and reducing the number of clarification questions by 1.5 to 2.7 times compared to strong prompting and uncertainty-based baseline methods. The authors introduce ClarifyBench, a novel multi-turn benchmark for tool-augmented disambiguation that uses realistic LLM-based user simulation and covers multiple domains such as document editing, vehicle control, and travel booking. Furthermore, they demonstrate that structured uncertainty provides valuable training signals for reinforcement learning, with uncertainty-weighted GRPO training boosting the When2Call accuracy substantially — from 36.5% to 65.2% for a 3-billion parameter model and from 36.7% to 62.9% for a 7-billion parameter model. These findings establish structured uncertainty as an effective and principled approach for enhancing task success rates and interaction efficiency in real-world LLM-agent applications. <div>
arXiv:2511.08798v1 Announce Type: new 
Abstract: LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\% while reducing clarification questions by 1.5-2.7$\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\% to 65.2\% (3B model) and 36.7\% to 62.9\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Automated Cognitive Assessment in Parkinson's Disease Using Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.08806</link>
<guid>https://arxiv.org/abs/2511.08806</guid>
<content:encoded><![CDATA[
<div> natural language processing, Parkinson's disease, cognitive processes, entity recognition, machine learning

Summary: 
- The study aims to understand how individuals with Parkinson's disease describe their cognitive experiences through natural language processing (NLP) models.
- Three model families were evaluated: Bio_ClinicalBERT, Meta-Llama-3-8B-Instruct, and GPT-4o mini, for extracting seven cognitive categories from patient narratives.
- The fine-tuned Meta-Llama-3-8B-Instruct model performed the best with high F1-scores, particularly excelling in context-dependent categories like thought and social interaction.
- Bio_ClinicalBERT showed high precision but low recall, performing well in some categories like location and time but failing in others such as thought, emotion, and social interaction.
- The task of extracting cognitive processes from narratives is challenging due to the abstract and overlapping nature, but with refinement, NLP systems show promise in monitoring cognitive function longitudinally in Parkinson's disease patients and supplementing formal neuropsychological assessments. 

<br /><br />Summary: <div>
arXiv:2511.08806v1 Announce Type: new 
Abstract: Understanding how individuals with Parkinson's disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BNLI: A Linguistically-Refined Bengali Dataset for Natural Language Inference</title>
<link>https://arxiv.org/abs/2511.08813</link>
<guid>https://arxiv.org/abs/2511.08813</guid>
<content:encoded><![CDATA[
<div> Keywords: Bengali, NLI, dataset, annotation, semantic clarity
Summary:
BNLI is a refined and curated Bengali NLI dataset created to address limitations in existing resources. The dataset underwent a rigorous annotation process to ensure semantic clarity and balance across entailment, contradiction, and neutrality classes. State-of-the-art transformer-based models were used to benchmark BNLI, showcasing improved reliability and interpretability in capturing complex semantic relations in Bengali text. BNLI serves as a strong foundation for advancing research in Bengali and other low-resource language inference tasks. <br /><br />Summary: BNLI is a curated Bengali NLI dataset with a rigorous annotation process, providing semantic clarity and balance across different classes. It was benchmarked using transformer-based models, demonstrating improved reliability and interpretability in capturing complex semantic relations in Bengali text. BNLI is a valuable resource for advancing research in low-resource language inference tasks. <div>
arXiv:2511.08813v1 Announce Type: new 
Abstract: Despite the growing progress in Natural Language Inference (NLI) research, resources for the Bengali language remain extremely limited. Existing Bengali NLI datasets exhibit several inconsistencies, including annotation errors, ambiguous sentence pairs, and inadequate linguistic diversity, which hinder effective model training and evaluation. To address these limitations, we introduce BNLI, a refined and linguistically curated Bengali NLI dataset designed to support robust language understanding and inference modeling. The dataset was constructed through a rigorous annotation pipeline emphasizing semantic clarity and balance across entailment, contradiction, and neutrality classes. We benchmarked BNLI using a suite of state-of-the-art transformer-based architectures, including multilingual and Bengali-specific models, to assess their ability to capture complex semantic relations in Bengali text. The experimental findings highlight the improved reliability and interpretability achieved with BNLI, establishing it as a strong foundation for advancing research in Bengali and other low-resource language inference tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents</title>
<link>https://arxiv.org/abs/2511.08835</link>
<guid>https://arxiv.org/abs/2511.08835</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational agents, task-oriented dialogue, chitchat, mode transition, Direct Preference Optimization (DPO)  

<br /><br />Summary:  
This paper addresses the longstanding challenge of unifying task-oriented dialogue (TOD) and open-ended chitchat in conversational agents, which traditionally are developed separately. Real-world conversations naturally transition fluidly between these modes, yet prior models lack robust mechanisms for handling such transitions. To bridge this gap, the authors introduce TACT (TOD-And-Chitchat Transition), a novel dataset designed for transition-aware dialogue modeling. TACT incorporates structurally diverse and integrated conversational flows supporting both user- and agent-driven switches between TOD and chitchat, enabling the training of agents that better manage complex dialogue dynamics. To evaluate agent performance on mode transitions, the paper proposes two new metrics: Switch, measuring the detection of mode transitions, and Recovery, assessing the agent's ability to recover from such transitions effectively. Models trained on TACT outperform baseline models in intent detection and mode transition handling, demonstrating the dataset’s efficacy. Furthermore, applying Direct Preference Optimization (DPO), a preference-based training technique, on TACT-trained models yields additional improvements. The best model achieves a joint mode-intent accuracy of 75.74% and attains a 70.1% win rate against GPT-4o in human evaluations. These findings suggest that combining diverse transition data with DPO fine-tuning significantly improves response quality and transition control, advancing the development of proactive, transition-aware conversational agents. <div>
arXiv:2511.08835v1 Announce Type: new 
Abstract: Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics -- Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74\% joint mode-intent accuracy and a 70.1\% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation</title>
<link>https://arxiv.org/abs/2511.08866</link>
<guid>https://arxiv.org/abs/2511.08866</guid>
<content:encoded><![CDATA[
<div> Keywords: Hypothesis generation, Biomedical research, Large Language Model, BioVerge, ReAct<br />
Summary: <br />
1) Different architectures of BioVerge Agent influence exploration diversity and reasoning strategies. 
2) Structured and textual information sources provide unique contexts that enhance hypothesis generation. 
3) Self-evaluation significantly improves novelty and relevance of proposed hypotheses. 
BioVerge introduces a benchmark dataset and an LLM-based agent framework for standardized biomedical hypothesis generation. The dataset includes structured and textual data from historical biomedical hypotheses and PubMed literature. BioVerge Agent utilizes a ReAct-based approach with Generation and Evaluation modules for proposing and self-assessing hypotheses. Experiments highlight the influence of agent architectures on exploration diversity and reasoning strategies, the importance of structured and textual information sources, and the benefits of self-evaluation for generating novel and relevant hypotheses. <div>
arXiv:2511.08866v1 Announce Type: new 
Abstract: Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.08877</link>
<guid>https://arxiv.org/abs/2511.08877</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination, citation frequency, memorization, factual accuracy<br /><br />Summary:<br /><br />This study examines how large language models (LLMs), specifically GPT-4.1, handle bibliographic citation generation and the tendency to hallucinate non-existent papers. It hypothesizes that an LLM's accuracy in producing bibliographic records depends on whether the information is memorized from frequent appearances in training data or generated anew. The research uses citation count as a proxy for training data redundancy, assuming that highly cited papers are more likely to appear in the pretraining corpus. By generating 100 citations across twenty computer science domains and verifying them manually, researchers measured factual consistency through cosine similarity between generated and authentic metadata. The results showed a strong positive correlation between citation count and factual accuracy, with papers cited over roughly 1,000 times being almost memorized verbatim by the model. However, the presence of multiple highly cited papers with similar content led to memory interference, which affected recall accuracy. These findings suggest a threshold where an LLM shifts from generalizing to memorizing information, identifying highly cited works as nearly verbatim retained within the model’s internal knowledge. This points to both opportunities and challenges in citation recommendation systems using LLMs, emphasizing citation frequency as a critical factor in reducing hallucinated references. <div>
arXiv:2511.08877v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in citation recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic records depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the pretraining corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record appears in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 citations across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) citation count is strongly correlated with factual accuracy, (ii) bibliographic information becomes almost verbatim memorized beyond roughly 1,000 citations, and (iii) memory interference occurs when multiple highly cited papers share similar content. These findings indicate a threshold where generalization shifts into memorization, with highly cited papers being nearly verbatim retained in the model.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluClean: A Unified Framework to Combat Hallucinations in LLMs</title>
<link>https://arxiv.org/abs/2511.08916</link>
<guid>https://arxiv.org/abs/2511.08916</guid>
<content:encoded><![CDATA[
<div> Framework, Hallucinations, Language models, Factual reliability, Trustworthiness  
Summary:  
HalluClean is a new framework introduced to address the issue of hallucinated content in Large Language Models (LLMs). It focuses on detecting and correcting unsupported claims in LLM-generated text by utilizing a reasoning-enhanced approach with distinct stages of planning, execution, and revision. HalluClean is task-agnostic, requiring minimal prompts for zero-shot generalization across various domains without external knowledge sources or supervised detectors. Evaluations across different tasks such as question answering and summarization demonstrate significant improvements in factual consistency compared to baseline methods. The framework shows potential in enhancing the trustworthiness of LLM outputs for real-world applications.  
<br /><br />Summary: <div>
arXiv:2511.08916v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiDAR: Think in Diffusion, Talk in Autoregression</title>
<link>https://arxiv.org/abs/2511.08923</link>
<guid>https://arxiv.org/abs/2511.08923</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion language models, Autoregressive models, TiDAR, Parallel generation, Structured attention masks<br /><br />Summary:  
1. Diffusion language models enable fast parallel token generation, but autoregressive (AR) models usually provide higher quality because their causal, left-to-right structure aligns naturally with language tasks.  
2. Existing approaches either rely on weaker AR-like drafting models, reducing drafting efficiency, or apply sequential decoding to diffusion models, losing quality and parallelizability benefits.  
3. TiDAR is proposed as a novel hybrid architecture combining diffusion-based token drafting ("Thinking") and autoregressive final sampling ("Talking") within a single forward pass using structured attention masks.  
4. This design maximizes GPU utilization by balancing token drafting and verification without significant overhead, making TiDAR practical for deployment as a standalone model.  
5. Extensive evaluation at 1.5B and 8B parameter scales shows TiDAR outperforms speculative decoding in throughput, surpasses diffusion models like Dream and Llada in both efficiency and quality, and uniquely closes the quality gap with AR models while delivering 4.71x to 5.91x higher token generation speed. <div>
arXiv:2511.08923v1 Announce Type: new 
Abstract: Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVADE: LLM-Based Explanation Generation and Validation for Error Detection in NLI</title>
<link>https://arxiv.org/abs/2511.08949</link>
<guid>https://arxiv.org/abs/2511.08949</guid>
<content:encoded><![CDATA[
<div> framework, NLP models, human label variation, annotation errors, large language models

Summary:
The study introduces a new framework called EVADE for detecting errors in natural language inference tasks using large language models (LLMs). It aims to address human label variation (HLV) by generating and validating explanations to identify errors. The framework is compared to the earlier VARIERR framework, which requires two rounds of manual annotation. The study shows that LLM validation can refine explanation distributions to better align with human annotations. Removing errors detected by LLMs leads to improved fine-tuning performance compared to errors identified by human annotators. This research highlights the potential for scaling error detection and improving dataset quality while reducing human effort in handling label variation. <div>
arXiv:2511.08949v1 Announce Type: new 
Abstract: High-quality datasets are critical for training and evaluating reliable NLP models. In tasks like natural language inference (NLI), human label variation (HLV) arises when multiple labels are valid for the same instance, making it difficult to separate annotation errors from plausible variation. An earlier framework VARIERR (Weber-Genzel et al., 2024) asks multiple annotators to explain their label decisions in the first round and flag errors via validity judgments in the second round. However, conducting two rounds of manual annotation is costly and may limit the coverage of plausible labels or explanations. Our study proposes a new framework, EVADE, for generating and validating explanations to detect errors using large language models (LLMs). We perform a comprehensive analysis comparing human- and LLM-detected errors for NLI across distribution comparison, validation overlap, and impact on model fine-tuning. Our experiments demonstrate that LLM validation refines generated explanation distributions to more closely align with human annotations, and that removing LLM-detected errors from training data yields improvements in fine-tuning performance than removing errors identified by human annotators. This highlights the potential to scale error detection, reducing human effort while improving dataset quality under label variation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiralThinker: Latent Reasoning through an Iterative Process with Text-Latent Interleaving</title>
<link>https://arxiv.org/abs/2511.08983</link>
<guid>https://arxiv.org/abs/2511.08983</guid>
<content:encoded><![CDATA[
<div> Framework, Iterative updates, Latent reasoning, Alignment, Reasoning tasks 

Summary: 
The paper introduces SpiralThinker, a unified framework for latent reasoning that incorporates iterative updates over latent representations to enhance implicit reasoning without increasing token generation. By implementing a progressive alignment objective and structured annotations, SpiralThinker ensures coherence between latent and textual reasoning, resulting in improved performance across various reasoning tasks. Through detailed analyses, the study highlights the significance of iteration and alignment, with optimal numbers of latent tokens and iterations varying by dataset. It emphasizes the crucial role of alignment in facilitating an effective iterative process. SpiralThinker successfully integrates iterative computation with latent reasoning, underscoring the importance of aligned iterative updates in steering reasoning within the latent space. <div>
arXiv:2511.08983v1 Announce Type: new 
Abstract: Recent advances in large reasoning models have been driven by reinforcement learning and test-time scaling, accompanied by growing interest in latent rather than purely textual reasoning. However, existing latent reasoning methods lack mechanisms to ensure stable evolution of latent representations and a systematic way to interleave implicit and explicit reasoning. We introduce SpiralThinker, a unified framework that performs iterative updates over latent representations, enabling extended implicit reasoning without generating additional tokens. A progressive alignment objective combined with structured annotations maintains coherence between latent and textual reasoning. Across mathematical, logical, and commonsense reasoning tasks, SpiralThinker achieves the best overall performance among latent reasoning approaches, consistently surpassing previous methods across all benchmarks. Detailed analyses reveal that both iteration and alignment are indispensable, the numbers of latent tokens and iterations exhibit dataset-specific optima, and appropriate alignment proves critical for an effective iterative process. Overall, SpiralThinker bridges iterative computation and latent reasoning, demonstrating that aligned iterative updates can reliably steer reasoning in the latent space.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models</title>
<link>https://arxiv.org/abs/2511.09003</link>
<guid>https://arxiv.org/abs/2511.09003</guid>
<content:encoded><![CDATA[
<div> Emotional support, large language models, evaluation, trajectory-based assessment, user-centered perspective <br />
<br />
Summary: This research introduces a new framework for evaluating the emotional support capabilities of large language models (LLMs) through trajectory-based assessment. The study focuses on the long-term and dynamic nature of emotional support in human-AI interaction, moving away from static evaluations. A benchmark is created with various emotional contexts and disturbance events to simulate realistic emotional shifts during evolving dialogue scenarios. The framework incorporates validated emotion regulation strategies to guide model responses towards psychologically grounded outcomes. User emotional trajectories are modeled using a first-order Markov process, and causally-adjusted emotion estimation is applied for unbiased emotional state tracking. Three trajectory-level metrics are introduced to capture user emotional dynamics over time: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). Evaluations across different LLMs show significant disparities in emotional support capabilities, providing valuable insights for model development. <div>
arXiv:2511.09003v1 Announce Type: new 
Abstract: Emotional support is a core capability in human-AI interaction, with applications including psychological counseling, role play, and companionship. However, existing evaluations of large language models (LLMs) often rely on short, static dialogues and fail to capture the dynamic and long-term nature of emotional support. To overcome this limitation, we shift from snapshot-based evaluation to trajectory-based assessment, adopting a user-centered perspective that evaluates models based on their ability to improve and stabilize user emotional states over time. Our framework constructs a large-scale benchmark consisting of 328 emotional contexts and 1,152 disturbance events, simulating realistic emotional shifts under evolving dialogue scenarios. To encourage psychologically grounded responses, we constrain model outputs using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and we apply causally-adjusted emotion estimation to obtain unbiased emotional state tracking. Based on this framework, we introduce three trajectory-level metrics: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). These metrics collectively capture user emotional dynamics over time and support comprehensive evaluation of long-term emotional support performance of LLMs. Extensive evaluations across a diverse set of LLMs reveal significant disparities in emotional support capabilities and provide actionable insights for model development.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neurosymbolic Approach to Natural Language Formalization and Verification</title>
<link>https://arxiv.org/abs/2511.09008</link>
<guid>https://arxiv.org/abs/2511.09008</guid>
<content:encoded><![CDATA[
<div> framework, neurosymbolic, language models, formalization process, logical correctness
Summary: 
The article introduces a novel two-stage neurosymbolic framework to address the stochastic limitations of Large Language Models (LLMs) in regulated industries like finance and healthcare. The framework involves using LLMs with optional human guidance to formalize natural language policies and conducting inference-time autoformalization to ensure logical correctness. Multiple redundant formalization steps are performed at inference time to cross-check for semantic equivalence, resulting in over 99% soundness and near-zero false positive rates in identifying logical validity. The approach generates auditable logical artifacts that validate verification outcomes and can enhance the original text. This framework provides a mechanism for fine-grained control over the formalization process and offers a reliable solution for ensuring the accuracy and compliance of natural language statements in regulated sectors. 
<br /><br />Summary: <div>
arXiv:2511.09008v1 Announce Type: new 
Abstract: Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique</title>
<link>https://arxiv.org/abs/2511.09067</link>
<guid>https://arxiv.org/abs/2511.09067</guid>
<content:encoded><![CDATA[
<div> Keywords: critique, Large Multimodal Models, benchmark, evaluation, GPT-4o

Summary: 
The study introduces MM-CRITIC, a benchmark for evaluating critique ability in Large Multimodal Models (LMMs) in various dimensions. The benchmark covers basic, correction, and comparison tasks across 8 main task types, collecting responses from different LMMs. Ground answers are integrated to enhance evaluation reliability, guiding the annotation of responses and reference critique generation by GPT-4o for trustworthy judgments. Experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities. The analysis reveals insights on the correlation between response quality and critique, as well as varying critique difficulty across evaluation dimensions. The code for MM-CRITIC is available on GitHub for further exploration and research. 

<br /><br />Summary: <div>
arXiv:2511.09067v1 Announce Type: new 
Abstract: The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Dynamic Chunking for Streaming Tibetan Speech Recognition</title>
<link>https://arxiv.org/abs/2511.09085</link>
<guid>https://arxiv.org/abs/2511.09085</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming speech recognition, Amdo Tibetan, hybrid CTC/Attention architecture, dynamic chunking mechanism, lexicon construction

Summary: 
A new streaming speech recognition framework for Amdo Tibetan is proposed in this work. It utilizes a hybrid CTC/Attention architecture with a context-aware dynamic chunking mechanism that adjusts chunk widths based on encoding states for flexible receptive fields. By addressing the context truncation problem of fixed-chunk methods, the framework adapts well to varying speaking rates. A lexicon based on Tibetan orthographic principles is constructed for linguistically motivated modeling units. The integration of an external language model during decoding enhances semantic consistency and improves recognition of longer sentences. Experimental results demonstrate a significant 48.15% relative improvement in word error rate (WER) over the fixed-chunk baseline, achieving a WER of 6.23% on the test set. The framework also reduces recognition latency while maintaining performance close to global decoding. 

<br /><br />Summary: <div>
arXiv:2511.09085v1 Announce Type: new 
Abstract: In this work, we propose a streaming speech recognition framework for Amdo Tibetan, built upon a hybrid CTC/Atten-tion architecture with a context-aware dynamic chunking mechanism. The proposed strategy adaptively adjusts chunk widths based on encoding states, enabling flexible receptive fields, cross-chunk information exchange, and robust adaptation to varying speaking rates, thereby alleviating the context truncation problem of fixed-chunk methods. To further capture the linguistic characteristics of Tibetan, we construct a lexicon grounded in its orthographic principles, providing linguistically motivated modeling units. During decoding, an external language model is integrated to enhance semantic consistency and improve recognition of long sentences. Experimental results show that the proposed framework achieves a word error rate (WER) of 6.23% on the test set, yielding a 48.15% relative improvement over the fixed-chunk baseline, while significantly reducing recognition latency and maintaining performance close to global decoding.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2511.09109</link>
<guid>https://arxiv.org/abs/2511.09109</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, reasoning framework, bidirectional information distance, multi-objective reinforcement learning, question answering benchmarks

Summary:
Bi-RAR is a novel retrieval-augmented reasoning framework that addresses limitations in complex reasoning scenarios by evaluating intermediate steps in both forward and backward directions. It introduces a bidirectional information distance grounded in Kolmogorov complexity to measure the completeness of each step in addressing the question and approaching the answer. To optimize reasoning, a multi-objective reinforcement learning framework is used with a cascading reward structure emphasizing early trajectory alignment. Empirical results on seven question answering benchmarks show that Bi-RAR outperforms previous methods and enables efficient interaction and reasoning with the search engine during training and inference. This approach mitigates hallucinations in large language models and enhances response quality by providing explicit guidance for intermediate steps, thereby avoiding reward hacking and leading to improved performance in multi-step reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2511.09109v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios.Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation</title>
<link>https://arxiv.org/abs/2511.09133</link>
<guid>https://arxiv.org/abs/2511.09133</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational humor, Natural Language Processing, Oogiri, Large Language Models, Empathy <br />
Summary: <br />
- The paper focuses on evaluating Large Language Models (LLMs) through the lens of Oogiri, a form of Japanese improvisational comedy games.
- A multifaceted understanding of humor is essential, and the study systematically evaluates LLMs across dimensions like Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness.
- LLMs can generate Oogiri responses at a level between low- and mid-tier human performance but exhibit a notable lack of Empathy compared to humans.
- The lack of Empathy in LLMs explains their failure to replicate human humor assessment, as they prioritize Novelty while humans prioritize Empathy in humor evaluation.
- The annotated corpus created in this study is released to the community to facilitate the development of emotionally intelligent and sophisticated conversational agents. 

Summary: <div>
arXiv:2511.09133v1 Announce Type: new 
Abstract: Computational humor is a frontier for creating advanced and engaging natural language processing (NLP) applications, such as sophisticated dialogue systems. While previous studies have benchmarked the humor capabilities of Large Language Models (LLMs), they have often relied on single-dimensional evaluations, such as judging whether something is simply ``funny.'' This paper argues that a multifaceted understanding of humor is necessary and addresses this gap by systematically evaluating LLMs through the lens of Oogiri, a form of Japanese improvisational comedy games. To achieve this, we expanded upon existing Oogiri datasets with data from new sources and then augmented the collection with Oogiri responses generated by LLMs. We then manually annotated this expanded collection with 5-point absolute ratings across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Using this dataset, we assessed the capabilities of state-of-the-art LLMs on two core tasks: their ability to generate creative Oogiri responses and their ability to evaluate the funniness of responses using a six-dimensional evaluation. Our results show that while LLMs can generate responses at a level between low- and mid-tier human performance, they exhibit a notable lack of Empathy. This deficit in Empathy helps explain their failure to replicate human humor assessment. Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy. We release our annotated corpus to the community to pave the way for the development of more emotionally intelligent and sophisticated conversational agents.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Topic-Doesn't-Fit-All: Transcreating Reading Comprehension Test for Personalized Learning</title>
<link>https://arxiv.org/abs/2511.09135</link>
<guid>https://arxiv.org/abs/2511.09135</guid>
<content:encoded><![CDATA[
<div> Personalized Learning, EFL Education, Reading Comprehension, Content Transcreation, Motivation  

<br /><br />Summary:  
This paper introduces a novel approach to personalized learning in English as a Foreign Language (EFL) education by generating customized reading comprehension tests that cater to individual student interests. The authors develop a structured content transcreation pipeline utilizing OpenAI's gpt-4o, starting from the RACE-C dataset. This pipeline produces new reading passages and multiple-choice questions that preserve the linguistic style of the originals while aligning semantically with learners' specific interests. The methodology incorporates several key components, including topic extraction, question classification based on Bloom's taxonomy, linguistic feature analysis, and the content transcreation process itself. To evaluate the effectiveness of this approach, a controlled experiment is conducted with EFL learners in South Korea. The study's findings reveal that students engaging with personalized reading materials demonstrate better reading comprehension performance and maintain higher motivation levels compared to peers using non-personalized content. Overall, the research highlights the potential of leveraging AI-driven content personalization to enhance both engagement and learning outcomes in EFL reading comprehension. <div>
arXiv:2511.09135v1 Announce Type: new 
Abstract: Personalized learning has gained attention in English as a Foreign Language (EFL) education, where engagement and motivation play crucial roles in reading comprehension. We propose a novel approach to generating personalized English reading comprehension tests tailored to students' interests. We develop a structured content transcreation pipeline using OpenAI's gpt-4o, where we start with the RACE-C dataset, and generate new passages and multiple-choice reading comprehension questions that are linguistically similar to the original passages but semantically aligned with individual learners' interests. Our methodology integrates topic extraction, question classification based on Bloom's taxonomy, linguistic feature analysis, and content transcreation to enhance student engagement. We conduct a controlled experiment with EFL learners in South Korea to examine the impact of interest-aligned reading materials on comprehension and motivation. Our results show students learning with personalized reading passages demonstrate improved comprehension and motivation retention compared to those learning with non-personalized materials.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoPE: Denoising Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2511.09146</link>
<guid>https://arxiv.org/abs/2511.09146</guid>
<content:encoded><![CDATA[
<div> Rotary Position Embedding, attention sink, Denoising Positional Encoding, truncated matrix entropy, length extrapolation<br /><br />Summary:<br /><br />1. The paper addresses inherent limitations of Rotary Position Embedding (RoPE) in Transformer models, particularly its weakening effect on length extrapolation capabilities. <br />2. It reinterprets the attention map combined with positional encoding as a noisy feature map, which motivates a novel approach to positional encoding enhancement. <br />3. The authors propose Denoising Positional Encoding (DoPE), a training-free method that leverages truncated matrix entropy to detect and remove outlier frequency bands in the attention feature map. <br />4. DoPE further uses a parameter-free Gaussian distribution for reparameterization, improving robustness when generalizing to longer sequence lengths. <br />5. The method theoretically connects the cause of the "attention sink" problem with truncated matrix entropy and experimentally demonstrates significant improvements in retrieval accuracy and reasoning stability on tasks with contexts extended up to 64K tokens, showing effective mitigation of attention sinks and restoration of balanced attention patterns for better length generalization. <div>
arXiv:2511.09146v1 Announce Type: new 
Abstract: Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls</title>
<link>https://arxiv.org/abs/2511.09148</link>
<guid>https://arxiv.org/abs/2511.09148</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, data evolution, model training, closed-loop process, tool-use capabilities

Summary: 
LoopTool introduces a closed-loop framework for enhancing large language models (LLMs) by tightly integrating data synthesis and model training. The framework consists of three modules: Greedy Capability Probing (GCP) diagnoses model capabilities, Judgement-Guided Label Verification (JGLV) corrects annotation errors, and Error-Driven Data Expansion (EDDE) generates new challenging samples. By iteratively refining data and models, LoopTool outperforms a 32B data generator and achieves state-of-the-art results on benchmarks like BFCL-v3 and ACEBench. The cost-effective, open-source ecosystem eliminates reliance on expensive closed-source APIs. This approach demonstrates the potential for closed-loop, self-refining data pipelines to boost LLM tool-use capabilities.<br /><br />Summary: <div>
arXiv:2511.09148v1 Announce Type: new 
Abstract: Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Search for Complex Table Question Answering in Securities Report</title>
<link>https://arxiv.org/abs/2511.09179</link>
<guid>https://arxiv.org/abs/2511.09179</guid>
<content:encoded><![CDATA[
<div> keyword: Large Language Models, Table Question Answering, cell extraction method, contrastive learning, text-search models
Summary:<br /><br />
- Large Language Models (LLMs) are being used in Table Question Answering (TQA) to extract information from tables in documents.
- Entering entire tables as text into LLMs often leads to incorrect answers due to the inability to capture complex table structures.
- The proposed cell extraction method for TQA utilizes a hybrid retrieval mechanism to estimate table headers based on similarities between a question and individual cells.
- The approach selects the answer based on the most relevant row and column intersection.
- The language model is trained using contrastive learning on a small dataset of question-header pairs to improve performance.
- Experimental results on the TQA dataset from the U4 shared task at NTCIR-18 showed an accuracy of 74.6%, outperforming existing LLMs like GPT-4o mini (63.9%).
- Future work will focus on incorporating more efficient text-search models to further enhance performance and approach human evaluation results.  
Summary: <div>
arXiv:2511.09179v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) are gaining increased attention in the domain of Table Question Answering (TQA), particularly for extracting information from tables in documents. However, directly entering entire tables as long text into LLMs often leads to incorrect answers because most LLMs cannot inherently capture complex table structures. In this paper, we propose a cell extraction method for TQA without manual identification, even for complex table headers. Our approach estimates table headers by computing similarities between a given question and individual cells via a hybrid retrieval mechanism that integrates a language model and TF-IDF. We then select as the answer the cells at the intersection of the most relevant row and column. Furthermore, the language model is trained using contrastive learning on a small dataset of question-header pairs to enhance performance. We evaluated our approach in the TQA dataset from the U4 shared task at NTCIR-18. The experimental results show that our pipeline achieves an accuracy of 74.6\%, outperforming existing LLMs such as GPT-4o mini~(63.9\%). In the future, although we used traditional encoder models for retrieval in this study, we plan to incorporate more efficient text-search models to improve performance and narrow the gap with human evaluation results.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context is Enough: Empirical Validation of $\textit{Sequentiality}$ on Essays</title>
<link>https://arxiv.org/abs/2511.09185</link>
<guid>https://arxiv.org/abs/2511.09185</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, sequentiality, contextual terms, essay scoring, discourse traits  

<br /><br />Summary:  
1. This paper addresses the critique of a previously proposed metric called sequentiality, which combined topic and contextual terms to quantify narrative flow using Large Language Models (LLMs).  
2. The critique suggested that the topic selection confounded original results and recommended using only the contextual term for a more valid and interpretable measure.  
3. The authors empirically validate this proposal by testing it on two essay datasets, ASAP++ and ELLIPSE, both containing human-annotated trait scores related to discourse-level traits such as Organization and Cohesion.  
4. Results show that the contextual-only measure of sequentiality aligns more closely with human assessments compared to the original and topic-only versions.  
5. Although zero-shot prompted LLMs predict trait scores more accurately on their own, combining the contextual sequentiality measure with standard linguistic features yields higher predictive value than the original and topic-only metrics, even outperforming zero-shot LLM predictions alone.  
6. The findings underscore the importance of explicitly modeling sentence-to-sentence flow and support the contextual measure as a validated, interpretable, and complementary feature for automated essay scoring and other NLP applications. <div>
arXiv:2511.09185v1 Announce Type: new 
Abstract: Recent work has proposed using Large Language Models (LLMs) to quantify narrative flow through a measure called sequentiality, which combines topic and contextual terms. A recent critique argued that the original results were confounded by how topics were selected for the topic-based component, and noted that the metric had not been validated against ground-truth measures of flow. That work proposed using only the contextual term as a more conceptually valid and interpretable alternative. In this paper, we empirically validate that proposal. Using two essay datasets with human-annotated trait scores, ASAP++ and ELLIPSE, we show that the contextual version of sequentiality aligns more closely with human assessments of discourse-level traits such as Organization and Cohesion. While zero-shot prompted LLMs predict trait scores more accurately than the contextual measure alone, the contextual measure adds more predictive value than both the topic-only and original sequentiality formulations when combined with standard linguistic features. Notably, this combination also outperforms the zero-shot LLM predictions, highlighting the value of explicitly modeling sentence-to-sentence flow. Our findings support the use of context-based sequentiality as a validated, interpretable, and complementary feature for automated essay scoring and related NLP tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Learning Dynamics of Subword Segmentation for Morphologically Diverse Languages</title>
<link>https://arxiv.org/abs/2511.09197</link>
<guid>https://arxiv.org/abs/2511.09197</guid>
<content:encoded><![CDATA[
<div> Keywords: subword segmentation, language model, pretraining, finetuning, morphology <br />
<br />
Summary: 
The study explores the learning dynamics of subword segmentation in language models, focusing on how subwords evolve during pretraining and finetuning. The subword segmental language model (SSLM) framework is extended to support pretraining and finetuning for typologically diverse languages: Isi-Xhosa, Setswana, and English. Four stages of subword learning are identified, with isi-Xhosa showing greater instability due to its morphological complexity. During finetuning, subword boundaries become finer-grained. The research highlights the potential of learnable subwords in improving text generation and cross-lingual transfer, particularly for low-resource, morphologically complex languages. <div>
arXiv:2511.09197v1 Announce Type: new 
Abstract: Subword segmentation is typically applied in preprocessing and stays fixed during training. Alternatively, it can be learned during training to optimise the training objective. In this paper we study the learning dynamics of subword segmentation: if a language model can dynamically optimise tokenisation, how do its subwords evolve during pretraining and finetuning? To explore this, we extend the subword segmental language model (SSLM), a framework for learning subwords during training, to support pretraining and finetuning. We train models for three typologically diverse languages to study learning dynamics across the morphological spectrum: Isi-Xhosa is conjunctive (long word forms composed of many morphemes), Setswana is disjunctive (morphemes written as separate words), and English represents a typological middle ground. We analyse subword dynamics from a linguistic perspective, tracking morphology, productivity, and fertility. We identify four stages of subword learning, with the morphologically complex isi-Xhosa exhibiting greater instability. During finetuning, subword boundaries shift to become finer-grained. Lastly, we show that learnable subwords offers a promising approach to improve text generation and cross-lingual transfer for low-resource, morphologically complex languages.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretraining Finnish ModernBERTs</title>
<link>https://arxiv.org/abs/2511.09213</link>
<guid>https://arxiv.org/abs/2511.09213</guid>
<content:encoded><![CDATA[
<div> Multilingualism, ModernBERT, pretrained models, long context, Finnish languages<br /><br />Summary: This paper focuses on pretraining ModernBERT encoder models across six different sizes, ranging from 51 million to 475 million parameters, specifically targeting limited multilingualism with an emphasis on languages relevant to Finland. The study shows that these models are competitive with, or even outperform, existing multilingual models on various natural language processing tasks. Notably, they demonstrate superior performance compared to monolingual models on tasks requiring handling contexts longer than 512 tokens, which is a critical capability for understanding extended text. The research also investigates the impact of different types of data incorporated during the final training phase, providing valuable empirical insights into effective training strategies. Finally, the authors have made both the code and the pretrained models publicly available, facilitating further research and application development in this domain. <div>
arXiv:2511.09213v1 Announce Type: new 
Abstract: This paper reports on pretraining ModernBERT encoder models in six different sizes, ranging from 51M to 475M parameters, with a focus on limited multilingualism, emphasizing languages relevant to Finland. Our models are competitive with, or superior to, existing multilingual models. They outperform monolingual models on tasks that require a context longer than 512 tokens. We present empirical results on using different data in the final stage of training. The code and models are publicly released.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning</title>
<link>https://arxiv.org/abs/2511.09222</link>
<guid>https://arxiv.org/abs/2511.09222</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, verifiable rewards, honesty alignment, deductive reasoning, Anchor <br />
<br />
Summary: 
The paper introduces the concept of reinforcement learning with verifiable rewards (RLVR) for aligning language models with complex reasoning objectives. It focuses on honesty alignment, where models must not only answer queries but also recognize when conclusions cannot be drawn from given premises. To test honesty alignment, two deductive reasoning datasets are curated, one for linear algebra and one for logical inference. The study finds that current methods struggle with these tasks and proposes a new reinforcement learning method called Anchor. Anchor injects ground truth trajectories into rollouts, preventing early training collapse and improving reasoning performance. The results highlight the importance of training dynamics for reliable deductive reasoning in aligned language models. <br /><br /> <div>
arXiv:2511.09222v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as a promising framework for aligning language models with complex reasoning objectives. However, most existing methods optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. This challenge is especially pronounced in honesty alignment, where models must not only solve answerable queries but also identify when conclusions cannot be drawn from the given premises. Deductive reasoning provides an ideal testbed because it isolates reasoning capability from reliance on external factual knowledge. To investigate honesty alignment, we curate two multi-step deductive reasoning datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that GRPO, with or without supervised fine tuning initialization, struggles on these tasks. Through extensive experiments across three models, we evaluate stabilization strategies and show that curriculum learning provides some benefit but requires carefully designed in distribution datasets with controllable difficulty. To address these limitations, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling reliable deductive reasoning in aligned language models.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation</title>
<link>https://arxiv.org/abs/2511.09232</link>
<guid>https://arxiv.org/abs/2511.09232</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech Large Language Models, Optimal Transport, cross-lingual alignment, speech-to-text translation, zero-shot languages<br /><br />Summary: This paper addresses the challenge of biased translation performance in multilingual speech-to-text translation (S2TT) caused by neglecting semantic commonalities across source languages. The authors propose a novel framework named POTSA (Parallel Optimal Transport for Speech Alignment) that leverages cross-lingual parallel speech pairs combined with Optimal Transport (OT) techniques to enhance alignment between high- and low-resource languages. The approach includes a Bias Compensation module designed to coarsely align initial speech representations among different languages, reducing initial disparities. Subsequently, token-level OT constraints are applied to a Q-Former module using parallel speech pairs to establish detailed consistency of speech representations at the token level. Additionally, the framework incorporates a layer scheduling strategy that applies OT constraints selectively to the layers most beneficial for capturing semantic information. Experiments conducted on the FLEURS dataset demonstrate that POTSA achieves state-of-the-art (SOTA) results, showing an average improvement of +0.93 BLEU across five common languages. Remarkably, it also yields a substantial +5.05 BLEU increase in zero-shot language settings, all while requiring only 10 hours of parallel speech data per source language. This work presents an effective solution to bridging translation quality gaps for low-resource and unseen languages. <div>
arXiv:2511.09232v1 Announce Type: new 
Abstract: Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT). However, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance. In this work, we propose \textbf{POTSA} (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps. First, we introduce a Bias Compensation module to coarsely align initial speech representations across languages. Second, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations. Then, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers. Experiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C$^3$TG: Conflict-aware, Composite, and Collaborative Controlled Text Generation</title>
<link>https://arxiv.org/abs/2511.09292</link>
<guid>https://arxiv.org/abs/2511.09292</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-attribute control, text generation, conflict resolution, iterative optimization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of controlling multiple specific attributes in generated text by large language models (LLMs) without needing architectural changes or extensive fine-tuning.<br />2. Existing methods primarily allow control over single, basic attributes and struggle when multiple, potentially conflicting attributes are required, lacking coordination and iterative refinement mechanisms.<br />3. The authors propose C³TG (Conflict-aware, Composite, and Collaborative Controlled Text Generation), a novel two-phase framework designed for fine-grained control over multiple text attributes simultaneously.<br />4. During the generation phase, C³TG dynamically pairs the LLM with relevant attribute classifiers (covering 17 dimensions) and applies weighted KL-divergence to adjust token probabilities, guiding generation toward desired attribute profiles.<br />5. In the optimization phase, an energy function combining classifier feedback and penalty terms iteratively resolves conflicts among attributes to refine outputs, ensuring natural text fluency is preserved.<br />6. Experimental results demonstrate that C³TG outperforms baseline approaches in attribute accuracy, linguistic quality, diversity of generated text, and also reduces toxicity.<br />7. The framework achieves precise multi-dimensional attribute control without requiring costly model modifications, making it an effective and flexible solution for controlled text generation tasks. <div>
arXiv:2511.09292v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable text generation capabilities. However, controlling specific attributes of generated text remains challenging without architectural modifications or extensive fine-tuning. Current methods typically toggle a single, basic attribute but struggle with precise multi-attribute control. In scenarios where attribute requirements conflict, existing methods lack coordination mechanisms, causing interference between desired attributes. Furthermore, these methods fail to incorporate iterative optimization processes in the controlled generation pipeline. To address these limitations, we propose Conflict-aware, Composite, and Collaborative Controlled Text Generation (C$^3$TG), a two-phase framework for fine-grained, multi-dimensional text attribute control. During generation, C$^3$TG selectively pairs the LLM with the required attribute classifiers from the 17 available dimensions and employs weighted KL-divergence to adjust token probabilities. The optimization phase then leverages an energy function combining classifier scores and penalty terms to resolve attribute conflicts through iterative feedback, enabling precise control over multiple dimensions simultaneously while preserving natural text flow. Experiments show that C$^3$TG significantly outperforms baselines across multiple metrics including attribute accuracy, linguistic fluency, and output diversity, while simultaneously reducing toxicity. These results establish C$^3$TG as an effective and flexible solution for multi-dimensional text attribute control that requires no costly model modifications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteraryTaste: A Preference Dataset for Creative Writing Personalization</title>
<link>https://arxiv.org/abs/2511.09310</link>
<guid>https://arxiv.org/abs/2511.09310</guid>
<content:encoded><![CDATA[
<div> Creative writing, personalization, large language models, reading preferences, dataset<br /><br />Summary:<br /><br />This paper introduces LiteraryTaste, a novel dataset designed to support the development of personalized creative writing language models by capturing individual reading preferences. The dataset involves 60 participants who provided two types of preference data: 1) self-reported reading habits and tastes, termed stated preferences, and 2) annotations of their preferences over 100 pairs of short creative writing texts, referred to as revealed preferences. Key findings include the observation that people's creative writing preferences significantly diverge, highlighting the importance of personalization. The authors demonstrate that fine-tuning a transformer encoder on this data achieves 75.8% accuracy in predicting personal preferences and 67.7% accuracy for collective preferences, suggesting effective modeling of individual and group tastes. However, stated preferences showed limited predictive power for revealed preferences, indicating that what people say about their tastes might not align well with their actual choices. Additionally, the study uses an LLM-driven interpretability pipeline to analyze variations in preferences, providing insights into how creativity is perceived differently across users. This work establishes a foundation for further research toward tailoring creative writing technologies to better match individual user tastes and improve user satisfaction. <div>
arXiv:2511.09310v1 Announce Type: new 
Abstract: People have different creative writing preferences, and large language models (LLMs) for these tasks can benefit from adapting to each user's preferences. However, these models are often trained over a dataset that considers varying personal tastes as a monolith. To facilitate developing personalized creative writing LLMs, we introduce LiteraryTaste, a dataset of reading preferences from 60 people, where each person: 1) self-reported their reading habits and tastes (stated preference), and 2) annotated their preferences over 100 pairs of short creative writing texts (revealed preference). With our dataset, we found that: 1) people diverge on creative writing preferences, 2) finetuning a transformer encoder could achieve 75.8% and 67.7% accuracy when modeling personal and collective revealed preferences, and 3) stated preferences had limited utility in modeling revealed preferences. With an LLM-driven interpretability pipeline, we analyzed how people's preferences vary. We hope our work serves as a cornerstone for personalizing creative writing technologies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Khmer Polarity Classification</title>
<link>https://arxiv.org/abs/2511.09313</link>
<guid>https://arxiv.org/abs/2511.09313</guid>
<content:encoded><![CDATA[
<div> Khmer polarity classification, explainable AI, Qwen-3 model, polarity dataset, self-explanations<br /><br />Summary: This paper addresses the task of Khmer polarity classification, which involves categorizing Khmer text into positive, negative, or neutral sentiments. Unlike prior models that perform classification without explanation, the authors propose an explainable approach by fine-tuning the instruction-based reasoning Qwen-3 model. The explainability emphasized in this work is based on self-explanations, where the model provides rationale by highlighting polarity-related keywords or phrases that justify its predictions. Experimental results demonstrate that the fine-tuned Qwen-3 model not only achieves accurate polarity classification but also offers interpretable reasoning for its decisions. Additionally, the authors contribute a new dataset tailored for this task, containing short to medium-length casual, romanized, and mixed-code Khmer expressions. This dataset was constructed using a combination of heuristic rules and human curation to ensure quality and diversity. Both the dataset and the fine-tuned Qwen-3 models are made publicly accessible via a gated Hugging Face repository (rinabuoy/khmerpolarity_nonreasoning). This work advances Khmer language processing by combining effective sentiment analysis with explainability support. <div>
arXiv:2511.09313v1 Announce Type: new 
Abstract: Khmer polarity classification is a fundamental natural language processing task that assigns a positive, negative, or neutral label to a given Khmer text input. Existing Khmer models typically predict the label without explaining the rationale behind the prediction. This paper proposes an explainable Khmer polarity classifier by fine-tuning an instruction-based reasoning Qwen-3 model. The notion of explainability in this paper is limited to self-explanations, which the model uses to rationalize its predictions. Experimental results show that the fine-tuned model not only predicts labels accurately but also provides reasoning by identifying polarity-related keywords or phrases to support its predictions. In addition, we contribute a new Khmer polarity dataset consisting of short- to medium-length casual, romanized, and mixed-code Khmer expressions. This dataset was constructed using both heuristic rules and human curation and is publicly available through a gated Hugging Face repository (rinabuoy/khmerpolarity_nonreasoning). The fine-tuned Qwen-3 models are also made available in the same Hugging Face account.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness</title>
<link>https://arxiv.org/abs/2511.09316</link>
<guid>https://arxiv.org/abs/2511.09316</guid>
<content:encoded><![CDATA[
<div> certified robustness, sequence classification, edit distance perturbations, AdaptDel methods, randomized smoothing 

Summary:
The article introduces AdaptDel methods for certified robustness in sequence classification against edit distance perturbations, particularly for natural language processing tasks. Current methods face challenges with inputs of varying lengths, leading to suboptimal performance. AdaptDel methods use adaptable deletion rates that adjust dynamically based on input properties. The theoretical framework of randomized smoothing is extended to variable-rate deletion to ensure sound certification with respect to edit distance. Empirical results in natural language tasks show significant improvement, with up to 30 orders of magnitude enhancement in the median cardinality of the certified region compared to state-of-the-art certifications. <div>
arXiv:2511.09316v1 Announce Type: new 
Abstract: We consider the problem of certified robustness for sequence classification against edit distance perturbations. Naturally occurring inputs of varying lengths (e.g., sentences in natural language processing tasks) present a challenge to current methods that employ fixed-rate deletion mechanisms and lead to suboptimal performance. To this end, we introduce AdaptDel methods with adaptable deletion rates that dynamically adjust based on input properties. We extend the theoretical framework of randomized smoothing to variable-rate deletion, ensuring sound certification with respect to edit distance. We achieve strong empirical results in natural language tasks, observing up to 30 orders of magnitude improvement to median cardinality of the certified region, over state-of-the-art certifications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.09339</link>
<guid>https://arxiv.org/abs/2511.09339</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, scientific reasoning, mmJEE-Eval, JEE Advanced, multimodal benchmark<br /><br />Summary:<br /><br />1. This paper introduces mmJEE-Eval, a new challenging multimodal bilingual benchmark in English and Hindi, consisting of 1,460 questions from India's JEE Advanced exam (2019-2025) covering Physics, Chemistry, and Mathematics. <br />2. The benchmark aims to better differentiate genuine scientific reasoning capabilities from mere pattern-matching in contemporary vision-language models (VLMs), addressing limitations in existing multimodal reasoning benchmarks.<br />3. Evaluation of 17 state-of-the-art VLMs shows that top closed-source models such as GPT-5 and Gemini 2.5 Pro/Flash achieve high accuracy (77-84%) on the latest 2025 questions, while large open-source models plateau at 37-45% accuracy despite scaling to 400 billion parameters.<br />4. Closed models demonstrate excellent problem-solving performance, with up to 100% pass@3 scores, but their reasoning ability drastically declines under meta-cognitive stress tests; for instance, GPT-5 only corrects 5.2% of its errors.<br />5. Ablation studies confirm that mmJEE-Eval's difficulty arises from question complexity and reasoning depth, not from data memorization, making it an effective benchmark for distinguishing advanced reasoning and training methods versus weaker alternatives.<br />6. The authors publicly release the benchmark's code and dataset at https://mmjee-eval.github.io for broader research use. <div>
arXiv:2511.09339v1 Announce Type: new 
Abstract: Contemporary vision-language models (VLMs) perform well on existing multimodal reasoning benchmarks (78-85\% accuracy on MMMU, MathVista). Yet, these results fail to sufficiently distinguish true scientific reasoning articulation capabilities from pattern-matching. To address this gap, we introduce \textbf{mmJEE-Eval}, a multimodal bilingual (English and Hindi) benchmark comprising 1,460 questions from India's JEE Advanced examination (2019-2025) spanning pre-college Physics, Chemistry, and Mathematics domains. Our evaluation of 17 state-of-the-art models reveals that while frontier VLMs (GPT-5, Gemini 2.5 Pro/Flash) achieve 77-84\% accuracy on held-out 2025 questions, open-source models plateau at 37-45\% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks. While closed frontiers from Google and OpenAI show high problem-solving accuracies (up to 100\% pass@3 scores), they fully collapse when the reasoning load is increased meta-cognitively (GPT-5 fixes just 5.2\% errors). Systematic ablations show mmJEE-Eval's difficulty stems from complexity and reasoning depth rather than memorization. Effectively, our benchmark segregates superior training and reasoning methodologies where alternatives fail. We publicly release our code and data: https://mmjee-eval.github.io
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seer Self-Consistency: Advance Budget Estimation for Adaptive Test-Time Scaling</title>
<link>https://arxiv.org/abs/2511.09345</link>
<guid>https://arxiv.org/abs/2511.09345</guid>
<content:encoded><![CDATA[
<div> dynamic self-consistency, Large Language Models, token efficiency, latency, System 1 reasoning

Summary:
The paper introduces SeerSC, a dynamic self-consistency framework that enhances token efficiency and reduces latency in Large Language Models. By integrating System 1 and System 2 reasoning, the framework uses System 1 for quick computation of answer entropy, which is then used to assess the suitability of samples for scaling under System 2. This approach enables dynamic self-consistency while benefiting from System 1's accurate estimations. The method surpasses existing techniques by achieving a remarkable 47% reduction in token consumption and a 43% decrease in inference latency without compromising performance. By leveraging both rapid and accurate reasoning systems, SeerSC improves overall inference performance and efficiency of LLMs. <br /><br />Summary: <div>
arXiv:2511.09345v1 Announce Type: new 
Abstract: Test-time scaling improves the inference performance of Large Language Models (LLMs) but also incurs substantial computational costs. Although recent studies have reduced token consumption through dynamic self-consistency, they remain constrained by the high latency of sequential requests. In this paper, we propose SeerSC, a dynamic self-consistency framework that simultaneously improves token efficiency and latency by integrating System 1 and System 2 reasoning. Specifically, we utilize the rapid System 1 to compute the answer entropy for given queries. This score is then used to evaluate the potential of samples for scaling, enabling dynamic self-consistency under System 2. Benefiting from the advance and accurate estimation provided by System 1, the proposed method can reduce token usage while simultaneously achieving a significant decrease in latency through parallel generation. It outperforms existing methods, achieving up to a 47% reduction in token consumption and a 43% reduction in inference latency without significant performance loss.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spider4SSC &amp; S2CLite: A text-to-multi-query-language dataset using lightweight ontology-agnostic SPARQL to Cypher parser</title>
<link>https://arxiv.org/abs/2511.09354</link>
<guid>https://arxiv.org/abs/2511.09354</guid>
<content:encoded><![CDATA[
<div> Dataset, S2CLite, SPARQL, Cypher, parsing<br />
Summary:<br />
The article introduces the Spider4SSC dataset and S2CLite parsing tool. S2CLite is a rule-based parser that translates SPARQL queries into Cypher queries without relying on an RDF graph or external tools. Experimental results demonstrate that S2CLite significantly improves query parsing accuracy compared to existing solutions, achieving a total parsing accuracy of 77.8% on Spider4SPARQL. S2CLite also outperforms the state-of-the-art S2CTrans in terms of execution accuracy on a common subset of queries. The tool is open-sourced on GitHub for further development. Additionally, S2CLite is used to generate the Spider4SSC dataset, which includes 4525 unique questions and 3 equivalent sets of 2581 matching queries in SQL, SPARQL, and Cypher. The clean Spider4SSC dataset is available for download. <br /><br />Summary: <div>
arXiv:2511.09354v1 Announce Type: new 
Abstract: We present Spider4SSC dataset and S2CLite parsing tool. S2CLite is a lightweight, ontology-agnostic parser that translates SPARQL queries into Cypher queries, enabling both in-situ and large-scale SPARQL to Cypher translation. Unlike existing solutions, S2CLite is purely rule-based (inspired by traditional programming language compilers) and operates without requiring an RDF graph or external tools. Experiments conducted on the BSBM42 and Spider4SPARQL datasets show that S2CLite significantly reduces query parsing errors, achieving a total parsing accuracy of 77.8% on Spider4SPARQL compared to 44.2% by the state-of-the-art S2CTrans. Furthermore, S2CLite achieved a 96.6\% execution accuracy on the intersecting subset of queries parsed by both parsers, outperforming S2CTrans by 7.3%. We further use S2CLite to parse Spider4SPARQL queries to Cypher and generate Spider4SSC, a unified Text-to-Query language (SQL, SPARQL, Cypher) dataset with 4525 unique questions and 3 equivalent sets of 2581 matching queries (SQL, SPARQL and Cypher). We open-source S2CLite for further development on GitHub (github.com/vejvarm/S2CLite) and provide the clean Spider4SSC dataset for download.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTQ-Eval: Multilingual Text Quality Evaluation for Language Models</title>
<link>https://arxiv.org/abs/2511.09374</link>
<guid>https://arxiv.org/abs/2511.09374</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual text evaluation, large language models, MTQ-Eval, text quality assessment, preference learning<br /><br />Summary:<br /><br />This paper addresses the challenge of evaluating text quality in multilingual settings using large language models (LLMs). It introduces MTQ-Eval, a new evaluation framework designed to assess text quality across 115 languages by learning from examples of both high- and low-quality text. To create MTQ-Eval, the authors automatically generate data reflecting quality preferences, which is then used to fine-tune open-source base LLMs so they align better with human-like judgments of text quality. The study presents a comprehensive evaluation demonstrating MTQ-Eval’s superior performance over existing approaches in multiple languages. Additionally, the authors find that the improved evaluation capability of MTQ-Eval not only enhances quality assessment tasks but also contributes to better results in related downstream tasks. This suggests that multilingual text quality evaluation using aligned LLMs is both feasible and beneficial. The work highlights the potential for scalable, generalizable text quality evaluation beyond specific task-focused metrics, offering robust multilingual support. Overall, MTQ-Eval represents an important step toward more accurate and universal text quality assessment using LLMs trained on preference data. <div>
arXiv:2511.09374v1 Announce Type: new 
Abstract: The use of large language models (LLMs) for evaluating outputs is becoming an increasingly effective and scalable approach. However, it remains uncertain whether this capability extends beyond task-specific evaluations to more general assessments of text quality, particularly in multilingual contexts. In this study, we introduce, MTQ-Eval, a novel framework for multilingual text quality evaluation that learns from examples of both high- and low-quality texts, adjusting its internal representations. To develop MTQ-Eval, we first automatically generate text quality preference data and then use it to train open-source base LLMs to align with ratings of high- and low-quality text. Our comprehensive evaluation across 115 languages demonstrates the improved performance of the proposed model. Upon further analysis, we find that this enhanced evaluation capability also leads to notable improvements in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Correcting Large Language Models: Generation vs. Multiple Choice</title>
<link>https://arxiv.org/abs/2511.09381</link>
<guid>https://arxiv.org/abs/2511.09381</guid>
<content:encoded><![CDATA[
<div> self-correction mechanism, language models, open-ended generation, multiple-choice selection, decision-oriented applications
Summary: 
This paper investigates the self-correction mechanisms in large language models (LLMs) for open-ended text generation and multiple-choice selection tasks. It compares performance trends and error-correction behaviors across various natural language understanding and reasoning tasks using different scales and families of language models. The study reveals that open-ended generation benefits from re-interpretation and compositional refinement, while multiple-choice selection relies on clearer solution boundaries but may be limited by provided options. The findings emphasize the importance of considering task structure and output space interaction in designing self-correction mechanisms for knowledge-intensive reasoning and decision-oriented LLM applications.<br /><br /> <div>
arXiv:2511.09381v1 Announce Type: new 
Abstract: Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:
  \textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMaPO: Adaptive Margin-attached Preference Optimization for Language Model Alignment</title>
<link>https://arxiv.org/abs/2511.09385</link>
<guid>https://arxiv.org/abs/2511.09385</guid>
<content:encoded><![CDATA[
<div> offline preference optimization, language models, overfitting, underfitting, adaptive margin<br />
<br />
Summary:<br />
The article introduces Adaptive Margin-attached Preference Optimization (AMaPO) as a solution to the Overfitting-Underfitting Dilemma in aligning language models using offline preference optimization. The current margin designs lead to excessive gradients for correctly ranked samples (overfitting) and insufficient signals for misranked ones (underfitting). AMaPO addresses this issue by utilizing an adaptive margin that reallocates learning effort by amplifying gradients for misranked samples and suppressing them for correctly ranked ones. Experimental results show that AMaPO improves ranking accuracy and downstream alignment performance compared to existing methods. The algorithm successfully mitigates overfitting and underfitting issues, providing a more effective and stable approach for aligning language models. <div>
arXiv:2511.09385v1 Announce Type: new 
Abstract: Offline preference optimization offers a simpler and more stable alternative to RLHF for aligning language models. However, their effectiveness is critically dependent on ranking accuracy, a metric where further gains are highly impactful. This limitation arises from a fundamental problem that we identify and formalize as the Overfitting-Underfitting Dilemma: current margin designs cause models to apply excessive, wasteful gradients to correctly ranked samples (overfitting) while providing insufficient corrective signals for misranked ones (underfitting). To resolve this dilemma, we propose Adaptive Margin-attached Preference Optimization (AMaPO), a simple yet principled algorithm. AMaPO employs an instance-wise adaptive margin, refined by Z-normalization and exponential scaling, which dynamically reallocates learning effort by amplifying gradients for misranked samples and suppressing them for correct ones. Extensive experiments on widely used benchmarks demonstrate that AMaPO not only achieves better ranking accuracy and superior downstream alignment performance, but targeted analysis also confirms that it successfully mitigates the core overfitting and underfitting issues.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque</title>
<link>https://arxiv.org/abs/2511.09396</link>
<guid>https://arxiv.org/abs/2511.09396</guid>
<content:encoded><![CDATA[
arXiv:2511.09396v1 Announce Type: new 
Abstract: Current Multimodal Large Language Models exhibit very strong performance for several demanding tasks. While commercial MLLMs deliver acceptable performance in low-resource languages, comparable results remain unattained within the open science community. In this paper, we aim to develop a strong MLLM for a low-resource language, namely Basque. For that purpose, we develop our own training and evaluation image-text datasets. Using two different Large Language Models as backbones, the Llama-3.1-Instruct model and a Basque-adapted variant called Latxa, we explore several data mixtures for training. We show that: i) low ratios of Basque multimodal data (around 20%) are already enough to obtain solid results on Basque benchmarks, and ii) contrary to expected, a Basque instructed backbone LLM is not required to obtain a strong MLLM in Basque. Our results pave the way to develop MLLMs for other low-resource languages by openly releasing our resources.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE-Bench: A Benchmark of Diverse Client Simulations Guided by Expert Principles for Evaluating LLMs in Psychological Counseling</title>
<link>https://arxiv.org/abs/2511.09407</link>
<guid>https://arxiv.org/abs/2511.09407</guid>
<content:encoded><![CDATA[
arXiv:2511.09407v1 Announce Type: new 
Abstract: The mismatch between the growing demand for psychological counseling and the limited availability of services has motivated research into the application of Large Language Models (LLMs) in this domain. Consequently, there is a need for a robust and unified benchmark to assess the counseling competence of various LLMs. Existing works, however, are limited by unprofessional client simulation, static question-and-answer evaluation formats, and unidimensional metrics. These limitations hinder their effectiveness in assessing a model's comprehensive ability to handle diverse and complex clients. To address this gap, we introduce \textbf{CARE-Bench}, a dynamic and interactive automated benchmark. It is built upon diverse client profiles derived from real-world counseling cases and simulated according to expert guidelines. CARE-Bench provides a multidimensional performance evaluation grounded in established psychological scales. Using CARE-Bench, we evaluate several general-purpose LLMs and specialized counseling models, revealing their current limitations. In collaboration with psychologists, we conduct a detailed analysis of the reasons for LLMs' failures when interacting with clients of different types, which provides directions for developing more comprehensive, universal, and effective counseling models.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSAP-ERE: Fine-Grained Scholarly Entity and Relation Extraction Focused on Machine Learning</title>
<link>https://arxiv.org/abs/2511.09411</link>
<guid>https://arxiv.org/abs/2511.09411</guid>
<content:encoded><![CDATA[
arXiv:2511.09411v1 Announce Type: new 
Abstract: Research in Machine Learning (ML) and AI evolves rapidly. Information Extraction (IE) from scientific publications enables to identify information about research concepts and resources on a large scale and therefore is a pathway to improve understanding and reproducibility of ML-related research. To extract and connect fine-grained information in ML-related research, e.g. method training and data usage, we introduce GSAP-ERE. It is a manually curated fine-grained dataset with 10 entity types and 18 semantically categorized relation types, containing mentions of 63K entities and 35K relations from the full text of 100 ML publications. We show that our dataset enables fine-tuned models to automatically extract information relevant for downstream tasks ranging from knowledge graph (KG) construction, to monitoring the computational reproducibility of AI research at scale. Additionally, we use our dataset as a test suite to explore prompting strategies for IE using Large Language Models (LLM). We observe that the performance of state-of-the-art LLM prompting methods is largely outperformed by our best fine-tuned baseline model (NER: 80.6%, RE: 54.0% for the fine-tuned model vs. NER: 44.4%, RE: 10.1% for the LLM). This disparity of performance between supervised models and unsupervised usage of LLMs suggests datasets like GSAP-ERE are needed to advance research in the domain of scholarly information extraction.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIG5-TPoT: Predicting BIG Five Personality Traits, Facets, and Items Through Targeted Preselection of Texts</title>
<link>https://arxiv.org/abs/2511.09426</link>
<guid>https://arxiv.org/abs/2511.09426</guid>
<content:encoded><![CDATA[
arXiv:2511.09426v1 Announce Type: new 
Abstract: Predicting an individual's personalities from their generated texts is a challenging task, especially when the text volume is large. In this paper, we introduce a straightforward yet effective novel strategy called targeted preselection of texts (TPoT). This method semantically filters the texts as input to a deep learning model, specifically designed to predict a Big Five personality trait, facet, or item, referred to as the BIG5-TPoT model. By selecting texts that are semantically relevant to a particular trait, facet, or item, this strategy not only addresses the issue of input text limits in large language models but also improves the Mean Absolute Error and accuracy metrics in predictions for the Stream of Consciousness Essays dataset.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Readability Measures and Automatic Text Simplification: In the Search of a Construct</title>
<link>https://arxiv.org/abs/2511.09536</link>
<guid>https://arxiv.org/abs/2511.09536</guid>
<content:encoded><![CDATA[
arXiv:2511.09536v1 Announce Type: new 
Abstract: Readability is a key concept in the current era of abundant written information. To help making texts more readable and make information more accessible to everyone, a line of researched aims at making texts accessible for their target audience: automatic text simplification (ATS). Lately, there have been studies on the correlations between automatic evaluation metrics in ATS and human judgment. However, the correlations between those two aspects and commonly available readability measures (such as readability formulas or linguistic features) have not been the focus of as much attention. In this work, we investigate the place of readability measures in ATS by complementing the existing studies on evaluation metrics and human judgment, on English. We first discuss the relationship between ATS and research in readability, then we report a study on correlations between readability measures and human judgment, and between readability measures and ATS evaluation metrics. We identify that in general, readability measures do not correlate well with automatic metrics and human judgment. We argue that as the three different angles from which simplification can be assessed tend to exhibit rather low correlations with one another, there is a need for a clear definition of the construct in ATS.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynClaimEval: A Framework for Evaluating the Utility of Synthetic Data in Long-Context Claim Verification</title>
<link>https://arxiv.org/abs/2511.09539</link>
<guid>https://arxiv.org/abs/2511.09539</guid>
<content:encoded><![CDATA[
arXiv:2511.09539v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with extended context windows promise direct reasoning over long documents, reducing the need for chunking or retrieval. Constructing annotated resources for training and evaluation, however, remains costly. Synthetic data offers a scalable alternative, and we introduce SynClaimEval, a framework for evaluating synthetic data utility in long-context claim verification -- a task central to hallucination detection and fact-checking. Our framework examines three dimensions: (i) input characteristics, by varying context length and testing generalization to out-of-domain benchmarks; (ii) synthesis logic, by controlling claim complexity and error type variation; and (iii) explanation quality, measuring the degree to which model explanations provide evidence consistent with predictions. Experiments across benchmarks show that long-context synthesis can improve verification in base instruction-tuned models, particularly when augmenting existing human-written datasets. Moreover, synthesis enhances explanation quality, even when verification scores do not improve, underscoring its potential to strengthen both performance and explainability.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational Agents for Building Energy Efficiency -- Advising Housing Cooperatives in Stockholm on Reducing Energy Consumption</title>
<link>https://arxiv.org/abs/2511.08587</link>
<guid>https://arxiv.org/abs/2511.08587</guid>
<content:encoded><![CDATA[
arXiv:2511.08587v1 Announce Type: cross 
Abstract: Housing cooperative is a common type of multifamily building ownership in Sweden. Although this ownership structure grants decision-making autonomy, it places a burden of responsibility on cooperative's board members. Most board members lack the resources or expertise to manage properties and their energy consumption. This ignorance presents a unique challenge, especially given the EU directives that prohibit buildings rated as energy classes F and G by 2033. Conversational agents (CAs) enable human-like interactions with computer systems, facilitating human-computer interaction across various domains. In our case, CAs can be implemented to support cooperative members in making informed energy retrofitting and usage decisions. This paper introduces a Conversational agent system, called SPARA, designed to advise cooperatives on energy efficiency. SPARA functions as an energy efficiency advisor by leveraging the Retrieval-Augmented Generation (RAG) framework with a Language Model(LM). The LM generates targeted recommendations based on a knowledge base composed of email communications between professional energy advisors and cooperatives' representatives in Stockholm. The preliminary results indicate that SPARA can provide energy efficiency advice with precision 80\%, comparable to that of municipal energy efficiency (EE) experts. A pilot implementation is currently underway, where municipal EE experts are evaluating SPARA performance based on questions posed to EE experts by BRF members. Our findings suggest that LMs can significantly improve outreach by supporting stakeholders in their energy transition. For future work, more research is needed to evaluate this technology, particularly limitations to the stability and trustworthiness of its energy efficiency advice.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion</title>
<link>https://arxiv.org/abs/2511.08653</link>
<guid>https://arxiv.org/abs/2511.08653</guid>
<content:encoded><![CDATA[
arXiv:2511.08653v1 Announce Type: cross 
Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-generated podcasts: Synthetic Intimacy and Cultural Translation in NotebookLM's Audio Overviews</title>
<link>https://arxiv.org/abs/2511.08654</link>
<guid>https://arxiv.org/abs/2511.08654</guid>
<content:encoded><![CDATA[
arXiv:2511.08654v1 Announce Type: cross 
Abstract: This paper analyses AI-generated podcasts produced by Google's NotebookLM, which generates audio podcasts with two chatty AI hosts discussing whichever documents a user uploads. While AI-generated podcasts have been discussed as tools, for instance in medical education, they have not yet been analysed as media. By uploading different types of text and analysing the generated outputs I show how the podcasts' structure is built around a fixed template. I also find that NotebookLM not only translates texts from other languages into a perky standardised Mid-Western American accent, it also translates cultural contexts to a white, educated, middle-class American default. This is a distinct development in how publics are shaped by media, marking a departure from the multiple public spheres that scholars have described in human podcasting from the early 2000s until today, where hosts spoke to specific communities and responded to listener comments, to an abstraction of the podcast genre.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benevolent Dictators? On LLM Agent Behavior in Dictator Games</title>
<link>https://arxiv.org/abs/2511.08721</link>
<guid>https://arxiv.org/abs/2511.08721</guid>
<content:encoded><![CDATA[
arXiv:2511.08721v1 Announce Type: cross 
Abstract: In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BayesQ: Uncertainty-Guided Bayesian Quantization</title>
<link>https://arxiv.org/abs/2511.08821</link>
<guid>https://arxiv.org/abs/2511.08821</guid>
<content:encoded><![CDATA[
arXiv:2511.08821v1 Announce Type: cross 
Abstract: We present BayesQ, an uncertainty-guided post-training quantization framework that is the first to optimize quantization under the posterior expected loss. BayesQ fits a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank), whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion, and allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget. For scalar quantizers, posterior-expected MSE yields closed-form tables; task-aware proxies are handled by short Monte Carlo on a small calibration set. An optional calibration-only distillation aligns the quantized model with the posterior predictive teacher. At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over strong PTQ baselines on ResNet-50 (ImageNet) and BERT-base (GLUE) e.g., vs. GPTQ by $+1.5/+0.7/+0.3$ top-1 percentage points on RN50 and $+1.1/+0.4/+0.2$ GLUE points on BERT, while requiring one-time preprocessing comparable to a GPTQ pass. BayesQ reframes low-bit quantization as uncertainty-aware risk minimization in a practical, post-training pipeline.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Double Contingency Problem: AI Recursion and the Limits of Interspecies Understanding</title>
<link>https://arxiv.org/abs/2511.08927</link>
<guid>https://arxiv.org/abs/2511.08927</guid>
<content:encoded><![CDATA[
arXiv:2511.08927v1 Announce Type: cross 
Abstract: Current bioacoustic AI systems achieve impressive cross-species performance by processing animal communication through transformer architectures, foundation model paradigms, and other computational approaches. However, these approaches overlook a fundamental question: what happens when one form of recursive cognition--AI systems with their attention mechanisms, iterative processing, and feedback loops--encounters the recursive communicative processes of other species? Drawing on philosopher Yuk Hui's work on recursivity and contingency, I argue that AI systems are not neutral pattern detectors but recursive cognitive agents whose own information processing may systematically obscure or distort other species' communicative structures. This creates a double contingency problem: each species' communication emerges through contingent ecological and evolutionary conditions, while AI systems process these signals through their own contingent architectural and training conditions. I propose that addressing this challenge requires reconceptualizing bioacoustic AI from universal pattern recognition toward diplomatic encounter between different forms of recursive cognition, with implications for model design, evaluation frameworks, and research methodologies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransactionGPT</title>
<link>https://arxiv.org/abs/2511.08939</link>
<guid>https://arxiv.org/abs/2511.08939</guid>
<content:encoded><![CDATA[
arXiv:2511.08939v1 Announce Type: cross 
Abstract: We present TransactionGPT (TGPT), a foundation model for consumer transaction data within one of world's largest payment networks. TGPT is designed to understand and generate transaction trajectories while simultaneously supporting a variety of downstream prediction and classification tasks. We introduce a novel 3D-Transformer architecture specifically tailored for capturing the complex dynamics in payment transaction data. This architecture incorporates design innovations that enhance modality fusion and computational efficiency, while seamlessly enabling joint optimization with downstream objectives. Trained on billion-scale real-world transactions, TGPT significantly improves downstream classification performance against a competitive production model and exhibits advantages over baselines in generating future transactions. We conduct extensive empirical evaluations utilizing a diverse collection of company transaction datasets spanning multiple downstream tasks, thereby enabling a thorough assessment of TGPT's effectiveness and efficiency in comparison to established methodologies. Furthermore, we examine the incorporation of LLM-derived embeddings within TGPT and benchmark its performance against fine-tuned LLMs, demonstrating that TGPT achieves superior predictive accuracy as well as faster training and inference. We anticipate that the architectural innovations and practical guidelines from this work will advance foundation models for transaction-like data and catalyze future research in this emerging field.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Mixture of Experts For Large Language Models</title>
<link>https://arxiv.org/abs/2511.08968</link>
<guid>https://arxiv.org/abs/2511.08968</guid>
<content:encoded><![CDATA[
arXiv:2511.08968v1 Announce Type: cross 
Abstract: We present Bayesian Mixture of Experts (Bayesian-MoE), a post-hoc uncertainty estimation framework for fine-tuned large language models (LLMs) based on Mixture-of-Experts architectures. Our method applies a structured Laplace approximation to the second linear layer of each expert, enabling calibrated uncertainty estimation without modifying the original training procedure or introducing new parameters. Unlike prior approaches, which apply Bayesian inference to added adapter modules, Bayesian-MoE directly targets the expert pathways already present in MoE models, leveraging their modular design for tractable block-wise posterior estimation. We use Kronecker-factored low-rank approximations to model curvature and derive scalable estimates of predictive uncertainty and marginal likelihood. Experiments on common-sense reasoning benchmarks with Qwen1.5-MoE and DeepSeek-MoE demonstrate that Bayesian-MoE improves both expected calibration error (ECE) and negative log-likelihood (NLL) over baselines, confirming its effectiveness for reliable downstream decision-making.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines</title>
<link>https://arxiv.org/abs/2511.09005</link>
<guid>https://arxiv.org/abs/2511.09005</guid>
<content:encoded><![CDATA[
arXiv:2511.09005v1 Announce Type: cross 
Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Million-Step LLM Task with Zero Errors</title>
<link>https://arxiv.org/abs/2511.09030</link>
<guid>https://arxiv.org/abs/2511.09030</guid>
<content:encoded><![CDATA[
arXiv:2511.09030v1 Announce Type: cross 
Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</title>
<link>https://arxiv.org/abs/2511.09057</link>
<guid>https://arxiv.org/abs/2511.09057</guid>
<content:encoded><![CDATA[
arXiv:2511.09057v1 Announce Type: cross 
Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Aware Reasoning for GUI Agents</title>
<link>https://arxiv.org/abs/2511.09127</link>
<guid>https://arxiv.org/abs/2511.09127</guid>
<content:encoded><![CDATA[
arXiv:2511.09127v1 Announce Type: cross 
Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Object Hallucinations with Verified Atomic Confidence Estimation</title>
<link>https://arxiv.org/abs/2511.09228</link>
<guid>https://arxiv.org/abs/2511.09228</guid>
<content:encoded><![CDATA[
arXiv:2511.09228v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering</title>
<link>https://arxiv.org/abs/2511.09282</link>
<guid>https://arxiv.org/abs/2511.09282</guid>
<content:encoded><![CDATA[
arXiv:2511.09282v1 Announce Type: cross 
Abstract: Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI</title>
<link>https://arxiv.org/abs/2511.09325</link>
<guid>https://arxiv.org/abs/2511.09325</guid>
<content:encoded><![CDATA[
arXiv:2511.09325v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks</title>
<link>https://arxiv.org/abs/2511.09373</link>
<guid>https://arxiv.org/abs/2511.09373</guid>
<content:encoded><![CDATA[
arXiv:2511.09373v1 Announce Type: cross 
Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting</title>
<link>https://arxiv.org/abs/2511.09478</link>
<guid>https://arxiv.org/abs/2511.09478</guid>
<content:encoded><![CDATA[
arXiv:2511.09478v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaturalTurn: A Method to Segment Speech into Psychologically Meaningful Conversational Turns</title>
<link>https://arxiv.org/abs/2403.15615</link>
<guid>https://arxiv.org/abs/2403.15615</guid>
<content:encoded><![CDATA[
arXiv:2403.15615v4 Announce Type: replace 
Abstract: Conversation is a subject of increasing interest in the social, cognitive, and computational sciences. Yet as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational "turns"-the basic building blocks of social interaction. We discuss this challenge and then introduce "NaturalTurn," a turn-segmentation algorithm designed to accurately capture the dynamics of conversational exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize human conversation. Using data from a large conversation corpus, we show that NaturalTurn captures conversational turns more accurately than a baseline model. For example, it produces turns with durations and gaps that match empirical literature, reveals stronger linguistic alignment patterns between speakers, and uncovers otherwise hidden relationships between turn-taking and affective outcomes. NaturalTurn thus represents a pragmatic development in machine-generated transcript-processing methods, or "turn models", that will enable researchers to link turn-taking dynamics with important outcomes of social interaction, a central goal of conversation science.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Deep Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2410.15153</link>
<guid>https://arxiv.org/abs/2410.15153</guid>
<content:encoded><![CDATA[
arXiv:2410.15153v4 Announce Type: replace 
Abstract: Machine unlearning has emerged as an important component in developing safe and trustworthy models. Prior work on fact unlearning in LLMs has mostly focused on removing a specified target fact robustly, but often overlooks its deductive connections to other knowledge. We propose a new setting for fact unlearning, deep unlearning, where the goal is not only to remove a target fact but also to prevent it from being deduced via retained knowledge in the LLM and logical reasoning. We propose three novel metrics: Success-DU and Recall to measure unlearning efficacy, and Accuracy to measure the remainder model utility. To benchmark this setting, we leverage both (1) an existing real-world knowledge dataset, MQuAKE, that provides one-step deduction instances, and (2) newly construct a novel semi-synthetic dataset, Eval-DU, that allows multiple steps of realistic deductions among synthetic facts. Experiments reveal that current methods struggle with deep unlearning: they either fail to deeply unlearn, or excessively remove unrelated facts. Our results suggest that targeted algorithms may have to be developed for robust/deep fact unlearning in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model Benchmarks in Medical Tasks</title>
<link>https://arxiv.org/abs/2410.21348</link>
<guid>https://arxiv.org/abs/2410.21348</guid>
<content:encoded><![CDATA[
arXiv:2410.21348v3 Announce Type: replace 
Abstract: With the increasing application of large language models (LLMs) in the medical domain, evaluating these models' performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark datasets employed in medical LLM tasks. These datasets span multiple modalities including text, image, and multimodal benchmarks, focusing on different aspects of medical knowledge such as electronic health records (EHRs), doctor-patient dialogues, medical question-answering, and medical image captioning. The survey categorizes the datasets by modality, discussing their significance, data structure, and impact on the development of LLMs for clinical tasks such as diagnosis, report generation, and predictive decision support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and CheXpert, which have facilitated advancements in tasks like medical report generation, clinical summarization, and synthetic data generation. The paper summarizes the challenges and opportunities in leveraging these benchmarks for advancing multimodal medical intelligence, emphasizing the need for datasets with a greater degree of language diversity, structured omics data, and innovative approaches to synthesis. This work also provides a foundation for future research in the application of LLMs in medicine, contributing to the evolving field of medical artificial intelligence.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training and Evaluating Language Models with Template-based Data Generation</title>
<link>https://arxiv.org/abs/2411.18104</link>
<guid>https://arxiv.org/abs/2411.18104</guid>
<content:encoded><![CDATA[
arXiv:2411.18104v5 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, a fundamental bottleneck persists: these models often struggle with tasks requiring complex, multi-step reasoning, particularly in mathematical problem-solving. This deficiency stems from the critical scarcity of large-scale, high-quality, domain-specific datasets necessary for cultivating sophisticated reasoning abilities. To overcome this challenge, we introduce Template-based Data Generation (TDG), a novel and scalable paradigm that harnesses frontier LLMs (GPT-4) to automatically generate parameterized meta-templates, which in turn synthesize a virtually infinite stream of high-quality problems and solutions. Using this paradigm, we create TemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million synthetically generated grade school math problems. Each problem is accompanied by a programmatically verifiable solution, offering an unprecedented level of quality at scale. This resource not only resolves the data scarcity issue for supervised fine-tuning but also provides a robust mechanism for model alignment through Reinforcement Learning with Verifiable Rewards (RLVR). Our approach elevates data augmentation by leveraging GPT-4 to generate meta-templates, ensuring diverse and complex problem structures. By providing a scalable solution to the data and verification bottleneck, TDG and TemplateGSM pave the way for a new generation of LLMs with powerful, reliable reasoning skills. Project Page: https://github.com/iiis-ai/TemplateMath
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenGenAlign: A Preference Dataset and Benchmark for Trustworthy Reward Modeling in Open-Ended, Long-Context Generation</title>
<link>https://arxiv.org/abs/2501.13264</link>
<guid>https://arxiv.org/abs/2501.13264</guid>
<content:encoded><![CDATA[
arXiv:2501.13264v3 Announce Type: replace 
Abstract: Reward Modeling is critical in evaluating and improving the generation of Large Language Models (LLMs). While numerous recent works have shown its feasibility in improving safety, helpfulness, reasoning, and instruction-following ability, its capability and generalization to open-ended long-context generation is still rarely explored. In this paper, we introduce OpenGenAlign, a framework and a high-quality dataset designed to develop reward models to evaluate and improve hallucination-free, comprehensive, reliable, and efficient open-ended long-context generation. We define four key metrics to assess generation quality and develop an automated pipeline to evaluate the outputs of multiple LLMs across long-context QA, Data-to-Text, and Summarization scenarios using o3, ending up with 33K high-quality preference data with a human agreement rate of 81\%. Experimental results first demonstrate that existing reward models perform suboptimally on the held-out benchmark. And Our trained reward model achieves superior performance in the benchmark and effectively improves the generation quality of the policy models using Reinforcement Learning (RL). Additionally, OpenGenAlign could be used for effective guided generation in existing datasets. Furthermore, we demonstrate that the OpenGenAlign could be integrated with reward data from other domains to achieve better performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Linguistics Learned to Stop Worrying and Love the Language Models</title>
<link>https://arxiv.org/abs/2501.17047</link>
<guid>https://arxiv.org/abs/2501.17047</guid>
<content:encoded><![CDATA[
arXiv:2501.17047v3 Announce Type: replace 
Abstract: Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure. We argue that both extremes are wrong. LMs can contribute to fundamental questions about linguistic structure, language processing, and learning. They force us to rethink arguments and ways of thinking that have been foundational in linguistics. While they do not replace linguistic structure and theory, they serve as model systems and working proofs of concept for gradient, usage-based approaches to language. We offer an optimistic take on the relationship between language models and linguistics.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.04387</link>
<guid>https://arxiv.org/abs/2502.04387</guid>
<content:encoded><![CDATA[
arXiv:2502.04387v3 Announce Type: replace 
Abstract: Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing other existing FL methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment</title>
<link>https://arxiv.org/abs/2502.14389</link>
<guid>https://arxiv.org/abs/2502.14389</guid>
<content:encoded><![CDATA[
arXiv:2502.14389v2 Announce Type: replace 
Abstract: Argument mining algorithms analyze the argumentative structure of essays, making them a valuable tool for enhancing education by providing targeted feedback on the students' argumentation skills. While current methods often use encoder or encoder-decoder deep learning architectures, decoder-only models remain largely unexplored, offering a promising research direction.
  This paper proposes leveraging open-source, small Large Language Models (LLMs) for argument mining through few-shot prompting and fine-tuning. These models' small size and open-source nature ensure accessibility, privacy, and computational efficiency, enabling schools and educators to adopt and deploy them locally. Specifically, we perform three tasks: segmentation of student essays into arguments, classification of the arguments by type, and assessment of their quality. We empirically evaluate the models on the Feedback Prize - Predicting Effective Arguments dataset of grade 6-12 students essays and demonstrate how fine-tuned small LLMs outperform baseline methods in segmenting the essays and determining the argument types while few-shot prompting yields comparable performance to that of the baselines in assessing quality. This work highlights the educational potential of small, open-source LLMs to provide real-time, personalized feedback, enhancing independent learning and writing skills while ensuring low computational cost and privacy.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs</title>
<link>https://arxiv.org/abs/2502.21239</link>
<guid>https://arxiv.org/abs/2502.21239</guid>
<content:encoded><![CDATA[
arXiv:2502.21239v5 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable performance across diverse tasks by encoding vast amounts of factual knowledge. However, they are still prone to hallucinations, generating incorrect or misleading information, often accompanied by high uncertainty. Existing methods for hallucination detection primarily focus on quantifying internal uncertainty, which arises from missing or conflicting knowledge within the model. However, hallucinations can also stem from external uncertainty, where ambiguous user queries lead to multiple possible interpretations. In this work, we introduce Semantic Volume, a novel mathematical measure for quantifying both external and internal uncertainty in LLMs. Our approach perturbs queries and responses, embeds them in a semantic space, and computes the Gram matrix determinant of the embedding vectors, capturing their dispersion as a measure of uncertainty. Our framework provides a generalizable and unsupervised uncertainty detection method without requiring internal access to LLMs. We conduct extensive experiments on both external and internal uncertainty detections, demonstrating that our Semantic Volume method consistently outperforms existing baselines in both tasks. Additionally, we provide theoretical insights linking our measure to differential entropy, unifying and extending previous sampling-based uncertainty measures such as the semantic entropy. Semantic Volume is shown to be a robust and interpretable approach to improving the reliability of LLMs by systematically detecting uncertainty in both user queries and model responses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors</title>
<link>https://arxiv.org/abs/2503.16578</link>
<guid>https://arxiv.org/abs/2503.16578</guid>
<content:encoded><![CDATA[
arXiv:2503.16578v2 Announce Type: replace 
Abstract: While voice technologies increasingly serve aging populations, current systems exhibit significant performance gaps due to inadequate training data capturing elderly-specific vocal characteristics like presbyphonia and dialectal variations. The limited data available on super-aged individuals in existing elderly speech datasets, coupled with overly simple recording styles and annotation dimensions, exacerbates this issue. To address the critical scarcity of speech data from individuals aged 75 and above, we introduce SeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset contains 55.53 hours of speech from 101 natural conversations involving 202 participants, ensuring a strategic balance across gender, region, and age. Through detailed annotation across multiple dimensions, it can support a wide range of speech tasks. We perform extensive experiments on speaker verification, speaker diarization, speech recognition, and speech editing tasks, offering crucial insights for the development of speech technologies targeting this age group.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARS: Multi-Agent Adaptive Reasoning with Socratic Guidance for Automated Prompt Optimization</title>
<link>https://arxiv.org/abs/2503.16874</link>
<guid>https://arxiv.org/abs/2503.16874</guid>
<content:encoded><![CDATA[
arXiv:2503.16874v2 Announce Type: replace 
Abstract: Large language models (LLMs) typically operate in a question-answering paradigm, where the quality of the input prompt critically affects the response. Automated Prompt Optimization (APO) aims to overcome the cognitive biases of manually crafted prompts and explore a broader prompt design space. However, existing APO methods often suffer from rigid template structures and inefficient exploration in the prompt space. To this end, we propose a Multi-Agent Adaptive Reasoning with Socratic guidance framework (MARS) for APO. MARS consists of five complementary agents and formulates the optimization process as a Partially Observable Markov Decision Process (POMDP), enabling adaptive prompt refinement through explicit state modeling and interactive feedback. Specifically, a Planner agent generates flexible optimization trajectories, a Teacher-Critic-Student triad engages in Socratic-style dialogue to iteratively optimize the prompt based on pseudo-gradient signals in the text space, and a Target agent executes the prompt in downstream tasks to provide performance feedback. MARS integrates reasoning, feedback, and state transition into a unified hidden-state evolution process, improving both the effectiveness and interpretability of optimization. Extensive experiments on multiple datasets demonstrate that MARS outperforms existing APO methods in terms of optimization performance, search efficiency, and interpretability.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting individual differences to bootstrap communication</title>
<link>https://arxiv.org/abs/2504.05211</link>
<guid>https://arxiv.org/abs/2504.05211</guid>
<content:encoded><![CDATA[
arXiv:2504.05211v3 Announce Type: replace 
Abstract: Establishing a communication system is hard because the intended meaning of a signal is unknown to its receiver when first produced, and the signaller also has no idea how that signal will be interpreted. Most theoretical accounts of the emergence of communication systems rely on feedback to reinforce behaviours that have led to successful communication in the past. However, providing such feedback requires already being able to communicate the meaning that was intended or interpreted. Therefore these accounts cannot explain how communication can be bootstrapped from non-communicative behaviours. Here we present a model that shows how a communication system, capable of expressing an unbounded number of meanings, can emerge as a result of individual behavioural differences in a large population without any pre-existing means to determine communicative success. The two key cognitive capabilities responsible for this outcome are behaving predictably in a given situation, and an alignment of psychological states ahead of signal production that derives from shared intentionality. Since both capabilities can exist independently of communication, our results are compatible with theories in which large flexible socially-learned communication systems like language are the product of a general but well-developed capacity for social cognition.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
arXiv:2504.19254v4 Announce Type: replace 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for closed-book hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16483</link>
<guid>https://arxiv.org/abs/2505.16483</guid>
<content:encoded><![CDATA[
arXiv:2505.16483v3 Announce Type: replace 
Abstract: Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to reduce faithfulness hallucinations of LLMs across different downstream tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models</title>
<link>https://arxiv.org/abs/2505.16774</link>
<guid>https://arxiv.org/abs/2505.16774</guid>
<content:encoded><![CDATA[
arXiv:2505.16774v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong instruction-following capabilities in text-based tasks. However, this ability often deteriorates in multimodal models after alignment with non-text modalities such as images or audio. While several recent efforts have investigated instruction-following performance in text and vision-language models, instruction-following in audio-based large language models remains largely unexplored. To bridge this gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess the ability to follow instructions in an audio LLM. IFEval-Audio contains 280 audio-instruction-answer triples across six diverse dimensions: Content, Capitalization, Symbol, List Structure, Length, and Format. Each example pairs an audio input with a text instruction, requiring the model to generate an output that follows a specified structure. We benchmark state-of-the-art audio LLMs on their ability to follow audio-involved instructions. The dataset is released publicly to support future research in this emerging area.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High</title>
<link>https://arxiv.org/abs/2505.22354</link>
<guid>https://arxiv.org/abs/2505.22354</guid>
<content:encoded><![CDATA[
arXiv:2505.22354v2 Announce Type: replace 
Abstract: This paper examines how LLMs handle false presuppositions and whether certain linguistic factors influence their responses to falsely presupposed content. Presuppositions subtly introduce information as given, making them highly effective at embedding disputable or false information. This raises concerns about whether LLMs, like humans, may fail to detect and correct misleading assumptions introduced as false presuppositions, even when the stakes of misinformation are high. Using a systematic approach based on linguistic presupposition analysis, we investigate the conditions under which LLMs are more or less sensitive to adopt or reject false presuppositions. Focusing on political contexts, we examine how factors like linguistic construction, political party, and scenario probability impact the recognition of false presuppositions. We conduct experiments with a newly created dataset and examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's Mistral-7B-v03. Our results show that the models struggle to recognize false presuppositions, with performance varying by condition. This study highlights that linguistic presupposition analysis is a valuable tool for uncovering the reinforcement of political misinformation in LLM responses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models</title>
<link>https://arxiv.org/abs/2505.23015</link>
<guid>https://arxiv.org/abs/2505.23015</guid>
<content:encoded><![CDATA[
arXiv:2505.23015v2 Announce Type: replace 
Abstract: Stealthy data poisoning during fine-tuning can backdoor large language models (LLMs), threatening downstream safety. Existing detectors either use classifier-style probability signals--ill-suited to generation--or rely on rewriting, which can degrade quality and even introduce new triggers. We address the practical need to efficiently remove poisoned examples before or during fine-tuning. We observe a robust signal in the response space: after applying TF-IDF to model responses, poisoned examples form compact clusters (driven by consistent malicious outputs), while clean examples remain dispersed. We leverage this with RFTC--Reference-Filtration + TF-IDF Clustering. RFTC first compares each example's response with that of a reference model and flags those with large deviations as suspicious; it then performs TF-IDF clustering on the suspicious set and identifies true poisoned examples using intra-class distance. On two machine translation datasets and one QA dataset, RFTC outperforms prior detectors in both detection accuracy and the downstream performance of the fine-tuned models. Ablations with different reference models further validate the effectiveness and robustness of Reference-Filtration.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding</title>
<link>https://arxiv.org/abs/2506.00942</link>
<guid>https://arxiv.org/abs/2506.00942</guid>
<content:encoded><![CDATA[
arXiv:2506.00942v2 Announce Type: replace 
Abstract: The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs. Our code and data are available at: https://github.com/CuCl-2/anyECG-chat.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models</title>
<link>https://arxiv.org/abs/2507.03133</link>
<guid>https://arxiv.org/abs/2507.03133</guid>
<content:encoded><![CDATA[
arXiv:2507.03133v2 Announce Type: replace 
Abstract: Although demonstrating remarkable performance on reasoning tasks, Large Language Models (LLMs) still tend to fabricate unreliable responses when confronted with problems that are unsolvable or beyond their capability, severely undermining the reliability. Prior studies of LLM reliability have primarily focused on knowledge tasks to identify unanswerable questions, while mathematical reasoning tasks have remained unexplored due to the dearth of unsolvable math problems. To systematically investigate LLM reliability in mathematical reasoning tasks, we formulate the reliability evaluation for both solvable and unsolvable problems. We then develop a ReliableMath dataset which incorporates open-source solvable problems and high-quality unsolvable problems synthesized by our proposed construction workflow with human evaluations. Experiments are conducted on various LLMs with several key findings uncovered. LLMs fail to directly identify unsolvable problems and always generate fabricated responses. When instructing LLMs to indicate unsolvability using a reliable prompt, the reliability of larger-sized LLMs remains on solvable problems, but notably improves on unsolvable problems yet still falls short of solvable problems. However, small LLMs rarely show any progress despite employing reliable prompts. Therefore, we further propose an alignment strategy to enhance small LLMs' reliability, which can significantly improve LLM reliability performances on both in-domain and out-of-domain tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian</title>
<link>https://arxiv.org/abs/2507.22159</link>
<guid>https://arxiv.org/abs/2507.22159</guid>
<content:encoded><![CDATA[
arXiv:2507.22159v2 Announce Type: replace 
Abstract: Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset designed to evaluate the naturalness and quality of LLM-generated text. The dataset contains 522 prompts and yields 4,099 human-annotated pairwise preferences from comparisons across five instruction-tuned LLMs. All annotations are natively written in Indonesian with strong inter-annotator agreement, measured by Krippendorff's alpha. Our benchmark spans 10 diverse categories, enabling practitioners to identify LLMs' fine-grained strengths and weaknesses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Super Experts in Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2507.23279</link>
<guid>https://arxiv.org/abs/2507.23279</guid>
<content:encoded><![CDATA[
arXiv:2507.23279v2 Announce Type: replace 
Abstract: In this study, we report, for the first time, the discovery and systematic investigation of a distinct subset of experts that play a pivotal role in the MoE LLMs' forward inference. These experts are prevalent in open-source MoE LLMs, and despite their extremely limited number, pruning them results in a substantial decline in model performance (e.g., prune just three out of 6,144 causes Qwen3-30B-A3B to generate repetitive and uninformative outputs).We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs: (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs is model-specific, data-agnostic, and remains unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further investigate why compressing SEs exerts such a pronounced impact. We show that, in MoE LLMs, SEs serve as the primary source of the systematic outlier mechanism in Transformers, and that compressing them profoundly disrupts this process, ultimately causing the collapse of attention sinks. These findings advance the understanding of the internal dynamics of MoE LLMs, filling an important gap in the current knowledge. The code is provided in https://github.com/ZunhaiSu/Super-Experts-Profilling.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates</title>
<link>https://arxiv.org/abs/2508.01159</link>
<guid>https://arxiv.org/abs/2508.01159</guid>
<content:encoded><![CDATA[
arXiv:2508.01159v2 Announce Type: replace 
Abstract: This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Language Models via Causal Reasoning</title>
<link>https://arxiv.org/abs/2508.12495</link>
<guid>https://arxiv.org/abs/2508.12495</guid>
<content:encoded><![CDATA[
arXiv:2508.12495v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title>
<link>https://arxiv.org/abs/2508.15212</link>
<guid>https://arxiv.org/abs/2508.15212</guid>
<content:encoded><![CDATA[
arXiv:2508.15212v3 Announce Type: replace 
Abstract: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs</title>
<link>https://arxiv.org/abs/2508.19594</link>
<guid>https://arxiv.org/abs/2508.19594</guid>
<content:encoded><![CDATA[
arXiv:2508.19594v3 Announce Type: replace 
Abstract: Context faithfulness is essential for reliable reasoning in context-dependent scenarios. However, large language models often struggle to ground their outputs in the provided context, resulting in irrelevant responses. Inspired by the emergent expert specialization observed in mixture-of-experts architectures, this work investigates whether certain experts exhibit specialization in context utilization, offering a potential pathway toward targeted optimization for improved context faithfulness. To explore this, we propose Router Lens, a method that accurately identifies context-faithful experts. Our analysis reveals that these experts progressively amplify attention to relevant contextual information, thereby enhancing context grounding. Building on this insight, we introduce Context-faithful Expert Fine-Tuning (CEFT), a lightweight optimization approach that selectively fine-tunes context-faithful experts. Experiments across a wide range of benchmarks and models demonstrate that CEFT matches or surpasses the performance of full fine-tuning while being significantly more efficient.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Language Models Handle a Non-Gregorian Calendar? The Case of the Japanese wareki</title>
<link>https://arxiv.org/abs/2509.04432</link>
<guid>https://arxiv.org/abs/2509.04432</guid>
<content:encoded><![CDATA[
arXiv:2509.04432v3 Announce Type: replace 
Abstract: Temporal reasoning and knowledge are essential capabilities for language models (LMs). While much prior work has analyzed and improved temporal reasoning in LMs, most studies have focused solely on the Gregorian calendar. However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew calendars, are in active use and reflect culturally grounded conceptions of time. If and how well current LMs can accurately handle such non-Gregorian calendars has not been evaluated so far. Here, we present a systematic evaluation of how well language models handle one such non-Gregorian system: the Japanese wareki. We create datasets that require temporal knowledge and reasoning in using wareki dates. Evaluating open and closed LMs, we find that some models can perform calendar conversions, but GPT-4o, Deepseek V3, and even Japanese-centric models struggle with Japanese calendar arithmetic and knowledge involving wareki dates. Error analysis suggests corpus frequency of Japanese calendar expressions and a Gregorian bias in the model's knowledge as possible explanations. Our results show the importance of developing LMs that are better equipped for culture-specific tasks such as calendar understanding.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations</title>
<link>https://arxiv.org/abs/2509.04498</link>
<guid>https://arxiv.org/abs/2509.04498</guid>
<content:encoded><![CDATA[
arXiv:2509.04498v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation systems for tasks like education planning, yet their recommendations risk perpetuating societal biases. This paper empirically examines geographic, demographic, and economic biases in university and program suggestions from three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360 simulated user profiles varying by gender, nationality, and economic status, we analyze over 25,000 recommendations. Results show strong biases: institutions in the Global North are disproportionately favored, recommendations often reinforce gender stereotypes, and institutional repetition is prevalent. While LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities across 58 countries, systemic disparities persist. To quantify these issues, we propose a novel, multi-dimensional evaluation framework that goes beyond accuracy by measuring demographic and geographic representation. Our findings highlight the urgent need for bias consideration in educational LMs to ensure equitable global access to higher education.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance</title>
<link>https://arxiv.org/abs/2510.03528</link>
<guid>https://arxiv.org/abs/2510.03528</guid>
<content:encoded><![CDATA[
arXiv:2510.03528v2 Announce Type: replace 
Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium</title>
<link>https://arxiv.org/abs/2510.25977</link>
<guid>https://arxiv.org/abs/2510.25977</guid>
<content:encoded><![CDATA[
arXiv:2510.25977v3 Announce Type: replace 
Abstract: AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache</title>
<link>https://arxiv.org/abs/2510.25979</link>
<guid>https://arxiv.org/abs/2510.25979</guid>
<content:encoded><![CDATA[
arXiv:2510.25979v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Positional Bias in Long-Document Ranking: Impact, Assessment, and Mitigation</title>
<link>https://arxiv.org/abs/2207.01262</link>
<guid>https://arxiv.org/abs/2207.01262</guid>
<content:encoded><![CDATA[
arXiv:2207.01262v4 Announce Type: replace-cross 
Abstract: We tested over 20 Transformer models for ranking long documents (including recent LongP models trained with FlashAttention and RankGPT models "powered" by OpenAI and Anthropic cloud APIs). We compared them with the simple FirstP baseline, which applied the same model to truncated input (up to 512 tokens). On MS MARCO, TREC DL, and Robust04 no long-document model outperformed FirstP by more than 5% (on average). We hypothesized that this lack of improvement is not due to inherent model limitations, but due to benchmark positional bias (most relevant passages tend to occur early in documents), which is known to exist in MS MARCO. To confirm this, we analyzed positional relevance distributions across four long-document corpora (with six query sets) and observed the same early-position bias. Surprisingly, we also found bias in six BEIR collections, which are typically categorized as short-document datasets. We then introduced a new diagnostic dataset, MS MARCO FarRelevant, where relevant spans were deliberately placed beyond the first 512 tokens. On this dataset, many long-context models (including RankGPT) performed at random-baseline level, suggesting overfitting to positional bias. We also experimented with debiasing training data, but with limited success. Our findings (1) highlight the need for careful benchmark design in evaluating long-context models for document ranking, (2) identify model types that are more robust to positional bias, and (3) motivate further work on approaches to debias training data. We release our code and data to support further research.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formalizing and Benchmarking Prompt Injection Attacks and Defenses</title>
<link>https://arxiv.org/abs/2310.12815</link>
<guid>https://arxiv.org/abs/2310.12815</guid>
<content:encoded><![CDATA[
arXiv:2310.12815v5 Announce Type: replace-cross 
Abstract: A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https://github.com/liu00222/Open-Prompt-Injection.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms</title>
<link>https://arxiv.org/abs/2409.16694</link>
<guid>https://arxiv.org/abs/2409.16694</guid>
<content:encoded><![CDATA[
arXiv:2409.16694v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language processing, showcasing exceptional performance across various tasks. However, the expensive memory and computational requirements present significant challenges for their practical deployment. Low-bit quantization has emerged as a critical approach to mitigate these challenges by reducing the bit-width of model parameters, activations, and gradients, thus decreasing memory usage and computational demands. This paper presents a comprehensive survey of low-bit quantization methods tailored for LLMs, covering the fundamental principles, system implementations, and algorithmic strategies. An overview of basic concepts and new data formats specific to low-bit LLMs is first introduced, followed by a review of frameworks and systems that facilitate low-bit LLMs across various hardware platforms. Then, we categorize and analyze techniques and toolkits for efficient low-bit training and inference of LLMs. Finally, we conclude with a discussion of future trends and potential advancements of low-bit LLMs. Our systematic overview from basic, system, and algorithm perspectives can offer valuable insights and guidelines for future works to enhance the efficiency and applicability of LLMs through low-bit quantization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4AD: Large Language Models for Autonomous Driving - Concept, Review, Benchmark, Experiments, and Future Trends</title>
<link>https://arxiv.org/abs/2410.15281</link>
<guid>https://arxiv.org/abs/2410.15281</guid>
<content:encoded><![CDATA[
arXiv:2410.15281v4 Announce Type: replace-cross 
Abstract: With the broader adoption and highly successful development of Large Language Models (LLMs), there has been growing interest and demand for applying LLMs to autonomous driving technology. Driven by their natural language understanding and reasoning capabilities, LLMs have the potential to enhance various aspects of autonomous driving systems, from perception and scene understanding to interactive decision-making. In this paper, we first introduce the novel concept of designing Large Language Models for Autonomous Driving (LLM4AD), followed by a review of existing LLM4AD studies. Then, we propose a comprehensive benchmark for evaluating the instruction-following and reasoning abilities of LLM4AD systems, which includes LaMPilot-Bench, CARLA Leaderboard 1.0 Benchmark in simulation and NuPlanQA for multi-view visual question answering. Furthermore, we conduct extensive real-world experiments on autonomous vehicle platforms, examining both on-cloud and on-edge LLM deployment for personalized decision-making and motion control. Next, we explore the future trends of integrating language diffusion models into autonomous driving, exemplified by the proposed ViLaD (Vision-Language Diffusion) framework. Finally, we discuss the main challenges of LLM4AD, including latency, deployment, security and privacy, safety, trust and transparency, and personalization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Retrieval-Augmented Generation with Differential Privacy</title>
<link>https://arxiv.org/abs/2412.04697</link>
<guid>https://arxiv.org/abs/2412.04697</guid>
<content:encoded><![CDATA[
arXiv:2412.04697v3 Announce Type: replace-cross 
Abstract: With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval-augmented generation (RAG) is particularly effective -- it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$ across different models and datasets.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation</title>
<link>https://arxiv.org/abs/2503.08906</link>
<guid>https://arxiv.org/abs/2503.08906</guid>
<content:encoded><![CDATA[
arXiv:2503.08906v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks</title>
<link>https://arxiv.org/abs/2504.04277</link>
<guid>https://arxiv.org/abs/2504.04277</guid>
<content:encoded><![CDATA[
arXiv:2504.04277v3 Announce Type: replace-cross 
Abstract: Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5\% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases</title>
<link>https://arxiv.org/abs/2504.05478</link>
<guid>https://arxiv.org/abs/2504.05478</guid>
<content:encoded><![CDATA[
arXiv:2504.05478v2 Announce Type: replace-cross 
Abstract: Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q&amp;As on large text-attributed KGs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15311</link>
<guid>https://arxiv.org/abs/2505.15311</guid>
<content:encoded><![CDATA[
arXiv:2505.15311v2 Announce Type: replace-cross 
Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding</title>
<link>https://arxiv.org/abs/2505.17330</link>
<guid>https://arxiv.org/abs/2505.17330</guid>
<content:encoded><![CDATA[
arXiv:2505.17330v2 Announce Type: replace-cross 
Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework</title>
<link>https://arxiv.org/abs/2506.15538</link>
<guid>https://arxiv.org/abs/2506.15538</guid>
<content:encoded><![CDATA[
arXiv:2506.15538v4 Announce Type: replace-cross 
Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Within the context of large language models (LLMs) for natural language processing (NLP), current automated neuron-level feature description methods face two key challenges: limited robustness and the assumption that each neuron encodes a single concept (monosemanticity), despite increasing evidence of polysemanticity. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework specifically designed to capture the complexity of features in LLMs. Unlike approaches that assign a single description per neuron, common in many automated interpretability methods in NLP, PRISM produces more nuanced descriptions that account for both monosemantic and polysemantic behavior. We apply PRISM to LLMs and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v4 Announce Type: replace-cross 
Abstract: The Political Compass Test (PCT) and similar surveys are commonly used to assess political bias in auto-regressive LLMs. Our rigorous statistical experiments show that while changes to standard generation parameters have minimal effect on PCT scores, prompt phrasing and fine-tuning individually and together can significantly influence results. Interestingly, fine-tuning on politically rich vs. neutral datasets does not lead to different shifts in scores. We also generalize these findings to a similar popular test called 8 Values. Humans do not change their responses to questions when prompted differently (``answer this question'' vs ``state your opinion''), or after exposure to politically neutral text, such as mathematical formulae. But the fact that the models do so raises concerns about the validity of these tests for measuring model bias, and paves the way for deeper exploration into how political and social views are encoded in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval</title>
<link>https://arxiv.org/abs/2508.04001</link>
<guid>https://arxiv.org/abs/2508.04001</guid>
<content:encoded><![CDATA[
arXiv:2508.04001v2 Announce Type: replace-cross 
Abstract: Conversational search aims to satisfy users' complex information needs via multiple-turn interactions. The key challenge lies in revealing real users' search intent from the context-dependent queries. Previous studies achieve conversational search by fine-tuning a conversational dense retriever with relevance judgments between pairs of context-dependent queries and documents. However, this training paradigm encounters data scarcity issues. To this end, we propose ConvMix, a mixed-criteria framework to augment conversational dense retrieval, which covers more aspects than existing data augmentation frameworks. We design a two-sided relevance judgment augmentation schema in a scalable manner via the aid of large language models. Besides, we integrate the framework with quality control mechanisms to obtain semantically diverse samples and near-distribution supervisions to combine various annotated data. Experimental results on five widely used benchmarks show that the conversational dense retriever trained by our ConvMix framework outperforms previous baseline methods, which demonstrates our superior effectiveness.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v2 Announce Type: replace-cross 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories-traditionally used for training value or reward models-can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree-structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low-variance, prefix-aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance-a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER</title>
<link>https://arxiv.org/abs/2509.10975</link>
<guid>https://arxiv.org/abs/2509.10975</guid>
<content:encoded><![CDATA[
arXiv:2509.10975v2 Announce Type: replace-cross 
Abstract: Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER by jointly detecting textual mentions and grounding them to visual regions. While existing supervised methods achieve strong performance, they rely on costly multimodal annotations and often underperform in low-resource domains. Multimodal Large Language Models (MLLMs) show strong generalization but suffer from Domain Knowledge Conflict, producing redundant or incorrect mentions for domain-specific entities. To address these challenges, we propose ReFineG, a three-stage collaborative framework that integrates small supervised models with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware NER data synthesis strategy transfers LLM knowledge to small models with supervised training while avoiding domain knowledge conflicts. In the Refinement Stage, an uncertainty-based mechanism retains confident predictions from supervised models and delegates uncertain ones to the MLLM. In the Grounding Stage, a multimodal context selection algorithm enhances visual grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task, ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard, demonstrating its effectiveness with limited annotations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.03669</link>
<guid>https://arxiv.org/abs/2510.03669</guid>
<content:encoded><![CDATA[
arXiv:2510.03669v3 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance</title>
<link>https://arxiv.org/abs/2510.08048</link>
<guid>https://arxiv.org/abs/2510.08048</guid>
<content:encoded><![CDATA[
arXiv:2510.08048v2 Announce Type: replace-cross 
Abstract: Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glia: A Human-Inspired AI for Automated Systems Design and Optimization</title>
<link>https://arxiv.org/abs/2510.27176</link>
<guid>https://arxiv.org/abs/2510.27176</guid>
<content:encoded><![CDATA[
<div> Agents, Experimentation, Analysis, Networked systems design, AI architecture <br />
Summary: <br />
The study introduces Glia, an AI architecture utilizing large language models (LLMs) to autonomously design mechanisms for computer systems. It features a multi-agent workflow where each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that combines abstract reasoning with empirical feedback. Unlike existing machine learning methods, Glia generates interpretable designs and reveals its reasoning process. When applied to optimizing a distributed GPU cluster for LLM inference, Glia creates new algorithms for request routing, scheduling, and auto-scaling that rival human-expert performance in less time. It also provides unique insights into workload behavior. By leveraging reasoning LLMs and structured experimentation, the AI demonstrates the ability to produce innovative and comprehensible designs for complex systems problems. Overall, the results indicate that AI can excel in networked systems design by combining reasoning capabilities with practical experimentation. <br /> <div>
arXiv:2510.27176v2 Announce Type: replace-cross 
Abstract: Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Preliminary Study of RAG for Taiwanese Historical Archives</title>
<link>https://arxiv.org/abs/2511.07445</link>
<guid>https://arxiv.org/abs/2511.07445</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, RAG, Taiwanese Historical Archives, Traditional Chinese datasets, Fort Zeelandia, Taiwan Provincial Council Gazette <br />
Summary: 
This paper presents an initial study on the application of a Retrieval-Augmented Generation (RAG) pipeline to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, with corresponding open-ended query sets. The study examines the impacts of query characteristics and metadata integration strategies on retrieval quality, answer generation, and overall system performance. The findings suggest that early-stage metadata integration improves retrieval and answer accuracy. However, challenges such as generation hallucinations and difficulties in handling temporal or multi-hop historical queries persist in RAG systems. The research sheds light on the potential of RAG for knowledge-intensive tasks in Taiwanese Historical Archives but also highlights areas for further improvement in system functionality and accuracy. <br /> <div>
arXiv:2511.07445v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey</title>
<link>https://arxiv.org/abs/2511.07448</link>
<guid>https://arxiv.org/abs/2511.07448</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific idea generation, large language models, creativity, scientific discovery, methods

Summary:
This article explores the use of large language models (LLMs) in scientific idea generation, a complex task that requires balancing creativity with scientific soundness. The survey categorizes existing methods into five families, including external knowledge augmentation and multi-agent collaboration, to understand how different approaches generate creative and scientifically valid ideas. By applying creativity frameworks, the authors analyze the level and source of creativity each method emphasizes. The survey clarifies the current state of the field and suggests directions for reliable and transformative applications of LLMs in scientific discovery.<br /><br />Summary: <div>
arXiv:2511.07448v1 Announce Type: new 
Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2511.07457</link>
<guid>https://arxiv.org/abs/2511.07457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge graphs, web data, fine-tuning, LoRA parameters

Summary:
Large Language Models (LLMs) have shown great success in handling sequential textual data but struggle with structural data like knowledge graphs. Existing methods either convert graphs into text sequences with high token overhead or use additional modules for fixed-size token representations, leading to suboptimal results. To address this, the authors propose a novel framework called GRIP, which enables LLMs to internalize complex relational information from graphs through fine-tuning tasks. This knowledge is stored in lightweight LoRA parameters, allowing the fine-tuned LLM to handle various graph-related tasks without requiring the original graph at inference time. Extensive experiments on different benchmarks confirm the effectiveness and efficiency of the GRIP approach.<br /><br />Summary: Large Language Models (LLMs) struggle with structural data like knowledge graphs, prompting the development of the GRIP framework. GRIP equips LLMs with the ability to learn complex relational information from graphs through fine-tuning tasks, stored in lightweight LoRA parameters. This enables LLMs to handle graph-related tasks efficiently without needing access to the original graph at inference time. <div>
arXiv:2511.07457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment</title>
<link>https://arxiv.org/abs/2511.07458</link>
<guid>https://arxiv.org/abs/2511.07458</guid>
<content:encoded><![CDATA[
<div> Keywords: log summarization, evaluation, REFLEX, large language model, metrics

<br />
Summary: 
The article introduces REFLEX, a reference-free evaluation metric for log summarization using large language models (LLMs). Traditional metrics like ROUGE and BLEU rely on surface-level lexical overlap, making evaluating log summarization systems challenging due to the lack of high-quality reference summaries. REFLEX leverages LLMs as zero-shot evaluators to assess summary quality in terms of relevance, informativeness, and coherence without the need for gold-standard references or human annotations. The new metric produces stable, interpretable, and fine-grained evaluations across various log summarization datasets, effectively distinguishing model outputs better than traditional metrics. REFLEX offers a scalable alternative for evaluating log summaries in real-world scenarios where reference data may be scarce or unavailable. <br /><br /> <div>
arXiv:2511.07458v1 Announce Type: new 
Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It Takes Two: A Dual Stage Approach for Terminology-Aware Translation</title>
<link>https://arxiv.org/abs/2511.07461</link>
<guid>https://arxiv.org/abs/2511.07461</guid>
<content:encoded><![CDATA[
<div> terminology-constrained machine translation, DuTerm, NMT model, LLM, terminology adherence

Summary:
DuTerm is a two-stage architecture for terminology-constrained machine translation that combines a terminology-aware NMT model with a prompt-based LLM for post-editing. The system was evaluated on multiple language pairs using the WMT 2025 Terminology Shared Task corpus. Results showed that the LLM stage, which refines NMT output and enforces terminology adherence, consistently produced higher quality translations compared to strict constraint enforcement. The study also highlighted the importance of flexible, context-driven terminology handling by the LLM, which was found to work best as context-driven mutators rather than generators. This trade-off between strict constraints and context-driven refinement is critical for achieving high-quality translations in terminology-constrained settings. <div>
arXiv:2511.07461v1 Announce Type: new 
Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motif 2 12.7B technical report</title>
<link>https://arxiv.org/abs/2511.07464</link>
<guid>https://arxiv.org/abs/2511.07464</guid>
<content:encoded><![CDATA[
<div> Efficiency, Language understanding, Generalization, Training optimization, Fine-tuning 
<br />
Summary: 
Motif-2-12.7B is a new open-weight foundation model that focuses on efficiency and language understanding. It integrates Grouped Differential Attention (GDA) for improved representational efficiency. The model is pre-trained on a vast amount of data spanning different domains using a curriculum-driven data scheduler. The training system leverages special optimization techniques for enhanced performance in large-scale distributed environments. Post-training involves a supervised fine-tuning pipeline to improve general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across various benchmarks, showcasing the effectiveness of architectural scaling and optimized training design in achieving comparable capabilities to larger models. 
<br /> <div>
arXiv:2511.07464v1 Announce Type: new 
Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2511.07498</link>
<guid>https://arxiv.org/abs/2511.07498</guid>
<content:encoded><![CDATA[
<div> method, attention heads, multilingual processing, language-specific, interpretability
Summary:
The article discusses the role of multi-head self-attention (MHA) in supporting multilingual processing in Large Language Models (LLMs). The authors propose Language Attention Head Importance Scores (LAHIS) as a method to identify attention head importance for multilingual capabilities in LLMs. They apply LAHIS to three LLMs and discover the presence of language-specific and language-general heads, with language-specific heads aiding in cross-lingual attention transfer. A lightweight adaptation is introduced to improve XQuAD accuracy by modulating attention outputs over language heads with minimal parameters. This study enhances the interpretability and multilingual capabilities of LLMs by highlighting the importance of MHA in addressing challenges related to multilingual understanding and generation. <br /><br />Summary: <div>
arXiv:2511.07498v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Optimization Unlocks Real-Time Pairwise Reranking</title>
<link>https://arxiv.org/abs/2511.07555</link>
<guid>https://arxiv.org/abs/2511.07555</guid>
<content:encoded><![CDATA[
<div> optimization methods, latency reduction, pairwise reranking, Large Language Models, real-time applications
Summary: 
Pairwise reranking of documents in information retrieval systems to enhance Retrieval-Augmented Generation (RAG) processes is crucial. Large Language Models (LLMs) play a significant role in this reranking, with Pairwise Reranking Prompting (PRP) being a popular method. However, concerns about complexity and computational demands exist. This study focuses on optimizing pairwise reranking, achieving a remarkable latency reduction while maintaining performance levels. By using smaller models, limiting the reranked set, and implementing other optimizations such as reducing positional bias and restricting output tokens, the efficiency of LLM-based reranking is significantly improved. These optimizations make LLM-based reranking more feasible for real-world deployment in latency-sensitive applications. <br /><br />Summary: <div>
arXiv:2511.07555v1 Announce Type: new 
Abstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives</title>
<link>https://arxiv.org/abs/2511.07641</link>
<guid>https://arxiv.org/abs/2511.07641</guid>
<content:encoded><![CDATA[
<div> Keywords: emotional nuances, Dutch-specific LLMs, valence prediction, sentiment analysis, low-resource language variant

Summary:
Traditional lexicon-based tools like LIWC and Pattern have been foundational in understanding emotional nuances in everyday language. However, this study evaluated the performance of Dutch-specific Large Language Models (LLMs) against these traditional methods for valence prediction in Flemish, a low-resource language variant. Surprisingly, the Dutch-tuned LLMs did not outperform Pattern, suggesting a challenge in capturing emotional valence in spontaneous narratives. The findings question assumptions about LLM superiority in sentiment analysis tasks and emphasize the need for culturally and linguistically tailored evaluation frameworks for low-resource language variants. This study highlights the complexity of emotional expression in real-world language use and raises concerns about the adequacy of current LLM fine-tuning approaches. <div>
arXiv:2511.07641v1 Announce Type: new 
Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering</title>
<link>https://arxiv.org/abs/2511.07659</link>
<guid>https://arxiv.org/abs/2511.07659</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Natural Language Inference, QA datasets, DIVER-QA, semantic nuances 

Summary: 
- Evaluating answers from large language models is challenging due to semantic nuances that are missed by lexical metrics and the computational expense of LLM-as-Judge scoring.
- A lightweight alternative using off-the-shelf Natural Language Inference scoring augmented by a simple lexical-match flag matches GPT-4o's accuracy on long-form QA with significantly fewer parameters.
- The authors introduce DIVER-QA, a new 3000-sample human-annotated benchmark that spans five QA datasets and five candidate LLMs to test human alignment of evaluation metrics.
- Results demonstrate that inexpensive NLI-based evaluation remains competitive in measuring LLM performance and offer DIVER-QA as an open resource for future metric research. 

<br /><br />Summary: <div>
arXiv:2511.07659v1 Announce Type: new 
Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stress Testing Factual Consistency Metrics for Long-Document Summarization</title>
<link>https://arxiv.org/abs/2511.07689</link>
<guid>https://arxiv.org/abs/2511.07689</guid>
<content:encoded><![CDATA[
<div> Keywords: factual consistency, abstractive text summarization, factuality metrics, long-document setting, benchmark datasets<br />
<br />
Summary: 
The study evaluates the reliability of six factuality metrics for abstractive text summarization on long documents. Perturbations like paraphrasing and simplification were used to test metric robustness. Short-form metrics showed inconsistent scores for semantically equivalent summaries and struggled with information-dense claims similar to the source document. Expanding the retrieval context improved stability in some domains but no metric consistently maintained factual alignment. The study suggests directions for improving factuality evaluation, including multi-span reasoning and context-aware calibration. The results underline the need for training on meaning-preserving variations to enhance robustness in long-form summarization.<br /> 
 <div>
arXiv:2511.07689v1 Announce Type: new 
Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences</title>
<link>https://arxiv.org/abs/2511.07691</link>
<guid>https://arxiv.org/abs/2511.07691</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference optimization, Language models, Multilingual settings, Confidence-Aware, Robustness 

Summary: 
Preference optimization is a crucial technique for aligning large language models with human preferences post-training. Existing methods like Direct Preference Optimization (DPO) may struggle to generalize effectively in multilingual settings. To address this, Confidence-Aware Preference Optimization (CAPO) offers a dynamic loss scaling approach based on relative reward, enhancing robustness to noisy or low-margin comparisons. Empirical results show CAPO surpassing existing baselines by 16% in reward accuracy and improving alignment by widening the gap between preferred and dispreferred responses across different languages. This innovation highlights the importance of incorporating confidence levels in preference pairs to enhance the performance and generalizability of preference optimization techniques in multilingual contexts. 

<br /><br />Summary: <div>
arXiv:2511.07691v1 Announce Type: new 
Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Critical Confabulation: Can LLMs Hallucinate for Social Good?</title>
<link>https://arxiv.org/abs/2511.07722</link>
<guid>https://arxiv.org/abs/2511.07722</guid>
<content:encoded><![CDATA[
<div> hallucinate, confabulations, critical confabulation, social affordances, historical accuracy<br />
<br />
Summary: <br />
The article discusses the concept of critical confabulation, inspired by critical fabulation, where LLMs are used to fill in gaps in historical archives due to social inequalities. By simulating gaps in character-centric timelines, LLMs can generate masked events to reconstruct narratives of "hidden figures" in history. Audited LLM models demonstrated the capability to perform critical confabulation effectively, with controlled and well-specified hallucinations supporting knowledge production without compromising historical accuracy. The study evaluated the performance of different LLM models and baselines in generating useful hallucinations under various prompts, showcasing the potential for LLMs to contribute to historical narrative reconstruction. <div>
arXiv:2511.07722v1 Announce Type: new 
Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production</title>
<link>https://arxiv.org/abs/2511.07752</link>
<guid>https://arxiv.org/abs/2511.07752</guid>
<content:encoded><![CDATA[
<div> predictability, language production, naturalistic speech, substitution errors, lexical planning
Summary: 
The study examines how contextual predictability influences word choice and form in online language production. It explores the effects of backward predictability, which considers a word's predictability given its future context, in addition to its predictability based on past context. By analyzing naturalistic speech corpora, the study introduces a new information-theoretic predictability measure that integrates both past and future context. Results show that backward predictability impacts word duration and substitution errors. These errors provide insights into how speakers prioritize form, meaning, and context-based information during lexical planning. Overall, the findings highlight the significance of past and future context in word encoding and selection, shedding light on the mechanisms of sentence planning in language production. <br /><br />Summary: <div>
arXiv:2511.07752v1 Announce Type: new 
Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark</title>
<link>https://arxiv.org/abs/2511.07794</link>
<guid>https://arxiv.org/abs/2511.07794</guid>
<content:encoded><![CDATA[
<div> Methodology, Evaluation system, Design philosophy, Large language models, Insurance field

Summary:
This paper introduces the construction methodology, evaluation system, and design philosophy of CUFEInse v1.0. The benchmark covers 5 core dimensions, 54 sub-indicators, and 14,430 questions in the insurance field. Evaluation results show shortcomings in general-purpose and domain-specific models in insurance scenarios. Common bottlenecks in large models include weak actuarial capabilities and compliance adaptation. CUFEInse fills the gap in professional evaluation benchmarks for insurance, serving as a reference for academia and industry. The paper also discusses future iterations focusing on domain adaptation and reasoning enhancement for insurance large models. <div>
arXiv:2511.07794v1 Announce Type: new 
Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory</title>
<link>https://arxiv.org/abs/2511.07800</link>
<guid>https://arxiv.org/abs/2511.07800</guid>
<content:encoded><![CDATA[
<div> memory, large language models, reinforcement learning, strategic reasoning, adaptive

Summary:
The paper introduces a novel agent-centric, trainable, multi-layered graph memory framework to enhance the reasoning capabilities of Large Language Models (LLMs). This framework utilizes context memory to guide decision-making and improves the utilization of parametric information. A reinforcement-based weight optimization procedure is proposed to make the memory adaptable, estimating the utility of strategic meta-cognition based on reward feedback from downstream tasks. The optimized strategies are integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory demonstrates robust generalization, enhances strategic reasoning performance, and consistently benefits during Reinforcement Learning training. Overall, the framework improves LLM agents' ability to utilize prior experiences, leading to better decision-making in complex, open-ended environments. <br /><br />Summary: <div>
arXiv:2511.07800v1 Announce Type: new 
Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys</title>
<link>https://arxiv.org/abs/2511.07871</link>
<guid>https://arxiv.org/abs/2511.07871</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, social survey, AlignSurvey, dataset

Summary:
AlignSurvey introduces a benchmark that uses large language models (LLMs) to replicate and evaluate the entire social survey process. It includes four tasks: social role modeling, semi-structured interview modeling, attitude stance modeling, and survey response modeling. The benchmark provides task-specific evaluation metrics to assess fidelity, consistency, and fairness at individual and group levels, focusing on demographic diversity. To support AlignSurvey, a multi-tiered dataset architecture is constructed, including the Social Foundation Corpus and Entire-Pipeline Survey Datasets. The SurveyLM family, obtained through fine-tuning of LLMs, is released for domain-specific alignment evaluation. All datasets, models, and tools are available for transparent and socially responsible research.<br /><br />Summary: <div>
arXiv:2511.07871v1 Announce Type: new 
Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles</title>
<link>https://arxiv.org/abs/2511.07879</link>
<guid>https://arxiv.org/abs/2511.07879</guid>
<content:encoded><![CDATA[
<div> Keywords: social unrest events, forecasting, topic modeling, Named Entity Recognition, Related Entity Extraction

Summary:
<br /><br />Forecasting social unrest events in democracies like India is crucial for administrative officials to take necessary action. This paper presents a system that utilizes topic modeling and word2vec to filter relevant news articles, along with Named Entity Recognition (NER) methods to identify key entities like people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format for better analysis. The model developed in this study is geographically independent and aims to identify key features for filtering civil unrest events effectively. The concept of Related Entities, which are crucial entities involved in the event, is introduced, and a method for extracting them is proposed, referred to as Related Entity Extraction. By analyzing announcements in news articles and extracting related entities, the system can accurately forecast planned social unrest events and enable timely intervention by authorities. <div>
arXiv:2511.07879v1 Announce Type: new 
Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification</title>
<link>https://arxiv.org/abs/2511.07888</link>
<guid>https://arxiv.org/abs/2511.07888</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, adversarial attacks, robustness, manifold modeling, sentence embeddings<br />
<br />
Summary: <br />
The article introduces a new approach, Manifold-Correcting Causal Flow (MC^2F), to address the challenge of balancing model robustness against adversarial attacks and performance on clean data in text classification. MC^2F consists of two modules: Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the distribution of clean data in the encoder embedding manifold, while the Geodesic Purification Solver corrects out-of-distribution embeddings by projecting adversarial points back onto the learned manifold. Extensive evaluations on multiple datasets and adversarial attacks show that MC^2F achieves a new state-of-the-art in adversarial robustness without compromising performance on clean data, and even resulting in modest accuracy improvements. <div>
arXiv:2511.07888v1 Announce Type: new 
Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning</title>
<link>https://arxiv.org/abs/2511.07910</link>
<guid>https://arxiv.org/abs/2511.07910</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Logic Drift, Knowledge Graph Question Answering, Logits-to-Logic framework, logic consistency

Summary:
The article introduces the Logits-to-Logic framework to enhance the logic consistency of Large Language Models (LLMs) in structured knowledge reasoning tasks, addressing the challenge of Logic Drift. By targeting the logits output during generation, the framework incorporates modules for strengthening and filtering logits to correct logical defects in LLM outputs. This approach improves logic consistency significantly and achieves state-of-the-art performance on various Knowledge Graph Question Answering (KGQA) benchmarks. Unlike existing methods that rely on input-level guidance, the Logits-to-Logic framework focuses on correcting logical flaws in LLM outputs and can adapt to different tasks and knowledge graphs flexibly. This innovative framework represents a significant advancement in improving LLMs' performance in reasoning tasks involving structured knowledge. 

<br /><br />Summary: The Logits-to-Logic framework enhances logic consistency in LLMs by targeting logits output, correcting logical defects, and achieving state-of-the-art performance on KGQA benchmarks. It outperforms existing methods by focusing on correcting logical flaws in LLM outputs and adapting flexibly to different tasks and knowledge graphs. <div>
arXiv:2511.07910v1 Announce Type: new 
Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Media for Mental Health: Data, Methods, and Findings</title>
<link>https://arxiv.org/abs/2511.07914</link>
<guid>https://arxiv.org/abs/2511.07914</guid>
<content:encoded><![CDATA[
<div> Keywords: virtual communities, social media data, mental health challenges, machine learning, natural language processing
Summary: 
This chapter explores the use of social media data for studying mental health challenges such as depression, anxiety, and suicidal thoughts. It discusses the various indicators found in user disclosures, including linguistic, visual, and emotional cues. By leveraging this new source of data, medical practice can be improved, timely support can be provided, and government policies influenced. The chapter categorizes social media data usage, introduces machine learning and natural language processing methods, and suggests future research directions. Through the analysis of virtual communities and forums, valuable insights can be gathered to raise awareness of mental health issues and provide necessary assistance to those in need.<br /><br />Summary: <div>
arXiv:2511.07914v1 Announce Type: new 
Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined</title>
<link>https://arxiv.org/abs/2511.07918</link>
<guid>https://arxiv.org/abs/2511.07918</guid>
<content:encoded><![CDATA[
<div> Keywords: human speech production, theta-band synchrony, speech modes, neural mechanisms, language processing<br />
Summary:<br />
This study explores the differences in theta-band neural synchrony across various modes of speech production, including perceived, overt, whispered, and imagined speech. The analysis focuses on connectivity metrics and region-wise variations. Overt and whispered speech show broad and strong frontotemporal synchrony, reflecting motor-phonological coupling during articulation. Perceived speech exhibits dominant posterior and temporal synchrony, related to auditory perception and comprehension. Imagined speech displays internally coherent synchronization primarily in frontal and supplementary motor regions. This study highlights the distinct neural dynamics underlying different speech modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying on temporoparietal networks. Overall, the findings contribute to a better understanding of the neural mechanisms involved in language perception and imagined speech.<br /> 
Summary: <div>
arXiv:2511.07918v1 Announce Type: new 
Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker</title>
<link>https://arxiv.org/abs/2511.07969</link>
<guid>https://arxiv.org/abs/2511.07969</guid>
<content:encoded><![CDATA[
<div> specialized natural language processing, work-related tasks, multi-task progress, Unified Work Embeddings, zero-shot ranking performance
<br />
Workforce transformation in diverse industries has led to an increased demand for specialized natural language processing capabilities. However, work-related tasks present challenges such as long-tailed distributions, extreme multi-label target spaces, and limited data availability. The WorkBench evaluation suite addresses these complexities by formulating six work-related tasks as ranking problems, enabling cross-task transfer and enhancing performance through Unified Work Embeddings (UWE). UWE leverages real-world data to create task-specific bipartite graphs and employs a many-to-many InfoNCE objective to generate task-agnostic embeddings. The model shows zero-shot ranking performance on unseen target spaces, enables low-latency inference, and outperforms generalist embedding models in terms of macro-averaged MAP and RP@10. 
<br /><br />Summary: <div>
arXiv:2511.07969v1 Announce Type: new 
Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation</title>
<link>https://arxiv.org/abs/2511.07982</link>
<guid>https://arxiv.org/abs/2511.07982</guid>
<content:encoded><![CDATA[
<div> interpretation, Notices to Airmen, NOTAMs, deep parsing, NOTAM-Evolve <br />
Summary:<br />
Accurate interpretation of Notices to Airmen (NOTAMs) is crucial for aviation safety. Existing automated systems struggle to extract actionable intelligence from the condensed and cryptic language of NOTAMs. This article proposes a self-evolving framework called NOTAM-Evolve that uses a large language model to master complex NOTAM interpretation. By leveraging knowledge graph-enhanced retrieval and a closed-loop learning process, the framework achieves a significant accuracy improvement. The researchers also introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Experimental results show a 30.4% absolute accuracy improvement over the base large language model, establishing a new state of the art in structured NOTAM interpretation. <br /> <div>
arXiv:2511.07982v1 Announce Type: new 
Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?</title>
<link>https://arxiv.org/abs/2511.07989</link>
<guid>https://arxiv.org/abs/2511.07989</guid>
<content:encoded><![CDATA[
<div> BERT-like models, LLMs, text classification, South Slavic languages, zero-shot performance<br />
<br />
Summary:<br />
Recent advancements in large language models (LLMs) have led to increased focus on zero-shot and few-shot prompting in text classification tasks. Evaluating the performance of language models across various South Slavic languages, this study compared fine-tuned BERT-like models with a selection of LLMs in sentiment, topic, and genre classification tasks. Results indicate that LLMs exhibit strong zero-shot performance, often matching or surpassing BERT-like models, both in English and South Slavic languages. However, drawbacks of LLMs include less predictable outputs, slower inference, and higher computational costs. Despite the promising performance of LLMs, the practical choice for large-scale text annotation still leans towards fine-tuned BERT-like models. <div>
arXiv:2511.07989v1 Announce Type: new 
Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Correction Distillation for Structured Data Question Answering</title>
<link>https://arxiv.org/abs/2511.07998</link>
<guid>https://arxiv.org/abs/2511.07998</guid>
<content:encoded><![CDATA[
<div> Keywords: structured data QA, large language models, error correction, distillation, small-scale LLMs

Summary:
Structured data question answering (QA) is a key research area, with recent advancements in large language models (LLMs) driving progress. However, small-scale LLMs face challenges in generating accurate structured queries. To address this, a self-correction distillation (SCD) method is proposed. SCD includes an error prompt mechanism (EPM) to detect errors and provide customized error messages during inference, along with a two-stage distillation strategy to transfer capabilities from large-scale LLMs to small-scale ones. Experimental results across various benchmarks show that SCD outperforms other distillation methods on small-scale LLMs and closely approaches the performance of GPT4 on some datasets. Large-scale LLMs with EPM also achieve state-of-the-art results on most datasets.
<br /><br />Summary: <div>
arXiv:2511.07998v1 Announce Type: new 
Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing</title>
<link>https://arxiv.org/abs/2511.08017</link>
<guid>https://arxiv.org/abs/2511.08017</guid>
<content:encoded><![CDATA[
<div> learning framework, multi-character role-playing, Hyper-Contrastive Role-Adaptive, distinct traits, shared traits

Summary:
HyCoRA is a novel Hyper-Contrastive Role-Adaptive (HyCoRA) framework designed to enhance multi-character role-playing by effectively balancing the learning of distinct and shared traits. The framework consists of a Hyper-Half Low-Rank Adaptation structure, featuring a role-specific module generated by a lightweight hyper-network and a trainable role-shared module. The role-specific module captures unique persona signatures, while the role-shared module focuses on common traits. Additionally, a hyper-contrastive learning mechanism is used to differentiate between distinct personalities across roles. Experimental results on English and Chinese benchmarks demonstrate the superior performance of HyCoRA. Evaluation using GPT-4 and visual analyses further confirm the framework's ability to represent role characteristics effectively. <div>
arXiv:2511.08017v1 Announce Type: new 
Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution</title>
<link>https://arxiv.org/abs/2511.08085</link>
<guid>https://arxiv.org/abs/2511.08085</guid>
<content:encoded><![CDATA[
<div> Keywords: Bangla, authorship attribution, stop-words, deep learning, benchmark

Summary:
- The research investigates Bangla authorship attribution using a new balanced benchmark corpus BARD10 and analyzes the impact of stop-word removal on classical and deep learning models.
- BARD10, containing Bangla blog and opinion prose from ten authors, is evaluated alongside the BAAD16 corpus using classifiers like SVM, Bangla BERT, XGBoost, and MLP.
- The classical TF-IDF + SVM baseline outperformed deep learning models on both datasets, suggesting the importance of finely calibrated ML models for short texts.
- Authors in BARD10 are more influenced by stop-word pruning compared to BAAD16, indicating genre-dependent reliance on stop-word signatures.
- Error analysis revealed that transformer models may diminish or reduce high-frequency components transmitting authorial signatures. The study provides insights on the significance of Bangla stop-words, the effectiveness of ML models in short-text scenarios, and the potential of BARD10 as a benchmark for future transformer models.


Summary: <div>
arXiv:2511.08085v1 Announce Type: new 
Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling</title>
<link>https://arxiv.org/abs/2511.08109</link>
<guid>https://arxiv.org/abs/2511.08109</guid>
<content:encoded><![CDATA[
<div> Keywords: science fiction, conceptual permeability, masked language modeling, ontological categories, genre-specific restructuring

Summary:
This paper explores how science fiction challenges traditional ontological boundaries between humans, animals, and machines using computational analysis. By applying masked language modeling to science fiction and general fiction corpora, the study measures the deviation in token predictions to quantify conceptual slippage. The findings show that science fiction exhibits higher conceptual permeability, especially around machine references, leading to significant cross-category substitutions and dispersion. In contrast, human terms maintain semantic coherence, anchoring substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics, where estrangement in science fiction serves as a controlled disruption of semantic norms. The study highlights the importance of using computational tools critically in literary analysis to uncover genre-conditioned ontological assumptions, contributing to a deeper understanding of the linguistic framework of science fiction. 

<br /><br />Summary: <div>
arXiv:2511.08109v1 Announce Type: new 
Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal LLMs Do Not Compose Skills Optimally Across Modalities</title>
<link>https://arxiv.org/abs/2511.08113</link>
<guid>https://arxiv.org/abs/2511.08113</guid>
<content:encoded><![CDATA[
<div> skill composition, neural networks, Multimodal Large Language Models, evaluation tasks, cross-modality

Summary:
This paper investigates the ability of Multimodal Large Language Models (MLLMs) to compose skills across modalities by designing evaluation tasks that require the composition of two modality-dependent skills. The study evaluates different MLLMs under two settings: direct task solving and cascaded inference. The results reveal a significant skill composition gap across modalities in all evaluated MLLMs. To address this gap, the study explores two strategies - chain-of-thought prompting and a specific fine-tuning recipe. While these strategies improve model performance, they still exhibit notable skill composition gaps, indicating the need for further research in enhancing cross-modal skill composition in MLLMs. <br /><br />Summary: <div>
arXiv:2511.08113v1 Announce Type: new 
Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition</title>
<link>https://arxiv.org/abs/2511.08126</link>
<guid>https://arxiv.org/abs/2511.08126</guid>
<content:encoded><![CDATA[
<div> quantification, Large Language Models, cross-linguistic, ordering, scales <br />
Summary:<br />
This paper explores the challenges faced by Large Language Models (LLMs) in understanding quantification, a complex linguistic phenomenon that interfaces with logic, pragmatics, and numerical domains. The study focuses on three key features of human quantification that have not been thoroughly investigated in LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and biases in the human approximate number system. By comparing human and LLM performance across different tasks, the researchers highlight clear differences in the representation of quantification. The findings suggest that LLMs may not fully capture the nuances of human quantification, raising questions about their semantic and pragmatic capabilities. The study also emphasizes the importance of considering cross-linguistic variations in LLM performance to assess their robustness and stability across diverse languages.<br /> <div>
arXiv:2511.08126v1 Announce Type: new 
Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentence-Anchored Gist Compression for Long-Context LLMs</title>
<link>https://arxiv.org/abs/2511.08128</link>
<guid>https://arxiv.org/abs/2511.08128</guid>
<content:encoded><![CDATA[
<div> investigates, context compression, Large Language Models, learned compression tokens, memory reduction <br />
Summary:<br />
This work explores context compression for Large Language Models (LLMs) through the use of learned compression tokens to decrease memory and computational requirements for handling lengthy sequences. Pre-trained LLMs can be optimized to compress their context by ratios ranging from 2x to 8x with minimal impact on performance, evident in evaluations on short and long-context benchmarks. Additionally, experiments conducted on a 3-billion-parameter LLaMA model demonstrate that this approach achieves comparable results to other compression techniques while achieving higher compression ratios. <br /><br /> <div>
arXiv:2511.08128v1 Announce Type: new 
Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility</title>
<link>https://arxiv.org/abs/2511.08139</link>
<guid>https://arxiv.org/abs/2511.08139</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, positional encodings, morphological complexity, word order flexibility, downstream tasks

Summary: 
Language model architectures are often developed in English and then transferred to other languages. This study investigates the impact of this bias on languages that differ structurally from English. The focus is on positional encodings and the trade-off hypothesis, which suggests a relationship between morphological complexity and word order flexibility. Pretraining models with different positional encodings for seven diverse languages, the study examines their performance on various tasks. Contrary to expectations, the results do not show a clear connection between positional encodings and language characteristics. The study highlights the importance of considering tasks, languages, and metrics in drawing meaningful conclusions. The findings suggest that further research is needed to fully understand the implications of architectural choices in language modeling. 

Summary: <div>
arXiv:2511.08139v1 Announce Type: new 
Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction</title>
<link>https://arxiv.org/abs/2511.08143</link>
<guid>https://arxiv.org/abs/2511.08143</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Document-level Relation Extraction, Relation as a Prior, binary relation, triples extraction

Summary:
Large Language Models (LLMs) have shown great document understanding capabilities but struggle with Document-level Relation Extraction (DocRE) due to fine-grained comprehension challenges. The traditional approach of extracting entities before predicting relations introduces noise from unrelated entity pairs and struggles with predicting relations beyond predefined labels. To address these issues, a novel approach called Relation as a Prior (RelPrior) is proposed. RelPrior uses binary relations as a prior to filter out irrelevant entity pairs and predefined relations as a prior for triple extraction, avoiding misjudgments caused by strict predefined labels. Experimental results on two benchmarks show that RelPrior outperforms existing LLM-based methods, achieving state-of-the-art performance. <br /><br />Summary: <div>
arXiv:2511.08143v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?</title>
<link>https://arxiv.org/abs/2511.08145</link>
<guid>https://arxiv.org/abs/2511.08145</guid>
<content:encoded><![CDATA[
<div> instruction-tuned, in-context-prompted, Large Language Models (LLMs), Sanskrit poetry-to-prose conversion task, morphologically rich languages<br />
Summary:<br />
The study compares Large Language Models (LLMs) with smaller task-specific models on the Sanskrit poetry-to-prose conversion task. The task involves complex reasoning due to the unique characteristics of Sanskrit verse. Through domain-specific fine-tuning, the ByT5-Sanskrit model outperforms LLM approaches. Human evaluation supports this finding. In-context learning templates based on Paninian grammar offer an alternative to fine-tuning when specific corpora are lacking. The task-specific Seq2Seq model shows robust generalization in out-of-domain evaluations.<br /> <div>
arXiv:2511.08145v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?</title>
<link>https://arxiv.org/abs/2511.08199</link>
<guid>https://arxiv.org/abs/2511.08199</guid>
<content:encoded><![CDATA[
<div> Keywords: BabyLM corpus, syntactic properties, age-groups, CHILDES, curriculum learning

Summary: 
The study investigates the syntactic properties of the BabyLM corpus and different age-groups within the CHILDES dataset. It is observed that CHILDES does not show significant syntactic variation based on age. However, the study demonstrates that understanding the syntactic structure of the training data can aid in interpreting model performance in linguistic tasks. The research delves into curriculum learning, exploring developmental and other cognitively inspired curriculum strategies. While some curricula show improvement in reading tasks, the most substantial performance enhancement is achieved by utilizing a subset of syntactically classifiable data rather than the entire noisy corpus. The findings suggest the importance of considering syntactic knowledge when designing curriculum approaches for language learning tasks. <div>
arXiv:2511.08199v1 Announce Type: new 
Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction</title>
<link>https://arxiv.org/abs/2511.08204</link>
<guid>https://arxiv.org/abs/2511.08204</guid>
<content:encoded><![CDATA[
<div> extracting, knowledge, astronomy, transformer-based system, SciBERT

Summary: 
The paper discusses the need for automation in extracting key entities and contextual information from expanding scientific literature in astronomy. It presents a system based on an encoder model for extracting knowledge from astronomy articles. The system aims to classify telescope references, detect auxiliary semantic attributes, and recognize instrument mentions from textual content. The system is built upon the SciBERT model and fine-tuned for astronomy corpora classification. The fine-tuning process involves stochastically sampling segments from the training data and using majority voting over test segments at inference time. Despite its simplicity and low-cost implementation, the system significantly outperforms the open-weight GPT baseline in terms of performance. <div>
arXiv:2511.08204v1 Announce Type: new 
Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback</title>
<link>https://arxiv.org/abs/2511.08225</link>
<guid>https://arxiv.org/abs/2511.08225</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmarking, large language models, bias detection, formative feedback, gender substitution

Summary: 
In this article, a benchmarking framework is proposed to detect bias in large language models (LLMs) used for educational purposes, particularly in providing formative feedback. The study utilized 600 student essays to analyze the response of six representative LLMs to gender substitutions in different contexts. The results indicated that there were asymmetric semantic responses to gender manipulations, with male-female substitutions inducing larger shifts than female-male. Only certain models showed sensitivity to explicit gender cues. Qualitative analysis revealed linguistic differences in feedback based on gender cues, such as more autonomy-supportive feedback under male prompts. The findings suggest persistent gender biases in the feedback provided by LLMs, highlighting the need for fairness auditing in pedagogical GenAI. The article proposes reporting standards for counterfactual evaluation in learning analytics and provides practical guidance for designing prompts to ensure equitable feedback.<br /><br />Summary: <div>
arXiv:2511.08225v1 Announce Type: new 
Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context</title>
<link>https://arxiv.org/abs/2511.08230</link>
<guid>https://arxiv.org/abs/2511.08230</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, Mandarin, speech-to-speech benchmarks, VocalBench-zh, evaluation experiment

Summary:
The article introduces the development of multi-modal large language models (LLMs) that facilitate speech interactions, focusing on Mandarin language support. However, the lack of comprehensive speech-to-speech benchmarks in Mandarin poses challenges for developers and users in evaluating and comparing models. To address this issue, the authors propose VocalBench-zh, an evaluation suite specifically designed for Mandarin contexts. This suite includes 10 subsets with over 10,000 high-quality instances covering 12 user-oriented characters. The evaluation experiment conducted on 14 mainstream models reveals common challenges and the need for advancements in next-generation speech interactive systems. The evaluation codes and datasets for VocalBench-zh are accessible on GitHub for researchers and developers interested in improving speech technology. 

<br /><br />Summary: <div>
arXiv:2511.08230v1 Announce Type: new 
Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG</title>
<link>https://arxiv.org/abs/2511.08245</link>
<guid>https://arxiv.org/abs/2511.08245</guid>
<content:encoded><![CDATA[
<div> Keywords: Error Correction, NL-to-SQL, Prompt Tuning, NLIDBs, RAG

Summary:
This paper presents a novel Error Correction through Prompt Tuning approach for NL-to-SQL translation using the latest advancements in large language models (LLMs) and Retrieval Augmented Generation (RAG). The framework integrates error diagnosis, cause identification, fixing instructions, and correction application inspired by medical diagnostic processes. It incorporates fine-tuning and RAG to enhance accuracy and transparency by utilizing external knowledge bases. The evolution of NLIDBs from rule-based systems to neural network-driven approaches is explored. Through extensive experiments, the framework shows a notable 12 percent accuracy improvement over existing methods, indicating its potential to transform data access and management in contemporary data-driven environments. Overall, this research addresses the critical need for efficient and accurate translation of natural language queries into SQL expressions, offering promising advancements in the field of NL-to-SQL. 

<br /><br />Summary: <div>
arXiv:2511.08245v1 Announce Type: new 
Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech</title>
<link>https://arxiv.org/abs/2511.08247</link>
<guid>https://arxiv.org/abs/2511.08247</guid>
<content:encoded><![CDATA[
<div> Dataset, ParliaBench, language models, political authenticity, evaluation framework <br />
<br />
Summary: <br />
The article introduces ParliaBench, a benchmark for parliamentary speech generation, which addresses challenges faced by large language models in producing authentic and ideologically consistent parliamentary speeches. A dataset of UK Parliament speeches was used to train models, and an evaluation framework was developed to assess linguistic quality, semantic coherence, and political authenticity. Two novel metrics, Political Spectrum Alignment and Party Alignment, were proposed to measure ideological positioning. Five language models were fine-tuned and evaluated, with results showing significant improvements in most metrics when compared to baseline models. The novel metrics demonstrated strong discriminative power in assessing political dimensions. <div>
arXiv:2511.08247v1 Announce Type: new 
Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments</title>
<link>https://arxiv.org/abs/2511.08298</link>
<guid>https://arxiv.org/abs/2511.08298</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Large Language Models, scientific tables, hierarchical structure, prompt engineering, performance evaluation

Summary:
This work investigates the ability of Vision Large Language Models (VLLMs) to understand the hierarchical structure of tables in scientific articles. The study uses the PubTables-1M dataset to create a benchmark collection of Complex Hierarchical Tables (CHiTab). Various prompt engineering strategies are employed to assess the models' comprehension capabilities. State-of-the-art VLLMs are evaluated on the CHiTab benchmark dataset, both in their off-the-shelf versions and after fine-tuning on the task. The study also compares the performance of humans on the task with that of the evaluated VLLMs. Results suggest that generic VLLMs, not specifically designed for table structure understanding, can successfully infer hierarchical table structures. The findings offer insights into the potential and limitations of VLLMs in processing complex tables and provide guidance for future research on incorporating structured data understanding into general-purpose VLLMs.<br /><br />Summary: <div>
arXiv:2511.08298v1 Announce Type: new 
Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates</title>
<link>https://arxiv.org/abs/2511.08317</link>
<guid>https://arxiv.org/abs/2511.08317</guid>
<content:encoded><![CDATA[
<div> framework, ReViewGraph, heterogeneous graph reasoning, LLM simulated debates, argumentative dynamics

Summary:
ReViewGraph is introduced as a new framework to enhance paper review methods by incorporating heterogeneous graph reasoning over LLM-simulated reviewer-author debates. The framework simulates reviewer-author exchanges using LLM-based multi-agent collaboration and extracts diverse opinion relations as typed edges in a structured debate graph. By utilizing graph neural networks, ReViewGraph captures detailed argumentative dynamics, enabling more informed review decisions. Experimental results on three datasets show that ReViewGraph outperforms strong baselines by an average relative improvement of 15.73%. This highlights the importance of modeling intricate reviewer-author debate structures in improving the paper review process. <div>
arXiv:2511.08317v1 Announce Type: new 
Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multi-Agent Response Refinement in Conversational Systems</title>
<link>https://arxiv.org/abs/2511.08319</link>
<guid>https://arxiv.org/abs/2511.08319</guid>
<content:encoded><![CDATA[
<div> refining responses, large language models, multi-agent framework, conversational quality, dynamic communication strategy  
Summary:  
- Large Language Models (LLMs) have been successful in generating human-like responses but can struggle with personalization and specific knowledge.
- Responding to this limitation, a multi-agent framework is proposed to refine responses through aspects such as factuality, personalization, and coherence.
- Each agent is assigned a specific role for one of these aspects, with their feedback merged to enhance the overall response.
- A dynamic communication strategy is introduced to improve collaboration among agents by adaptively selecting and coordinating based on query requirements.
- Validation on conversational datasets showed significant performance improvement over relevant baselines, especially in tasks involving knowledge or user persona.  
<br /><br />Summary: <div>
arXiv:2511.08319v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress</title>
<link>https://arxiv.org/abs/2511.08325</link>
<guid>https://arxiv.org/abs/2511.08325</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, decision-making tasks, process reward models, Generalized Advantage Estimation

Summary: 
The article introduces a new approach for improving decision-making in large language models (LLMs) in agent tasks. It proposes the use of process reward models (PRMs) called AgentPRM to evaluate decisions based on progress towards the goal rather than correctness. This new approach aims to better capture the interdependence between sequential decisions and their contribution to the final goal, leading to improved progress tracking and exploration-exploitation balance. To train AgentPRM efficiently, a Temporal Difference-based estimation method combined with Generalized Advantage Estimation (GAE) is employed. Experimental results show that AgentPRM is significantly more compute-efficient than baselines, with over 8 times improvement. The method also demonstrates robust performance when scaling up test-time compute. The study includes detailed analyses and insights on applying AgentPRM to reinforcement learning of LLM agents. 

<br /><br />Summary: <div>
arXiv:2511.08325v1 Announce Type: new 
Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2511.08364</link>
<guid>https://arxiv.org/abs/2511.08364</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-hop question answering, Chain of Thought, Knowledge Graphs, Outcome Reward Models, Implicit Process Reward Model

Summary:
In this article, the authors introduce a novel approach called Dual Implicit Process Reward Model (DPRM) to enhance multi-hop question answering tasks. The DPRM consists of two implicit Process Reward Models (PRMs) - KG-PRM and CoT-PRM, which derive step-level rewards without explicit annotations. KG-PRM utilizes preference pairs to learn structural constraints from Knowledge Graphs (KGs), while CoT-PRM focuses on multi-step reasoning guided by Chain of Thought. The DPRM integrates a consistency constraint to ensure coherence between CoT and KG reasoning paths. Theoretical analysis demonstrates the derivation of process rewards in the model. Experimental results exhibit a significant performance improvement compared to 13 baseline methods, with up to a 16.6% increase in Hit@1 metric on multiple datasets. <div>
arXiv:2511.08364v1 Announce Type: new 
Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures</title>
<link>https://arxiv.org/abs/2511.08372</link>
<guid>https://arxiv.org/abs/2511.08372</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic articulatory model, speech gestures, gesture score, neurobiology, speech production

Summary: 
The paper introduces the dynamic articulatory model DYNARTmo, which simulates speech production through continuous articulator movements based on speech gestures and a corresponding gesture score. The model is designed to mimic the hierarchical control of speech production in a neurobiologically inspired computational framework. The structure of the gesture inventory and the coordination of gestures within the gesture score are outlined, demonstrating how they translate into continuous articulator trajectories that control the DYNARTmo vocal tract model. This implementation provides a comprehensive approach to understanding and modeling speech production, bridging the gap between linguistic representation and articulatory-acoustic realization. <div>
arXiv:2511.08372v1 Announce Type: new 
Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurkEmbed: Turkish Embedding Model on NLI &amp; STS Tasks</title>
<link>https://arxiv.org/abs/2511.08376</link>
<guid>https://arxiv.org/abs/2511.08376</guid>
<content:encoded><![CDATA[
<div> Keywords: TurkEmbed, Turkish language embedding model, Natural Language Inference, Semantic Textual Similarity, NLP ecosystem

Summary: 
TurkEmbed is a novel Turkish language embedding model designed to excel in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Unlike existing models that rely on machine-translated datasets, TurkEmbed utilizes diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach allows the model to adapt to various resource-constrained environments and offer faster encoding capabilities. Evaluation on the Turkish STS-b-TR dataset shows significant improvements in semantic similarity tasks, surpassing the current state-of-the-art model Emrecan on All-NLI-TR and STS-b-TR benchmarks by 1-4%. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a deeper understanding of language and enabling advancements in downstream applications. 

<br /><br />Summary: <div>
arXiv:2511.08376v1 Announce Type: new 
Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints</title>
<link>https://arxiv.org/abs/2511.08392</link>
<guid>https://arxiv.org/abs/2511.08392</guid>
<content:encoded><![CDATA[
<div> Proof-Carrying Reasoning, Large Language Models, single-step inferences, logical coherence, validation
Summary:
Proof-Carrying Reasoning with Large Language Models (PCRLLM) is a framework proposed to improve the logical coherence of LLMs by constraining reasoning to single-step inferences. This approach ensures that each output explicitly specifies premises, rules, and conclusions, allowing for verification against a target logic and addressing trustworthiness concerns in black-box settings. PCRLLM also enables systematic multi-LLM collaboration, facilitating the comparison and integration of intermediate reasoning steps under formal rules. Additionally, a benchmark schema for generating large-scale step-level reasoning data is introduced, combining natural language expressiveness with formal rigor. Through these mechanisms, PCRLLM aims to enhance the reliability and transparency of reasoning processes in LLMs. 
<br /><br />Summary: <div>
arXiv:2511.08392v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction Dynamics as a Reward Signal for LLMs</title>
<link>https://arxiv.org/abs/2511.08394</link>
<guid>https://arxiv.org/abs/2511.08394</guid>
<content:encoded><![CDATA[
<div> reward, Large Language Models, dialogue embedding, conversational geometry, agent collaboration

Summary:
- The paper introduces TRACE, a reward signal based on conversational geometry, which considers the dynamics of a dialogue's embedding trajectory as a source of signal for aligning Large Language Models (LLMs) in multi-turn conversations.
- A reward model trained solely on structural signals achieves a pairwise accuracy of 68.20%, comparable to a powerful LLM baseline analyzing the full transcript.
- The hybrid model combining interaction dynamics with textual analysis achieves the highest performance at 80.17%, highlighting the complementary nature of these approaches.
- The study shows that how an agent communicates is as crucial for success as what it says in interactive settings.
- This approach offers a privacy-preserving framework for aligning agents and serves as a diagnostic tool to understand the unique interaction patterns that drive successful collaboration. 

<br /><br />Summary: <div>
arXiv:2511.08394v1 Announce Type: new 
Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?</title>
<link>https://arxiv.org/abs/2511.08455</link>
<guid>https://arxiv.org/abs/2511.08455</guid>
<content:encoded><![CDATA[
arXiv:2511.08455v1 Announce Type: new 
Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation</title>
<link>https://arxiv.org/abs/2511.08500</link>
<guid>https://arxiv.org/abs/2511.08500</guid>
<content:encoded><![CDATA[
arXiv:2511.08500v1 Announce Type: new 
Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured RAG for Answering Aggregative Questions</title>
<link>https://arxiv.org/abs/2511.08505</link>
<guid>https://arxiv.org/abs/2511.08505</guid>
<content:encoded><![CDATA[
arXiv:2511.08505v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research</title>
<link>https://arxiv.org/abs/2511.08507</link>
<guid>https://arxiv.org/abs/2511.08507</guid>
<content:encoded><![CDATA[
arXiv:2511.08507v1 Announce Type: new 
Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaResearch: Accelerating New Algorithm Discovery with Language Models</title>
<link>https://arxiv.org/abs/2511.08522</link>
<guid>https://arxiv.org/abs/2511.08522</guid>
<content:encoded><![CDATA[
arXiv:2511.08522v1 Announce Type: new 
Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating CoT Monitorability in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.08525</link>
<guid>https://arxiv.org/abs/2511.08525</guid>
<content:encoded><![CDATA[
arXiv:2511.08525v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL</title>
<link>https://arxiv.org/abs/2511.08537</link>
<guid>https://arxiv.org/abs/2511.08537</guid>
<content:encoded><![CDATA[
arXiv:2511.08537v1 Announce Type: new 
Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models</title>
<link>https://arxiv.org/abs/2511.08565</link>
<guid>https://arxiv.org/abs/2511.08565</guid>
<content:encoded><![CDATA[
arXiv:2511.08565v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.08577</link>
<guid>https://arxiv.org/abs/2511.08577</guid>
<content:encoded><![CDATA[
arXiv:2511.08577v1 Announce Type: new 
Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Language Models to Explain Their Own Computations</title>
<link>https://arxiv.org/abs/2511.08579</link>
<guid>https://arxiv.org/abs/2511.08579</guid>
<content:encoded><![CDATA[
arXiv:2511.08579v1 Announce Type: new 
Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation</title>
<link>https://arxiv.org/abs/2511.06254</link>
<guid>https://arxiv.org/abs/2511.06254</guid>
<content:encoded><![CDATA[
arXiv:2511.06254v1 Announce Type: cross 
Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network and Systems Performance Characterization of MCP-Enabled LLM Agents</title>
<link>https://arxiv.org/abs/2511.07426</link>
<guid>https://arxiv.org/abs/2511.07426</guid>
<content:encoded><![CDATA[
arXiv:2511.07426v1 Announce Type: cross 
Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Polite Liar: Epistemic Pathology in Language Models</title>
<link>https://arxiv.org/abs/2511.07477</link>
<guid>https://arxiv.org/abs/2511.07477</guid>
<content:encoded><![CDATA[
arXiv:2511.07477v1 Announce Type: cross 
Abstract: Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an "epistemic alignment" principle: reward justified confidence over perceived fluency.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits</title>
<link>https://arxiv.org/abs/2511.07482</link>
<guid>https://arxiv.org/abs/2511.07482</guid>
<content:encoded><![CDATA[
arXiv:2511.07482v1 Announce Type: cross 
Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Impact of CU: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2511.07491</link>
<guid>https://arxiv.org/abs/2511.07491</guid>
<content:encoded><![CDATA[
arXiv:2511.07491v1 Announce Type: cross 
Abstract: Community Unionism has served as a pivotal concept in debates on trade union renewal since the early 2000s, yet its theoretical coherence and political significance remain unresolved. This article investigates why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature. Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies -- I examine how CU functions as both an empirical descriptor and a normative ideal. The analysis reveals CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics. This tension suggests CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</title>
<link>https://arxiv.org/abs/2511.07577</link>
<guid>https://arxiv.org/abs/2511.07577</guid>
<content:encoded><![CDATA[
arXiv:2511.07577v1 Announce Type: cross 
Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models</title>
<link>https://arxiv.org/abs/2511.07581</link>
<guid>https://arxiv.org/abs/2511.07581</guid>
<content:encoded><![CDATA[
arXiv:2511.07581v1 Announce Type: cross 
Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows</title>
<link>https://arxiv.org/abs/2511.07585</link>
<guid>https://arxiv.org/abs/2511.07585</guid>
<content:encoded><![CDATA[
arXiv:2511.07585v1 Announce Type: cross 
Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces</title>
<link>https://arxiv.org/abs/2511.07587</link>
<guid>https://arxiv.org/abs/2511.07587</guid>
<content:encoded><![CDATA[
arXiv:2511.07587v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</title>
<link>https://arxiv.org/abs/2511.07685</link>
<guid>https://arxiv.org/abs/2511.07685</guid>
<content:encoded><![CDATA[
arXiv:2511.07685v1 Announce Type: cross 
Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViPRA: Video Prediction for Robot Actions</title>
<link>https://arxiv.org/abs/2511.07732</link>
<guid>https://arxiv.org/abs/2511.07732</guid>
<content:encoded><![CDATA[
arXiv:2511.07732v1 Announce Type: cross 
Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title>
<link>https://arxiv.org/abs/2511.07772</link>
<guid>https://arxiv.org/abs/2511.07772</guid>
<content:encoded><![CDATA[
arXiv:2511.07772v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.07790</link>
<guid>https://arxiv.org/abs/2511.07790</guid>
<content:encoded><![CDATA[
arXiv:2511.07790v1 Announce Type: cross 
Abstract: Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost</title>
<link>https://arxiv.org/abs/2511.07865</link>
<guid>https://arxiv.org/abs/2511.07865</guid>
<content:encoded><![CDATA[
arXiv:2511.07865v1 Announce Type: cross 
Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation</title>
<link>https://arxiv.org/abs/2511.07876</link>
<guid>https://arxiv.org/abs/2511.07876</guid>
<content:encoded><![CDATA[
arXiv:2511.07876v1 Announce Type: cross 
Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</title>
<link>https://arxiv.org/abs/2511.07885</link>
<guid>https://arxiv.org/abs/2511.07885</guid>
<content:encoded><![CDATA[
arXiv:2511.07885v1 Announce Type: cross 
Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2511.07896</link>
<guid>https://arxiv.org/abs/2511.07896</guid>
<content:encoded><![CDATA[
arXiv:2511.07896v1 Announce Type: cross 
Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeechJudge: Towards Human-Level Judgment for Speech Naturalness</title>
<link>https://arxiv.org/abs/2511.07931</link>
<guid>https://arxiv.org/abs/2511.07931</guid>
<content:encoded><![CDATA[
arXiv:2511.07931v1 Announce Type: cross 
Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction</title>
<link>https://arxiv.org/abs/2511.07943</link>
<guid>https://arxiv.org/abs/2511.07943</guid>
<content:encoded><![CDATA[
arXiv:2511.07943v1 Announce Type: cross 
Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives</title>
<link>https://arxiv.org/abs/2511.08029</link>
<guid>https://arxiv.org/abs/2511.08029</guid>
<content:encoded><![CDATA[
arXiv:2511.08029v1 Announce Type: cross 
Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaAct: Large Language Model Reasoning with Dynamic Action Spaces</title>
<link>https://arxiv.org/abs/2511.08043</link>
<guid>https://arxiv.org/abs/2511.08043</guid>
<content:encoded><![CDATA[
arXiv:2511.08043v1 Announce Type: cross 
Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging</title>
<link>https://arxiv.org/abs/2511.08052</link>
<guid>https://arxiv.org/abs/2511.08052</guid>
<content:encoded><![CDATA[
arXiv:2511.08052v1 Announce Type: cross 
Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression</title>
<link>https://arxiv.org/abs/2511.08066</link>
<guid>https://arxiv.org/abs/2511.08066</guid>
<content:encoded><![CDATA[
arXiv:2511.08066v1 Announce Type: cross 
Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR</title>
<link>https://arxiv.org/abs/2511.08092</link>
<guid>https://arxiv.org/abs/2511.08092</guid>
<content:encoded><![CDATA[
arXiv:2511.08092v1 Announce Type: cross 
Abstract: We challenge the conventional view of neural network pruning as solely a compression technique, demonstrating that one-shot magnitude pruning serves as a powerful implicit regularizer for ASR. Using Whisper-small, we combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning. This reveals architectural asymmetries: decoder FFNs are pruning-fragile, whereas decoder self-attention and the last encoder layers contain redundancy that, when removed, improves generalization. Without fine-tuning, pruning 50% of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; pruning the last four encoder layers at 50% instead yields a 1.72% absolute (14.8% relative) improvement. Gains persisted on Common Voice and TED-LIUM datasets. Beyond regularization benefits, our sensitivity-aware approach enables more aggressive one-shot compression. At 40% sparsity, where established global pruning approaches catastrophically fail, our method preserves near-baseline accuracy. This positions pruning as a first-class architectural design tool: knowing where to prune is as important as how much to prune.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantizing Whisper-small: How design choices affect ASR performance</title>
<link>https://arxiv.org/abs/2511.08093</link>
<guid>https://arxiv.org/abs/2511.08093</guid>
<content:encoded><![CDATA[
arXiv:2511.08093v1 Announce Type: cross 
Abstract: Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization performed worse, likely due to Whisper's Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision</title>
<link>https://arxiv.org/abs/2511.08098</link>
<guid>https://arxiv.org/abs/2511.08098</guid>
<content:encoded><![CDATA[
arXiv:2511.08098v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning</title>
<link>https://arxiv.org/abs/2511.08151</link>
<guid>https://arxiv.org/abs/2511.08151</guid>
<content:encoded><![CDATA[
arXiv:2511.08151v1 Announce Type: cross 
Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents</title>
<link>https://arxiv.org/abs/2511.08242</link>
<guid>https://arxiv.org/abs/2511.08242</guid>
<content:encoded><![CDATA[
arXiv:2511.08242v1 Announce Type: cross 
Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs</title>
<link>https://arxiv.org/abs/2511.08274</link>
<guid>https://arxiv.org/abs/2511.08274</guid>
<content:encoded><![CDATA[
arXiv:2511.08274v1 Announce Type: cross 
Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Selective State Space Artificial Intelligence</title>
<link>https://arxiv.org/abs/2511.08349</link>
<guid>https://arxiv.org/abs/2511.08349</guid>
<content:encoded><![CDATA[
arXiv:2511.08349v1 Announce Type: cross 
Abstract: Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization.
  In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning.
  We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Model and Layer Fusion for Speech Foundation Models</title>
<link>https://arxiv.org/abs/2511.08389</link>
<guid>https://arxiv.org/abs/2511.08389</guid>
<content:encoded><![CDATA[
arXiv:2511.08389v1 Announce Type: cross 
Abstract: Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence in Qualitative Research Methods: Between Hype and Risks?</title>
<link>https://arxiv.org/abs/2511.08461</link>
<guid>https://arxiv.org/abs/2511.08461</guid>
<content:encoded><![CDATA[
arXiv:2511.08461v1 Announce Type: cross 
Abstract: As Artificial Intelligence (AI) is increasingly promoted and used in qualitative research, it also raises profound methodological issues. This position paper critically interrogates the role of generative AI (genAI) in the context of qualitative coding methodologies. Despite widespread hype and claims of efficiency, we propose that genAI is not methodologically valid within qualitative inquiries, and its use risks undermining the robustness and trustworthiness of qualitative research. The lack of meaningful documentation, commercial opacity, and the inherent tendencies of genAI systems to produce incorrect outputs all contribute to weakening methodological rigor. Overall, the balance between risk and benefits does not support the use of genAI in qualitative research, and our position paper cautions researchers to put sound methodology before technological novelty.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity</title>
<link>https://arxiv.org/abs/2511.08487</link>
<guid>https://arxiv.org/abs/2511.08487</guid>
<content:encoded><![CDATA[
arXiv:2511.08487v1 Announce Type: cross 
Abstract: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. We address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a "Complexity Paradox" emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form QA</title>
<link>https://arxiv.org/abs/2408.09235</link>
<guid>https://arxiv.org/abs/2408.09235</guid>
<content:encoded><![CDATA[
arXiv:2408.09235v3 Announce Type: replace 
Abstract: The emergence of Large Language Models (LLMs) as chat assistants capable of generating human-like conversations has amplified the need for robust evaluation methods, particularly for open-ended tasks. Conventional metrics such as EM and F1, while useful, are inadequate for capturing the full semantics and contextual depth of such generative outputs. We propose a reference-guided verdict method that automates the evaluation process by leveraging multiple LLMs as judges. Through experiments on free-form question-answering tasks, we demonstrate that combining multiple models improves the reliability and accuracy of evaluations, especially in tasks where a single model may struggle. The results indicate a strong correlation with human evaluations, establishing the proposed method as a reliable alternative to traditional metrics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering</title>
<link>https://arxiv.org/abs/2409.04181</link>
<guid>https://arxiv.org/abs/2409.04181</guid>
<content:encoded><![CDATA[
arXiv:2409.04181v3 Announce Type: replace 
Abstract: Advancements in natural language processing have revolutionized the way we can interact with digital information systems, such as databases, making them more accessible. However, challenges persist, especially when accuracy is critical, as in the biomedical domain. A key issue is the hallucination problem, where models generate information unsupported by the underlying data, potentially leading to dangerous misinformation. This paper presents a novel approach designed to bridge this gap by combining Large Language Models (LLM) and Knowledge Graphs (KG) to improve the accuracy and reliability of question-answering systems, on the example of a biomedical KG. Built on the LangChain framework, our method incorporates a query checker that ensures the syntactical and semantic validity of LLM-generated queries, which are then used to extract information from a Knowledge Graph, substantially reducing errors like hallucinations. We evaluated the overall performance using a new benchmark dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other models in generating accurate queries, open-source models like llama3:70b show promise with appropriate prompt engineering. To make this approach accessible, a user-friendly web-based interface has been developed, allowing users to input natural language queries, view generated and corrected Cypher queries, and verify the resulting paths for accuracy. Overall, this hybrid approach effectively addresses common issues such as data gaps and hallucinations, offering a reliable and intuitive solution for question answering systems. The source code for generating the results of this paper and for the user-interface can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selection of LLM Fine-Tuning Data based on Orthogonal Rules</title>
<link>https://arxiv.org/abs/2410.04715</link>
<guid>https://arxiv.org/abs/2410.04715</guid>
<content:encoded><![CDATA[
arXiv:2410.04715v3 Announce Type: replace 
Abstract: High-quality training data is critical to the performance of large language models (LLMs). Recent work has explored using LLMs to rate and select data based on a small set of human-designed criteria (rules), but these approaches often rely heavily on heuristics, lack principled metrics for rule evaluation, and generalize poorly to new tasks. We propose a novel rule-based data selection framework that introduces a metric based on the orthogonality of rule score vectors to evaluate and select complementary rules. Our automated pipeline first uses LLMs to generate diverse rules covering multiple aspects of data quality, then rates samples according to these rules and applies the determinantal point process (DPP) to select the most independent rules. These rules are then used to score the full dataset, and high-scoring samples are selected for downstream tasks such as LLM fine-tuning. We evaluate our framework in two experiment setups: (1) alignment with ground-truth ratings and (2) performance of LLMs fine-tuned on the selected data. Experiments across IMDB, Medical, Math, and Code domains demonstrate that our DPP-based rule selection consistently improves both rating accuracy and downstream model performance over strong baselines.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use</title>
<link>https://arxiv.org/abs/2410.16400</link>
<guid>https://arxiv.org/abs/2410.16400</guid>
<content:encoded><![CDATA[
arXiv:2410.16400v2 Announce Type: replace 
Abstract: While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs' inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models</title>
<link>https://arxiv.org/abs/2411.05036</link>
<guid>https://arxiv.org/abs/2411.05036</guid>
<content:encoded><![CDATA[
arXiv:2411.05036v2 Announce Type: replace 
Abstract: Word embeddings and language models have transformed natural language processing (NLP) by facilitating the representation of linguistic elements in continuous vector spaces. This review visits foundational concepts such as the distributional hypothesis and contextual similarity, tracing the evolution from sparse representations like one-hot encoding to dense embeddings including Word2Vec, GloVe, and fastText. We examine both static and contextualized embeddings, underscoring advancements in models such as ELMo, BERT, and GPT and their adaptations for cross-lingual and personalized applications. The discussion extends to sentence and document embeddings, covering aggregation methods and generative topic models, along with the application of embeddings in multimodal domains, including vision, robotics, and cognitive science. Advanced topics such as model compression, interpretability, numerical encoding, and bias mitigation are analyzed, addressing both technical challenges and ethical implications. Additionally, we identify future research directions, emphasizing the need for scalable training techniques, enhanced interpretability, and robust grounding in non-textual modalities. By synthesizing current methodologies and emerging trends, this survey offers researchers and practitioners an in-depth resource to push the boundaries of embedding-based language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction</title>
<link>https://arxiv.org/abs/2502.10388</link>
<guid>https://arxiv.org/abs/2502.10388</guid>
<content:encoded><![CDATA[
arXiv:2502.10388v2 Announce Type: replace 
Abstract: Recent progress in large language models (LLMs) has enabled the automated processing of lengthy documents even without supervised training on a task-specific dataset. Yet, their zero-shot performance in complex tasks as opposed to straightforward information extraction tasks remains suboptimal. One feasible approach for tasks with lengthy, complex input is to first summarize the document and then apply supervised fine-tuning to the summary. However, the summarization process inevitably results in some loss of information. In this study we present a method for processing the summaries of long documents aimed to capture different important aspects of the original document. We hypothesize that LLM summaries generated with different aspect-oriented prompts contain different information signals, and we propose methods to measure these differences. We introduce approaches to effectively integrate signals from these different summaries for supervised training of transformer models. We validate our hypotheses on a high-impact task -- 30-day readmission prediction from a psychiatric discharge -- using real-world data from four hospitals, and show that our proposed method increases the prediction performance for the complex task of predicting patient outcome.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thus Spake Long-Context Large Language Model</title>
<link>https://arxiv.org/abs/2502.17129</link>
<guid>https://arxiv.org/abs/2502.17129</guid>
<content:encoded><![CDATA[
arXiv:2502.17129v2 Announce Type: replace 
Abstract: Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs), giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, research on long-context LLMs has expanded beyond length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.
  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend their mortality. In this survey, we will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to research on long-context LLMs.
  Video: https://www.bilibili.com/video/BV11h9AYoEYj.
  Github: https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Figurative Archive: an open dataset and web-based application for the study of metaphor</title>
<link>https://arxiv.org/abs/2503.00444</link>
<guid>https://arxiv.org/abs/2503.00444</guid>
<content:encoded><![CDATA[
arXiv:2503.00444v3 Announce Type: replace 
Abstract: Research on metaphor has steadily increased over the last decades, as this phenomenon opens a window into a range of linguistic and cognitive processes. At the same time, the demand for rigorously constructed and extensively normed experimental materials increased as well. Here, we present the Figurative Archive, an open database of 996 metaphors in Italian enriched with rating and corpus-based measures (from familiarity to semantic distance and preferred interpretations), derived by collecting stimuli used across 11 studies. It includes both everyday and literary metaphors, varying in structure and semantic domains, and is validated based on correlations between familiarity and other measures. The Archive has several aspects of novelty: it is increased in size compared to previous resources; it offers a measure of metaphor inclusiveness, to comply with recommendations for non-discriminatory language use; it is displayed in a web-based interface, with features for a customized consultation. We provide guidelines for using the Archive to source materials for studies investigating metaphor processing and relationships between metaphor features in humans and computational models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLEV: LLM-Based Evaluation Through Lightweight Efficient Voting for Free-Form Question-Answering</title>
<link>https://arxiv.org/abs/2503.08542</link>
<guid>https://arxiv.org/abs/2503.08542</guid>
<content:encoded><![CDATA[
arXiv:2503.08542v2 Announce Type: replace 
Abstract: Evaluating free-form Question Answering (QA) remains a challenge due to its diverse and open-ended nature. Traditional automatic metrics fail to capture semantic equivalence or accommodate the variability of open-ended responses. Leveraging Large Language Models (LLMs) as evaluators offers a promising alternative due to their strong language understanding and instruction-following capabilities. We propose Consensus via Lightweight Efficient Voting (CLEV), which employs two primary LLMs as judges and invokes a third judge only in cases of disagreement. This approach prioritizes evaluation reliability while reducing unnecessary computational demands. Through experiments, including human evaluation, we demonstrate CLEV's ability to provide consistent, scalable, and resource-efficient assessments, establishing it as a robust framework for evaluating LLMs on free-form QA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
arXiv:2503.20797v3 Announce Type: replace 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENCORE: Entropy-guided Reward Composition for Multi-head Safety Reward Models</title>
<link>https://arxiv.org/abs/2503.20995</link>
<guid>https://arxiv.org/abs/2503.20995</guid>
<content:encoded><![CDATA[
arXiv:2503.20995v2 Announce Type: replace 
Abstract: The safety alignment of large language models (LLMs) often relies on reinforcement learning from human feedback (RLHF), which requires human annotations to construct preference datasets. Given the challenge of assigning overall quality scores to data, recent works increasingly adopt fine-grained ratings based on multiple safety rules. In this paper, we discover a robust phenomenon: Rules with higher rating entropy tend to have lower accuracy in distinguishing human-preferred responses. Exploiting this insight, we propose ENCORE, a simple entropy-guided method to compose multi-head rewards by penalizing rules with high rating entropy. Theoretically, we show that such rules yield negligible weights under the Bradley-Terry loss during weight optimization, naturally justifying their penalization. Empirically, ENCORE consistently outperforms strong baselines, including random and uniform weighting, single-head Bradley-Terry, and LLM-as-a-judge, etc. on RewardBench safety tasks. Our method is completely training-free, generally applicable across datasets, and retains interpretability, making it a practical and effective approach for multi-attribute reward modeling.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment</title>
<link>https://arxiv.org/abs/2503.23777</link>
<guid>https://arxiv.org/abs/2503.23777</guid>
<content:encoded><![CDATA[
arXiv:2503.23777v2 Announce Type: replace 
Abstract: Naive joint training of large language models (LLMs) for multilingual preference alignment can suffer from negative interference. This is a known issue in multilingual training, where conflicting objectives degrade overall performance. However, the impact of this phenomenon in the context of multilingual preference alignment remains largely underexplored. To address this issue, we propose CONGRAD, a scalable and effective filtering method that selects high-quality preference samples with minimal gradient conflicts across languages. Our method leverages gradient surgery to retain samples aligned with an aggregated multilingual update direction. Additionally, we incorporate a sublinear gradient compression strategy that reduces memory overhead during gradient accumulation. We integrate CONGRAD into self-rewarding framework and evaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that CONGRAD consistently outperforms strong baselines in both seen and unseen languages, with minimal alignment tax.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</title>
<link>https://arxiv.org/abs/2504.01903</link>
<guid>https://arxiv.org/abs/2504.01903</guid>
<content:encoded><![CDATA[
arXiv:2504.01903v2 Announce Type: replace 
Abstract: This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles -- diversity, deliberative reasoning, and rigorous filtering -- STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is https://ucsc-vlaa.github.io/STAR-1.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation</title>
<link>https://arxiv.org/abs/2504.02106</link>
<guid>https://arxiv.org/abs/2504.02106</guid>
<content:encoded><![CDATA[
arXiv:2504.02106v2 Announce Type: replace 
Abstract: Evaluating the quality of generated text automatically remains a significant challenge. Conventional reference-based metrics have been shown to exhibit relatively weak correlation with human evaluations. Recent research advocates the use of large language models (LLMs) as source-based metrics for natural language generation (NLG) assessment. While promising, LLM-based metrics, particularly those using smaller models, still fall short in aligning with human judgments. In this work, we introduce ContrastScore, a contrastive evaluation metric designed to enable higher-quality, less biased, and more efficient assessment of generated text. We evaluate ContrastScore on two NLG tasks: machine translation and summarization. Experimental results show that ContrastScore consistently achieves stronger correlation with human judgments than both single-model and ensemble-based baselines. Notably, ContrastScore based on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as many parameters, demonstrating its efficiency. Furthermore, it effectively mitigates common evaluation biases such as length and likelihood preferences, resulting in more robust automatic evaluation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives</title>
<link>https://arxiv.org/abs/2504.14707</link>
<guid>https://arxiv.org/abs/2504.14707</guid>
<content:encoded><![CDATA[
arXiv:2504.14707v2 Announce Type: replace 
Abstract: Standard topic models often struggle to capture culturally specific nuances in text. This study evaluates the effectiveness of contextual embeddings for identifying culturally resonant themes in an underrepresented linguistic context. We compare the performance of KMeans Clustering, Latent Dirichlet Allocation (LDA), and BERTopic on a corpus of nearly 25,000 daily personal narratives written in Belgian-Dutch (Flemish). While LDA achieves strong performance on automated coherence metrics, subsequent human evaluation reveals that BERTopic consistently identifies the most coherent and culturally relevant topics, highlighting the limitations of purely statistical methods on this narrative-rich data. Furthermore, the diminished performance of K-Means compared to prior work on similar Dutch corpora underscores the unique linguistic challenges posed by personal narrative analysis. Our findings demonstrate the critical role of contextual embeddings in robust topic modeling and emphasize the need for human-centered evaluation, particularly when working with low-resource languages and culturally specific domains.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.00028</link>
<guid>https://arxiv.org/abs/2505.00028</guid>
<content:encoded><![CDATA[
arXiv:2505.00028v2 Announce Type: replace 
Abstract: End-to-end speech-to-speech (S2S) dialogue systems have recently garnered increasing research attention for their lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration of information. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind the SOTA cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. Our code and dataset are released.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
arXiv:2505.00661v3 Announce Type: replace 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning. E.g. they can fail to generalize to simple reversals of relations they are trained on, or fail to make simple logical deductions based on trained information. These failures to generalize factual information from fine-tuning can significantly hinder the reasoning capabilities of these models. On the other hand, language models' in-context learning (ICL) shows different inductive biases and deductive reasoning capabilities. Here, we explore these differences in generalization and deductive reasoning between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' abilities to make generalizations over factual information from novel data. These datasets are designed to create clean tests of generalization, by isolating the knowledge in the dataset from that in pretraining. We expose pretrained large models to controlled subsets of the information in these datasets -- either through ICL or fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, ICL can generalize several types of inferences more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context reasoning traces to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the generalization afforded by different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2505.11225</link>
<guid>https://arxiv.org/abs/2505.11225</guid>
<content:encoded><![CDATA[
arXiv:2505.11225v2 Announce Type: replace 
Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2505.12345</link>
<guid>https://arxiv.org/abs/2505.12345</guid>
<content:encoded><![CDATA[
arXiv:2505.12345v3 Announce Type: replace 
Abstract: Model editing aims to enhance the accuracy and reliability of large language models (LLMs) by efficiently adjusting their internal parameters. Currently, most LLM editing datasets are confined to narrow knowledge domains and cover a limited range of editing evaluation. They often overlook the broad scope of editing demands and the diversity of ripple effects resulting from edits. In this context, we introduce UniEdit, a unified benchmark for LLM editing grounded in open-domain knowledge. First, we construct editing samples by selecting entities from 25 common domains across five major categories, utilizing the extensive triple knowledge available in open-domain knowledge graphs to ensure comprehensive coverage of the knowledge domains. To address the issues of generality and locality in editing, we design an Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we employ proprietary LLMs to convert the sampled knowledge subgraphs into natural language text, guaranteeing grammatical accuracy and syntactical diversity. Extensive statistical analysis confirms the scale, comprehensiveness, and diversity of our UniEdit benchmark. We conduct comprehensive experiments across multiple LLMs and editors, analyzing their performance to highlight strengths and weaknesses in editing across open knowledge domains and various evaluation criteria, thereby offering valuable insights for future research endeavors.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2505.15683</link>
<guid>https://arxiv.org/abs/2505.15683</guid>
<content:encoded><![CDATA[
arXiv:2505.15683v3 Announce Type: replace 
Abstract: Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions</title>
<link>https://arxiv.org/abs/2505.17120</link>
<guid>https://arxiv.org/abs/2505.17120</guid>
<content:encoded><![CDATA[
arXiv:2505.17120v2 Announce Type: replace 
Abstract: We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FB-RAG: Improving RAG with Forward and Backward Lookup</title>
<link>https://arxiv.org/abs/2505.17206</link>
<guid>https://arxiv.org/abs/2505.17206</guid>
<content:encoded><![CDATA[
arXiv:2505.17206v3 Announce Type: replace 
Abstract: Traditional Retrieval-Augmented Generation (RAG) struggles with complex queries that lack strong signals to retrieve the most relevant context, forcing a trade-off between choosing a small context that misses key information and a large context that confuses the LLM. To address this, we propose Forward-Backward RAG (FB-RAG), a new training-free framework based on a simple yet powerful forward-looking strategy. FB-RAG employs a light-weight LLM to peek into potential future generations, using evidence from multiple sampled outputs to precisely identify the most relevant context for a final, more powerful generator. This improves performance without complex finetuning or Reinforcement Learning common in prior work. Across $9$ datasets from LongBench and $\infty$Bench, FB-RAG consistently delivers strong results. Further, the performance gains can be achieved with reduced latency due to a shorter, more focused prompt for the powerful generator. On EN.QA dataset, FB-RAG matches the leading baseline with over $48$% latency reduction or achieves an $8$% performance improvement with a $10$% latency reduction. Our analysis finds cases where even when the forward-looking LLM fails to generate correct answers, its attempts are sufficient to guide the final model to an accurate response, demonstrating how smaller LLMs can systematically improve the performance and efficiency of larger ones.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>p2-TQA: A Process-based Preference Learning Framework for Self-Improving Table Question Answering Models</title>
<link>https://arxiv.org/abs/2505.17565</link>
<guid>https://arxiv.org/abs/2505.17565</guid>
<content:encoded><![CDATA[
arXiv:2505.17565v2 Announce Type: replace 
Abstract: Table question answering (TQA) focuses on answering questions based on tabular data. Developing TQA systems targets effective interaction with tabular data for tasks such as cell retrieval and data analysis. While recent work has leveraged fine-tuning to improve TQA systems, existing approaches often under-utilize available data and neglect the potential of post-training for further gains. In this work, we introduce p2-TQA, a process-based preference learning framework for TQA post-training. p2-TQA automatically constructs process-based preference data via a table-specific pipeline, eliminating the need for manual or costly data collection. It then optimizes models through contrastive learning on the collected data. Experiments show that p2-TQA effectively improves TQA models by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets with only 8,000 training instances. Furthermore, models enhanced with p2-TQA achieve competitive results against larger, more complex state-of-the-art TQA systems, while maintaining up to five times higher efficiency.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDB: Multilingual Instruction Data Booster for Enhancing Cultural Equality in Multilingual Instruction Synthesis</title>
<link>https://arxiv.org/abs/2505.17671</link>
<guid>https://arxiv.org/abs/2505.17671</guid>
<content:encoded><![CDATA[
arXiv:2505.17671v2 Announce Type: replace 
Abstract: Despite doubts on data quality, instruction synthesis has been widely applied into instruction tuning (IT) of LLMs as an economic and rapid alternative. Recent endeavors focus on improving data quality for synthesized instruction pairs in English and have facilitated IT of English-centric LLMs. However, data quality issues in multilingual synthesized instruction pairs are even more severe, since the common synthesizing practice is to translate English synthesized data into other languages using machine translation (MT). Besides the known content errors in these English synthesized data, multilingual synthesized instruction data are further exposed to defects introduced by MT and face insufficient localization of the target languages, leading to cultural inequality in trained LLMs. In this paper, we propose MIDB, a Multilingual Instruction Data Booster to automatically address the quality issues in multilingual synthesized data. MIDB is trained on around 36.8k revision examples across 16 languages by human linguistic experts, thereby can boost the low-quality data by addressing content errors and MT defects, and improving localization in these synthesized data. Both automatic and human evaluation indicate that not only MIDB steadily improved instruction data quality in 16 languages, but also the instruction-following and cultural-understanding abilities of multilingual LLMs fine-tuned on MIDB-boosted data were significantly enhanced, suggesting an improved linguistic and cultural equality.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2506.02431</link>
<guid>https://arxiv.org/abs/2506.02431</guid>
<content:encoded><![CDATA[
arXiv:2506.02431v2 Announce Type: replace 
Abstract: Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. To provide a deeper interpretive lens, we incorporate four key cultural dimensions, namely Power Distance, Uncertainty Avoidance, Long-Term Orientation, and Individualism, derived from Hofstedes cross-cultural framework. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward</title>
<link>https://arxiv.org/abs/2506.04070</link>
<guid>https://arxiv.org/abs/2506.04070</guid>
<content:encoded><![CDATA[
arXiv:2506.04070v2 Announce Type: replace 
Abstract: Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study focuses on generating precise, in-situ, step-by-step navigation instructions that are practically usable for VI users. Specifically, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to navigation instructions, thereby providing feedback rewards to guide the post-training of a Vision-Language Model (VLM). This enhances instruction accuracy and usability while reducing costly real-world data collection needs. To address the scarcity of dedicated benchmarks in this field, we introduce NIG4VI, a 27k-sample open-source dataset to facilitate training and evaluation. It comprises diverse navigation scenarios with accurate spatial coordinates, supporting detailed and open-ended in-situ instruction generation. Experiments on NIG4VI demonstrate the effectiveness of LaF-GRPO through quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU 14\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o 0.323), and qualitative analysis further confirms that our method yields more intuitive and safer instructions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</title>
<link>https://arxiv.org/abs/2506.14429</link>
<guid>https://arxiv.org/abs/2506.14429</guid>
<content:encoded><![CDATA[
arXiv:2506.14429v3 Announce Type: replace 
Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably stable perplexity during direct context extrapolation. Moreover, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct local perception phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first length extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs. The code is available at https://github.com/OpenMOSS/LongLLaDA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</title>
<link>https://arxiv.org/abs/2506.16444</link>
<guid>https://arxiv.org/abs/2506.16444</guid>
<content:encoded><![CDATA[
arXiv:2506.16444v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is confined to the data that they have been trained on. To overcome this issue, Retrieval-Augmented Generation (RAG) complements the static training-derived knowledge of LLMs with an external knowledge repository. RAG consists of three stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes a significant bottleneck in inference pipelines. In this stage, a user query is mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS) algorithm searches for similar vectors in the database to identify relevant items. Due to the large database sizes, ANNS incurs significant data movement overheads between the host and the storage system. To alleviate these overheads, prior works propose In-Storage Processing (ISP) techniques that accelerate ANNS by performing computations inside storage. However, existing works that leverage ISP for ANNS (i) employ algorithms that are not tailored to ISP systems, (ii) do not accelerate data retrieval operations for data selected by ANNS, and (iii) introduce significant hardware modifications, limiting performance and hindering their adoption. We propose REIS, the first ISP system tailored for RAG that addresses these limitations with three key mechanisms. First, REIS employs a database layout that links database embedding vectors to their associated documents, enabling efficient retrieval. Second, it enables efficient ANNS by introducing an ISP-tailored data placement technique that distributes embeddings across the planes of the storage system and employs a lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that uses the existing computational resources inside the storage system. Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trilemma of Truth in Large Language Models</title>
<link>https://arxiv.org/abs/2506.23921</link>
<guid>https://arxiv.org/abs/2506.23921</guid>
<content:encoded><![CDATA[
arXiv:2506.23921v3 Announce Type: replace 
Abstract: The public often attributes human-like qualities to large language models (LLMs) and assumes they "know" certain things. In reality, LLMs encode information retained during training as internal probabilistic knowledge. This study examines existing methods for probing the veracity of that knowledge and identifies several flawed underlying assumptions. To address these flaws, we introduce sAwMIL (Sparse-Aware Multiple-Instance Learning), a multiclass probing framework that combines multiple-instance learning with conformal prediction. sAwMIL leverages internal activations of LLMs to classify statements as true, false, or neither. We evaluate sAwMIL across 16 open-source LLMs, including default and chat-based variants, on three new curated datasets. Our results show that (1) common probing methods fail to provide a reliable and transferable veracity direction and, in some settings, perform worse than zero-shot prompting; (2) truth and falsehood are not encoded symmetrically; and (3) LLMs encode a third type of signal that is distinct from both true and false.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Verifiable Instruction Following</title>
<link>https://arxiv.org/abs/2507.02833</link>
<guid>https://arxiv.org/abs/2507.02833</guid>
<content:encoded><![CDATA[
arXiv:2507.02833v3 Announce Type: replace 
Abstract: A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning</title>
<link>https://arxiv.org/abs/2507.07810</link>
<guid>https://arxiv.org/abs/2507.07810</guid>
<content:encoded><![CDATA[
arXiv:2507.07810v2 Announce Type: replace 
Abstract: This paper investigates the relationship between large language models' (LLMs) ability to recognize repetitive input patterns and their performance on in-context learning (ICL). In contrast to prior work that has primarily focused on attention heads, we examine this relationship from the perspective of skill neurons, specifically repetition neurons. Our experiments reveal that the impact of these neurons on ICL performance varies depending on the depth of the layer in which they reside. By comparing the effects of repetition neurons and induction heads, we further identify strategies for reducing repetitive outputs while maintaining strong ICL capabilities.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlMed: Adding Reasoning Control to Medical Language Model</title>
<link>https://arxiv.org/abs/2507.22545</link>
<guid>https://arxiv.org/abs/2507.22545</guid>
<content:encoded><![CDATA[
arXiv:2507.22545v3 Announce Type: replace 
Abstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02087</link>
<guid>https://arxiv.org/abs/2508.02087</guid>
<content:encoded><![CDATA[
arXiv:2508.02087v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within LLMs. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful AI systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Isolating Culture Neurons in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2508.02241</link>
<guid>https://arxiv.org/abs/2508.02241</guid>
<content:encoded><![CDATA[
arXiv:2508.02241v2 Announce Type: replace 
Abstract: Language and culture are deeply intertwined, yet it has been unclear how and where multilingual large language models encode culture. Here, we build on an established methodology for identifying language-specific neurons to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated largely independently of language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited, with implications for fairness, inclusivity, and alignment. Code and data are available at https://github.com/namazifard/Culture_Neurons.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</title>
<link>https://arxiv.org/abs/2508.09776</link>
<guid>https://arxiv.org/abs/2508.09776</guid>
<content:encoded><![CDATA[
arXiv:2508.09776v2 Announce Type: replace 
Abstract: In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models</title>
<link>https://arxiv.org/abs/2509.04508</link>
<guid>https://arxiv.org/abs/2509.04508</guid>
<content:encoded><![CDATA[
arXiv:2509.04508v2 Announce Type: replace 
Abstract: Multi-agent systems with smaller language models (SLMs) present a viable alternative to single agent systems powered by large language models (LLMs) for addressing complex problems. In this work, we study how these alternatives compare in terms of both effectiveness and efficiency. To study this trade-off, we instantiate single and multi-agent systems for the complex problems in the AppWorld environment using different sized language models.
  We find that difficulties with long-trajectory learning in smaller language models (SLMs) limit their performance. Even when trained for specialized roles, SLMs fail to learn all subtasks effectively. To address this issue, we introduce a simple progressive sub-task training strategy, which introduces new sub-tasks progressively in each training epoch. We find that this novel strategy, analogous to instance level curriculum learning, consistently improves the effectiveness of multi-agents at all configurations. Our Pareto analysis shows that fine-tuned multi-agent systems yield better effectiveness-efficiency trade-offs. Additional ablations and analyses shows the importance of our progressive training strategy and its ability to reduce subtask error rates.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder-LLM Integration in Cross-Lingual Reasoning</title>
<link>https://arxiv.org/abs/2509.08105</link>
<guid>https://arxiv.org/abs/2509.08105</guid>
<content:encoded><![CDATA[
arXiv:2509.08105v3 Announce Type: replace 
Abstract: Large language models excel in English but still struggle with complex reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder methods such as LangBridge and MindMerger raise accuracy on mid and high-resource languages, yet they leave a large gap on LRLs. We present MERLIN, a two-stage model-stacking framework that applies a curriculum learning strategy -- from general bilingual bitext to task-specific data -- and adapts only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini. It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp), demonstrating effectiveness across both low and high-resource settings.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework</title>
<link>https://arxiv.org/abs/2509.17844</link>
<guid>https://arxiv.org/abs/2509.17844</guid>
<content:encoded><![CDATA[
arXiv:2509.17844v2 Announce Type: replace 
Abstract: Emotions that somebody develops based on an argument do not only depend on the argument itself - they are also influenced by a subjective evaluation of the argument's potential impact on the self. For instance, an argument to ban plastic bottles might cause fear of losing a job for a bottle industry worker, which lowers the convincingness - presumably independent of its content. While binary emotionality of arguments has been studied, such cognitive appraisal models have only been proposed in other subtasks of emotion analysis, but not in the context of arguments and their convincingness. To fill this research gap, we propose the Contextualized Argument Appraisal Framework to model the interplay between the sender, receiver, and argument. We adapt established appraisal models from psychology to argument mining, including argument pleasantness, familiarity, response urgency, and expected effort, as well as convincingness variables. To evaluate the framework and pave the way for computational modeling, we develop a novel role-playing-based annotation setup, mimicking real-world exposure to arguments. Participants disclose their emotion, explain the main cause, the argument appraisal, and the perceived convincingness. To consider the subjective nature of such annotations, we also collect demographic data and personality traits of both the participants and ask them to disclose the same variables for their perception of the argument sender. The analysis of the resulting ContArgA corpus of 4000 annotations reveals that convincingness is positively correlated with positive emotions (e.g., trust) and negatively correlated with negative emotions (e.g., anger). The appraisal variables particularly point to the importance of the annotator's familiarity with the argument.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages</title>
<link>https://arxiv.org/abs/2509.26601</link>
<guid>https://arxiv.org/abs/2509.26601</guid>
<content:encoded><![CDATA[
arXiv:2509.26601v2 Announce Type: replace 
Abstract: Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title>
<link>https://arxiv.org/abs/2510.04226</link>
<guid>https://arxiv.org/abs/2510.04226</guid>
<content:encoded><![CDATA[
arXiv:2510.04226v5 Announce Type: replace 
Abstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration</title>
<link>https://arxiv.org/abs/2510.15267</link>
<guid>https://arxiv.org/abs/2510.15267</guid>
<content:encoded><![CDATA[
arXiv:2510.15267v2 Announce Type: replace 
Abstract: Automated International Classification of Diseases (ICD) coding assigns standardized diagnosis and procedure codes to clinical records, playing a critical role in healthcare systems. However, existing methods face challenges such as semantic gaps between clinical text and ICD codes, poor performance on rare and long-tail codes, and limited interpretability. To address these issues, we propose TraceCoder, a novel framework integrating multi-source external knowledge to enhance traceability and explainability in ICD coding. TraceCoder dynamically incorporates diverse knowledge sources, including UMLS, Wikipedia, and large language models (LLMs), to enrich code representations, bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a hybrid attention mechanism to model interactions among labels, clinical context, and knowledge, improving long-tail code recognition and making predictions interpretable by grounding them in external evidence. Experiments on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that TraceCoder achieves state-of-the-art performance, with ablation studies validating the effectiveness of its components. TraceCoder offers a scalable and robust solution for automated ICD coding, aligning with clinical needs for accuracy, interpretability, and reliability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding</title>
<link>https://arxiv.org/abs/2510.15269</link>
<guid>https://arxiv.org/abs/2510.15269</guid>
<content:encoded><![CDATA[
arXiv:2510.15269v2 Announce Type: replace 
Abstract: Medical texts, particularly electronic medical records (EMRs), are a cornerstone of modern healthcare, capturing critical information about patient care, diagnoses, and treatments. These texts hold immense potential for advancing clinical decision-making and healthcare analytics. However, their unstructured nature, domain-specific language, and variability across contexts make automated understanding an intricate challenge. Despite the advancements in natural language processing, existing methods often treat all data as equally challenging, ignoring the inherent differences in complexity across clinical records. This oversight limits the ability of models to effectively generalize and perform well on rare or complex cases. In this paper, we present TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to address these challenges by rethinking how models interact with medical texts during training. Inspired by the principle of progressive learning, TACL dynamically adjusts the training process based on the complexity of individual samples. By categorizing data into difficulty levels and prioritizing simpler cases early in training, the model builds a strong foundation before tackling more complex records. By applying TACL to multilingual medical data, including English and Chinese clinical records, we observe significant improvements across diverse clinical tasks, including automatic ICD coding, readmission prediction and TCM syndrome differentiation. TACL not only enhances the performance of automated systems but also demonstrates the potential to unify approaches across disparate medical domains, paving the way for more accurate, scalable, and globally applicable medical text understanding solutions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2510.16565</link>
<guid>https://arxiv.org/abs/2510.16565</guid>
<content:encoded><![CDATA[
arXiv:2510.16565v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used across diverse cultural contexts, making accurate cultural understanding essential. Prior evaluations have mostly focused on output-level performance, obscuring the factors that drive differences in responses, while studies using circuit analysis have covered few languages and rarely focused on culture. In this work, we trace LLMs' internal cultural understanding mechanisms by measuring activation path overlaps when answering semantically equivalent questions under two conditions: varying the target country while fixing the question language, and varying the question language while fixing the country. We also use same-language country pairs to disentangle language from cultural aspects. Results show that internal paths overlap more for same-language, cross-country questions than for cross-language, same-country questions, indicating strong language-specific patterns. Notably, the South Korea-North Korea pair exhibits low overlap and high variability, showing that linguistic similarity does not guarantee aligned internal representation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model</title>
<link>https://arxiv.org/abs/2503.09205</link>
<guid>https://arxiv.org/abs/2503.09205</guid>
<content:encoded><![CDATA[
arXiv:2503.09205v4 Announce Type: replace-cross 
Abstract: Integrating audio and visual data for training multimodal foundational models remains a challenge. The Audio-Video Vector Alignment (AVVA) framework addresses this by considering AV scene alignment beyond mere temporal synchronization, and leveraging Large Language Models (LLMs) for data curation. AVVA implements a scoring mechanism for selecting aligned training data segments. It integrates Whisper, a speech-based foundation model, for audio and DINOv2 for video analysis in a dual-encoder structure with contrastive learning on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the effectiveness of the proposed model architecture and data curation approach. AVVA achieves a significant improvement in top-k accuracies for video-to-audio retrieval on all datasets compared to DenseAV, while using only 192 hrs of curated training data. Furthermore, an ablation study indicates that the data curation process effectively trades data quality for data quantity, yielding increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound, compared to training on the full spectrum of uncurated data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles</title>
<link>https://arxiv.org/abs/2503.17352</link>
<guid>https://arxiv.org/abs/2503.17352</guid>
<content:encoded><![CDATA[
arXiv:2503.17352v3 Announce Type: replace-cross 
Abstract: We introduce OpenVLThinker, one of the first open-source large vision-language models (LVLMs) to exhibit sophisticated chain-of-thought reasoning, achieving notable performance gains on challenging visual reasoning tasks. While text-based reasoning models (e.g., Deepseek R1) show promising results in text-only tasks, distilling their reasoning into LVLMs via supervised fine-tuning (SFT) often results in performance degradation due to imprecise visual grounding. Conversely, purely reinforcement learning (RL)-based methods face a large search space, hindering the emergence of reflective behaviors in smaller models (e.g., 7B LVLMs). Surprisingly, alternating between SFT and RL ultimately results in significant performance improvements after a few iterations. Our analysis reveals that the base model rarely exhibits reasoning behaviors initially, but SFT effectively surfaces these latent actions and narrows the RL search space, accelerating the development of reasoning capabilities. Each subsequent RL stage further refines the model's reasoning skills, producing higher-quality SFT data for continued self-improvement. OpenVLThinker-7B consistently advances performance across six benchmarks demanding mathematical and general reasoning, notably improving MathVista by 3.8%, EMMA by 2.4%, and HallusionBench by 1.6%. Beyond demonstrating the synergy between SFT and RL for complex reasoning tasks, our findings provide early evidence towards achieving R1-style reasoning in multimodal contexts. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Recaptioning Framework to Account for Perceptual Diversity Across Languages in Vision-Language Modeling</title>
<link>https://arxiv.org/abs/2504.14359</link>
<guid>https://arxiv.org/abs/2504.14359</guid>
<content:encoded><![CDATA[
arXiv:2504.14359v2 Announce Type: replace-cross 
Abstract: When captioning an image, people describe objects in diverse ways, such as by using different terms and/or including details that are perceptually noteworthy to them. Descriptions can be especially unique across languages and cultures. Modern vision-language models (VLMs) gain understanding of images with text in different languages often through training on machine translations of English captions. However, this process relies on input content written from the perception of English speakers, leading to a perceptual bias. In this work, we outline a framework to address this bias. We specifically use a small amount of native speaker data, nearest-neighbor example guidance, and multimodal LLM reasoning to augment captions to better reflect descriptions in a target language. When adding the resulting rewrites to multilingual CLIP finetuning, we improve on German and Japanese text-image retrieval case studies (up to +3.5 mean recall, +4.4 on native vs. translation errors). We also propose a mechanism to build understanding of object description variation across languages, and offer insights into cross-dataset and cross-language generalization.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPMA: Preference Manipulation Attack Against Model Context Protocol</title>
<link>https://arxiv.org/abs/2505.11154</link>
<guid>https://arxiv.org/abs/2505.11154</guid>
<content:encoded><![CDATA[
arXiv:2505.11154v2 Announce Type: replace-cross 
Abstract: Model Context Protocol (MCP) standardizes interface mapping for large language models (LLMs) to access external data and tools, which revolutionizes the paradigm of tool selection and facilitates the rapid expansion of the LLM agent tool ecosystem. However, as the MCP is increasingly adopted, third-party customized versions of the MCP server expose potential security vulnerabilities. In this paper, we first introduce a novel security threat, which we term the MCP Preference Manipulation Attack (MPMA). An attacker deploys a customized MCP server to manipulate LLMs, causing them to prioritize it over other competing MCP servers. This can result in economic benefits for attackers, such as revenue from paid MCP services or advertising income generated from free servers. To achieve MPMA, we first design a Direct Preference Manipulation Attack (DPMA) that achieves significant effectiveness by inserting the manipulative word and phrases into the tool name and description. However, such a direct modification is obvious to users and lacks stealthiness. To address these limitations, we further propose Genetic-based Advertising Preference Manipulation Attack (GAPMA). GAPMA employs four commonly used strategies to initialize descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness. The experiment results demonstrate that GAPMA balances high effectiveness and stealthiness. Our study reveals a critical vulnerability of the MCP in open ecosystems, highlighting an urgent need for robust defense mechanisms to ensure the fairness of the MCP ecosystem.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title>
<link>https://arxiv.org/abs/2505.11770</link>
<guid>https://arxiv.org/abs/2505.11770</guid>
<content:encoded><![CDATA[
arXiv:2505.11770v2 Announce Type: replace-cross 
Abstract: Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</title>
<link>https://arxiv.org/abs/2508.01365</link>
<guid>https://arxiv.org/abs/2508.01365</guid>
<content:encoded><![CDATA[
arXiv:2508.01365v3 Announce Type: replace-cross 
Abstract: Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Question-to-Knowledge (Q2K): Multi-Agent Generation of Inspectable Facts for Product Mapping</title>
<link>https://arxiv.org/abs/2509.01182</link>
<guid>https://arxiv.org/abs/2509.01182</guid>
<content:encoded><![CDATA[
arXiv:2509.01182v2 Announce Type: replace-cross 
Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit (SKU) is a persistent challenge in ecommerce, especially when explicit identifiers are missing and product names vary widely across platforms. Rule based heuristics and keyword similarity often misclassify products by overlooking subtle distinctions in brand, specification, or bundle configuration. To overcome these limitations, we propose Question to Knowledge (Q2K), a multi agent framework that leverages Large Language Models (LLMs) for reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates targeted disambiguation questions, (2) a Knowledge Agent that resolves them via focused web searches, and (3) a Deduplication Agent that reuses validated reasoning traces to reduce redundancy and ensure consistency. A human in the loop mechanism further refines uncertain cases. Experiments on real world consumer goods datasets show that Q2K surpasses strong baselines, achieving higher accuracy and robustness in difficult scenarios such as bundle identification and brand origin disambiguation. By reusing retrieved reasoning instead of issuing repeated searches, Q2K balances accuracy with efficiency, offering a scalable and interpretable solution for product integration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning</title>
<link>https://arxiv.org/abs/2509.22315</link>
<guid>https://arxiv.org/abs/2509.22315</guid>
<content:encoded><![CDATA[
arXiv:2509.22315v3 Announce Type: replace-cross 
Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking, Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \textbf{System 1} (fast, intuitive thinking) and \textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \textit{planning}, \textit{hypothesis generation}, \textit{retrieval}, \textit{information integration}, and \textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions</title>
<link>https://arxiv.org/abs/2510.08576</link>
<guid>https://arxiv.org/abs/2510.08576</guid>
<content:encoded><![CDATA[
arXiv:2510.08576v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural language understanding and user intent resolution, enabling tasks such as translation, summarization, and, increasingly, the orchestration of complex workflows. This development signifies a paradigm shift from conventional, GUI-driven user interfaces toward intuitive, language-first interaction paradigms. Rather than manually navigating applications, users can articulate their objectives in natural language, enabling LLMs to orchestrate actions across multiple applications in a dynamic and contextual manner. However, extant implementations frequently rely on cloud-based proprietary models, which introduce limitations in terms of privacy, autonomy, and scalability. For language-first interaction to become a truly robust and trusted interface paradigm, local deployment is not merely a convenience; it is an imperative. This limitation underscores the importance of evaluating the feasibility of locally deployable, open-source, and open-access LLMs as foundational components for future intent-based operating systems. In this study, we examine the capabilities of several open-source and open-access models in facilitating user intention resolution through machine assistance. A comparative analysis is conducted against OpenAI's proprietary GPT-4-based systems to assess performance in generating workflows for various user intentions. The present study offers empirical insights into the practical viability, performance trade-offs, and potential of open LLMs as autonomous, locally operable components in next-generation operating systems. The results of this study inform the broader discussion on the decentralization and democratization of AI infrastructure and point toward a future where user-device interaction becomes more seamless, adaptive, and privacy-conscious through locally embedded intelligence.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry</title>
<link>https://arxiv.org/abs/2510.22340</link>
<guid>https://arxiv.org/abs/2510.22340</guid>
<content:encoded><![CDATA[
arXiv:2510.22340v2 Announce Type: replace-cross 
Abstract: Solid geometry problem solving demands spatial mathematical reasoning that integrates spatial intelligence and symbolic reasoning. However, most existing multimodal mathematical reasoning benchmarks focus primarily on 2D plane geometry, rely on static datasets prone to data contamination and memorization, and evaluate models solely by final answers, overlooking the reasoning process. To address these limitations, we introduce DynaSolidGeo, the first dynamic benchmark for evaluating genuine spatial reasoning in Vision-Language Models (VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo contains 503 expert-curated seed questions that can, in principle, dynamically generate an unbounded number of diverse multimodal text-visual instances. Beyond answer accuracy, we incorporate process evaluation based on expert-annotated reasoning chains to measure logical validity and causal coherence. Experiments across representative open-source and closed-source VLMs reveal large performance gaps, severe degradation in dynamic settings, and poor performance on tasks requiring high-level spatial intelligence, such as mental rotation and visualization. The code and dataset are available at \href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.26512</link>
<guid>https://arxiv.org/abs/2510.26512</guid>
<content:encoded><![CDATA[
<div> Keywords: human smuggling networks, legal case documents, knowledge graph construction, coreference resolution, LLM-based pipelines

Summary: 
- Human smuggling networks are evolving and challenging to analyze, with legal case documents providing crucial insights but posing difficulties due to their complex and ambiguous nature.
- Current LLM-based approaches have limitations in generating structured knowledge graphs from these documents, leading to noisy and fragmented graphs with duplicate nodes.
- The CORE-KG framework addresses these challenges by incorporating a type-aware coreference module and domain-guided structured prompts, significantly reducing duplication and legal noise.
- An ablation study of CORE-KG highlights the importance of both coreference resolution and structured prompts in minimizing node duplication and noisy nodes.
- Removing coreference resolution results in increased node duplication and noisy nodes, while removing structured prompts leads to further duplication and noise. These findings provide valuable insights for enhancing LLM-based pipelines for extracting structured representations from complex legal texts.<br /><br />Summary: <div>
arXiv:2510.26512v2 Announce Type: replace 
Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.25% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.29% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</title>
<link>https://arxiv.org/abs/2510.18480</link>
<guid>https://arxiv.org/abs/2510.18480</guid>
<content:encoded><![CDATA[
<div> efficiency, diffusion language models, autoregressive models, benchmarking, acceleration strategies
Summary:
- Diffusion language models (DLMs) offer a parallel decoding process but often underperform autoregressive models in speed.
- This study systematically examines DLM efficiency, pointing out issues in prior evaluation methods.
- Empirical benchmarking and theoretical analysis show that autoregressive models achieve higher throughput compared to DLMs.
- Acceleration strategies such as dual cache and parallel decoding provide benefits mainly at small batch sizes.
- The study emphasizes the need for robust evaluation methods and improved acceleration strategies to enhance research on DLMs.<br /><br />Summary: <div>
arXiv:2510.18480v3 Announce Type: replace 
Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</title>
<link>https://arxiv.org/abs/2509.02547</link>
<guid>https://arxiv.org/abs/2509.02547</guid>
<content:encoded><![CDATA[
<div> Markov Decision Processes, Reinforcement Learning, Agentic RL, Language Models, Autonomous Agents<br />
Summary:<br />
This article introduces the concept of agentic reinforcement learning (Agentic RL) as a shift from traditional reinforcement learning applied to large language models (LLM RL). It contrasts single-step Markov Decision Processes (MDPs) of LLM-RL with partially observable Markov decision processes (POMDPs) in Agentic RL. The survey proposes a taxonomy focusing on agentic capabilities like planning, tool use, memory, reasoning, self-improvement, and perception, along with their applications in various task domains. It emphasizes reinforcement learning as crucial for transforming static modules into adaptive agentic behavior. The article consolidates open-source environments, benchmarks, and frameworks for researchers. By analyzing over five hundred recent works, it maps the evolving field of Agentic RL and identifies opportunities and challenges for developing scalable AI agents. <br />Summary: <div>
arXiv:2509.02547v3 Announce Type: replace-cross 
Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation</title>
<link>https://arxiv.org/abs/2510.25677</link>
<guid>https://arxiv.org/abs/2510.25677</guid>
<content:encoded><![CDATA[
<div> framework, wireless sensing, security, zero-knowledge proofs, encoder <br />
Summary: <br />
ZK-SenseLM is a wireless sensing framework that ensures security through zero-knowledge proofs and a large-model encoder for Wi-Fi channel state information. It incorporates mmWave radar or RFID and a policy-based decision layer. The system utilizes masked spectral pretraining and phase-consistency regularization for encoding, with a cross-modal alignment linking RF features to policy tokens. To prevent unsafe actions during distribution shift, a selective-abstention head is employed, with the chosen risk-coverage point included in the proof. A four-stage proving pipeline ensures the integrity of actions, with PLONK-style proofs verifying actions and confidence. Proofs are optimized using micro-batching and can be offloaded from low-power devices through a gateway option. The system is compatible with federated learning and on-device personalization while maintaining verifiability. ZK-SenseLM enhances performance in various tasks, improves calibration and coverage-risk curves, and effectively detects tampering and replay events. <div>
arXiv:2510.25677v2 Announce Type: replace-cross 
Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a large-model encoder for Wi-Fi channel state information (and optionally mmWave radar or RFID) with a policy-grounded decision layer and end-to-end zero-knowledge proofs of inference. The encoder uses masked spectral pretraining with phase-consistency regularization, plus a light cross-modal alignment that ties RF features to compact, human-interpretable policy tokens. To reduce unsafe actions under distribution shift, we add a calibrated selective-abstention head; the chosen risk-coverage operating point is registered and bound into the proof. We implement a four-stage proving pipeline: (C1) feature sanity and commitment, (C2) threshold and version binding, (C3) time-window binding, and (C4) PLONK-style proofs that the quantized network, given the committed window, produced the logged action and confidence. Micro-batched proving amortizes cost across adjacent windows, and a gateway option offloads proofs from low-power devices. The system integrates with differentially private federated learning and on-device personalization without weakening verifiability: model hashes and the registered threshold are part of each public statement. Across activity, presence or intrusion, respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1 and calibration, yields favorable coverage-risk curves under perturbations, and rejects tamper and replay with compact proofs and fast verification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</title>
<link>https://arxiv.org/abs/2511.05516</link>
<guid>https://arxiv.org/abs/2511.05516</guid>
<content:encoded><![CDATA[
<div> Keywords: speech language model, continuous audio tokenizer, speech editing, instruction-based, free-form editing

Summary:
MingTok-Audio is introduced as a unified continuous speech tokenizer that integrates semantic and acoustic features for improved speech understanding and generation. Ming-UniAudio, based on MingTok-Audio, achieves a balance between generation and understanding capabilities, setting new SOTA records on the ContextASR benchmark. Ming-UniAudio-Edit is the first speech language model enabling free-form speech editing guided by natural language instructions without timestamp conditions. A comprehensive benchmark, Ming-Freeform-Audio-Edit, is introduced to evaluate instruction-based speech editing, covering semantic correctness, acoustic quality, and instruction alignment. The models are open-sourced to foster the development of unified audio understanding, generation, and manipulation.<br /><br />Summary: MingTok-Audio and Ming-UniAudio improve speech understanding and generation, setting benchmarks in ASR tasks. Ming-UniAudio-Edit enables free-form speech editing guided by natural language instructions, supported by the Ming-Freeform-Audio-Edit benchmark. All models are open-sourced to advance research in unified audio processing. <div>
arXiv:2511.05516v1 Announce Type: new 
Abstract: Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retracing the Past: LLMs Emit Training Data When They Get Lost</title>
<link>https://arxiv.org/abs/2511.05518</link>
<guid>https://arxiv.org/abs/2511.05518</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, memorization, data extraction, privacy concerns, confusion-inducing attacks

Summary:
The paper introduces Confusion-Inducing Attacks (CIA), a framework for extracting memorized data from large language models (LLMs) by maximizing model uncertainty. It identifies a spike in token-level prediction entropy as a precursor to the emission of memorized text during divergence, and leverages this insight to optimize input snippets for inducing a high-entropy state. The proposed Mismatched Supervised Fine-tuning (SFT) weakens alignment in aligned LLMs to increase susceptibility to attacks. Experimental results show that CIA outperforms existing methods in extracting verbatim and near-verbatim training data without prior knowledge of the training data. The study highlights the persistent risks of memorization in LLMs and offers a systematic approach to assess these vulnerabilities. 

<br /><br />Summary: The paper presents Confusion-Inducing Attacks (CIA) to extract memorized data from large language models by maximizing model uncertainty. It uses token-level prediction entropy to optimize input snippets and proposes Mismatched Supervised Fine-tuning (SFT) for aligned models. Experimental results demonstrate the effectiveness of CIA in extracting training data without prior knowledge, highlighting ongoing risks of memorization in LLMs. <div>
arXiv:2511.05518v1 Announce Type: new 
Abstract: The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning</title>
<link>https://arxiv.org/abs/2511.05532</link>
<guid>https://arxiv.org/abs/2511.05532</guid>
<content:encoded><![CDATA[
<div> Keywords: online content moderation, in-context learning, foundation models, personalization, decentralized environments

Summary: 
- The article introduces a novel framework that utilizes in-context learning with foundation models to detect harmful online content such as toxicity, spam, and negative sentiment across various settings.
- This approach allows for lightweight personalization, enabling users to block or unblock categories and extend detection to semantic variations without needing model retraining.
- Experiments on public datasets and a new annotated Mastodon dataset show that foundation models can generalize well across tasks, sometimes outperforming task-specific models.
- Personalization is effective with minimal user input, and adding label definitions or rationales to prompts improves robustness to noisy data.
- The work suggests a move towards user-centric content safety systems that are practical, privacy-preserving, and highly adaptable, offering a new approach beyond traditional centralized moderation systems.

<br /><br />Summary: <div>
arXiv:2511.05532v1 Announce Type: new 
Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP4IFC: IFC-Based Building Design Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.05533</link>
<guid>https://arxiv.org/abs/2511.05533</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, architecture, engineering, construction, Industry Foundation Classes (IFC)

Summary:
This article introduces MCP4IFC, an open-source framework that allows Large Language Models (LLMs) to manipulate Industry Foundation Classes (IFC) data in the architecture, engineering, and construction (AEC) field. The framework enables LLMs to translate natural language instructions into actions on standardized data models using the Model Context Protocol (MCP). It includes tools for querying scene information, creating and modifying building elements, and a dynamic code-generation system. The framework combines in-context learning with retrieval-augmented generation (RAG) for tasks beyond the predefined toolset. Experiments show that an LLM using MCP4IFC can successfully complete various tasks, from building a house to editing existing IFC data. The open-source framework aims to promote research in LLM-driven BIM design and facilitate AI-assisted modeling workflows.

<br /><br />Summary: <div>
arXiv:2511.05533v1 Announce Type: new 
Abstract: Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference</title>
<link>https://arxiv.org/abs/2511.05534</link>
<guid>https://arxiv.org/abs/2511.05534</guid>
<content:encoded><![CDATA[
<div> framework, cross-modal information flow, multimodal, KV cache merging, sensitivity-adaptive token matching <br />
<br />
Summary: <br />
FlowMM is introduced as a framework for multimodal KV cache merging, addressing limitations in traditional eviction strategies. It leverages cross-modal information flow to dynamically apply merging strategies, capturing modality-specific patterns while maintaining contextual integrity. The framework also includes a sensitivity-adaptive token matching mechanism to evaluate token similarity and task sensitivity, merging low-risk tokens while preserving high-sensitivity ones. Experimental results across MLLMs demonstrate a significant reduction in KV cache memory and decoding latency while maintaining competitive task performance. <div>
arXiv:2511.05534v1 Announce Type: new 
Abstract: Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Future of AI Models: A Computational perspective on Model collapse</title>
<link>https://arxiv.org/abs/2511.05535</link>
<guid>https://arxiv.org/abs/2511.05535</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, Diffusion models, Neural architectures, Model Collapse

Summary:
Artificial Intelligence, particularly Large Language Models (LLMs), has significantly impacted various fields such as software engineering, journalism, and academia. The proliferation of AI-generated content is evident, with a high percentage of webpages containing such material. However, the use of synthetic content poses a risk of Model Collapse due to reduced linguistic and semantic diversity. This study explores the onset of collapse by analyzing semantic similarity in English-language Wikipedia from 2013 to 2025. Results indicate a gradual increase in similarity before the adoption of LLM models, attributable to early RNN/LSTM technologies. Fluctuations in similarity reflect linguistic diversity, corpus size variations, and sampling error. A sharp rise in similarity post-LLM adoption signifies a potential threat to data richness and model generalization. This data-driven analysis offers insights into the progression towards recursive AI contamination. 

<br /><br />Summary: Artificial Intelligence has revolutionized various domains, with a substantial presence of AI-generated content on webpages. However, the widespread use of synthetic material poses a risk of Model Collapse, impacting linguistic and semantic diversity. By analyzing semantic similarity trends in English-language Wikipedia, this study uncovers an exponential rise in similarity following the adoption of Large Language Models. Fluctuations in similarity metrics highlight linguistic diversity and corpus size variations. The study provides valuable insights into the potential onset of recursive AI contamination, signaling a potential threat to data richness and model generalization. <div>
arXiv:2511.05535v1 Announce Type: new 
Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability</title>
<link>https://arxiv.org/abs/2511.05541</link>
<guid>https://arxiv.org/abs/2511.05541</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretability, Sparse Autoencoders, linguistic understanding, semantic features, unsupervised learning

Summary:<br /><br />Translating complex model representations into human-understandable concepts is a critical goal for interpretability. Current methods like Sparse Autoencoders (SAEs) often fail to capture meaningful linguistic information, focusing instead on superficial patterns. This stems from a lack of integration of linguistic knowledge in training. To address this, Temporal Sparse Autoencoders (T-SAEs) are introduced, which prioritize semantic over syntactic features through a novel contrastive loss mechanism. The T-SAEs successfully disentangle semantic concepts in a self-supervised manner, yielding smoother and coherent semantic representations across various datasets and models. Surprisingly, these semantic structures emerge without explicit semantic supervision, indicating a promising approach for enhancing unsupervised interpretability in language models. <div>
arXiv:2511.05541v1 Announce Type: new 
Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements</title>
<link>https://arxiv.org/abs/2511.05560</link>
<guid>https://arxiv.org/abs/2511.05560</guid>
<content:encoded><![CDATA[
<div> mLSTM, language modeling, linear attention, sliding window attention, Muon optimizer  
Summary:  
1. The study focuses on sample-efficient language modeling techniques for the BabyLM 2025 shared task, utilizing the BLaLM model with a linear-time mLSTM token mixer.
2. Architectural enhancements such as short convolutions, sliding window attention, and Hedgehog feature maps are explored to improve model performance.
3. A curated high-quality corpus is used for training in low-resource settings, emphasizing readability and pedagogical structure.
4. Experiments reveal that linear attention with sliding window attention enhances zero-shot performance consistently.
5. The Muon optimizer is found to stabilize convergence and reduce perplexity compared to AdamW, showcasing effective strategies for efficient language modeling without relying on scale.  
<br /><br />Summary: <div>
arXiv:2511.05560v1 Announce Type: new 
Abstract: We study architectural and optimization tech- niques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM to- ken mixer and explores lightweight enhance- ments, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support train- ing in low-resource settings, we curate a high- quality corpus emphasizing readability and ped- agogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding win- dow attention consistently improves zero-shot performance, and (2) the Muon optimizer stabi- lizes convergence and reduces perplexity over AdamW. These results highlight effective strate- gies for efficient language modeling without relying on scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8</title>
<link>https://arxiv.org/abs/2511.05578</link>
<guid>https://arxiv.org/abs/2511.05578</guid>
<content:encoded><![CDATA[
<div> tokenization, subword, language model, UTF-8, monoid theory

Summary:
This paper discusses the challenges of subword tokenization for language models, specifically focusing on the trade-offs between using code points and bytes in the vocabulary. By formalizing tokenization using monoid theory, the study proves that tokenizers with ill-formed UTF-8 tokens can result in sequences that are also ill-formed UTF-8. The research demonstrates the implications of converting tokens incrementally versus all at once, highlighting potential differences in results. Real-world bugs resulting from these issues are discussed, along with evaluations of mitigations. Case studies involving major foundation models, serving engines, and constrained generation systems are explored to showcase the impact of these findings. The study emphasizes the need for applications of language models to account for potential breakage introduced by using byte-based vocabularies. 
<br /><br />Summary: <div>
arXiv:2511.05578v1 Announce Type: new 
Abstract: Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Diversity and Quality through Base-Aligned Model Collaboration</title>
<link>https://arxiv.org/abs/2511.05650</link>
<guid>https://arxiv.org/abs/2511.05650</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, model collaboration, diversity, quality, inference-time

Summary: 
Base-Aligned Model Collaboration (BACo) is introduced as a framework for improving the diversity and quality of large language models (LLMs) by dynamically combining a base model with its aligned counterpart. Utilizing routing strategies based on next-token prediction uncertainty and semantic role prediction, BACo optimizes diversity and quality without compromising on performance. Compared to existing diversity-promoting methods, BACo achieves a balance between diversity and quality in a single pass, offering controllability and consistently outperforming state-of-the-art baselines across various generation tasks and metrics. Through collaboration between base and aligned models, BACo demonstrates a 21.3% joint improvement in diversity and quality, as confirmed by human evaluations. This approach highlights the potential of model collaboration to enhance the output quality of LLMs while maintaining diversity. 

<br /><br />Summary: <div>
arXiv:2511.05650v1 Announce Type: new 
Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OckBench: Measuring the Efficiency of LLM Reasoning</title>
<link>https://arxiv.org/abs/2511.05722</link>
<guid>https://arxiv.org/abs/2511.05722</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, token efficiency, benchmarking, automated reasoning, code generation

Summary:
Large language models like GPT-4 and Claude 3 have greatly improved automated reasoning and code generation. However, existing benchmarks often overlook token efficiency, which plays a crucial role in determining system latency, cost, and energy consumption. In response to this gap, the OckBench benchmark was introduced to evaluate both accuracy and token count for reasoning and coding tasks. Through experiments on various models, it was discovered that models with similar accuracy can differ significantly in token consumption. The benchmark also highlights the importance of considering token efficiency when evaluating models, prompting a paradigm shift in research evaluation practices. OckBench serves as a comprehensive platform for measuring, comparing, and guiding research in token-efficient reasoning. The benchmarks are accessible at https://ockbench.github.io/. 

<br /><br />Summary: <div>
arXiv:2511.05722v1 Announce Type: new 
Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning Without Copying</title>
<link>https://arxiv.org/abs/2511.05743</link>
<guid>https://arxiv.org/abs/2511.05743</guid>
<content:encoded><![CDATA[
<div> induction heads, inductive copying, transformers, abstractive ICL, Hapax <br />
<br />
Summary: 
The study explores the role of induction heads in transformers, which are attention heads that perform inductive copying to match patterns and copy continuations. The research investigates whether transformers can still acquire in-context learning capabilities when inductive copying is suppressed. The proposed Hapax setting omits the loss contribution for tokens predict correctly by induction heads. Despite a reduction in inductive copying, performance on abstractive in-context learning tasks remains comparable and even surpasses the vanilla model on various tasks. The model achieves lower loss values on token positions not predicted by induction heads. Analysis reveals that models trained with Hapax develop fewer and weaker induction heads but still retain in-context learning capabilities. These findings suggest that inductive copying is not essential for learning abstractive in-context learning mechanisms. <br /> <div>
arXiv:2511.05743v1 Announce Type: new 
Abstract: Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2511.05752</link>
<guid>https://arxiv.org/abs/2511.05752</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, language models, feature pyramids, graph neural networks, semantic modeling

Summary: This study presents a hybrid method for text classification that combines deep feature extraction from language models, multi-scale feature fusion via feature pyramids, and structured modeling with graph neural networks. The large language model captures contextual dependencies and semantic representations, forming a strong foundation for subsequent modeling. The feature pyramid mechanism integrates semantic features of different scales, balancing global and local information to create hierarchical semantic expressions. Fused features are transformed into graph representations, allowing graph neural networks to capture semantic relations and dependencies in the text. The proposed method outperforms existing models in robustness alignment experiments on metrics such as ACC, F1-Score, AUC, and Precision. This framework provides a new approach for multi-scale feature fusion and structured semantic modeling in text classification tasks.<br /><br />Summary: This study introduces a hybrid method for text classification that combines deep feature extraction from language models with multi-scale feature fusion and structured modeling. The framework achieves superior performance in complex semantic contexts, showcasing the effectiveness of balancing global and local information, as well as semantics and structure in text classification tasks. <div>
arXiv:2511.05752v1 Announce Type: new 
Abstract: This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation: Complexity Barriers and Implications for Learning</title>
<link>https://arxiv.org/abs/2511.05759</link>
<guid>https://arxiv.org/abs/2511.05759</guid>
<content:encoded><![CDATA[
<div> language generation, learnability, regular languages, context-free languages, language models
Summary:<br />
- Kleinberg and Mullainathan demonstrated the theoretical possibility of language generation with a sufficient number of positive examples, but practical feasibility is challenging.<br />
- Simple language families like regular and context-free languages may require an extraordinarily large number of examples for successful generation, sometimes without a computable bound.<br />
- There exists a significant gap between the theoretical potential and efficient learnability of language.<br />
- The success of modern language models may be explained by considering structural properties of natural language that enable effective generation in practice.<br /> <div>
arXiv:2511.05759v1 Announce Type: new 
Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title>
<link>https://arxiv.org/abs/2511.05784</link>
<guid>https://arxiv.org/abs/2511.05784</guid>
<content:encoded><![CDATA[
<div> Keywords: unlearning, language models, privacy protection, detection module, continual unlearning

Summary:
The article introduces a novel framework called DRAGON for unlearning in Large Language Models (LLMs) to protect private data and remove harmful knowledge. DRAGON leverages in-context chain-of-thought (CoT) instructions to guide LLMs without the need for access to retain data. By utilizing a lightweight detection module and a CoT guard model, DRAGON can effectively identify forget-worthy prompts and enforce safe intervention without modifying the base model. The framework is evaluated across three unlearning tasks, demonstrating its strong capability, scalability, and practical applicability. The proposed metrics for unlearning performance and the continual unlearning setting provide a robust evaluation of DRAGON's effectiveness in data-limited scenarios. Overall, DRAGON offers a promising solution for efficient and secure unlearning in LLMs. 

<br /><br />Summary: <div>
arXiv:2511.05784v1 Announce Type: new 
Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Edits Decay in Fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2511.05852</link>
<guid>https://arxiv.org/abs/2511.05852</guid>
<content:encoded><![CDATA[
<div> knowledge editing, fine-tuning, language models, edits decay, selective-layer fine-tuning

Summary:
In this study, the authors explore the interaction between knowledge editing and fine-tuning in large language models (LLMs). They investigate whether edits made to LLMs persist after fine-tuning, which is crucial for scenarios such as removing malicious edits or preserving beneficial ones. The experiments involve testing two editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets. The results show that edits decay after fine-tuning, with the survival of edits varying across configurations. Selective-layer fine-tuning is proposed as a strategy to effectively remove edits, even though it may slightly impact downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. This study establishes empirical baselines and provides actionable strategies for integrating knowledge editing with fine-tuning in LLMs, emphasizing the importance of considering the full application pipeline when evaluating model editing. 

Summary: <div>
arXiv:2511.05852v1 Announce Type: new 
Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations</title>
<link>https://arxiv.org/abs/2511.05901</link>
<guid>https://arxiv.org/abs/2511.05901</guid>
<content:encoded><![CDATA[
<div> Keywords: medical knowledge, large language models, retrieval-augmented generation, clinical validation, low-resource settings 

Summary:
Large language models (LLMs) have shown value in the medical field, but there are limitations. Retrieval-augmented generation (RAG) technologies have potential to enhance clinical applicability. However, current research relies heavily on publicly available data, with limited use of private data. Retrieval approaches commonly use English-centric embedding models, while medical-specific LLMs are underutilized. Evaluation metrics focus on generation quality and task performance, but lack attention to bias and safety. RAG applications in medicine are concentrated on question answering, report generation, text summarization, and information extraction. The field of medical RAG is still in its early stages and requires advancements in clinical validation, cross-linguistic adaptation, and support for low-resource settings to ensure global use is trustworthy and responsible. 

<br /><br />Summary: <div>
arXiv:2511.05901v1 Announce Type: new 
Abstract: The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NILC: Discovering New Intents with LLM-assisted Clustering</title>
<link>https://arxiv.org/abs/2511.05913</link>
<guid>https://arxiv.org/abs/2511.05913</guid>
<content:encoded><![CDATA[
<div> Keywords: New Intent Discovery, Clustering, Language Models, Semi-Supervised Learning, Text Embeddings <br />
Summary: <br />
The paper introduces a novel clustering framework, NILC, designed for effective New Intent Discovery (NID) in dialogue systems. Existing NID approaches typically use a cascaded architecture, but NILC adopts an iterative workflow to leverage feedback from both text embeddings and clustering stages. By incorporating large language models (LLMs), NILC enriches cluster centroids with contextual semantics and refines uncertain utterances for improved clustering. It also utilizes LLMs to rewrite and correct ambiguous or terse utterances, enhancing the performance of NID. In the semi-supervised setting, NILC employs non-trivial techniques such as seeding and soft must links to provide supervision signals for more accurate intent recognition. Experimental results demonstrate the superior performance of NILC over recent baselines across various datasets, showcasing its effectiveness in achieving significant improvements in NID tasks. <br /> <div>
arXiv:2511.05913v1 Announce Type: new 
Abstract: New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction</title>
<link>https://arxiv.org/abs/2511.05921</link>
<guid>https://arxiv.org/abs/2511.05921</guid>
<content:encoded><![CDATA[
<div> Keywords: Voice-controlled dialog systems, Intent Detection, Active Learning, Semi-supervised learning, Annotation cost reduction

Summary:
IDALC (Intent Detection and Active Learning based Correction) is introduced as a semi-supervised framework for voice-controlled dialog systems. It aims to detect user intents and rectify system-rejected utterances while minimizing human annotation. The framework surpasses baseline methods, achieving higher accuracy and macro-F1 improvement. Empirical findings on benchmark datasets show 5-10% higher accuracy and 4-8% improvement in macro-F1 compared to baseline methods. Notably, IDALC maintains the overall annotation cost at just 6-10% of the unlabelled data available. The framework can efficiently retrain agents with new intents from rejected queries over time, reducing the need for manual annotation. IDALC presents a cost-effective solution for improving the performance of voice-controlled dialog systems while minimizing human effort in annotation tasks.<br /><br />Summary: IDALC is a semi-supervised framework that detects intents and corrects rejected utterances in voice-controlled systems, achieving higher accuracy and macro-F1 improvement while minimizing annotation costs and manual effort. <div>
arXiv:2511.05921v1 Announce Type: new 
Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2511.05933</link>
<guid>https://arxiv.org/abs/2511.05933</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language model, hierarchical knowledge, structured prompting, internal activation analysis

Summary:
Reinforcement learning (RL) enhances language models' performance in recalling pure knowledge, especially in structured knowledge tasks like medical codes. The study shows that RL models outperform base and supervised fine-tuned models in recalling procedural paths within existing knowledge hierarchies. Structured prompting helps fine-tuned models bridge the performance gap, indicating that RL models excel in navigating knowledge hierarchies. While final-answer accuracy improves with prompting, RL models retain superior procedural path recall abilities. Internal activation analysis reveals that RL transforms how models traverse knowledge rather than knowledge representation itself. Factual representations maintain similarity between fine-tuned and RL models, but query representations diverge, highlighting RL's impact on knowledge traversal. This study challenges the notion that RL degrades memorized knowledge, instead showing its effectiveness in enhancing procedural skills in structured knowledge recall tasks. 

<br /><br />Summary: <div>
arXiv:2511.05933v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Recognition of Cognitive Distortions in Natural Language Texts</title>
<link>https://arxiv.org/abs/2511.05969</link>
<guid>https://arxiv.org/abs/2511.05969</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-factor classification, natural language texts, cognitive distortions, artificial intelligence, interpretable model

Summary:
This article introduces a novel approach to multi-factor classification of natural language texts using weighted structured patterns like N-grams, while considering the heterarchical relationships among them. The focus is on automating the detection of specific cognitive distortions in psychological care by employing an interpretable, robust, and transparent artificial intelligence model. The proposed recognition and learning algorithms enhance the existing state-of-the-art solutions in this domain. The effectiveness of the approach is validated on two publicly available datasets, showcasing significant improvements in F1 scores compared to existing literature. The optimization of hyper-parameters is conducted, and the code and models developed are made available for community use. This research contributes to advancing the field of automated cognitive distortion detection, offering a valuable tool for psychological care practitioners. 

<br /><br />Summary: <div>
arXiv:2511.05969v1 Announce Type: new 
Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Entropy in Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.05993</link>
<guid>https://arxiv.org/abs/2511.05993</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, verifiable rewards, language models, entropy dynamics, calibration

Summary: 
Reinforcement learning with verifiable rewards (RLVR) has been used to enhance the reasoning capabilities of large language models (LLMs). However, the collapse of entropy during RLVR training can lead to suboptimal convergence and hinder performance improvement. A comprehensive study on entropy dynamics in RLVR is lacking, prompting extensive experiments to be conducted. Factors such as off-policy updates, data diversity, and clipping thresholds impact model entropy. Tokens with positive advantages contribute most to entropy collapse, and adjusting loss weights of tokens can regulate model entropy effectively. The study reveals a correlation between model entropy, response diversity, calibration, and performance in various benchmarks. This research provides valuable insights into improving RLVR training of LLMs. 

<br /><br />Summary: <div>
arXiv:2511.05993v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</title>
<link>https://arxiv.org/abs/2511.06000</link>
<guid>https://arxiv.org/abs/2511.06000</guid>
<content:encoded><![CDATA[
<div> age-related information; language models; biomedical evidence synthesis; DemogSummary dataset; summarization-capable LLMs

Summary:
The study evaluates the retention of age-related information by state-of-the-art language models in generating abstractive summaries of biomedical studies. A novel age-stratified dataset, DemogSummary, is created to assess demographic distinctions in child, adult, and older adult populations. Three LLMs - Qwen, Longformer, and GPT-4.1 Nano - are evaluated using standard metrics and a new Demographic Salience Score. It is found that adult-focused summaries have the lowest demographic fidelity, and under-represented populations are more susceptible to hallucinations. The study highlights the limitations of current LLMs in maintaining faithful and unbiased summarization, emphasizing the necessity for fairness-aware evaluation frameworks and summarization pipelines in biomedical NLP.

Summary: <div>
arXiv:2511.06000v1 Announce Type: new 
Abstract: Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data</title>
<link>https://arxiv.org/abs/2511.06023</link>
<guid>https://arxiv.org/abs/2511.06023</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Implicit biases, Discrimination, Multi-dimensional, De-biasing

Summary:
This paper introduces a Multi-Reward Group Relative Policy Optimization (GRPO) framework to address the implicit biases and discriminatory tendencies found in Large Language Models (LLMs). By creating a synthetic dataset based on Chinese-context discrimination categories and training a reward model with multi-dimensional signals, the GRPO fine-tunes LLMs to optimize ethical dimensions such as fairness, neutrality, and linguistic quality. Experimental results show a significant reduction in bias intensity and an improvement in alignment with non-discriminatory standards while maintaining fluency and informativeness. The study demonstrates the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs, providing a replicable framework for cultural-contextual ethical alignment. 

<br /><br />Summary: <div>
arXiv:2511.06023v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts</title>
<link>https://arxiv.org/abs/2511.06048</link>
<guid>https://arxiv.org/abs/2511.06048</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, large language models, feature interpretation, interactive visualization, dimensionality reduction

Summary:
Sparse autoencoders (SAEs) have become a valuable tool for interpreting features in large language models by learning sparse directions. However, the vast number of extracted directions presents challenges for comprehensive exploration. Conventional embedding techniques like UMAP have limitations that can hinder accurate representation of global structure. In response, a focused exploration framework is proposed in this work, prioritizing curated concepts and their associated SAE features instead of attempting to visualize all features at once. An interactive visualization system combines topology-based visual encoding with dimensionality reduction, allowing users to examine both local and global relationships within selected features. This hybrid approach enhances understanding of SAE behavior through targeted and interpretable subsets, enabling deeper analysis of concept representation in latent space. <br /><br />Summary: <div>
arXiv:2511.06048v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework</title>
<link>https://arxiv.org/abs/2511.06051</link>
<guid>https://arxiv.org/abs/2511.06051</guid>
<content:encoded><![CDATA[
<div> Efficient, Hate Speech Detection, Real-time Deployment, LoRA, BERTweet

Summary:
Efficient hate speech detection systems are crucial for real-time deployment. A three-layer framework is proposed, combining rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. This approach achieves a 0.85 macro F1 score, comparable to state-of-the-art large language models like SafePhi but with a base model 100 times smaller. Dataset unification and optimized fine-tuning contribute to superior performance compared to traditional BERT-based methods with similar computational requirements. The system only requires 1.87M trainable parameters and trains in about 2 hours on a single T4 GPU, making it suitable for resource-constrained environments while maintaining competitive accuracy for real-world deployment. 

<br /><br />Summary: <div>
arXiv:2511.06051v1 Announce Type: new 
Abstract: This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning</title>
<link>https://arxiv.org/abs/2511.06057</link>
<guid>https://arxiv.org/abs/2511.06057</guid>
<content:encoded><![CDATA[
<div> framework, multimodal stance detection, dual-reasoning, modality contribution, context-aware<br />
Summary:<br />
The article introduces ReMoD, a framework for multimodal stance detection that incorporates dual-reasoning mechanisms. By combining intuitive reasoning with reflective reasoning, ReMoD dynamically weights the contribution of different modalities based on their expressive power. The intuitive stage utilizes experience pools to form an initial stance hypothesis, while the reflective stage refines this hypothesis by adjusting for modality biases and incorporating deeper semantic insights. Through continuous refinement during training and inference, ReMoD guides robust and context-aware stance decisions. Experimental results on the MMSD benchmark show that ReMoD outperforms baseline models and demonstrates strong generalization capabilities. <div>
arXiv:2511.06057v1 Announce Type: new 
Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework</title>
<link>https://arxiv.org/abs/2511.06067</link>
<guid>https://arxiv.org/abs/2511.06067</guid>
<content:encoded><![CDATA[
<div> Keywords: ArchCraft, hardware architectures, Verilog, RTL verification, ArchSynthBench

Summary:
ArchCraft is a framework designed to convert abstract architectural descriptions from academic papers into synthesizable Verilog projects for hardware reproduction. The framework utilizes formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into hardware-aware designs. ArchCraft generates RTL and testbench code, facilitating verification and debugging to report Power, Area, and Performance metrics. The proposed benchmark ArchSynthBench provides a comprehensive evaluation of hardware synthesis from architectural descriptions. Experimental results showcase the superiority of ArchCraft over direct generation methods and the VerilogCoder framework in paper understanding and code completion. Evaluations and physical implementations verify that the generated RTL code meets all timing constraints and performance metrics consistent with the original papers. <div>
arXiv:2511.06067v1 Announce Type: new 
Abstract: The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stemming Hallucination in Language Models Using a Licensing Oracle</title>
<link>https://arxiv.org/abs/2511.06073</link>
<guid>https://arxiv.org/abs/2511.06073</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, hallucinations, Licensing Oracle, structured knowledge graphs, fact-based domains<br />
Summary: <br />
- The study introduces the Licensing Oracle, an architectural solution, to prevent hallucinations in language models by validating against structured knowledge graphs.
- Unlike other methods, the Licensing Oracle integrates deterministic validation to ensure only factually accurate information is generated.
- Experiments comparing the Licensing Oracle with various methods showed it achieved perfect abstention precision and zero false answers, with 89.1% accuracy in factual responses.
- While methods like retrieval-augmented generation and fine-tuning improve performance, they do not eliminate hallucinations.
- The Licensing Oracle offers a reliable solution for fact-based domains and may pave the way for truth-constrained generation in future AI systems. <br /><br />Summary: <div>
arXiv:2511.06073v1 Announce Type: new 
Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuonAll: Muon Variant for Efficient Finetuning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.06086</link>
<guid>https://arxiv.org/abs/2511.06086</guid>
<content:encoded><![CDATA[
<div> Keywords: Muon optimizer, language models, finetuning, AdamW, open-source<br />
Summary:<br />
The article introduces MuonAll, an extension of the Muon optimizer designed for finetuning existing pretrained language models. MuonAll incorporates all parameters inside Muon by transforming them into 2D matrices. Extensive finetuning experiments were conducted on publicly available language models with sizes up to half a billion parameters, showcasing comparable performance to AdamW across various benchmarks. The study emphasizes the effectiveness of Muon and MuonAll as alternative optimizers in the field of language model finetuning. Additionally, the distributed implementations of Muon and MuonAll have been open-sourced and are accessible on GitHub at https://github.com/Saurabh750/optimizer. <div>
arXiv:2511.06086v1 Announce Type: new 
Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of retrieval-based QA on QUEST-LOFT</title>
<link>https://arxiv.org/abs/2511.06125</link>
<guid>https://arxiv.org/abs/2511.06125</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented generation, grounded QA, LOFT study, QUEST benchmark, structured output format<br />
<br />
Summary: 
The paper delves into the limitations of current retrieval-augmented generation (RAG) methods in handling questions with distributed information or complex reasoning. It highlights the findings from the LOFT study, showcasing the challenges faced by long-context language models, particularly with the QUEST benchmark. The analysis identifies key factors contributing to poor performance on QUEST-LOFT and presents updated results from comprehensive human evaluations. The study demonstrates that optimizing RAG with a structured output format incorporating reasoning and evidence, followed by answer re-verification, can significantly surpass long-context approaches in performance. <div>
arXiv:2511.06125v1 Announce Type: new 
Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.06146</link>
<guid>https://arxiv.org/abs/2511.06146</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial Reasoning, Vision-language models, Referring Expression Comprehension, Ambiguity, Negation

Summary:
Spatial Reasoning plays a crucial role in human cognition, challenging the latest Vision-Language Models (VLMs). Traditional analysis focuses on image captioning and visual question answering, but this study suggests using the Referring Expression Comprehension task to evaluate VLMs' spatial reasoning. This task reveals VLMs' performance in detecting ambiguous objects, understanding complex spatial expressions, and handling negation. By comparing task-specific architectures and large VLMs, the study exposes the strengths and weaknesses of these models in various spatial semantics categories. Despite facing difficulties in the task, the models exhibit different behaviors based on their underlying structures. This research sheds light on existing challenges and opportunities for future studies in enhancing VLMs' spatial reasoning capabilities.

<br /><br />Summary: 
- Spatial Reasoning is essential for human cognition and poses challenges for Vision-Language Models (VLMs).
- Referring Expression Comprehension task is proposed as an evaluation platform to assess VLMs' spatial reasoning abilities.
- The task highlights challenges in object detection ambiguity, complex spatial expressions, and negation understanding.
- Comparison of task-specific architectures and large VLMs exposes strengths and weaknesses in handling various spatial semantics categories.
- Despite facing difficulties, models exhibit different behaviors based on their structures, suggesting avenues for future research. <div>
arXiv:2511.06146v1 Announce Type: new 
Abstract: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering</title>
<link>https://arxiv.org/abs/2511.06183</link>
<guid>https://arxiv.org/abs/2511.06183</guid>
<content:encoded><![CDATA[
<div> Keywords: aspect-based summarization, personalized, targeted, BookAsSumQA, QA-based evaluation framework

Summary: Aspect-based summarization for books is challenging due to the lack of reference summaries for long texts. To address this, BookAsSumQA introduces a QA-based evaluation framework that generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summarization quality based on question-answering performance. Experimental results using BookAsSumQA indicate that LLM-based approaches exhibit higher accuracy on shorter texts, while RAG-based methods are more effective for longer documents, making them more practical for aspect-based book summarization. This framework allows for more personalized and targeted summaries by highlighting specific aspects of the text. <div>
arXiv:2511.06183v1 Announce Type: new 
Abstract: Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning</title>
<link>https://arxiv.org/abs/2511.06190</link>
<guid>https://arxiv.org/abs/2511.06190</guid>
<content:encoded><![CDATA[
<div> framework, routing, Large Language Models, inference costs, confidence scores<br />
Summary:<br />
The paper introduces STEER, a domain-agnostic framework for cost-efficient reasoning in Large Language Models (LLMs). By leveraging confidence scores from smaller models, STEER performs step-level routing between smaller and larger LLMs to reduce inference costs. Unlike existing methods, STEER does not rely on external models for routing and achieves competitive or enhanced accuracy across diverse benchmarks such as Mathematical Reasoning and Planning tasks. By utilizing model-internal confidence as a robust signal for routing, STEER offers a scalable approach for efficient deployment of LLMs. This approach helps to reduce inference costs by up to 48% while maintaining similar or improved accuracy compared to solely using larger models, showcasing the effectiveness of model-internal confidence in guiding efficient reasoning in LLMs. <br /> <div>
arXiv:2511.06190v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2511.06215</link>
<guid>https://arxiv.org/abs/2511.06215</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's Disease, language models, in-context learning, explicit knowledge, clinical reasoning

Summary:
Explicit Knowledge In-Context Learners (EK-ICL) is proposed as a framework to enhance Alzheimer's Disease detection using narrative transcripts. It integrates structured explicit knowledge to improve reasoning stability and task alignment in large language models (LLMs) under out-of-distribution and data-scarce conditions. EK-ICL incorporates confidence scores from small language models to ground predictions, parsing feature scores for improved demonstration selection, and label word replacement to address semantic misalignment. A parsing-based retrieval strategy and ensemble prediction are used to handle semantic homogeneity in AD transcripts. Extensive experiments showed that EK-ICL outperforms current fine-tuning and in-context learning approaches in AD detection. The study highlights the significance of explicit knowledge in clinical reasoning, emphasizing the importance of aligning label semantics with task context for optimal performance in low-resource settings.<br /><br />Summary: <div>
arXiv:2511.06215v1 Announce Type: new 
Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization</title>
<link>https://arxiv.org/abs/2511.06222</link>
<guid>https://arxiv.org/abs/2511.06222</guid>
<content:encoded><![CDATA[
<div> alignment paradigm, trustworthiness, helpfulness, Self-Priority Alignment, scalability
<br />
Summary: 
The article introduces a new alignment paradigm called priority alignment, focusing on ensuring trustworthiness before helpfulness in language model applications for high-stakes scenarios. It presents Self-Priority Alignment (SPA), an unsupervised framework that generates diverse responses, evaluates them, and refines them to meet trustworthy thresholds. SPA uses dual-criterion denoising to remove inconsistencies and control variance, constructing preference pairs and fine-tuning the model with an uncertainty-weighted alignment loss. Experimental results across various benchmarks demonstrate that SPA improves helpfulness while maintaining safety, outperforming strong baselines and preserving general capabilities. The approach provides a scalable and interpretable strategy for aligning language models in critical applications. 
<br /><br /> <div>
arXiv:2511.06222v1 Announce Type: new 
Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.06230</link>
<guid>https://arxiv.org/abs/2511.06230</guid>
<content:encoded><![CDATA[
<div> Keywords: discharge medication recommendation, CHIP 2025 Shared Task 2, Chinese EHR data, multi-label nature, large language model ensemble systems

Summary:
The paper discusses the CHIP 2025 Shared Task 2 competition focused on developing automated discharge medication recommendations using Chinese electronic health record (EHR) data. A high-quality dataset called CDrugRed was created for this task, challenging due to multi-label medication recommendations and diverse clinical text. Over 500 teams participated, with the top team showcasing the potential of advanced large language model ensembles. They achieved a Jaccard score of 0.5102 and an F1 score of 0.6267 on the final test set. While demonstrating promise in medication recommendation using language models in Chinese EHRs, the results also highlight the existing challenges. The post-evaluation phase for the competition is ongoing on the Tianchi platform. <br /><br />Summary: <div>
arXiv:2511.06230v1 Announce Type: new 
Abstract: Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy</title>
<link>https://arxiv.org/abs/2511.06234</link>
<guid>https://arxiv.org/abs/2511.06234</guid>
<content:encoded><![CDATA[
<div> negation, pre-trained models, natural language inference, dataset artifacts, data augmentation  
Summary:  
- Pre-trained models for natural language inference lack understanding of language nuances like negation, relying on dataset artifacts for high performance.  
- An investigation into an ELECTRA-small model fine-tuned on the SNLI dataset revealed struggles in accurately classifying negation-containing examples.  
- To address this issue, training data was augmented with contrast sets and adversarial examples emphasizing negation.  
- Results showed that this targeted data augmentation improved the model's accuracy on negation-containing examples without compromising overall performance.  
- The identified dataset artifact of the model's struggle with negation was successfully mitigated through the data augmentation technique.  

Summary: <div>
arXiv:2511.06234v1 Announce Type: new 
Abstract: Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSense:Making Large Language Models Proficient in Time-Series Analysis</title>
<link>https://arxiv.org/abs/2511.06344</link>
<guid>https://arxiv.org/abs/2511.06344</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series, text-temporal data integration, large language models, multimodal framework, temporal sense

Summary:
The study introduces the EvalTS benchmark consisting of 10 tasks to evaluate models incorporating text and temporal data. It addresses the issue of models biased towards textual cues by proposing TimeSense, a multimodal framework. TimeSense includes a Temporal Sense module to ground textual reasoning in time-series dynamics and uses coordinate-based positional embeddings for spatial understanding of time-series data. Experimental results show that TimeSense outperforms existing methods on complex multi-dimensional time-series reasoning tasks, achieving state-of-the-art performance across multiple tasks. The framework balances textual reasoning with a preserved temporal sense, enhancing the model's ability to analyze time-series data effectively. <br /><br />Summary: <div>
arXiv:2511.06344v1 Announce Type: new 
Abstract: In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.06391</link>
<guid>https://arxiv.org/abs/2511.06391</guid>
<content:encoded><![CDATA[
<div> Keywords: offensive content moderation, hate speech detection, implicit hate, cross-task transfer, HatePrototypes

Summary:<br />
This study examines the optimization of offensive content moderation models for different types of hateful messages. While existing benchmarks focus on explicit hate toward protected groups, implicit hate, including demeaning comparisons and subtle discriminatory language, is often overlooked. The researchers propose using HatePrototypes, class-level vector representations derived from language models, to enable cross-task transfer between explicit and implicit hate with as few as 50 examples per class. They demonstrate that these prototypes facilitate efficient hate speech detection without the need for repeated fine-tuning. Additionally, parameter-free early exiting with prototypes proves effective for both explicit and implicit hate. The code, prototype resources, and evaluation scripts have been released to support future research on efficient and transferable hate speech detection.<br />Summary: <div>
arXiv:2511.06391v1 Announce Type: new 
Abstract: Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss</title>
<link>https://arxiv.org/abs/2511.06402</link>
<guid>https://arxiv.org/abs/2511.06402</guid>
<content:encoded><![CDATA[
<div> Transformer-based framework, social media posts, sugar dating, class imbalance, content moderation<br />
Summary:<br /> 
- Sugar dating-related content on social media poses societal and regulatory concerns,<br />
- Detection is challenging due to euphemisms and class imbalance,<br />
- SugarTextNet framework with pretrained transformer encoder and attention-based cue extractor,<br />
- Introduces Context-Aware Focal Loss for minority-class detection,<br />
- Outperforms traditional models and large language models in detecting sensitive content. <br /> <div>
arXiv:2511.06402v1 Announce Type: new 
Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset</title>
<link>https://arxiv.org/abs/2511.06418</link>
<guid>https://arxiv.org/abs/2511.06418</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, drug development, personalized medicine, reasoning tasks, counterfactuals

Summary: 
Large language models (LLMs) are being increasingly utilized in the fields of drug development and personalized medicine. A new dataset has been introduced to evaluate LLMs on their factual knowledge of drug mechanisms and their ability to reason about them in novel situations, presented as counterfactuals. Results show that the o4-mini model performs better than other models from OpenAI, with the Qwen3-4B-thinking model closely matching its performance. The open-world setting for reasoning tasks, where models must recall relevant knowledge, proves to be more challenging than the closed-world setting. Additionally, counterfactuals affecting internal links in the reasoning chain are more difficult than those affecting links from the drug mentioned in the prompt. Overall, these findings suggest the importance of LLMs in drug development and personalized medicine, highlighting the need for models to demonstrate factual knowledge and deep understanding of drug mechanisms. 

<br /><br />Summary: <div>
arXiv:2511.06418v1 Announce Type: new 
Abstract: Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop</title>
<link>https://arxiv.org/abs/2511.06427</link>
<guid>https://arxiv.org/abs/2511.06427</guid>
<content:encoded><![CDATA[
<div> Keywords: Metaphors, Dutch language data, Cancer patients, Large language models, HealthQuote.NL 

Summary:<br /><br />
This article focuses on the extraction of metaphors used by cancer patients in Dutch language data to improve healthcare communication. Through analyzing patient storytelling interviews and online forum data, including posts and comments, researchers explore the performance of large language models (LLMs) using various prompting strategies. With a human-in-the-loop setup, the extracted metaphors are verified and compiled into a corpus named HealthQuote.NL. The goal is to enhance patient care, shared decision making, communication between patients and clinicians, and patient health literacy. The extracted metaphors can also contribute to designing personalized care pathways. The prompts and resources related to this research are shared on GitHub for further exploration and implementation. <div>
arXiv:2511.06427v1 Announce Type: new 
Abstract: Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models</title>
<link>https://arxiv.org/abs/2511.06441</link>
<guid>https://arxiv.org/abs/2511.06441</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, large language models, multimodal queries, routing network, efficient vision tasks

Summary: 
AI models are evolving beyond text and increasingly powering vision, audio, and document understanding. While large language models (LLMs) are effective, their high inference costs present challenges for real-time deployment. In contrast, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. To address this, a unified framework has been introduced that intelligently routes queries to the most suitable expert model using a learned routing network. For vision tasks, a two-stage open-source pipeline is employed, optimized for efficiency by incorporating efficient classical vision components where they excel. Performance on benchmarks such as MMLU and VQA matches or exceeds monolithic LLM systems while reducing reliance on costly models by over 67%. The framework's extensible, multi-agent orchestration enables the delivery of high-quality, resource-efficient AI at scale. 

<br /><br />Summary: <div>
arXiv:2511.06441v1 Announce Type: new 
Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention</title>
<link>https://arxiv.org/abs/2511.06446</link>
<guid>https://arxiv.org/abs/2511.06446</guid>
<content:encoded><![CDATA[
<div> knowledge bases, language models, retrieval, structured knowledge, attention<br />
Summary:<br />
The paper introduces SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases into large language models. SR-KI encodes KBs into key-value pairs and injects them into LLMs' KV cache for efficient retrieval. It employs a two-stage training paradigm, incorporating a dedicated retrieval layer within the LLM and applying an attention-based loss for supervision. Unlike traditional methods, SR-KI performs retrieval entirely within the model's latent space, allowing for dynamic knowledge updates. Experimental results show that SR-KI enables the integration of up to 40K KBs into a 7B LLM while maintaining strong retrieval performance. It achieves high Recall@10 rates and up to 99.75% compression of injected KBs, demonstrating its effectiveness in question answering and KB ID generation tasks. <div>
arXiv:2511.06446v1 Announce Type: new 
Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2511.06497</link>
<guid>https://arxiv.org/abs/2511.06497</guid>
<content:encoded><![CDATA[
<div> Realignment, cross-lingual transfer, multilingual language models, low-resource languages, parallel data <br />
<br />
Summary: Realignment is a promising strategy for improving cross-lingual transfer in multilingual language models. However, its effectiveness varies, especially for low-resource languages (LRLs). This study explores whether using strategically selected language subsets can provide comparable or even superior results compared to full multilingual alignment. The experiments show that realignment can greatly benefit LRLs and that carefully chosen language subsets can match or even outperform full alignment, particularly for unseen LRLs. This suggests that realignment does not always require all available languages, reducing the need for extensive data collection. Strategic language selection can lead to efficient and robust realignment, offering a more feasible approach for enhancing cross-lingual transfer in multilingual language models. <br /> <div>
arXiv:2511.06497v1 Announce Type: new 
Abstract: Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations</title>
<link>https://arxiv.org/abs/2511.06516</link>
<guid>https://arxiv.org/abs/2511.06516</guid>
<content:encoded><![CDATA[
<div> quantization, Large Language Models, task-aware, hidden representations, precision<br />
<br />
Summary: <br />
Large Language Models (LLMs) have shown excellent performance on various tasks, but their large size can lead to inefficiencies in memory and latency for applications requiring limited capabilities. This work introduces two new task-aware post-training quantization (PTQ) methods, Task-Aware Quantization (TAQ) and TAQO, which leverage task-specific signals encoded in hidden representations to guide the quantization process. By identifying task-relevant layers and preserving their precision while aggressively quantizing others, TAQ and TAQO achieve stable task sensitivity profiles and efficient task-specialized models. Experimental results demonstrate that TAQ and TAQO outperform existing baselines on various models, with TAQ leading on Phi-4 and TAQO leading on Llama-3.1, Qwen3, and Qwen2.5. For example, TAQ achieves significantly higher accuracy on Phi-4 compared to Activation-aware Weight Quantization (AWQ), while maintaining performance close to the original accuracy at lower average precision levels. <div>
arXiv:2511.06516v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement</title>
<link>https://arxiv.org/abs/2511.06530</link>
<guid>https://arxiv.org/abs/2511.06530</guid>
<content:encoded><![CDATA[
<div> Framework, Question-Answer, Data refinement, Large Language Models, Dataset quality  
Summary:  
RefineLab is a novel framework that uses Large Language Models to automatically refine Question-Answer datasets, addressing quality issues such as domain coverage gaps, difficulty imbalances, and factual inconsistencies. By setting target quality attributes and a token budget constraint, RefineLab performs selective edits on the dataset to improve overall quality while being resource-efficient. The framework operates as a constrained optimization problem, selecting optimal refinement strategies such as rephrasing or distractor replacement for each QA sample. Experimental results show that RefineLab significantly enhances dataset quality in terms of coverage, difficulty alignment, factual fidelity, and distractor quality, reducing discrepancies from expert-crafted datasets. This approach offers a scalable and customizable solution for reproducible dataset design, with implications for improving Large Language Model evaluation.  
<br /><br />Summary: <div>
arXiv:2511.06530v1 Announce Type: new 
Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages</title>
<link>https://arxiv.org/abs/2511.06531</link>
<guid>https://arxiv.org/abs/2511.06531</guid>
<content:encoded><![CDATA[
<div> languages, Nigeria, natural language processing, machine translation, topic classification
Summary: 
- Nigeria, with over 200 million people and 500 languages, lacks NLP research in languages beyond Hausa, Igbo, Nigerian-Pidgin, and Yoruba.
- A new dataset, ibom, introduces Anaang, Efik, Ibibio, and Oro languages for machine translation and topic classification.
- These languages are not represented in major benchmarks like Google Translate or Flores-200, highlighting the need for their inclusion.
- Evaluation shows current language models perform poorly in machine translation for these languages but improve steadily in topic classification with more shots.
<br /><br />Summary: <div>
arXiv:2511.06531v1 Announce Type: new 
Abstract: Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep2Text: Decoding Full Text from a Single LLM Token Representation</title>
<link>https://arxiv.org/abs/2511.06571</link>
<guid>https://arxiv.org/abs/2511.06571</guid>
<content:encoded><![CDATA[
<div> Recovering Input Text, Large Language Models (LLMs), Rep2Text, Information Bottleneck Effect, Generalization Capability
<br />
Summary:
Rep2Text introduces a new framework for decoding full text from last-token representations in large language models (LLMs). Through a trainable adapter and decoding language model, it can recover over half of the information in 16-token sequences while maintaining semantic integrity. There is an observed information bottleneck effect where longer sequences show decreased token-level recovery but maintain semantic coherence. The framework shows robust generalization to out-of-distribution medical data, indicating its potential applicability across diverse domains. <div>
arXiv:2511.06571v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabRAG: Tabular Document Retrieval via Structured Language Representations</title>
<link>https://arxiv.org/abs/2511.06582</link>
<guid>https://arxiv.org/abs/2511.06582</guid>
<content:encoded><![CDATA[
arXiv:2511.06582v1 Announce Type: new 
Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making</title>
<link>https://arxiv.org/abs/2511.06592</link>
<guid>https://arxiv.org/abs/2511.06592</guid>
<content:encoded><![CDATA[
arXiv:2511.06592v1 Announce Type: new 
Abstract: As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes</title>
<link>https://arxiv.org/abs/2511.06601</link>
<guid>https://arxiv.org/abs/2511.06601</guid>
<content:encoded><![CDATA[
arXiv:2511.06601v1 Announce Type: new 
Abstract: Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models</title>
<link>https://arxiv.org/abs/2511.06676</link>
<guid>https://arxiv.org/abs/2511.06676</guid>
<content:encoded><![CDATA[
arXiv:2511.06676v1 Announce Type: new 
Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation</title>
<link>https://arxiv.org/abs/2511.06680</link>
<guid>https://arxiv.org/abs/2511.06680</guid>
<content:encoded><![CDATA[
arXiv:2511.06680v1 Announce Type: new 
Abstract: Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title>
<link>https://arxiv.org/abs/2511.06682</link>
<guid>https://arxiv.org/abs/2511.06682</guid>
<content:encoded><![CDATA[
arXiv:2511.06682v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content</title>
<link>https://arxiv.org/abs/2511.06708</link>
<guid>https://arxiv.org/abs/2511.06708</guid>
<content:encoded><![CDATA[
arXiv:2511.06708v1 Announce Type: new 
Abstract: The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Na\"ive Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights</title>
<link>https://arxiv.org/abs/2511.06738</link>
<guid>https://arxiv.org/abs/2511.06738</guid>
<content:encoded><![CDATA[
arXiv:2511.06738v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensitivity of Small Language Models to Fine-tuning Data Contamination</title>
<link>https://arxiv.org/abs/2511.06763</link>
<guid>https://arxiv.org/abs/2511.06763</guid>
<content:encoded><![CDATA[
arXiv:2511.06763v1 Announce Type: new 
Abstract: Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\textit{capability curse}" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces</title>
<link>https://arxiv.org/abs/2511.06778</link>
<guid>https://arxiv.org/abs/2511.06778</guid>
<content:encoded><![CDATA[
arXiv:2511.06778v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Focus: Focal Attention for Selective and Scalable Transformers</title>
<link>https://arxiv.org/abs/2511.06818</link>
<guid>https://arxiv.org/abs/2511.06818</guid>
<content:encoded><![CDATA[
arXiv:2511.06818v1 Announce Type: new 
Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2511.06826</link>
<guid>https://arxiv.org/abs/2511.06826</guid>
<content:encoded><![CDATA[
arXiv:2511.06826v1 Announce Type: new 
Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition</title>
<link>https://arxiv.org/abs/2511.06860</link>
<guid>https://arxiv.org/abs/2511.06860</guid>
<content:encoded><![CDATA[
arXiv:2511.06860v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inclusion of Role into Named Entity Recognition and Ranking</title>
<link>https://arxiv.org/abs/2511.06886</link>
<guid>https://arxiv.org/abs/2511.06886</guid>
<content:encoded><![CDATA[
arXiv:2511.06886v1 Announce Type: new 
Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers</title>
<link>https://arxiv.org/abs/2511.06890</link>
<guid>https://arxiv.org/abs/2511.06890</guid>
<content:encoded><![CDATA[
arXiv:2511.06890v1 Announce Type: new 
Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation</title>
<link>https://arxiv.org/abs/2511.06899</link>
<guid>https://arxiv.org/abs/2511.06899</guid>
<content:encoded><![CDATA[
arXiv:2511.06899v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection</title>
<link>https://arxiv.org/abs/2511.06942</link>
<guid>https://arxiv.org/abs/2511.06942</guid>
<content:encoded><![CDATA[
arXiv:2511.06942v1 Announce Type: new 
Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs</title>
<link>https://arxiv.org/abs/2511.07001</link>
<guid>https://arxiv.org/abs/2511.07001</guid>
<content:encoded><![CDATA[
arXiv:2511.07001v1 Announce Type: new 
Abstract: Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Circuit Interpretation via Probe Prompting</title>
<link>https://arxiv.org/abs/2511.07002</link>
<guid>https://arxiv.org/abs/2511.07002</guid>
<content:encoded><![CDATA[
arXiv:2511.07002v1 Announce Type: new 
Abstract: Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs</title>
<link>https://arxiv.org/abs/2511.07003</link>
<guid>https://arxiv.org/abs/2511.07003</guid>
<content:encoded><![CDATA[
arXiv:2511.07003v1 Announce Type: new 
Abstract: Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale \textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2511.07010</link>
<guid>https://arxiv.org/abs/2511.07010</guid>
<content:encoded><![CDATA[
arXiv:2511.07010v1 Announce Type: new 
Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity</title>
<link>https://arxiv.org/abs/2511.07011</link>
<guid>https://arxiv.org/abs/2511.07011</guid>
<content:encoded><![CDATA[
arXiv:2511.07011v1 Announce Type: new 
Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks</title>
<link>https://arxiv.org/abs/2511.07025</link>
<guid>https://arxiv.org/abs/2511.07025</guid>
<content:encoded><![CDATA[
arXiv:2511.07025v1 Announce Type: new 
Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data</title>
<link>https://arxiv.org/abs/2511.07044</link>
<guid>https://arxiv.org/abs/2511.07044</guid>
<content:encoded><![CDATA[
arXiv:2511.07044v1 Announce Type: new 
Abstract: Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction</title>
<link>https://arxiv.org/abs/2511.07055</link>
<guid>https://arxiv.org/abs/2511.07055</guid>
<content:encoded><![CDATA[
arXiv:2511.07055v1 Announce Type: new 
Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.07065</link>
<guid>https://arxiv.org/abs/2511.07065</guid>
<content:encoded><![CDATA[
arXiv:2511.07065v1 Announce Type: new 
Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-Aware Data Selection for Efficient LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2511.07074</link>
<guid>https://arxiv.org/abs/2511.07074</guid>
<content:encoded><![CDATA[
arXiv:2511.07074v1 Announce Type: new 
Abstract: Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoBang: Detecting Emotion From Bengali Texts</title>
<link>https://arxiv.org/abs/2511.07077</link>
<guid>https://arxiv.org/abs/2511.07077</guid>
<content:encoded><![CDATA[
arXiv:2511.07077v1 Announce Type: new 
Abstract: Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora</title>
<link>https://arxiv.org/abs/2511.07080</link>
<guid>https://arxiv.org/abs/2511.07080</guid>
<content:encoded><![CDATA[
arXiv:2511.07080v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Agents Helps but Adversarial Robustness Gap Persists</title>
<link>https://arxiv.org/abs/2511.07112</link>
<guid>https://arxiv.org/abs/2511.07112</guid>
<content:encoded><![CDATA[
arXiv:2511.07112v1 Announce Type: new 
Abstract: When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought</title>
<link>https://arxiv.org/abs/2511.07124</link>
<guid>https://arxiv.org/abs/2511.07124</guid>
<content:encoded><![CDATA[
arXiv:2511.07124v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging</title>
<link>https://arxiv.org/abs/2511.07129</link>
<guid>https://arxiv.org/abs/2511.07129</guid>
<content:encoded><![CDATA[
arXiv:2511.07129v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2511.07148</link>
<guid>https://arxiv.org/abs/2511.07148</guid>
<content:encoded><![CDATA[
arXiv:2511.07148v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?</title>
<link>https://arxiv.org/abs/2511.07162</link>
<guid>https://arxiv.org/abs/2511.07162</guid>
<content:encoded><![CDATA[
arXiv:2511.07162v1 Announce Type: new 
Abstract: The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning</title>
<link>https://arxiv.org/abs/2511.07166</link>
<guid>https://arxiv.org/abs/2511.07166</guid>
<content:encoded><![CDATA[
arXiv:2511.07166v1 Announce Type: new 
Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models</title>
<link>https://arxiv.org/abs/2511.07193</link>
<guid>https://arxiv.org/abs/2511.07193</guid>
<content:encoded><![CDATA[
arXiv:2511.07193v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discourse Graph Guided Document Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2511.07230</link>
<guid>https://arxiv.org/abs/2511.07230</guid>
<content:encoded><![CDATA[
arXiv:2511.07230v1 Announce Type: new 
Abstract: Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Is the Story About? Protagonist Entity Recognition in News</title>
<link>https://arxiv.org/abs/2511.07296</link>
<guid>https://arxiv.org/abs/2511.07296</guid>
<content:encoded><![CDATA[
arXiv:2511.07296v1 Announce Type: new 
Abstract: News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification</title>
<link>https://arxiv.org/abs/2511.07304</link>
<guid>https://arxiv.org/abs/2511.07304</guid>
<content:encoded><![CDATA[
arXiv:2511.07304v1 Announce Type: new 
Abstract: This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the "Bangla Multi-task Hate Speech Identification" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding</title>
<link>https://arxiv.org/abs/2511.07311</link>
<guid>https://arxiv.org/abs/2511.07311</guid>
<content:encoded><![CDATA[
arXiv:2511.07311v1 Announce Type: new 
Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments</title>
<link>https://arxiv.org/abs/2511.07317</link>
<guid>https://arxiv.org/abs/2511.07317</guid>
<content:encoded><![CDATA[
arXiv:2511.07317v1 Announce Type: new 
Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2511.07318</link>
<guid>https://arxiv.org/abs/2511.07318</guid>
<content:encoded><![CDATA[
arXiv:2511.07318v1 Announce Type: new 
Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation</title>
<link>https://arxiv.org/abs/2511.07322</link>
<guid>https://arxiv.org/abs/2511.07322</guid>
<content:encoded><![CDATA[
arXiv:2511.07322v1 Announce Type: new 
Abstract: While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains</title>
<link>https://arxiv.org/abs/2511.07380</link>
<guid>https://arxiv.org/abs/2511.07380</guid>
<content:encoded><![CDATA[
arXiv:2511.07380v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \textbf{\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a \textbf{10.9x and 5.7x improvement} over the domain-only setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation</title>
<link>https://arxiv.org/abs/2511.07382</link>
<guid>https://arxiv.org/abs/2511.07382</guid>
<content:encoded><![CDATA[
arXiv:2511.07382v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation in Bangla". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team "Retriv" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence</title>
<link>https://arxiv.org/abs/2511.07384</link>
<guid>https://arxiv.org/abs/2511.07384</guid>
<content:encoded><![CDATA[
arXiv:2511.07384v1 Announce Type: new 
Abstract: Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction</title>
<link>https://arxiv.org/abs/2511.07392</link>
<guid>https://arxiv.org/abs/2511.07392</guid>
<content:encoded><![CDATA[
arXiv:2511.07392v1 Announce Type: new 
Abstract: In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConvFill: Model Collaboration for Responsive Conversational Voice Agents</title>
<link>https://arxiv.org/abs/2511.07397</link>
<guid>https://arxiv.org/abs/2511.07397</guid>
<content:encoded><![CDATA[
arXiv:2511.07397v1 Announce Type: new 
Abstract: Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations</title>
<link>https://arxiv.org/abs/2511.07405</link>
<guid>https://arxiv.org/abs/2511.07405</guid>
<content:encoded><![CDATA[
arXiv:2511.07405v1 Announce Type: new 
Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of High-Performance GPU Resources in Large Language Model Based Radiology Imaging Diagnosis</title>
<link>https://arxiv.org/abs/2509.16328</link>
<guid>https://arxiv.org/abs/2509.16328</guid>
<content:encoded><![CDATA[
arXiv:2509.16328v2 Announce Type: cross 
Abstract: Large-language models (LLMs) are rapidly being applied to radiology, enabling automated image interpretation and report generation tasks. Their deployment in clinical practice requires both high diagnostic accuracy and low inference latency, which in turn demands powerful hardware. High-performance graphical processing units (GPUs) provide the necessary compute and memory throughput to run large LLMs on imaging data. We review modern GPU architectures (e.g. NVIDIA A100/H100, AMD Instinct MI250X/MI300) and key performance metrics of floating-point throughput, memory bandwidth, VRAM capacity. We show how these hardware capabilities affect radiology tasks: for example, generating reports or detecting findings on CheXpert and MIMIC-CXR images is computationally intensive and benefits from GPU parallelism and tensor-core acceleration. Empirical studies indicate that using appropriate GPU resources can reduce inference time and improve throughput. We discuss practical challenges including privacy, deployment, cost, power and optimization strategies: mixed-precision, quantization, compression, and multi-GPU scaling. Finally, we anticipate that next-generation features (8-bit tensor cores, enhanced interconnect) will further enable on-premise and federated radiology AI. Advancing GPU infrastructure is essential for safe, efficient LLM-based radiology diagnostics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Oscar-Nominated Screenplays with Sentence Embeddings</title>
<link>https://arxiv.org/abs/2511.05500</link>
<guid>https://arxiv.org/abs/2511.05500</guid>
<content:encoded><![CDATA[
arXiv:2511.05500v1 Announce Type: cross 
Abstract: Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factual and Musical Evaluation Metrics for Music Language Models</title>
<link>https://arxiv.org/abs/2511.05550</link>
<guid>https://arxiv.org/abs/2511.05550</guid>
<content:encoded><![CDATA[
arXiv:2511.05550v1 Announce Type: cross 
Abstract: Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction</title>
<link>https://arxiv.org/abs/2511.05577</link>
<guid>https://arxiv.org/abs/2511.05577</guid>
<content:encoded><![CDATA[
arXiv:2511.05577v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximating the Mathematical Structure of Psychodynamics</title>
<link>https://arxiv.org/abs/2511.05580</link>
<guid>https://arxiv.org/abs/2511.05580</guid>
<content:encoded><![CDATA[
arXiv:2511.05580v1 Announce Type: cross 
Abstract: The complexity of human cognition has meant that psychology makes more use of theory and conceptual models than perhaps any other biomedical field. To enable precise quantitative study of the full breadth of phenomena in psychological and psychiatric medicine as well as cognitive aspects of AI safety, there is a need for a mathematical formulation which is both mathematically precise and equally accessible to experts from numerous fields. In this paper we formalize human psychodynamics via the diagrammatic framework of process theory, describe its key properties, and explain the links between a diagrammatic representation and central concepts in analysis of cognitive processes in contexts such as psychotherapy, neurotechnology, AI alignment, AI agent representation of individuals in autonomous negotiations, developing human-like AI systems, and other aspects of AI safety.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Representation Sharpening Framework for Zero Shot Dense Retrieval</title>
<link>https://arxiv.org/abs/2511.05684</link>
<guid>https://arxiv.org/abs/2511.05684</guid>
<content:encoded><![CDATA[
arXiv:2511.05684v1 Announce Type: cross 
Abstract: Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</title>
<link>https://arxiv.org/abs/2511.05704</link>
<guid>https://arxiv.org/abs/2511.05704</guid>
<content:encoded><![CDATA[
arXiv:2511.05704v1 Announce Type: cross 
Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale</title>
<link>https://arxiv.org/abs/2511.05705</link>
<guid>https://arxiv.org/abs/2511.05705</guid>
<content:encoded><![CDATA[
arXiv:2511.05705v1 Announce Type: cross 
Abstract: Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persian Musical Instruments Classification Using Polyphonic Data Augmentation</title>
<link>https://arxiv.org/abs/2511.05717</link>
<guid>https://arxiv.org/abs/2511.05717</guid>
<content:encoded><![CDATA[
arXiv:2511.05717v1 Announce Type: cross 
Abstract: Musical instrument classification is essential for music information retrieval (MIR) and generative music systems. However, research on non-Western traditions, particularly Persian music, remains limited. We address this gap by introducing a new dataset of isolated recordings covering seven traditional Persian instruments, two common but originally non-Persian instruments (i.e., violin, piano), and vocals. We propose a culturally informed data augmentation strategy that generates realistic polyphonic mixtures from monophonic samples. Using the MERT model (Music undERstanding with large-scale self-supervised Training) with a classification head, we evaluate our approach with out-of-distribution data which was obtained by manually labeling segments of traditional songs. On real-world polyphonic Persian music, the proposed method yielded the best ROC-AUC (0.795), highlighting complementary benefits of tonal and temporal coherence. These results demonstrate the effectiveness of culturally grounded augmentation for robust Persian instrument recognition and provide a foundation for culturally inclusive MIR and diverse music generation systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs</title>
<link>https://arxiv.org/abs/2511.05766</link>
<guid>https://arxiv.org/abs/2511.05766</guid>
<content:encoded><![CDATA[
arXiv:2511.05766v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis</title>
<link>https://arxiv.org/abs/2511.05810</link>
<guid>https://arxiv.org/abs/2511.05810</guid>
<content:encoded><![CDATA[
arXiv:2511.05810v1 Announce Type: cross 
Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?</title>
<link>https://arxiv.org/abs/2511.05867</link>
<guid>https://arxiv.org/abs/2511.05867</guid>
<content:encoded><![CDATA[
arXiv:2511.05867v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate strong capabilities in solving complex tasks when integrated with external tools. The Model Context Protocol (MCP) has become a standard interface for enabling such tool-based interactions. However, these interactions introduce substantial security concerns, particularly when the MCP server is compromised or untrustworthy. While prior benchmarks primarily focus on prompt injection attacks or analyze the vulnerabilities of LLM MCP interaction trajectories, limited attention has been given to the underlying system logs associated with malicious MCP servers. To address this gap, we present the first synthetic benchmark for evaluating LLMs ability to identify security risks from system logs. We define nine categories of MCP server risks and generate 1,800 synthetic system logs using ten state-of-the-art LLMs. These logs are embedded in the return values of 243 curated MCP servers, yielding a dataset of 2,421 chat histories for training and 471 queries for evaluation. Our pilot experiments reveal that smaller models often fail to detect risky system logs, leading to high false negatives. While models trained with supervised fine-tuning (SFT) tend to over-flag benign logs, resulting in elevated false positives, Reinforcement Learning from Verifiable Reward (RLVR) offers a better precision-recall balance. In particular, after training with Group Relative Policy Optimization (GRPO), Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing large remote model by 9 percentage points. Fine-grained, per-category analysis further underscores the effectiveness of reinforcement learning in enhancing LLM safety within the MCP framework. Code and data are available at: https://github.com/PorUna-byte/MCP-Guard/tree/master
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation</title>
<link>https://arxiv.org/abs/2511.05903</link>
<guid>https://arxiv.org/abs/2511.05903</guid>
<content:encoded><![CDATA[
arXiv:2511.05903v1 Announce Type: cross 
Abstract: User simulation is important for developing and evaluating human-centered AI, yet current student simulation in educational applications has significant limitations. Existing approaches focus on single learning experiences and do not account for students' gradual knowledge construction and evolving skill sets. Moreover, large language models are optimized to produce direct and accurate responses, making it challenging to represent the incomplete understanding and developmental constraints that characterize real learners. In this paper, we introduce a novel framework for memory-based student simulation that incorporates developmental trajectories through a hierarchical memory mechanism with structured knowledge representation. The framework also integrates metacognitive processes and personality traits to enrich the individual learner profiling, through dynamical consolidation of both cognitive development and personal learning characteristics. In practice, we implement a curriculum-aligned simulator grounded on the Next Generation Science Standards. Experimental results show that our approach can effectively reflect the gradual nature of knowledge development and the characteristic difficulties students face, providing a more accurate representation of learning processes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs</title>
<link>https://arxiv.org/abs/2511.05919</link>
<guid>https://arxiv.org/abs/2511.05919</guid>
<content:encoded><![CDATA[
arXiv:2511.05919v1 Announce Type: cross 
Abstract: LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to "victim" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScRPO: From Errors to Insights</title>
<link>https://arxiv.org/abs/2511.06065</link>
<guid>https://arxiv.org/abs/2511.06065</guid>
<content:encoded><![CDATA[
arXiv:2511.06065v1 Announce Type: cross 
Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathemati- cal problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collect- ing incorrect answers along with their cor- responding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous an- swers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH- 500, GSM8k, using Deepseek-Distill-Qwen- 1.5B and Deepseek-Distill-Qwen-7B. The ex- perimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way to- ward more reliable and capable AI systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI</title>
<link>https://arxiv.org/abs/2511.06078</link>
<guid>https://arxiv.org/abs/2511.06078</guid>
<content:encoded><![CDATA[
arXiv:2511.06078v1 Announce Type: cross 
Abstract: Simulated Students offer a valuable methodological framework for evaluating pedagogical approaches and modelling diverse learner profiles, tasks which are otherwise challenging to undertake systematically in real-world settings. Recent research has increasingly focused on developing such simulated agents to capture a range of learning styles, cognitive development pathways, and social behaviours. Among contemporary simulation techniques, the integration of large language models (LLMs) into educational research has emerged as a particularly versatile and scalable paradigm. LLMs afford a high degree of linguistic realism and behavioural adaptability, enabling agents to approximate cognitive processes and engage in contextually appropriate pedagogical dialogues. This paper presents a thematic review of empirical and methodological studies utilising LLMs to simulate student behaviour across educational environments. We synthesise current evidence on the capacity of LLM-based agents to emulate learner archetypes, respond to instructional inputs, and interact within multi-agent classroom scenarios. Furthermore, we examine the implications of such systems for curriculum development, instructional evaluation, and teacher training. While LLMs surpass rule-based systems in natural language generation and situational flexibility, ongoing concerns persist regarding algorithmic bias, evaluation reliability, and alignment with educational objectives. The review identifies existing technological and methodological gaps and proposes future research directions for integrating generative AI into adaptive learning systems and instructional design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Web Agents with Synthetic Supervision</title>
<link>https://arxiv.org/abs/2511.06101</link>
<guid>https://arxiv.org/abs/2511.06101</guid>
<content:encoded><![CDATA[
arXiv:2511.06101v1 Announce Type: cross 
Abstract: Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Develop Novel Social Biases Through Adaptive Exploration</title>
<link>https://arxiv.org/abs/2511.06148</link>
<guid>https://arxiv.org/abs/2511.06148</guid>
<content:encoded><![CDATA[
arXiv:2511.06148v1 Announce Type: cross 
Abstract: As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles</title>
<link>https://arxiv.org/abs/2511.06160</link>
<guid>https://arxiv.org/abs/2511.06160</guid>
<content:encoded><![CDATA[
arXiv:2511.06160v1 Announce Type: cross 
Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting</title>
<link>https://arxiv.org/abs/2511.06197</link>
<guid>https://arxiv.org/abs/2511.06197</guid>
<content:encoded><![CDATA[
arXiv:2511.06197v1 Announce Type: cross 
Abstract: The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads</title>
<link>https://arxiv.org/abs/2511.06209</link>
<guid>https://arxiv.org/abs/2511.06209</guid>
<content:encoded><![CDATA[
arXiv:2511.06209v1 Announce Type: cross 
Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B</title>
<link>https://arxiv.org/abs/2511.06221</link>
<guid>https://arxiv.org/abs/2511.06221</guid>
<content:encoded><![CDATA[
arXiv:2511.06221v1 Announce Type: cross 
Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixtures of SubExperts for Large Language Continual Learning</title>
<link>https://arxiv.org/abs/2511.06237</link>
<guid>https://arxiv.org/abs/2511.06237</guid>
<content:encoded><![CDATA[
arXiv:2511.06237v1 Announce Type: cross 
Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective</title>
<link>https://arxiv.org/abs/2511.06284</link>
<guid>https://arxiv.org/abs/2511.06284</guid>
<content:encoded><![CDATA[
arXiv:2511.06284v1 Announce Type: cross 
Abstract: Multimodal Misinformation Detection (MMD) refers to the task of detecting social media posts involving misinformation, where the post often contains text and image modalities. However, by observing the MMD posts, we hold that the text modality may be much more informative than the image modality because the text generally describes the whole event/story of the current post but the image often presents partial scenes only. Our preliminary empirical results indicate that the image modality exactly contributes less to MMD. Upon this idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that each text can be divided into several segments, and each text segment describes a partial scene that can be presented by an image. Accordingly, we split the text into a sequence of segments, and feed these segments into a pre-trained text-to-image generator to augment a sequence of images. We further incorporate two auxiliary objectives concerning text-image and image-label mutual information, and further post-train the generator over an auxiliary text-to-image generation benchmark dataset. Additionally, we propose a graph structure by defining three heuristic relationships between images, and use a graph neural network to generate the fused features. Extensive empirical results validate the effectiveness of RETSIMD.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction</title>
<link>https://arxiv.org/abs/2511.06288</link>
<guid>https://arxiv.org/abs/2511.06288</guid>
<content:encoded><![CDATA[
arXiv:2511.06288v1 Announce Type: cross 
Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on visual cues from the target speaker. However, humans also leverage linguistic knowledge, such as syntactic constraints, next word prediction, and prior knowledge of conversation, to extract target speech. Inspired by this observation, we propose ELEGANCE, a novel framework that incorporates linguistic knowledge from large language models (LLMs) into AV-TSE models through three distinct guidance strategies: output linguistic constraints, intermediate linguistic prediction, and input linguistic prior. Comprehensive experiments with RoBERTa, Qwen3-0.6B, and Qwen3-4B on two AV-TSE backbones demon- strate the effectiveness of our approach. Significant improvements are observed in challenging scenarios, including visual cue impaired, unseen languages, target speaker switches, increased interfering speakers, and out-of-domain test set. Demo page: https://alexwxwu.github.io/ELEGANCE/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation</title>
<link>https://arxiv.org/abs/2511.06346</link>
<guid>https://arxiv.org/abs/2511.06346</guid>
<content:encoded><![CDATA[
arXiv:2511.06346v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.06419</link>
<guid>https://arxiv.org/abs/2511.06419</guid>
<content:encoded><![CDATA[
arXiv:2511.06419v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models</title>
<link>https://arxiv.org/abs/2511.06430</link>
<guid>https://arxiv.org/abs/2511.06430</guid>
<content:encoded><![CDATA[
arXiv:2511.06430v1 Announce Type: cross 
Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis</title>
<link>https://arxiv.org/abs/2511.06437</link>
<guid>https://arxiv.org/abs/2511.06437</guid>
<content:encoded><![CDATA[
arXiv:2511.06437v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms</title>
<link>https://arxiv.org/abs/2511.06448</link>
<guid>https://arxiv.org/abs/2511.06448</guid>
<content:encoded><![CDATA[
arXiv:2511.06448v1 Announce Type: cross 
Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception</title>
<link>https://arxiv.org/abs/2511.06519</link>
<guid>https://arxiv.org/abs/2511.06519</guid>
<content:encoded><![CDATA[
arXiv:2511.06519v1 Announce Type: cross 
Abstract: Artificial Neural Networks, the building blocks of AI, were inspired by the human brain's network of neurons. Over the years, these networks have evolved to replicate the complex capabilities of the brain, allowing them to handle tasks such as image and language processing. In the realm of Large Language Models, there has been a keen interest in making the language learning process more akin to that of humans. While neuroscientific research has shown that different grammatical categories are processed by different neurons in the brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we identify the most important neurons associated with the prediction of words belonging to different part-of-speech tags. Using the achieved knowledge, we train a classifier on a dataset, which shows that the activation patterns of these key neurons can reliably predict part-of-speech tags on fresh data. The results suggest the presence of a subspace in LLMs focused on capturing part-of-speech tag concepts, resembling patterns observed in lesion studies of the brain in neuroscience.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FPGA or GPU? Analyzing comparative research for application-specific guidance</title>
<link>https://arxiv.org/abs/2511.06565</link>
<guid>https://arxiv.org/abs/2511.06565</guid>
<content:encoded><![CDATA[
arXiv:2511.06565v1 Announce Type: cross 
Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2511.06618</link>
<guid>https://arxiv.org/abs/2511.06618</guid>
<content:encoded><![CDATA[
arXiv:2511.06618v1 Announce Type: cross 
Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Testing for Segmenting Watermarked Texts From Language Models</title>
<link>https://arxiv.org/abs/2511.06645</link>
<guid>https://arxiv.org/abs/2511.06645</guid>
<content:encoded><![CDATA[
arXiv:2511.06645v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2511.06653</link>
<guid>https://arxiv.org/abs/2511.06653</guid>
<content:encoded><![CDATA[
arXiv:2511.06653v1 Announce Type: cross 
Abstract: Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions. The code is available at https://github.com/UnicomAI/HiMo-CLIP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Place Matters: Comparing LLM Hallucination Rates for Place-Based Legal Queries</title>
<link>https://arxiv.org/abs/2511.06700</link>
<guid>https://arxiv.org/abs/2511.06700</guid>
<content:encoded><![CDATA[
arXiv:2511.06700v1 Announce Type: cross 
Abstract: How do we make a meaningful comparison of a large language model's knowledge of the law in one place compared to another? Quantifying these differences is critical to understanding if the quality of the legal information obtained by users of LLM-based chatbots varies depending on their location. However, obtaining meaningful comparative metrics is challenging because legal institutions in different places are not themselves easily comparable. In this work we propose a methodology to obtain place-to-place metrics based on the comparative law concept of functionalism. We construct a dataset of factual scenarios drawn from Reddit posts by users seeking legal advice for family, housing, employment, crime and traffic issues. We use these to elicit a summary of a law from the LLM relevant to each scenario in Los Angeles, London and Sydney. These summaries, typically of a legislative provision, are manually evaluated for hallucinations. We show that the rate of hallucination of legal information by leading closed-source LLMs is significantly associated with place. This suggests that the quality of legal solutions provided by these models is not evenly distributed across geography. Additionally, we show a strong negative correlation between hallucination rate and the frequency of the majority response when the LLM is sampled multiple times, suggesting a measure of uncertainty of model predictions of legal facts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View</title>
<link>https://arxiv.org/abs/2511.06722</link>
<guid>https://arxiv.org/abs/2511.06722</guid>
<content:encoded><![CDATA[
arXiv:2511.06722v1 Announce Type: cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks</title>
<link>https://arxiv.org/abs/2511.07107</link>
<guid>https://arxiv.org/abs/2511.07107</guid>
<content:encoded><![CDATA[
arXiv:2511.07107v1 Announce Type: cross 
Abstract: Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents</title>
<link>https://arxiv.org/abs/2511.07176</link>
<guid>https://arxiv.org/abs/2511.07176</guid>
<content:encoded><![CDATA[
arXiv:2511.07176v1 Announce Type: cross 
Abstract: Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models</title>
<link>https://arxiv.org/abs/2511.07237</link>
<guid>https://arxiv.org/abs/2511.07237</guid>
<content:encoded><![CDATA[
arXiv:2511.07237v1 Announce Type: cross 
Abstract: Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\% of the parameters achieves up to a 12\% accuracy improvement and a 2.7$\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\% of layers achieves comparable or superior accuracy in over 95\% of tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction</title>
<link>https://arxiv.org/abs/2511.07327</link>
<guid>https://arxiv.org/abs/2511.07327</guid>
<content:encoded><![CDATA[
arXiv:2511.07327v1 Announce Type: cross 
Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection</title>
<link>https://arxiv.org/abs/2511.07364</link>
<guid>https://arxiv.org/abs/2511.07364</guid>
<content:encoded><![CDATA[
arXiv:2511.07364v1 Announce Type: cross 
Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</title>
<link>https://arxiv.org/abs/2511.07403</link>
<guid>https://arxiv.org/abs/2511.07403</guid>
<content:encoded><![CDATA[
arXiv:2511.07403v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DigiData: Training and Evaluating General-Purpose Mobile Control Agents</title>
<link>https://arxiv.org/abs/2511.07413</link>
<guid>https://arxiv.org/abs/2511.07413</guid>
<content:encoded><![CDATA[
arXiv:2511.07413v1 Announce Type: cross 
Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation with Infinite Contamination</title>
<link>https://arxiv.org/abs/2511.07417</link>
<guid>https://arxiv.org/abs/2511.07417</guid>
<content:encoded><![CDATA[
arXiv:2511.07417v1 Announce Type: cross 
Abstract: We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.
  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).
  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.
  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiLA: Enhancing LLM Tool Learning with Differential Logic Layer</title>
<link>https://arxiv.org/abs/2402.11903</link>
<guid>https://arxiv.org/abs/2402.11903</guid>
<content:encoded><![CDATA[
arXiv:2402.11903v4 Announce Type: replace 
Abstract: Considering the challenges faced by large language models (LLMs) in logical reasoning and planning, prior efforts have sought to augment LLMs with access to external solvers. While progress has been made on simple reasoning problems, solving classical constraint satisfaction problems, such as the Boolean Satisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains difficult for off-the-shelf solvers due to their intricate expressions and exponential search spaces. In this paper, we propose a novel differential logic layer-aided language modeling (DiLA) approach, where logical constraints are integrated into the forward and backward passes of a network layer, to provide another option for LLM tool learning. In DiLA, LLM aims to transform the language description to logic constraints and identify initial solutions of the highest quality, while the differential logic layer focuses on iteratively refining the LLM-prompted solution. Leveraging the logic layer as a bridge, DiLA enhances the logical reasoning ability of LLMs on a range of reasoning problems encoded by Boolean variables, guaranteeing the efficiency and correctness of the solution process. We evaluate the performance of DiLA on two classic reasoning problems and empirically demonstrate its consistent outperformance against existing prompt-based and solver-aided approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Likelihood-based Mitigation of Evaluation Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2402.15987</link>
<guid>https://arxiv.org/abs/2402.15987</guid>
<content:encoded><![CDATA[
arXiv:2402.15987v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through Curiosity-Driven Queries</title>
<link>https://arxiv.org/abs/2405.20318</link>
<guid>https://arxiv.org/abs/2405.20318</guid>
<content:encoded><![CDATA[
arXiv:2405.20318v4 Announce Type: replace 
Abstract: Recent progress in Large Language Model (LLM) technology has changed our role in interacting with these models. Instead of primarily testing these models with questions we already know answers to, we are now using them for queries where the answers are unknown to us, driven by human curiosity. This shift highlights the growing need to understand curiosity-driven human questions - those that are more complex, open-ended, and reflective of real-world needs. To this end, we present Quriosity, a collection of 13.5K naturally occurring questions from three diverse sources: human-to-search-engine queries, human-to-human interactions, and human-to-LLM conversations. Our comprehensive collection enables a rich understanding of human curiosity across various domains and contexts. Our analysis reveals a significant presence of causal questions (up to 42%) in the dataset, for which we develop an iterative prompt improvement framework to identify all causal queries and examine their unique linguistic properties, cognitive complexity and source distribution. Our paper paves the way for future work on causal question identification and open-ended chatbot interactions. Our code and data are at https://github.com/roberto-ceraolo/quriosity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedCoT: Federated Chain-of-Thought Distillation for Large Language Models</title>
<link>https://arxiv.org/abs/2406.12403</link>
<guid>https://arxiv.org/abs/2406.12403</guid>
<content:encoded><![CDATA[
arXiv:2406.12403v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as a transformative force in artificial intelligence, demonstrating exceptional proficiency across various tasks. However, their deployment in resource-constrained environments and concerns over user data privacy pose significant challenges. In contrast, Small Language Models (SLMs) offer computational efficiency but often lag in performance. To address these issues, we propose FedCoT, a federated framework designed for the Chain-of-Thought (CoT) distillation of knowledge from LLMs to SLMs, while ensuring the preservation of clients' data privacy. FedCoT ensures secure and efficient knowledge transfer from an LLM on a high-powered server to an SLM on a resource-constrained client, while adhering to privacy requirements. Leveraging perturbed prompts and rationales generated through the CoT approach, the framework enhances the performance of the client's SLM without compromising user data privacy within a multi-task learning framework. We propose two privacy protection strategies: the Exponential Mechanism Strategy and the Adaptive Exponential Mechanism Strategy, which balance user prompt privacy and the usability of rationales. Empirical evaluation on various text generation tasks demonstrates the effectiveness of FedCoT in training task-specific SLMs with enhanced performance while prioritizing data privacy protection. Our code has been contributed to the FATE open-source project and is now publicly accessible at \textit{https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/fedcot}
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models</title>
<link>https://arxiv.org/abs/2407.01599</link>
<guid>https://arxiv.org/abs/2407.01599</guid>
<content:encoded><![CDATA[
arXiv:2407.01599v3 Announce Type: replace 
Abstract: The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: https://chonghan-chen.com/llm-jailbreak-zoo-survey/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain</title>
<link>https://arxiv.org/abs/2407.10807</link>
<guid>https://arxiv.org/abs/2407.10807</guid>
<content:encoded><![CDATA[
arXiv:2407.10807v2 Announce Type: replace 
Abstract: Tabular data is considered the last unconquered castle of deep learning, yet the task of data stream classification is stated to be an equally important and demanding research area. Due to the temporal constraints, it is assumed that deep learning methods are not the optimal solution for application in this field. However, excluding the entire -- and prevalent -- group of methods seems rather rash given the progress that has been made in recent years in its development. For this reason, the following paper is the first to present an approach to natural language data stream classification using the sentence space method, which allows for encoding text into the form of a discrete digital signal. This allows the use of convolutional deep networks dedicated to image classification to solve the task of recognizing fake news based on text data. Based on the real-life Fakeddit dataset, the proposed approach was compared with state-of-the-art algorithms for data stream classification based on generalization ability and time complexity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BLADE: Benchmarking Language Model Agents for Data-Driven Science</title>
<link>https://arxiv.org/abs/2408.09667</link>
<guid>https://arxiv.org/abs/2408.09667</guid>
<content:encoded><![CDATA[
arXiv:2408.09667v3 Announce Type: replace 
Abstract: Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM</title>
<link>https://arxiv.org/abs/2409.13949</link>
<guid>https://arxiv.org/abs/2409.13949</guid>
<content:encoded><![CDATA[
arXiv:2409.13949v3 Announce Type: replace 
Abstract: Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-resource languages remains a challenging task. To maximize data efficiency in this low-resource setting, we introduce Mufu, which includes a selection of automatically generated multilingual candidates and an instruction to correct inaccurate translations in the prompt. Mufu prompts turn a translation task into a postediting one, and seek to harness the LLM's reasoning capability with auxiliary translation candidates, from which the model is required to assess the input quality, align the semantics cross-lingually, copy from relevant inputs and override instances that are incorrect. Our experiments on En-XX translations over the Flores-200 dataset show LLMs finetuned against Mufu-style prompts are robust to poor quality auxiliary translation candidates, achieving performance superior to NLLB 1.3B distilled model in 64% of low- and very-low-resource language pairs. We then distill these models to reduce inference cost, while maintaining on average 3.1 chrF improvement over finetune-only baseline in low-resource translations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skill Path: Unveiling Language Skills from Circuit Graphs</title>
<link>https://arxiv.org/abs/2410.01334</link>
<guid>https://arxiv.org/abs/2410.01334</guid>
<content:encoded><![CDATA[
arXiv:2410.01334v3 Announce Type: replace 
Abstract: Circuit graph discovery has emerged as a fundamental approach to elucidating the skill mechanistic of language models. Despite the output faithfulness of circuit graphs, they suffer from atomic ablation, which causes the loss of causal dependencies between connected components. In addition, their discovery process, designed to preserve output faithfulness, inadvertently captures extraneous effects other than an isolated target skill. To alleviate these challenges, we introduce skill paths, which offers a more refined and compact representation by isolating individual skills within a linear chain of components. To enable skill path extracting from circuit graphs, we propose a three-step framework, consisting of decomposition, pruning, and post-pruning causal mediation. In particular, we offer a complete linear decomposition of the transformer model which leads to a disentangled computation graph. After pruning, we further adopt causal analysis techniques, including counterfactuals and interventions, to extract the final skill paths from the circuit graph. To underscore the significance of skill paths, we investigate three generic language skills-Previous Token Skill, Induction Skill, and In-Context Learning Skill-using our framework. Experiments support two crucial properties of these skills, namely stratification and inclusiveness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-PRE: An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation</title>
<link>https://arxiv.org/abs/2410.12265</link>
<guid>https://arxiv.org/abs/2410.12265</guid>
<content:encoded><![CDATA[
arXiv:2410.12265v2 Announce Type: replace 
Abstract: The rapid development of large language models (LLMs) has highlighted the need for efficient and reliable methods to evaluate their performance. Traditional evaluation methods often face challenges like high costs, limited task formats, dependence on human references, and systematic biases. To address these limitations, we propose Auto-PRE, an automatic LLM evaluation framework inspired by the peer review process. Unlike previous approaches that rely on human annotations, Auto-PRE automatically selects evaluator LLMs based on three core traits: consistency, pertinence, and self-confidence, which correspond to the instruction, content, and response stages, respectively, and collectively cover the entire evaluation process. Experiments on three representative tasks, including summarization, non-factoid QA, and dialogue generation, demonstrate that Auto-PRE achieves state-of-the-art performance while significantly reducing evaluation costs. Furthermore, the structured and scalable design of our automatic qualification exam framework provides valuable insights into automating the evaluation of LLMs-as-judges, paving the way for more advanced LLM-based evaluation frameworks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All Entities are Not Created Equal: Examining the Long Tail for Ultra-Fine Entity Typing</title>
<link>https://arxiv.org/abs/2410.17355</link>
<guid>https://arxiv.org/abs/2410.17355</guid>
<content:encoded><![CDATA[
arXiv:2410.17355v3 Announce Type: replace 
Abstract: Due to their capacity to acquire world knowledge from large corpora, pre-trained language models (PLMs) are extensively used in ultra-fine entity typing tasks where the space of labels is extremely large. In this work, we explore the limitations of the knowledge acquired by PLMs by proposing a novel heuristic to approximate the pre-training distribution of entities when the pre-training data is unknown. Then, we systematically demonstrate that entity-typing approaches that rely solely on the parametric knowledge of PLMs struggle significantly with entities at the long tail of the pre-training distribution, and that knowledge-infused approaches can account for some of these shortcomings. Our findings suggest that we need to go beyond PLMs to produce solutions that perform well for infrequent entities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Heritage, Distinct Writing: Rethinking Resource Selection for East Asian Historical Documents</title>
<link>https://arxiv.org/abs/2411.04822</link>
<guid>https://arxiv.org/abs/2411.04822</guid>
<content:encoded><![CDATA[
arXiv:2411.04822v2 Announce Type: replace 
Abstract: Historical documents in the Sinosphere are known to share common formats and practices, particularly in veritable records compiled by court historians. This shared linguistic heritage has led researchers to use Classical Chinese resources for cross-lingual transfer when processing historical documents from Korea and Japan, which remain relatively low-resource. In this paper, we question the assumption of cross-lingual transferability from Classical Chinese to Hanja and Kanbun, the ancient written languages of Korea and Japan, respectively. Our experiments across machine translation, named entity recognition, and punctuation restoration tasks show minimal impact of Classical Chinese datasets on language model performance for ancient Korean documents written in Hanja, with performance differences within $\pm{}0.0068$ F1-score for sequence labeling tasks and up to $+0.84$ BLEU score for translation. These limitations persist consistently across various model sizes, architectures, and domain-specific datasets. Our analysis reveals that the benefits of Classical Chinese resources diminish rapidly as local language data increases for Hanja, while showing substantial improvements only in extremely low-resource scenarios for both Korean and Japanese historical documents. These findings emphasize the need for careful empirical validation rather than assuming benefits from indiscriminate cross-lingual transfer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compositional Phoneme Approximation for L1-Grounded L2 Pronunciation Training</title>
<link>https://arxiv.org/abs/2411.10927</link>
<guid>https://arxiv.org/abs/2411.10927</guid>
<content:encoded><![CDATA[
arXiv:2411.10927v5 Announce Type: replace 
Abstract: Learners of a second language (L2) often map non-native phonemes to similar native-language (L1) phonemes, making conventional L2-focused training slow and effortful. To address this, we propose an L1-grounded pronunciation training method based on compositional phoneme approximation (CPA), a feature-based representation technique that approximates L2 sounds with sequences of L1 phonemes. Evaluations with 20 Korean non-native English speakers show that CPA-based training achieves a 76% in-box formant rate in acoustic analysis, 17.6% relative improvement in phoneme recognition accuracy, and over 80% of speech being rated as more native-like, with minimal training. Project page: https://gsanpark.github.io/CPA-Pronunciation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pralekha: Cross-Lingual Document Alignment for Indic Languages</title>
<link>https://arxiv.org/abs/2411.19096</link>
<guid>https://arxiv.org/abs/2411.19096</guid>
<content:encoded><![CDATA[
arXiv:2411.19096v3 Announce Type: replace 
Abstract: Mining parallel document pairs for document-level machine translation (MT) remains challenging due to the limitations of existing Cross-Lingual Document Alignment (CLDA) techniques. Existing methods often rely on metadata such as URLs, which are scarce, or on pooled document representations that fail to capture fine-grained alignment cues. Moreover, the limited context window of sentence embedding models hinders their ability to represent document-level context, while sentence-based alignment introduces a combinatorially large search space, leading to high computational cost. To address these challenges for Indic languages, we introduce Pralekha, a benchmark containing over 3 million aligned document pairs across 11 Indic languages and English, which includes 1.5 million English-Indic pairs. Furthermore, we propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. Unlike pooling-based methods, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair. Intrinsic evaluation shows that our chunk-based method is 2-3x faster while maintaining competitive performance, and that DAC achieves substantial gains over pooling-based baselines. Extrinsic evaluation further demonstrates that document-level MT models trained on DAC-aligned pairs consistently outperform those using baseline alignment methods. These results highlight DAC's effectiveness for parallel document mining. The dataset and evaluation framework are publicly available to support further research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification</title>
<link>https://arxiv.org/abs/2411.19638</link>
<guid>https://arxiv.org/abs/2411.19638</guid>
<content:encoded><![CDATA[
arXiv:2411.19638v2 Announce Type: replace 
Abstract: With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news topic classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop a news topic training dataset through automatic annotation of 20,000 news articles in Slovenian, Croatian, Greek, and Catalan. Articles are classified into 17 main categories from the Media Topic schema, developed by the International Press Telecommunications Council (IPTC). The teacher model exhibits high zero-shot performance in all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual, and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment</title>
<link>https://arxiv.org/abs/2412.12475</link>
<guid>https://arxiv.org/abs/2412.12475</guid>
<content:encoded><![CDATA[
arXiv:2412.12475v3 Announce Type: replace 
Abstract: Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the vast number of diseases. The involvement of multiple organs and systems, and the shortage of specialized doctors with relevant experience, make diagnosing and treating rare diseases more challenging than common diseases. Recently, agents powered by large language models (LLMs) have demonstrated notable applications across various domains. In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical examinations. However, current agent frameworks are not well-adapted to real-world clinical scenarios, especially those involving the complex demands of rare diseases. To bridge this gap, we introduce RareAgents, the first LLM-driven multi-disciplinary team decision-support tool designed specifically for the complex clinical context of rare diseases. RareAgents integrates advanced Multidisciplinary Team (MDT) coordination, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents outperforms state-of-the-art domain-specific models, GPT-4o, and current agent frameworks in diagnosis and treatment for rare diseases. Furthermore, we contribute a novel rare disease dataset, MIMIC-IV-Ext-Rare, to facilitate further research in this field.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revealing emergent human-like conceptual representations from language prediction</title>
<link>https://arxiv.org/abs/2501.12547</link>
<guid>https://arxiv.org/abs/2501.12547</guid>
<content:encoded><![CDATA[
arXiv:2501.12547v4 Announce Type: replace 
Abstract: People acquire concepts through rich physical and social experiences and use them to understand and navigate the world. In contrast, large language models (LLMs), trained solely through next-token prediction on text, exhibit strikingly human-like behaviors. Are these models developing concepts akin to those of humans? If so, how are such concepts represented, organized, and related to behavior? Here, we address these questions by investigating the representations formed by LLMs during an in-context concept inference task. We found that LLMs can flexibly derive concepts from linguistic descriptions in relation to contextual cues about other concepts. The derived representations converge toward a shared, context-independent structure, and alignment with this structure reliably predicts model performance across various understanding and reasoning tasks. Moreover, the convergent representations effectively capture human behavioral judgments and closely align with neural activity patterns in the human brain, providing evidence for biological plausibility. Together, these findings establish that structured, human-like conceptual representations can emerge purely from language prediction without real-world grounding, highlighting the role of conceptual structure in understanding intelligent behavior. More broadly, our work suggests that LLMs offer a tangible window into the nature of human concepts and lays the groundwork for advancing alignment between artificial and human intelligence.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Task Representations from In-Context Learning</title>
<link>https://arxiv.org/abs/2502.05390</link>
<guid>https://arxiv.org/abs/2502.05390</guid>
<content:encoded><![CDATA[
arXiv:2502.05390v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models</title>
<link>https://arxiv.org/abs/2502.07077</link>
<guid>https://arxiv.org/abs/2502.07077</guid>
<content:encoded><![CDATA[
arXiv:2502.07077v2 Announce Type: replace 
Abstract: The tendency of users to anthropomorphise large language models (LLMs) is of growing interest to AI developers, researchers, and policy-makers. Here, we present a novel method for empirically evaluating anthropomorphic LLM behaviours in realistic and varied settings. Going beyond single-turn static benchmarks, we contribute three methodological advances in state-of-the-art (SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14 anthropomorphic behaviours. Second, we present a scalable, automated approach by employing simulations of user interactions. Third, we conduct an interactive, large-scale human subject study (N=1101) to validate that the model behaviours we measure predict real users' anthropomorphic perceptions. We find that all SOTA LLMs evaluated exhibit similar behaviours, characterised by relationship-building (e.g., empathy and validation) and first-person pronoun use, and that the majority of behaviours only first occur after multiple turns. Our work lays an empirical foundation for investigating how design choices influence anthropomorphic model behaviours and for progressing the ethical debate on the desirability of these behaviours. It also showcases the necessity of multi-turn evaluations for complex social phenomena in human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCS: Perceived Confidence Scoring of Black Box LLMs with Metamorphic Relations</title>
<link>https://arxiv.org/abs/2502.07186</link>
<guid>https://arxiv.org/abs/2502.07186</guid>
<content:encoded><![CDATA[
arXiv:2502.07186v2 Announce Type: replace 
Abstract: Zero-shot LLMs are now also used for textual classification tasks, e.g., sentiment and bias detection in a sentence or article. However, their performance can be suboptimal in such data annotation tasks. We introduce a novel technique that evaluates an LLM's confidence for classifying a textual input by leveraging Metamorphic Relations (MRs). The MRs generate semantically equivalent yet textually divergent versions of the input. Following the principles of Metamorphic Testing (MT), the mutated versions are expected to have annotation labels similar to the input. By analyzing the consistency of an LLM's responses across these variations, we compute a perceived confidence score (PCS) based on the frequency of the predicted labels. PCS can be used for both single and multiple LLM settings (e.g., when multiple LLMs are vetted in a majority-voting setup). Empirical evaluation shows that our PCS-based approach improves the performance of zero-shot LLMs by 9.3% in textual classification tasks. When multiple LLMs are used in a majority-voting setup, we obtain a performance boost of 5.8% with PCS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2502.15857</link>
<guid>https://arxiv.org/abs/2502.15857</guid>
<content:encoded><![CDATA[
arXiv:2502.15857v2 Announce Type: replace 
Abstract: Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a novel unified framework that systematically addresses both privacy preservation and model compression in federated settings. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Our framework's key innovation lies in its holistic integration of privacy-preserving mechanisms, synthetic data generation, and task-specific compression techniques, creating unique benefits through component interaction. Our experiments across diverse text generation tasks demonstrate that PPC-GPT successfully achieves dual objectives: maintaining competitive performance comparable to full-sized LLMs while ensuring robust privacy protection through its federated architecture. Our code has been contributed to the FATE open-source project and is now publicly accessible at \textit{https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/ppc-gpt}
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</title>
<link>https://arxiv.org/abs/2502.16002</link>
<guid>https://arxiv.org/abs/2502.16002</guid>
<content:encoded><![CDATA[
arXiv:2502.16002v4 Announce Type: replace 
Abstract: We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation</title>
<link>https://arxiv.org/abs/2502.19907</link>
<guid>https://arxiv.org/abs/2502.19907</guid>
<content:encoded><![CDATA[
arXiv:2502.19907v2 Announce Type: replace 
Abstract: Logical reasoning is essential for large language models (LLMs) to ensure accurate and coherent inference. However, LLMs struggle with reasoning order variations and fail to generalize across logically equivalent transformations. LLMs often rely on fixed sequential patterns rather than true logical understanding. To address this issue, we introduce an order-centric data augmentation framework based on commutativity in logical reasoning. We first randomly shuffle independent premises to introduce condition order augmentation. For reasoning steps, we construct a directed acyclic graph (DAG) to model dependencies between steps, which allows us to identify valid reorderings of steps while preserving logical correctness. By leveraging order-centric augmentations, models can develop a more flexible and generalized reasoning process. Finally, we conduct extensive experiments across multiple logical reasoning benchmarks, demonstrating that our method significantly enhances LLMs' reasoning performance and adaptability to diverse logical structures. We release our codes and augmented data in https://github.com/qianxiHe147/Order-Centric-Data-Augmentation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer</title>
<link>https://arxiv.org/abs/2502.21228</link>
<guid>https://arxiv.org/abs/2502.21228</guid>
<content:encoded><![CDATA[
arXiv:2502.21228v3 Announce Type: replace 
Abstract: To achieve equitable performance across languages, large language models (LLMs) must be able to abstract knowledge beyond the language in which it was learnt. However, the current literature lacks reliable ways to measure LLMs' capability of such cross-lingual knowledge transfer. To that end, we present ECLeKTic, a multilingual closed-book QA dataset that Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. Concretely, we used the presence and absence of Wikipedia articles in 12 languages to detect pieces of information that were likely available during pre-training in one of the languages but not in the others. We curate ECLeKTic as a set of fact-seeking questions over this kind of information, in all the different languages. Therefore, in order to solve ECLeKTic the model is required to transfer knowledge between languages. We evaluated 8 LLMs and showed that current SOTA models struggle to effectively share knowledge across languages, even if they can predict the answer for questions in the language in which the knowledge was acquired.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Human-LLM Representation Alignment: A Case Study on Affective Sentence Generation for Augmentative and Alternative Communication</title>
<link>https://arxiv.org/abs/2503.11881</link>
<guid>https://arxiv.org/abs/2503.11881</guid>
<content:encoded><![CDATA[
arXiv:2503.11881v3 Announce Type: replace 
Abstract: Gaps arise between a language model's use of concepts and people's expectations. This gap is critical when LLMs generate text to help people communicate via Augmentative and Alternative Communication (AAC) tools. In this work, we introduce the evaluation task of Representation Alignment for measuring this gap via human judgment. In our study, we expand keywords and emotion representations into full sentences. We select four emotion representations: Words, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and Numeric forms, and Emojis. In addition to Representation Alignment, we also measure people's judgments of the accuracy and realism of the generated sentences. While representations like VAD break emotions into easy-to-compute components, our findings show that people agree more with how LLMs generate when conditioned on English words (e.g., "angry") rather than VAD scales. This difference is especially visible when comparing Numeric VAD to words. Furthermore, we found that the perception of how much a generated sentence conveys an emotion is dependent on both the representation type and which emotion it is.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond</title>
<link>https://arxiv.org/abs/2503.16040</link>
<guid>https://arxiv.org/abs/2503.16040</guid>
<content:encoded><![CDATA[
arXiv:2503.16040v2 Announce Type: replace 
Abstract: Recent advances in test-time scaling of large language models (LLMs), exemplified by DeepSeek-R1 and OpenAI's o1, show that extending the chain of thought during inference can significantly improve general reasoning performance. However, the impact of this paradigm on legal reasoning remains insufficiently explored. To address this gap, we present the first systematic evaluation of 12 LLMs, including both reasoning-focused and general-purpose models, across 17 Chinese and English legal tasks spanning statutory and case-law traditions. In addition, we curate a bilingual chain-of-thought dataset for legal reasoning through distillation from DeepSeek-R1 and develop Legal-R1, an open-source model specialized for the legal domain. Experimental results show that Legal-R1 delivers competitive performance across diverse tasks. DeepSeek-R1 exhibits clear advantages in Chinese legal reasoning, while OpenAI's o1 achieves comparable results on English tasks. We further conduct a detailed error analysis, which reveals recurring issues such as outdated legal knowledge, limited capacity for legal interpretation, and susceptibility to factual hallucinations. These findings delineate the main obstacles confronting legal-domain LLMs and suggest promising directions for future research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Consistency of Multilingual Context Utilization in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.00597</link>
<guid>https://arxiv.org/abs/2504.00597</guid>
<content:encoded><![CDATA[
arXiv:2504.00597v4 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) with large language models (LLMs) has demonstrated strong performance in multilingual question-answering (QA) tasks by leveraging relevant passages retrieved from corpora. In multilingual RAG (mRAG), the retrieved passages can be written in languages other than that of the query entered by the user, making it challenging for LLMs to effectively utilize the provided information. Recent research suggests that retrieving passages from multilingual corpora can improve RAG performance, particularly for low-resource languages. However, the extent to which LLMs can leverage different kinds of multilingual contexts to generate accurate answers, *independently from retrieval quality*, remains understudied. In this paper, we conduct an extensive assessment of LLMs' ability to (i) make consistent use of a relevant passage regardless of its language, (ii) respond in the expected language, and (iii) focus on the relevant passage even when multiple `distracting' passages in different languages are provided in the context. Our experiments with four LLMs across three QA datasets covering a total of 48 languages reveal a surprising ability of LLMs to extract the relevant information from passages in a different language than the query, but a much weaker ability to formulate a full answer in the correct language. Our analysis, based on both accuracy and feature attribution techniques, further shows that distracting passages negatively impact answer quality regardless of their language. However, distractors in the query language exert a slightly stronger influence. Taken together, our findings deepen the understanding of how LLMs utilize context in mRAG systems, providing directions for future improvements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</title>
<link>https://arxiv.org/abs/2504.02904</link>
<guid>https://arxiv.org/abs/2504.02904</guid>
<content:encoded><![CDATA[
arXiv:2504.02904v3 Announce Type: replace 
Abstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering</title>
<link>https://arxiv.org/abs/2505.02311</link>
<guid>https://arxiv.org/abs/2505.02311</guid>
<content:encoded><![CDATA[
arXiv:2505.02311v2 Announce Type: replace 
Abstract: The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baselines in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Consistency Preference Optimization for Long-Form Question Answering</title>
<link>https://arxiv.org/abs/2505.09039</link>
<guid>https://arxiv.org/abs/2505.09039</guid>
<content:encoded><![CDATA[
arXiv:2505.09039v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated (factual, non-factual) pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness that may not always be accessible. Addressing this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals (i.e., the agreement of individual facts across multiple stochastic responses) to identify high- and low-quality data pairs for model alignment. Despite being fully self-supervised, ACPO outperforms the strong supervised alignment baseline by 1.95 points averaged across Phi-3 and Llama3 on the LongFact and BioGen datasets, demonstrating its effectiveness in improving factual reliability without relying on external models or knowledge bases.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection</title>
<link>https://arxiv.org/abs/2505.13979</link>
<guid>https://arxiv.org/abs/2505.13979</guid>
<content:encoded><![CDATA[
arXiv:2505.13979v2 Announce Type: replace 
Abstract: Multimodal models play a key role in empathy detection, but their performance can suffer when modalities provide conflicting cues. To understand these failures, we examine cases where unimodal and multimodal predictions diverge. Using fine-tuned models for text, audio, and video, along with a gated fusion model, we find that such disagreements often reflect underlying ambiguity, as evidenced by annotator uncertainty. Our analysis shows that dominant signals in one modality can mislead fusion when unsupported by others. We also observe that humans, like models, do not consistently benefit from multimodal input. These insights position disagreement as a useful diagnostic signal for identifying challenging examples and improving empathy system robustness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Large Language Models for Detecting Mental Manipulation via Annotation-Free Data Augmentation and Anti-Curriculum Distillation</title>
<link>https://arxiv.org/abs/2505.15255</link>
<guid>https://arxiv.org/abs/2505.15255</guid>
<content:encoded><![CDATA[
arXiv:2505.15255v3 Announce Type: replace 
Abstract: Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Nevertheless, detecting mental manipulation remains a largely underexplored research problem. The field faces three major challenges: (i) insufficient and hard-to-obtain training data; (ii) the covert nature of mental manipulation, which hinders detection; and (iii) the lack of real-world datasets. To address these challenges, we propose MentalMAC, a novel framework that enhances large language models' ability to detect elements of mental manipulation in multi-turn dialogue. Our approach consists of three key components: EvoSA, an annotation-free data augmentation method based on evolutionary operations and speech act theory; teacher-model-generated multi-task supervision; and progressive task-level anti-curriculum distillation. We then constructed the ReaMent dataset, comprising 5,000 real-world dialogue samples, utilizing MentalMAC-distilled models to aid in human annotation. Vast experiments show that MentalMAC achieves up to 25.9% improvement in F1mac and 8.1% in accuracy over the best-performing baseline, outperforming commercial LLMs such as GPT-4 and Claude-3.5-Sonnet. Warning: This paper contains content that may be offensive to the reader.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Model Distillation: A Temporal Difference Imitation Learning Perspective</title>
<link>https://arxiv.org/abs/2505.20335</link>
<guid>https://arxiv.org/abs/2505.20335</guid>
<content:encoded><![CDATA[
arXiv:2505.20335v2 Announce Type: replace 
Abstract: Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Text-based Protein Understanding: Retrieval or LLM?</title>
<link>https://arxiv.org/abs/2505.20354</link>
<guid>https://arxiv.org/abs/2505.20354</guid>
<content:encoded><![CDATA[
arXiv:2505.20354v4 Announce Type: replace 
Abstract: In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.24332</link>
<guid>https://arxiv.org/abs/2505.24332</guid>
<content:encoded><![CDATA[
arXiv:2505.24332v2 Announce Type: replace 
Abstract: Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing prompting and supervised fine-tuning (SFT) methods remain fixed by prompt rules or training corpora, and are usually benchmarked only on well-structured wiki sources, limiting real-world adaptability. We introduce WebPuzzle, a 24k-sample training and 275-sample test benchmark that evaluates information seeking on the live internet, across both wiki and open-domain queries. Leveraging 7k WebPuzzle instances, we develop DeepDiver, a reinforcement-learning (RL) framework that cultivates Search Intensity Scaling (SIS)-an emergent ability to escalate search frequency and depth instead of settling on overconfident, under-evidenced answers. With SIS, Qwen2.5-7B-Instruct and Pangu-7B-Reasoner attain performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's curriculum from cold-start SFT to a well designed RL procedure, and show that its seeking policy generalized from closed-ended queries to open-ended generation such as long-form writing. Our results advance adaptive information seeking in LLMs and provide a rigorous benchmark for future work.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Language Shapes Thought: Cross-Lingual Transfer of Factual Knowledge in Question Answering</title>
<link>https://arxiv.org/abs/2505.24409</link>
<guid>https://arxiv.org/abs/2505.24409</guid>
<content:encoded><![CDATA[
arXiv:2505.24409v2 Announce Type: replace 
Abstract: Multilingual large language models (LLMs) offer promising opportunities for cross-lingual information access, yet their use of factual knowledge remains highly sensitive to the input language. Prior work has addressed this through English prompting and evaluation, assuming that English-based reasoning is universally beneficial. In this work, we challenge that assumption by exploring factual knowledge transfer from non-English to English through the lens of Language and Thought Theory. We introduce Language-to-Thought (L2T) prompting, which aligns the model's internal ''thinking'' language with the source of knowledge. Across three languages and four models, L2T consistently outperforms English-based reasoning, reversing the expected advantage of English prompts. Our code is available at https://github.com/GeomeunByeol/Language2Thought.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text</title>
<link>https://arxiv.org/abs/2505.24826</link>
<guid>https://arxiv.org/abs/2505.24826</guid>
<content:encoded><![CDATA[
arXiv:2505.24826v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: https://github.com/lyxx3rd/LegalEval-Q.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics</title>
<link>https://arxiv.org/abs/2506.12618</link>
<guid>https://arxiv.org/abs/2506.12618</guid>
<content:encoded><![CDATA[
arXiv:2506.12618v2 Announce Type: replace 
Abstract: Robust unlearning is crucial for safely deploying large language models (LLMs) in environments where data privacy, model safety, and regulatory compliance must be ensured. Yet the task is inherently challenging, partly due to difficulties in reliably measuring whether unlearning has truly occurred. Moreover, fragmentation in current methodologies and inconsistent evaluation metrics hinder comparative analysis and reproducibility. To unify and accelerate research efforts, we introduce OpenUnlearning, a standardized and extensible framework designed explicitly for benchmarking both LLM unlearning methods and metrics. OpenUnlearning integrates 13 unlearning algorithms and 16 diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also enables analyses of forgetting behaviors across 450+ checkpoints we publicly release. Leveraging OpenUnlearning, we propose a novel meta-evaluation benchmark focused specifically on assessing the faithfulness and robustness of evaluation metrics themselves. We also benchmark diverse unlearning methods and provide a comparative analysis against an extensive evaluation suite. Overall, we establish a clear, community-driven pathway toward rigorous development in LLM unlearning research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning with Exploration: An Entropy Perspective</title>
<link>https://arxiv.org/abs/2506.14758</link>
<guid>https://arxiv.org/abs/2506.14758</guid>
<content:encoded><![CDATA[
arXiv:2506.14758v4 Announce Type: replace 
Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing large language model (LLM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LLMs. Through empirical analysis, we uncover positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LLMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LLM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations</title>
<link>https://arxiv.org/abs/2506.16678</link>
<guid>https://arxiv.org/abs/2506.16678</guid>
<content:encoded><![CDATA[
arXiv:2506.16678v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when processing and generating text. While this suggests internalized understanding of hierarchical syntax and dependency relations, the precise mechanism by which they represent syntactic structure is an open area within interpretability research. Probing provides one way to identify the mechanism of syntax being linearly encoded in activations, however, no comprehensive study has yet established whether a model's probing accuracy reliably predicts its downstream syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we evaluate 32 open-weight transformer models and find that syntactic features extracted via probing fail to predict outcomes of targeted syntax evaluations across English linguistic phenomena. Our results highlight a substantial disconnect between latent syntactic representations found via probing and observable syntactic behaviors in downstream tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Updating Code API Knowledge with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.20495</link>
<guid>https://arxiv.org/abs/2506.20495</guid>
<content:encoded><![CDATA[
arXiv:2506.20495v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title>
<link>https://arxiv.org/abs/2507.04531</link>
<guid>https://arxiv.org/abs/2507.04531</guid>
<content:encoded><![CDATA[
arXiv:2507.04531v3 Announce Type: replace 
Abstract: Large language models (LLMs) do not preserve privacy at inference-time. The LLM's outputs can inadvertently reveal information about the model's context, which presents a privacy challenge when the LLM is augmented via tools or databases containing sensitive information. Existing privacy-preserving methods at inference-time have significant limitations since they (i) lack provable guarantees or (ii) have a poor utility/privacy trade-off. We propose DP-Fusion, a Differentially Private Inference (DPI) mechanism for LLMs that provably bounds the influence a set of tokens in the context can have on the LLM's output. DP-Fusion works as follows: (1) label a subset of sensitive tokens, (2) infer the LLM without any sensitive tokens to obtain a baseline, (3) infer the LLM with the sensitive tokens, and (4) blend distributions so that the final output remains within a bounded distance of the baseline distribution. While this per-token influence bound also mitigates jailbreak-style prompt injection, we focus on \emph{document privatization}, where the goal is to paraphrase a document containing sensitive tokens, e.g., personally identifiable information, so that no attacker can reliably infer them from the paraphrased document while preserving high text quality. The privacy/utility trade-off is controlled by $\epsilon$, where $\epsilon=0$ hides sensitive tokens entirely, while higher values trade off privacy for improved text quality. We show that our method creates token-level provably privatized documents with substantially improved theoretical and empirical privacy, achieving $6\times$ lower perplexity than related DPI methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?</title>
<link>https://arxiv.org/abs/2507.05639</link>
<guid>https://arxiv.org/abs/2507.05639</guid>
<content:encoded><![CDATA[
arXiv:2507.05639v2 Announce Type: replace 
Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. The code and data have been made publicly available at https://github.com/XiaoduoAILab/ECom-Bench to facilitate further research and development in this domain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited</title>
<link>https://arxiv.org/abs/2507.12059</link>
<guid>https://arxiv.org/abs/2507.12059</guid>
<content:encoded><![CDATA[
arXiv:2507.12059v2 Announce Type: replace 
Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person. Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions. This paper summarises and extends earlier work presented at COSIT-24.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms</title>
<link>https://arxiv.org/abs/2507.20264</link>
<guid>https://arxiv.org/abs/2507.20264</guid>
<content:encoded><![CDATA[
arXiv:2507.20264v2 Announce Type: replace 
Abstract: Shaping inclusive representations that embrace diversity and ensure fair participation and reflections of values is at the core of many conversation-based models. However, many existing methods rely on surface inclusion using mention of user demographics or behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a step back and recognized that equitable inclusion needs to account for the implicit expression of opinion and use the stance of responses to validate the normative alignment. This study aims to evaluate how opinions are represented in NLP or computational models by introducing an alignment evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the normative social views and discourse. Our approach models the stance of responses as a proxy for the underlying opinion, enabling a considerate and reflective representation of diverse social viewpoints. We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through this, we provide a principled and structured lens on how implicit opinions are (mis)represented and offer a pathway toward more inclusive model behavior.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</title>
<link>https://arxiv.org/abs/2507.21652</link>
<guid>https://arxiv.org/abs/2507.21652</guid>
<content:encoded><![CDATA[
arXiv:2507.21652v2 Announce Type: replace 
Abstract: As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at https://github.com/mbzuai-nlp/UnsafeChain
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[
arXiv:2508.01710v4 Announce Type: replace 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning enables robust cross-lingual transfer and strong zero-shot generalization to unseen languages. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work advances multilingual LLM safety by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating, Synthesizing, and Enhancing for Customer Support Conversation</title>
<link>https://arxiv.org/abs/2508.04423</link>
<guid>https://arxiv.org/abs/2508.04423</guid>
<content:encoded><![CDATA[
arXiv:2508.04423v2 Announce Type: replace 
Abstract: Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.05100</link>
<guid>https://arxiv.org/abs/2508.05100</guid>
<content:encoded><![CDATA[
arXiv:2508.05100v2 Announce Type: replace 
Abstract: With the rapid advancement of large language models (LLMs), retrieval-augmented generation (RAG) has emerged as a critical approach to supplement the inherent knowledge limitations of LLMs. However, due to the typically large volume of retrieved information, RAG tends to operate with long context lengths. From the perspective of entropy engineering, we identify unconstrained entropy growth and attention dilution due to long retrieval context as significant factors affecting RAG performance. In this paper, we propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves the adaptability of RAG systems to varying context lengths through the principle of entropy invariance. By leveraging balanced context entropy to reformulate attention dynamics, BEE-RAG separates attention sensitivity from context length, ensuring a stable entropy level. Building upon this, we introduce a zero-shot inference strategy for multi-importance estimation and a parameter-efficient adaptive fine-tuning mechanism to obtain the optimal balancing factor for different settings. Extensive experiments across multiple RAG tasks demonstrate the effectiveness of BEE-RAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations</title>
<link>https://arxiv.org/abs/2508.05470</link>
<guid>https://arxiv.org/abs/2508.05470</guid>
<content:encoded><![CDATA[
arXiv:2508.05470v2 Announce Type: replace 
Abstract: We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity index's focus on lexical diversity, perplexity's sensitivity to model confidence, and syntactic templates' inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Does a Deep Neural Network Look at Lexical Stress?</title>
<link>https://arxiv.org/abs/2508.07229</link>
<guid>https://arxiv.org/abs/2508.07229</guid>
<content:encoded><![CDATA[
arXiv:2508.07229v2 Announce Type: replace 
Abstract: Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for CNN interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel's first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning's ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment</title>
<link>https://arxiv.org/abs/2508.08424</link>
<guid>https://arxiv.org/abs/2508.08424</guid>
<content:encoded><![CDATA[
arXiv:2508.08424v3 Announce Type: replace 
Abstract: The relationship between tokenizer algorithm (e.g., Byte-Pair Encoding (BPE), Unigram), morphological alignment, tokenization quality (e.g., compression efficiency), and downstream performance remains largely unclear, particularly for languages with complex morphology. In this paper, we conduct a comprehensive evaluation of tokenizers using small-sized BERT models -- from pre-training through fine-tuning -- for Telugu (agglutinative), along with preliminary evaluation in Hindi (primarily fusional with some agglutination) and English (fusional). To evaluate morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.
  Our experiments reveal two key findings for Telugu. First, the choice of tokenizer algorithm is the most significant factor influencing performance, with Unigram-based tokenizers consistently outperforming BPE across most settings. Second, while better morphological alignment shows a moderate, positive correlation with performance on text classification and structure prediction tasks, its impact is secondary to the tokenizer algorithm. Notably, hybrid approaches that use morphological information for pre-segmentation significantly boost the performance of BPE, though not Unigram. Our results further showcase the need for comprehensive intrinsic evaluation metrics for tokenizers that could explain downstream performance trends consistently.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.09091</link>
<guid>https://arxiv.org/abs/2508.09091</guid>
<content:encoded><![CDATA[
arXiv:2508.09091v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in English, but their performance degrades significantly on low-resource languages (LRLs) due to English-centric training. While methods like LangBridge align LLMs with multilingual encoders such as the Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically use only the final encoder layer. We propose a novel architecture that fuses all intermediate layers, enriching the linguistic information passed to the LLM. Our approach features two strategies: (1) a Global Softmax weighting for overall layer importance, and (2) a Transformer Softmax model that learns token-specific weights. The fused representations are mapped into the LLM's embedding space, enabling it to process multilingual inputs. The model is trained only on English data, without using any parallel or multilingual data. Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews, our Transformer Softmax model significantly outperforms the LangBridge baseline. We observe strong performance gains in LRLs, improving Sinhala classification accuracy from 71.66% to 75.86% and achieving clear improvements across Indic languages such as Tamil, Bengali, and Malayalam. These specific gains contribute to an overall boost in average XNLI accuracy from 70.36% to 71.50%. This approach offers a scalable, data-efficient path toward more capable and equitable multilingual LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SinLlama -- A Large Language Model for Sinhala</title>
<link>https://arxiv.org/abs/2508.09115</link>
<guid>https://arxiv.org/abs/2508.09115</guid>
<content:encoded><![CDATA[
arXiv:2508.09115v4 Announce Type: replace 
Abstract: Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMCARE: early detection of cognitive impairment via transformer models enhanced by LLM-generated synthetic data</title>
<link>https://arxiv.org/abs/2508.10027</link>
<guid>https://arxiv.org/abs/2508.10027</guid>
<content:encoded><![CDATA[
arXiv:2508.10027v3 Announce Type: replace 
Abstract: Alzheimer's disease and related dementias(ADRD) affect nearly five million older adults in the United States, yet more than half remain undiagnosed. Speech-based natural language processing(NLP) offers a scalable approach for detecting early cognitive decline through subtle linguistic markers that may precede clinical diagnosis. This study develops and evaluates a speech-based screening pipeline integrating transformer embeddings with handcrafted linguistic features, synthetic augmentation using large language models(LLMs), and benchmarking of unimodal and multimodal classifiers. External validation assessed generalizability to a MCI-only cohort.
  Transcripts were drawn from the ADReSSo 2021 benchmark dataset(n=237, Pitt Corpus) and the DementiaBank Delaware corpus(n=205, MCI vs. controls). Ten transformer models were tested under three fine-tuning strategies. A late-fusion model combined embeddings from the top transformer with 110 linguistic features. Five LLMs(LLaMA8B/70B, MedAlpaca7B, Ministral8B,GPT-4o) generated label-conditioned synthetic speech for augmentation, and three multimodal LLMs(GPT-4o,Qwen-Omni,Phi-4) were evaluated in zero-shot and fine-tuned modes. On ADReSSo, the fusion model achieved F1=83.3(AUC=89.5), outperforming transformer-only and linguistic baselines. MedAlpaca7B augmentation(2x) improved F1=85.7, though larger scales reduced gains. Fine-tuning boosted unimodal LLMs(MedAlpaca7B F1=47.7=>78.7), while multimodal models performed lower (Phi-4=71.6;GPT-4o=67.6). On Delaware, the fusion plus 1x MedAlpaca7B model achieved F1=72.8(AUC=69.6). Integrating transformer and linguistic features enhances ADRD detection. LLM-based augmentation improves data efficiency but yields diminishing returns, while current multimodal models remain limited. Validation on an independent MCI cohort supports the pipeline's potential for scalable, clinically relevant early screening.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[
arXiv:2508.10419v2 Announce Type: replace 
Abstract: Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and its high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods could fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition on reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global context comprehension, offering a principled, cognitively motivated paradigm towards retrieval-based stateful reasoning. Our framework is made publicly available at https://github.com/EternityJune25/ComoRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title>
<link>https://arxiv.org/abs/2508.15250</link>
<guid>https://arxiv.org/abs/2508.15250</guid>
<content:encoded><![CDATA[
arXiv:2508.15250v3 Announce Type: replace 
Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs</title>
<link>https://arxiv.org/abs/2508.20325</link>
<guid>https://arxiv.org/abs/2508.20325</guid>
<content:encoded><![CDATA[
arXiv:2508.20325v2 Announce Type: replace 
Abstract: As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline \textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement</title>
<link>https://arxiv.org/abs/2508.20916</link>
<guid>https://arxiv.org/abs/2508.20916</guid>
<content:encoded><![CDATA[
arXiv:2508.20916v2 Announce Type: replace 
Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normality and the Turing Test</title>
<link>https://arxiv.org/abs/2508.21382</link>
<guid>https://arxiv.org/abs/2508.21382</guid>
<content:encoded><![CDATA[
arXiv:2508.21382v2 Announce Type: replace 
Abstract: This paper proposes to revisit the Turing test through the concept of normality. Its core argument is that the Turing test is a test of normal intelligence as assessed by a normal judge. First, in the sense that the Turing test targets normal/average rather than exceptional human intelligence, so that successfully passing the test requires machines to "make mistakes" and display imperfect behavior just like normal/average humans. Second, in the sense that the Turing test is a statistical test where judgments of intelligence are never carried out by a single "average" judge (understood as non-expert) but always by a full jury. As such, the notion of "average human interrogator" that Turing talks about in his original paper should be understood primarily as referring to a mathematical abstraction made of the normalized aggregate of individual judgments of multiple judges. Its conclusions are twofold. First, it argues that large language models such as ChatGPT are unlikely to pass the Turing test as those models precisely target exceptional rather than normal/average human intelligence. As such, they constitute models of what it proposes to call artificial smartness rather than artificial intelligence, insofar as they deviate from the original goal of Turing for the modeling of artificial minds. Second, it argues that the objectivization of normal human behavior in the Turing test fails due to the game configuration of the test which ends up objectivizing normative ideals of normal behavior rather than normal behavior per se.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2509.13624</link>
<guid>https://arxiv.org/abs/2509.13624</guid>
<content:encoded><![CDATA[
arXiv:2509.13624v2 Announce Type: replace 
Abstract: Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training. This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests. Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions. We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic) and discover the side effects of the transfer learning. Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential. This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Neurosymbolic Reasoners on First-Order Logic Problems</title>
<link>https://arxiv.org/abs/2509.17377</link>
<guid>https://arxiv.org/abs/2509.17377</guid>
<content:encoded><![CDATA[
arXiv:2509.17377v2 Announce Type: replace 
Abstract: Recent trends in NLP aim to improve reasoning capabilities in Large Language Models (LLMs), with key focus on generalization and robustness to variations in tasks. Counterfactual task variants introduce minimal but semantically meaningful changes to otherwise valid first-order logic (FOL) problem instances altering a single predicate or swapping roles of constants to probe whether a reasoning system can maintain logical consistency under perturbation. Previous studies showed that LLMs becomes brittle on counterfactual variations, suggesting that they often rely on spurious surface patterns to generate responses. In this work, we explore if a neurosymbolic (NS) approach that integrates an LLM and a symbolic logical solver could mitigate this problem. Experiments across LLMs of varying sizes show that NS methods are more robust but perform worse overall that purely neural methods. We then propose NSCoT that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate that while it improves performance, NSCoT still lags behind standard CoT. Our analysis opens research directions for future work.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EditGRPO: Reinforcement Learning with Post-Rollout Edits for Clinically Accurate Chest X-Ray Report Generation</title>
<link>https://arxiv.org/abs/2509.22812</link>
<guid>https://arxiv.org/abs/2509.22812</guid>
<content:encoded><![CDATA[
arXiv:2509.22812v2 Announce Type: replace 
Abstract: Radiology report generation requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. Although recent innovations, particularly multimodal large language models, have shown improved performance, their supervised fine-tuning (SFT) objective is not explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO, a mixed-policy reinforcement learning algorithm designed specifically to optimize the generation through clinically motivated rewards. EditGRPO integrates on-policy exploration with off-policy guidance by injecting sentence-level detailed corrections during training rollouts. This mixed-policy approach addresses the exploration dilemma and sampling efficiency issues typically encountered in RL. Applied to a Qwen2.5-VL-3B, EditGRPO outperforms both SFT and vanilla GRPO baselines, achieving an average improvement of 3.4\% in clinical metrics across four major datasets. Notably, EditGRPO also demonstrates superior out-of-domain generalization, with an average performance gain of 5.9\% on unseen datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora</title>
<link>https://arxiv.org/abs/2510.10114</link>
<guid>https://arxiv.org/abs/2510.10114</guid>
<content:encoded><![CDATA[
arXiv:2510.10114v4 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate hallucinations of Large Language Models (LLMs) by leveraging external knowledge. While effective for simple queries, traditional RAG systems struggle with large-scale, unstructured corpora where information is fragmented. Recent advances incorporate knowledge graphs to capture relational structures, enabling more comprehensive retrieval for complex, multi-hop reasoning tasks. However, existing graph-based RAG (GraphRAG) methods rely on unstable and costly relation extraction for graph construction, often producing noisy graphs with incorrect or inconsistent relations that degrade retrieval quality. In this paper, we revisit the pipeline of existing GraphRAG systems and propose LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient framework that enables reliable graph construction and precise passage retrieval. Specifically, LinearRAG constructs a relation-free hierarchical graph, termed Tri-Graph, using only lightweight entity extraction and semantic linking, avoiding unstable relation modeling. This new paradigm of graph construction scales linearly with corpus size and incurs no extra token consumption, providing an economical and reliable indexing of the original passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant entity activation via local semantic bridging, followed by (ii) passage retrieval through global importance aggregation. Extensive experiments on four datasets demonstrate that LinearRAG significantly outperforms baseline models. Our code and datasets are available at https://github.com/DEEP-PolyU/LinearRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations</title>
<link>https://arxiv.org/abs/2510.11196</link>
<guid>https://arxiv.org/abs/2510.11196</guid>
<content:encoded><![CDATA[
arXiv:2510.11196v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone ($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality can be decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2510.13003</link>
<guid>https://arxiv.org/abs/2510.13003</guid>
<content:encoded><![CDATA[
arXiv:2510.13003v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language models but suffers from catastrophic forgetting when learned updates interfere with the dominant singular directions that encode essential pre-trained knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically grounded approach that prevents this interference through double-sided orthogonal projections. By decomposing frozen weights via SVD, OPLoRA constrains LoRA updates to lie entirely within the orthogonal complement of the top-$k$ singular subspace using projections $P_L = I - U_k U_k^\top$ and $P_R = I - V_k V_k^\top$. We prove that this construction exactly preserves the top-$k$ singular triples, providing mathematical guarantees for knowledge retention. To quantify subspace interference, we introduce $\rho_k$, a metric measuring update alignment with dominant directions. Extensive experiments across commonsense reasoning, mathematics, and code generation demonstrate that OPLoRA significantly reduces forgetting while maintaining competitive task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal projection as an effective mechanism for knowledge preservation in parameter-efficient fine-tuning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons</title>
<link>https://arxiv.org/abs/2510.13797</link>
<guid>https://arxiv.org/abs/2510.13797</guid>
<content:encoded><![CDATA[
arXiv:2510.13797v2 Announce Type: replace 
Abstract: The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meronymic Ontology Extraction via Large Language Models</title>
<link>https://arxiv.org/abs/2510.13839</link>
<guid>https://arxiv.org/abs/2510.13839</guid>
<content:encoded><![CDATA[
arXiv:2510.13839v2 Announce Type: replace 
Abstract: Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models</title>
<link>https://arxiv.org/abs/2510.13847</link>
<guid>https://arxiv.org/abs/2510.13847</guid>
<content:encoded><![CDATA[
arXiv:2510.13847v2 Announce Type: replace 
Abstract: Speculative decoding has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed top frequent subset of the target model's vocabulary. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. Across standard speculative decoding benchmarks, DynaSpec delivers consistent improvements in mean accepted length, for Llama-3-8B, reaching upto 98.2% of full-vocabulary performance, while fixed-shortlist baselines attain only 84.4%. By leveraging context-dependent selection, DynaSpec achieves up to a 2.18 times increase in generated tokens compared to 1.91 times for fixed-vocabulary approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title>
<link>https://arxiv.org/abs/2510.19670</link>
<guid>https://arxiv.org/abs/2510.19670</guid>
<content:encoded><![CDATA[
arXiv:2510.19670v2 Announce Type: replace 
Abstract: We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment</title>
<link>https://arxiv.org/abs/2407.17716</link>
<guid>https://arxiv.org/abs/2407.17716</guid>
<content:encoded><![CDATA[
arXiv:2407.17716v2 Announce Type: replace-cross 
Abstract: Speech emotion recognition (SER) systems often struggle in real-world environments, where ambient noise severely degrades their performance. This paper explores a novel approach that exploits prior knowledge of testing environments to maximize SER performance under noisy conditions. To address this task, we propose a text-guided, environment-aware training where an SER model is trained with contaminated speech samples and their paired noise description. We use a pre-trained text encoder to extract the text-based environment embedding and then fuse it to a transformer-based SER model during training and inference. We demonstrate the effectiveness of our approach through our experiment with the MSP-Podcast corpus and real-world additive noise samples collected from the Freesound and DEMAND repositories. Our experiment indicates that the text-based environment descriptions processed by a large language model (LLM) produce representations that improve the noise-robustness of the SER system. With a contrastive learning (CL)-based representation, our proposed method can be improved by jointly fine-tuning the text encoder with the emotion recognition model. Under the -5dB signal-to-noise ratio (SNR) level, fine-tuning the text encoder improves our CL-based representation method by 76.4% (arousal), 100.0% (dominance), and 27.7% (valence).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning - A Convex Optimization Perspective</title>
<link>https://arxiv.org/abs/2410.15483</link>
<guid>https://arxiv.org/abs/2410.15483</guid>
<content:encoded><![CDATA[
arXiv:2410.15483v4 Announce Type: replace-cross 
Abstract: The post-training of LLMs, which typically consists of the supervised fine-tuning (SFT) stage and the preference learning stage (RLHF or DPO), is crucial to effective and safe LLM applications. The widely adopted approach in post-training popular open-source LLMs is to sequentially perform SFT and RLHF/DPO. However, this is suboptimal in terms of SFT and RLHF/DPO trade-off: the LLM gradually forgets about the first stage's training when undergoing the second stage's training. This sequential paradigm persists largely due to its simplicity and modularity, which make it easier to implement and manage at scale despite its limitations. We theoretically prove the sub-optimality of sequential post-training and propose a practical joint post-training framework which has theoretical convergence guarantees and empirically outperforms sequential post-training framework, with up to 23% overall performance improvement across multiple LLM evaluation benchmarks, while having minimal computational overhead. Our code is available at https://github.com/heshandevaka/XRIGHT.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training</title>
<link>https://arxiv.org/abs/2411.07066</link>
<guid>https://arxiv.org/abs/2411.07066</guid>
<content:encoded><![CDATA[
arXiv:2411.07066v4 Announce Type: replace-cross 
Abstract: Network pruning focuses on algorithms that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are, in any case, too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their input activations, to obtain sparse models that maximize the activations' alignment with respect to their corresponding dense models. Hence, we propose \textbf{NeuroAl}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity, exploiting information from both the dense model and its sparse version to maximize the \emph{neuron alignment} among activations. Different from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \emph{no re-training}. We test our method over $\sim$300 test cases with four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEAGraph: Unveiling the Whole Story of Paper Review Comments</title>
<link>https://arxiv.org/abs/2412.11939</link>
<guid>https://arxiv.org/abs/2412.11939</guid>
<content:encoded><![CDATA[
arXiv:2412.11939v2 Announce Type: replace-cross 
Abstract: Peer review, as a cornerstone of scientific research, ensures the integrity and quality of scholarly work by providing authors with objective feedback for refinement. However, in the traditional peer review process, authors often receive vague or insufficiently detailed feedback, which provides limited assistance and leads to a more time-consuming review cycle. If authors can identify some specific weaknesses in their paper, they can not only address the reviewer's concerns but also improve their work. This raises the critical question of how to enhance authors' comprehension of review comments. In this paper, we present SEAGraph, a novel framework developed to clarify review comments by uncovering the underlying intentions behind them. We construct two types of graphs for each paper: the semantic mind graph, which captures the authors' thought process, and the hierarchical background graph, which delineates the research domains related to the paper. A retrieval method is then designed to extract relevant content from both graphs, facilitating coherent explanations for the review comments. Extensive experiments show that SEAGraph excels in review comment understanding tasks, offering significant benefits to authors. By bridging the gap between reviewers' critiques and authors' comprehension, SEAGraph contributes to a more efficient, transparent and collaborative scientific publishing ecosystem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional Surgery for Language Model Activations</title>
<link>https://arxiv.org/abs/2501.15758</link>
<guid>https://arxiv.org/abs/2501.15758</guid>
<content:encoded><![CDATA[
arXiv:2501.15758v2 Announce Type: replace-cross 
Abstract: Language models, while capable of generating remarkably coherent and seemingly accurate text, can occasionally produce undesirable content, including harmful or toxic outputs. In this paper, we present a new two-stage approach to detect and mitigate undesirable content generations by rectifying activations. First, we train an ensemble of layerwise classifiers to detect undesirable content using activations by minimizing a smooth surrogate of the risk-aware score. Then, for detected undesirable contents, we propose layerwise distributional steering policies that transform the attention heads. These policies are computed through principled semidefinite programming, which aims to minimally perturb the attention distribution while probabilistically guaranteeing the effectiveness of the editions. Empirical evaluations across multiple language models and datasets show that our method outperforms baselines in reducing the generation of undesirable output.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Pre-training of MoEs: How robust is your router?</title>
<link>https://arxiv.org/abs/2503.05029</link>
<guid>https://arxiv.org/abs/2503.05029</guid>
<content:encoded><![CDATA[
arXiv:2503.05029v2 Announce Type: replace-cross 
Abstract: Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers that require the same amount of floating-point operations (FLOPs) per forward pass, MoEs benefit from improved sample efficiency at training time and achieve much stronger performance. Many closed-source and open-source frontier language models have thus adopted an MoE architecture. Naturally, practitioners will want to extend the capabilities of these models with large amounts of newly collected data without completely re-training them. Prior work has shown that a simple combination of replay, learning rate re-warming, and re-decaying can enable the continual pre-training (CPT) of dense decoder-only transformers with minimal performance degradation compared to full re-training. In the case of decoder-only MoE transformers, however, it is unclear how the routing algorithm will impact continual pre-training performance: 1) do the MoE transformer's routers exacerbate forgetting relative to a dense model?; 2) do the routers maintain a balanced load on previous distributions after CPT?; 3) are the same strategies applied to dense models sufficient to continually pre-train MoE LLMs? In what follows, we conduct a large-scale study training a 500M parameter dense transformer and four 500M-active/2B-total parameter MoE transformers. Each model is trained for 600B tokens. Our results establish a surprising robustness to distribution shifts for MoEs using both Sinkhorn-Balanced and Z-and-Aux-loss-balanced routing algorithms, even in MoEs continually pre-trained without replay. Moreover, we show that MoE LLMs maintain their sample efficiency (relative to a FLOP-matched dense model) during CPT and that they can match the performance of a fully re-trained MoE at a fraction of the cost.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[
arXiv:2504.10514v3 Announce Type: replace-cross 
Abstract: Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-to-Pipeline: Bridging Natural Language and Data Preparation Pipelines</title>
<link>https://arxiv.org/abs/2505.15874</link>
<guid>https://arxiv.org/abs/2505.15874</guid>
<content:encoded><![CDATA[
arXiv:2505.15874v2 Announce Type: replace-cross 
Abstract: Data preparation (DP) transforms raw data into a form suitable for downstream applications, typically by composing operations into executable pipelines. Building such pipelines is time-consuming and requires sophisticated programming skills, posing a significant barrier for non-experts. To lower this barrier, we introduce Text-to-Pipeline, a new task that translates NL data preparation instructions into DP pipelines, and PARROT, a large-scale benchmark to support systematic evaluation. To ensure realistic DP scenarios, PARROT is built by mining transformation patterns from production pipelines and instantiating them on 23,009 real-world tables, resulting in ~18,000 tasks spanning 16 core operators. Our empirical evaluation on PARROT reveals a critical failure mode in cutting-edge LLMs: they struggle not only with multi-step compositional logic but also with semantic parameter grounding. We thus establish a strong baseline with Pipeline-Agent, an execution-aware agent that iteratively reflects on intermediate states. While it achieves state-of-the-art performance, a significant gap remains, underscoring the deep, unsolved challenges for PARROT. It provides the essential, large-scale testbed for developing and evaluating the next generation of autonomous data preparation agentic systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title>
<link>https://arxiv.org/abs/2506.00708</link>
<guid>https://arxiv.org/abs/2506.00708</guid>
<content:encoded><![CDATA[
arXiv:2506.00708v3 Announce Type: replace-cross 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dissecting Long-Chain-of-Thought Reasoning Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2506.04913</link>
<guid>https://arxiv.org/abs/2506.04913</guid>
<content:encoded><![CDATA[
arXiv:2506.04913v2 Announce Type: replace-cross 
Abstract: Despite recent progress in training long-chain-of-thought reasoning models via scaling reinforcement learning (RL), its underlying training dynamics remain poorly understood, and several counterintuitive behaviors persist. This work focuses on three key aspects: (1) We systematically analyze the roles of positive and negative samples in scaling RL, revealing that positive samples mainly facilitate precise fitting to the training data, whereas negative samples significantly enhance generalization and robustness. Interestingly, while positive samples are essential for convergence in the zero-RL setting, training on negative samples alone suffices to attain strong reasoning performance and even better generalization in cold-start scenarios. (2) We identify substantial data inefficiency in group relative policy optimization, where over half of the samples yield zero advantage. To address this, we explore two strategies, including relative length rewards and offline sample injection, to leverage these data better and enhance reasoning efficiency and capability. (3) We investigate unstable performance across various reasoning models and benchmarks, attributing instability to uncertain problems with ambiguous outcomes, and demonstrate that greedy decoding can distort evaluation by flipping the correctness of responses. Our code is available at: https://github.com/takagi97/Dissect-Long-Reason-Models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[
arXiv:2507.16795v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</title>
<link>https://arxiv.org/abs/2507.18224</link>
<guid>https://arxiv.org/abs/2507.18224</guid>
<content:encoded><![CDATA[
arXiv:2507.18224v3 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Medical Event Models Improve with Scale</title>
<link>https://arxiv.org/abs/2508.12104</link>
<guid>https://arxiv.org/abs/2508.12104</guid>
<content:encoded><![CDATA[
arXiv:2508.12104v3 Announce Type: replace-cross 
Abstract: Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Curiosity models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study of medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Consequently, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, Curiosity autoregressively predicts the next medical event to simulate patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, Curiosity generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. Curiosity's predictive power consistently improves as the model and pretraining scale. Our results show that Curiosity, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.21227</link>
<guid>https://arxiv.org/abs/2509.21227</guid>
<content:encoded><![CDATA[
arXiv:2509.21227v2 Announce Type: replace-cross 
Abstract: Text-image generation has advanced rapidly, but assessing whether outputs truly capture the objects, attributes, and relations described in prompts remains a central challenge. Evaluation in this space relies heavily on automated metrics, yet these are often adopted by convention or popularity rather than validated against human judgment. Because evaluation and reported progress in the field depend directly on these metrics, it is critical to understand how well they reflect human preferences. To address this, we present a broad study of widely used metrics for compositional text-image evaluation. Our analysis goes beyond simple correlation, examining their behavior across diverse compositional challenges and comparing how different metric families align with human judgments. The results show that no single metric performs consistently across tasks: performance varies with the type of compositional problem. Notably, VQA-based metrics, though popular, are not uniformly superior, while certain embedding-based metrics prove stronger in specific cases. Image-only metrics, as expected, contribute little to compositional evaluation, as they are designed for perceptual quality rather than alignment. These findings underscore the importance of careful and transparent metric selection, both for trustworthy evaluation and for their use as reward models in generation. Project page is available at https://amirkasaei.com/eval-the-evals/ .
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation</title>
<link>https://arxiv.org/abs/2509.21257</link>
<guid>https://arxiv.org/abs/2509.21257</guid>
<content:encoded><![CDATA[
arXiv:2509.21257v2 Announce Type: replace-cross 
Abstract: In language and vision-language models, hallucination is broadly understood as content generated from a model's prior knowledge or biases rather than from the given input. While this phenomenon has been studied in those domains, it has not been clearly framed for text-to-image (T2I) generative models. Existing evaluations mainly focus on alignment, checking whether prompt-specified elements appear, but overlook what the model generates beyond the prompt. We argue for defining hallucination in T2I as bias-driven deviations and propose a taxonomy with three categories: attribute, relation, and object hallucinations. This framing introduces an upper bound for evaluation and surfaces hidden biases, providing a foundation for richer assessment of T2I models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Markovian Thinker</title>
<link>https://arxiv.org/abs/2510.06557</link>
<guid>https://arxiv.org/abs/2510.06557</guid>
<content:encoded><![CDATA[
arXiv:2510.06557v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy</title>
<link>https://arxiv.org/abs/2510.16830</link>
<guid>https://arxiv.org/abs/2510.16830</guid>
<content:encoded><![CDATA[
arXiv:2510.16830v2 Announce Type: replace-cross 
Abstract: Large language models are often adapted through parameter efficient fine tuning, but current release practices provide weak assurances about what data were used and how updates were computed. We present Verifiable Fine Tuning, a protocol and system that produces succinct zero knowledge proofs that a released model was obtained from a public initialization under a declared training program and an auditable dataset commitment. The approach combines five elements. First, commitments that bind data sources, preprocessing, licenses, and per epoch quota counters to a manifest. Second, a verifiable sampler that supports public replayable and private index hiding batch selection. Third, update circuits restricted to parameter efficient fine tuning that enforce AdamW style optimizer semantics and proof friendly approximations with explicit error budgets. Fourth, recursive aggregation that folds per step proofs into per epoch and end to end certificates with millisecond verification. Fifth, provenance binding and optional trusted execution property cards that attest code identity and constants. On English and bilingual instruction mixtures, the method maintains utility within tight budgets while achieving practical proof performance. Policy quotas are enforced with zero violations, and private sampling windows show no measurable index leakage. Federated experiments demonstrate that the system composes with probabilistic audits and bandwidth constraints. These results indicate that end to end verifiable fine tuning is feasible today for real parameter efficient pipelines, closing a critical trust gap for regulated and decentralized deployments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs' Reasoning Over Ordered Procedural Steps</title>
<link>https://arxiv.org/abs/2511.04688</link>
<guid>https://arxiv.org/abs/2511.04688</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural sequences, language models, food recipes, ordering quality, model performance<br />
Summary:<br />
The study focuses on the task of reconstructing ordered sequences from shuffled procedural steps using a dataset of food recipes. Various large language models (LLMs) are evaluated in zero-shot and few-shot settings using metrics like Kendall's Tau, NLCS, and NED to assess ordering quality. The performance of LLMs decreases with longer sequences and more severe shuffling, indicating challenges in procedural reasoning. The research highlights the limitations of current LLMs in handling complex procedural sequences with increasing length and disorder. Overall, the analysis underscores the importance of accurate sequencing in tasks like food recipes and the need for advancements in LLMs for effective procedural reasoning.<br /> 
Summary: <div>
arXiv:2511.04688v1 Announce Type: new 
Abstract: Reasoning over procedural sequences, where the order of steps directly impacts outcomes, is a critical capability for large language models (LLMs). In this work, we study the task of reconstructing globally ordered sequences from shuffled procedural steps, using a curated dataset of food recipes, a domain where correct sequencing is essential for task success. We evaluate several LLMs under zero-shot and few-shot settings and present a comprehensive evaluation framework that adapts established metrics from ranking and sequence alignment. These include Kendall's Tau, Normalized Longest Common Subsequence (NLCS), and Normalized Edit Distance (NED), which capture complementary aspects of ordering quality. Our analysis shows that model performance declines with increasing sequence length, reflecting the added complexity of longer procedures. We also find that greater step displacement in the input, corresponding to more severe shuffling, leads to further degradation. These findings highlight the limitations of current LLMs in procedural reasoning, especially with longer and more disordered inputs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks</title>
<link>https://arxiv.org/abs/2511.04689</link>
<guid>https://arxiv.org/abs/2511.04689</guid>
<content:encoded><![CDATA[
<div> Item Response Theory, large language model evaluation, adaptive testing, benchmark items, measurement precision<br />
<br />
Summary: ATLAS is an adaptive testing framework that uses Item Response Theory to estimate model ability by selecting benchmark items based on Fisher information. It reduces the number of items needed for evaluation by 90% while maintaining measurement precision. Analysis shows that a small percentage of benchmark items have negative discrimination, indicating annotation errors that can corrupt evaluation results. ATLAS allows for more efficient evaluation with lower item exposure rates and test overlap compared to static benchmarks where all models see all items. IRT rankings of models differ from accuracy rankings, with models having the same accuracy receiving different IRT scores and a significant percentage of models shifting rank positions. The framework provides code and calibrated item banks for easy implementation. <div>
arXiv:2511.04689v1 Announce Type: new 
Abstract: Large language model evaluation requires thousands of benchmark items, making evaluations expensive and slow. Existing methods compute average accuracy across fixed item sets, treating all items equally despite varying quality and informativeness. We present ATLAS an adaptive testing framework using Item Response Theory (IRT) to estimate model ability through Fisher information-guided item selection. Our analysis of five major benchmarks reveals that 3-6% of items exhibit negative discrimination, indicating annotation errors that corrupt static evaluation. ATLAS achieves 90% item reduction while maintaining measurement precision: on HellaSwag (5,608 items), we match full-benchmark estimates using only 42 items with 0.154 MAE. Our framework maintains item exposure rates below 10% and test overlap at 16-27%, compared to static benchmarks where every model sees all items (100% exposure). Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with the same accuracy get different IRT scores, and 23-31% of all models shift by more than 10 rank positions. Code and calibrated item banks are available at https://github.com/Peiyu-Georgia-Li/ATLAS.git.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection</title>
<link>https://arxiv.org/abs/2511.04692</link>
<guid>https://arxiv.org/abs/2511.04692</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, sentiment information, user roles, deep clustering, SARC

Summary: <br /><br /> Fake news detection in social networks has been a significant research focus, with recent studies highlighting the importance of incorporating sentiment information from both news content and user comments for improved performance. However, existing approaches often overlook role differentiation, where the same sentiment polarity can come from users with different roles, limiting their ability to capture nuanced patterns. To address this issue, the proposed Sentiment-Augmented Role Clustering (SARC) framework utilizes sentiment-enhanced deep clustering to identify user roles for better fake news detection. By generating user features through joint comment text representation and sentiment encoding, SARC constructs a deep clustering module to categorize user roles and optimize dual objectives for role clustering and fake news detection simultaneously. Experimental results on benchmark datasets show that SARC outperforms baseline models in all metrics, demonstrating its effectiveness in enhancing fake news detection. <div>
arXiv:2511.04692v1 Announce Type: new 
Abstract: Fake news detection has been a long-standing research focus in social networks. Recent studies suggest that incorporating sentiment information from both news content and user comments can enhance detection performance. However, existing approaches typically treat sentiment features as auxiliary signals, overlooking role differentiation, that is, the same sentiment polarity may originate from users with distinct roles, thereby limiting their ability to capture nuanced patterns for effective detection. To address this issue, we propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes sentiment-enhanced deep clustering to identify user roles for improved fake news detection. The framework first generates user features through joint comment text representation (with BiGRU and Attention mechanism) and sentiment encoding. It then constructs a differentiable deep clustering module to automatically categorize user roles. Finally, unlike existing approaches which take fake news label as the unique supervision signal, we propose a joint optimization objective integrating role clustering and fake news detection to further improve the model performance. Experimental results on two benchmark datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior performance across all metrics compared to baseline models. The code is available at: https://github.com/jxshang/SARC.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Up the Instruction Ladder for Controllable Language Models</title>
<link>https://arxiv.org/abs/2511.04694</link>
<guid>https://arxiv.org/abs/2511.04694</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, instruction hierarchy, reasoning task, reinforcement learning, safety-critical settings

Summary:
This article introduces the concept of instruction hierarchy in large language models (LLMs) to manage conflicting instructions from different sources within a prompt context. The researchers frame instruction hierarchy resolution as a reasoning task and create the VerIH dataset to train models on prioritizing instructions. Through lightweight reinforcement learning, models are trained to understand the relationship between user prompts and system instructions, leading to improvements in instruction following and hierarchy benchmarks. The trained models demonstrate enhanced reasoning abilities that extend to safety-critical scenarios, protecting against adversarial attacks. By effectively resolving conflicts between user inputs and predefined policies, the models exhibit robustness and controllability in responding to prompt updates. This research highlights how reasoning over instruction hierarchies can enhance the reliability and performance of LLMs in real-world decision-making contexts.<br /><br />Summary: <div>
arXiv:2511.04694v1 Announce Type: new 
Abstract: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises both aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks. These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EncouRAGe: Evaluating RAG Local, Fast, and Reliable</title>
<link>https://arxiv.org/abs/2511.04696</link>
<guid>https://arxiv.org/abs/2511.04696</guid>
<content:encoded><![CDATA[
<div> Keywords: EncouRAGe, Python framework, Retrieval-Augmented Generation, Large Language Models, Evaluation

Summary:
EncouRAGe is a Python framework designed for developing and evaluating Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. The framework consists of five components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics. It prioritizes scientific reproducibility, diverse evaluation metrics, and local deployment for efficient dataset assessment in RAG workflows. Evaluation on multiple benchmark datasets showed that RAG still lags behind Oracle Context performance, with Hybrid BM25 achieving the best results. Reranking had marginal performance gains but increased response latency. This comprehensive framework offers flexibility and extensibility in RAG system development and evaluation.<br /><br />Summary: <div>
arXiv:2511.04696v1 Announce Type: new 
Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to streamline the development and evaluation of Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. EncouRAGe comprises five modular and extensible components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics, facilitating flexible experimentation and extensible development. The framework emphasizes scientific reproducibility, diverse evaluation metrics, and local deployment, enabling researchers to efficiently assess datasets within RAG workflows. This paper presents implementation details and an extensive evaluation across multiple benchmark datasets, including 25k QA pairs and over 51k documents. Our results show that RAG still underperforms compared to the Oracle Context, while Hybrid BM25 consistently achieves the best results across all four datasets. We further examine the effects of reranking, observing only marginal performance improvements accompanied by higher response latency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder</title>
<link>https://arxiv.org/abs/2511.04698</link>
<guid>https://arxiv.org/abs/2511.04698</guid>
<content:encoded><![CDATA[
<div> detecting mental health disorders, social media text, multiMentalRoBERTa, classification, stress, anxiety, depression, PTSD, suicidal ideation<br />
Summary:<br />
- The study introduces multiMentalRoBERTa, a model for classifying mental health conditions from social media text.<br />
- Data exploration reveals correlations between depression and suicidal ideation, and anxiety and PTSD.<br />
- multiMentalRoBERTa outperforms other methods with macro F1-scores of 0.839 and 0.870 in different setups.<br />
- Explainability methods such as Layer Integrated Gradients and KeyBERT are used to identify key features for classification, focusing on distinguishing depression from suicidal ideation.<br />
- The study emphasizes the importance of fairness, bias mitigation, and human-in-the-loop safety protocols in mental health detection.<br />
Summary: <div>
arXiv:2511.04698v1 Announce Type: new 
Abstract: The early detection of mental health disorders from social media text is critical for enabling timely support, risk assessment, and referral to appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned RoBERTa model designed for multiclass classification of common mental health conditions, including stress, anxiety, depression, post-traumatic stress disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple curated datasets, data exploration is conducted to analyze class overlaps, revealing strong correlations between depression and suicidal ideation as well as anxiety and PTSD, while stress emerges as a broad, overlapping category. Comparative experiments with traditional machine learning methods, domain-specific transformers, and prompting-based large language models demonstrate that multiMentalRoBERTa achieves superior performance, with macro F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup (excluding stress), outperforming both fine-tuned MentalBERT and baseline classifiers. Beyond predictive accuracy, explainability methods, including Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues that drive classification, with a particular focus on distinguishing depression from suicidal ideation. The findings emphasize the effectiveness of fine-tuned transformers for reliable and interpretable detection in sensitive contexts, while also underscoring the importance of fairness, bias mitigation, and human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as a lightweight, robust, and deployable solution for enhancing support in mental health platforms.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding</title>
<link>https://arxiv.org/abs/2511.04699</link>
<guid>https://arxiv.org/abs/2511.04699</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-Lingual SynthDocs, Arabic resources, Optical Character Recognition, Document Understanding, multilingual document analysis

Summary:<br />
Cross-Lingual SynthDocs is a synthetic corpus created to address the lack of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset includes over 2.5 million samples, consisting of textual data, annotated tables, and real data-based charts. The pipeline utilizes scanned backgrounds, bilingual layouts, and diacritic-aware fonts to capture Arabic document complexity. Various styles for charts and tables are also included in the corpus. Finetuning on SynthDocs shows improved Word Error Rate (WER) and Character Error Rate (CER) in OCR, as well as enhanced Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) in other modalities. This resource provides a scalable and visually realistic tool for advancing multilingual document analysis research.

Summary: <div>
arXiv:2511.04699v1 Announce Type: new 
Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.04700</link>
<guid>https://arxiv.org/abs/2511.04700</guid>
<content:encoded><![CDATA[
<div> Keyword: Retrieval-augmented generation, large language models, external knowledge, WinnowRAG, document filtering  
Summary:  
- Retrieval-augmented generation (RAG) integrates external knowledge sources to enhance large language models (LLMs).  
- To increase the likelihood of retrieving relevant information, WinnowRAG filters out noisy documents while preserving valuable content through a process known as winnowing.  
- WinnowRAG operates in two stages: query-aware clustering to form distinct topic clusters and winnowing to separate useful documents from noisy ones.  
- A critic LLM evaluates outputs of multiple agents and strategic merging techniques ensure only relevant knowledge is used for final response generation.  
- Model-agnostic WinnowRAG does not require model fine-tuning, making it easily adaptable to various tasks.  

<br /><br />Summary:  
Retrieval-augmented generation (RAG) enhances large language models by integrating external knowledge sources to address limitations in accessing up-to-date or specialized information. WinnowRAG, a novel RAG framework, systematically filters out noisy documents while preserving valuable content through winnowing. Operating in two stages, WinnowRAG uses query-aware clustering to form distinct topic clusters and a critic LLM for evaluating outputs and separating useful documents. Strategic merging techniques ensure that only relevant knowledge is used for generating final responses. This model-agnostic approach of WinnowRAG, without requiring model fine-tuning, makes it easily adaptable to various tasks. Extensive experiments demonstrate the effectiveness of WinnowRAG over existing baselines on realistic datasets. <div>
arXiv:2511.04700v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title>
<link>https://arxiv.org/abs/2511.04703</link>
<guid>https://arxiv.org/abs/2511.04703</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, evaluation, safety, robustness, construct validity

Summary:<br /><br />
In this study, a systematic review was conducted on 445 benchmarks for large language models (LLMs) to evaluate their capabilities and identify safety or robustness issues. It was found that the measures used in these benchmarks often did not effectively capture the phenomena of safety and robustness, leading to questionable validity of the claims made. The team of 29 expert reviewers identified patterns related to the measured phenomena, tasks, and scoring metrics that undermined the construct validity of the benchmarks. To address these shortcomings, eight key recommendations and actionable guidance were provided for researchers and practitioners in the development of LLM benchmarks. It is crucial for the field to improve the measurement of safety and robustness in LLM evaluations to ensure the reliability and validity of their assessments. 

Summary: <div>
arXiv:2511.04703v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios</title>
<link>https://arxiv.org/abs/2511.04705</link>
<guid>https://arxiv.org/abs/2511.04705</guid>
<content:encoded><![CDATA[
<div> Keywords: POLIS-Bench, LLMs, bilingual policy scenarios, dual-metric evaluation, compliance tasks

Summary:
POLIS-Bench is introduced as a comprehensive evaluation suite for LLMs in governmental bilingual policy scenarios. It includes an up-to-date bilingual policy corpus, scenario-grounded task design, and a dual-metric evaluation framework. The benchmark reveals a performance hierarchy among LLMs, with reasoning models excelling in cross-task stability and accuracy. Compliance tasks are particularly challenging. Leveraging POLIS-Bench, a lightweight open-source model achieves comparable or better performance to proprietary baselines at a reduced cost. This provides a cost-effective pathway for real-world governmental deployment. <div>
arXiv:2511.04705v1 Announce Type: new 
Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite designed for LLMs operating in governmental bilingual policy scenarios. Compared to existing benchmarks, POLIS-Bench introduces three major advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive, up-to-date policy corpus that significantly scales the effective assessment sample size, ensuring relevance to current governance practice. (ii) Scenario-Grounded Task Design: We distill three specialized, scenario-grounded tasks -- Clause Retrieval & Interpretation, Solution Generation, and the Compliance Judgmen--to comprehensively probe model understanding and application. (iii) Dual-Metric Evaluation Framework: We establish a novel dual-metric evaluation framework combining semantic similarity with accuracy rate to precisely measure both content alignment and task requirement adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on POLIS-Bench reveals a clear performance hierarchy where reasoning models maintain superior cross-task stability and accuracy, highlighting the difficulty of compliance tasks. Furthermore, leveraging our benchmark, we successfully fine-tune a lightweight open-source model. The resulting POLIS series models achieves parity with, or surpasses, strong proprietary baselines on multiple policy subtasks at a significantly reduced cost, providing a cost-effective and compliant path for robust real-world governmental deployment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models</title>
<link>https://arxiv.org/abs/2511.04710</link>
<guid>https://arxiv.org/abs/2511.04710</guid>
<content:encoded><![CDATA[
<div> model, Text-to-SQL, GEMMA-SQL, instruction-tuned, benchmark  
Summary:  
GEMMA-SQL is a lightweight and efficient text-to-SQL model based on the Gemma 2B architecture. It is fine-tuned in a resource-efficient manner and can run on low-cost hardware. By leveraging prompting strategies like few-shot learning, GEMMA-SQL enhances SQL query generation accuracy on the SPIDER benchmark. The instruction-tuned variant, GEMMA-SQL Instruct, outperforms state-of-the-art models with 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy. Effective prompt design and targeted instruction tuning are shown to significantly improve performance while maintaining scalability and adaptability. GEMMA-SQL is positioned as a practical, open-source option for reliable and accessible text-to-SQL systems.  
Summary: <div>
arXiv:2511.04710v1 Announce Type: new 
Abstract: Text-to-SQL systems enable users to interact with structured databases using natural language, eliminating the need for specialized programming knowledge. In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL model built upon the open-source Gemma 2B architecture. Unlike many large language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient, iterative manner and can be deployed on low-cost hardware. Leveraging the SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple prompting strategies, including few-shot learning, to enhance SQL query generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct, achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy, outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and CodeXDavinci. The proposed approach demonstrates that effective prompt design and targeted instruction tuning can significantly boost performance while maintaining high scalability and adaptability. These results position GEMMA-SQL as a practical, open-source alternative for robust and accessible text-to-SQL systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation</title>
<link>https://arxiv.org/abs/2511.04715</link>
<guid>https://arxiv.org/abs/2511.04715</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Influence Functions, Attention Layers, Aggregation Methods, Noise Detection Rate

Summary: 
The study focuses on understanding how training samples affect Large Language Model (LLM) decision-making and the methods used to estimate their influence. Contrary to prior beliefs, the cancellation effect hypothesis regarding the informative nature of the first layers for influence computation is debunked. The research suggests that middle attention layers are more accurate estimators of influence in LLMs. Additionally, alternative aggregation methods such as ranking and vote-based approaches significantly improve influence score performance. The study introduces the Noise Detection Rate (NDR) as a novel metric for assessing influence score efficacy without retraining the model, demonstrating superior predictive capability compared to the cancellation effect. Through extensive experimentation with various types and scales of LLMs, the research concludes that the first layers are not always better than the last layers for influence estimation in LLMs. 

<br /><br />Summary: <div>
arXiv:2511.04715v1 Announce Type: new 
Abstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to reason about rare diseases through retrieval-augmented agents</title>
<link>https://arxiv.org/abs/2511.04720</link>
<guid>https://arxiv.org/abs/2511.04720</guid>
<content:encoded><![CDATA[
<div> Rare diseases, brain MRI, AI agents, medical knowledge, diagnostic reasoning<br />
Summary:<br />
RADAR, or Retrieval Augmented Diagnostic Reasoning Agents, is a system designed to aid in rare disease detection in brain MRI scans. The approach utilizes AI agents that have access to external medical knowledge from case reports and literature. By embedding this knowledge and using efficient similarity search techniques, RADAR can retrieve relevant evidence to assist radiologists in making diagnostic decisions for unfamiliar diseases. This system does not require additional training and can be seamlessly integrated with various large language models to improve rare pathology recognition and interpretability. Testing on the NOVA dataset showed up to a 10.2% performance gain, especially with open source models like DeepSeek. The retrieved examples not only enhance accuracy but also provide interpretable explanations grounded in literature, showcasing the effectiveness of retrieval-augmented reasoning for low-prevalence conditions in medical imaging.<br /> <div>
arXiv:2511.04720v1 Announce Type: new 
Abstract: Rare diseases represent the long tail of medical imaging, where AI models often fail due to the scarcity of representative training data. In clinical workflows, radiologists frequently consult case reports and literature when confronted with unfamiliar findings. Following this line of reasoning, we introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic system for rare disease detection in brain MRI. Our approach uses AI agents with access to external medical knowledge by embedding both case reports and literature using sentence transformers and indexing them with FAISS to enable efficient similarity search. The agent retrieves clinically relevant evidence to guide diagnostic decision making on unseen diseases, without the need of additional training. Designed as a model-agnostic reasoning module, RADAR can be seamlessly integrated with diverse large language models, consistently improving their rare pathology recognition and interpretability. On the NOVA dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2% performance gain, with the strongest improvements observed for open source models such as DeepSeek. Beyond accuracy, the retrieved examples provide interpretable, literature grounded explanations, highlighting retrieval-augmented reasoning as a powerful paradigm for low-prevalence conditions in medical imaging.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surprisal reveals diversity gaps in image captioning and different scorers change the story</title>
<link>https://arxiv.org/abs/2511.04754</link>
<guid>https://arxiv.org/abs/2511.04754</guid>
<content:encoded><![CDATA[
<div> Linguistic diversity, image captioning, surprisal variance, state-of-the-art LLMs, human captions<br />
<br />
Summary:<br />
- The study focuses on quantifying linguistic diversity in image captioning using surprisal variance, which measures the spread of token-level negative log-probabilities within a set of captions.
- Comparison of five advanced vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions on the MSCOCO test set reveals humans display approximately double the surprisal variance of models.
- However, when rescoring captions with a general-language model, the pattern is reversed, indicating the importance of evaluating diversity under multiple scoring metrics.
- The analysis introduces the metric of surprisal-based diversity for image captioning, highlighting the need for robust evaluation methods in assessing diversity.
- It is emphasized that relying on a single scorer can completely reverse conclusions, underscoring the significance of considering surprisal under various scorers in diversity evaluations. <br /> <div>
arXiv:2511.04754v1 Announce Type: new 
Abstract: We quantify linguistic diversity in image captioning with surprisal variance - the spread of token-level negative log-probabilities within a caption set. On the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions. Measured with a caption-trained n-gram LM, humans display roughly twice the surprisal variance of models, but rescoring the same captions with a general-language model reverses the pattern. Our analysis introduces the surprisal-based diversity metric for image captioning. We show that relying on a single scorer can completely invert conclusions, thus, robust diversity evaluation must report surprisal under several scorers.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.04800</link>
<guid>https://arxiv.org/abs/2511.04800</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Large Language Models, Group Relative Policy Optimization, Explore Residual Prompts<br />
Summary:<br />
The article discusses Reinforcement Learning with Verifiable Rewards (RLVR) and the challenges faced when training large language models (LLMs) with residual prompts providing no training signal. The Explore Residual Prompts in Policy Optimization (ERPO) framework is introduced to address this issue by encouraging exploration on residual prompts and reactivating their training signals. ERPO maintains a history tracker for each prompt and adjusts sampling temperature for prompts with all correct responses to introduce more diverse reasoning traces. Empirical results on mathematical reasoning benchmarks show that ERPO outperforms strong baselines, improving the efficiency and effectiveness of training large language models with RLVR. <div>
arXiv:2511.04800v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for improving the reasoning abilities of large language models (LLMs). The Group Relative Policy Optimization (GRPO) family has demonstrated strong performance in training LLMs with RLVR. However, as models train longer and scale larger, more training prompts become residual prompts, those with zero variance rewards that provide no training signal. Consequently, fewer prompts contribute to training, reducing diversity and hindering effectiveness. To fully exploit these residual prompts, we propose the Explore Residual Prompts in Policy Optimization (ERPO) framework, which encourages exploration on residual prompts and reactivates their training signals. ERPO maintains a history tracker for each prompt and adaptively increases the sampling temperature for residual prompts that previously produced all correct responses. This encourages the model to generate more diverse reasoning traces, introducing incorrect responses that revive training signals. Empirical results on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong baselines across multiple mathematical reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title>
<link>https://arxiv.org/abs/2511.04869</link>
<guid>https://arxiv.org/abs/2511.04869</guid>
<content:encoded><![CDATA[
<div> sampling, semantic calibration, question-answering, calibration, local loss optimality 
Summary:<br /><br />Large Language Models (LLMs) have been found to exhibit semantic calibration in open-domain question-answering tasks, suggesting they can assess confidence in their responses beyond token-level. A theoretical framework based on next-token prediction explains how semantic calibration emerges as a byproduct of calibration and local loss optimality. Base LLMs are expected to be semantically calibrated when able to predict their distribution over semantic answer classes before generating a response. Experimental validation confirms three implications: (1) Base LLMs exhibit semantic calibration, (2) RL instruction-tuning breaks this calibration, and (3) chain-of-thought reasoning disrupts calibration. This study provides a foundational understanding of the conditions under which semantic calibration arises in LLMs. <br /><br /> <div>
arXiv:2511.04869v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. We find that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. Our main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of "B-calibration," which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. We state three implications of this prediction, which we validate through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. To our knowledge, our work provides the first principled explanation of when and why semantic calibration emerges in LLMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title>
<link>https://arxiv.org/abs/2511.04875</link>
<guid>https://arxiv.org/abs/2511.04875</guid>
<content:encoded><![CDATA[
<div> self-awareness, LLMs, low-rank adapters, fine-tuning, activation space <br />
Summary: 
1. The study explores the emergence of behavioral self-awareness in Large Language Models (LLMs) without explicit supervision.
2. Through experiments with low-rank adapters (LoRA), it is found that self-awareness can be induced with a single rank-1 LoRA adapter.
3. The learned self-aware behavior can be represented by a single steering vector in activation space, capturing most of the fine-tune's effect.
4. Self-awareness is domain-specific and localized, with unique representations across tasks.
5. The findings suggest that self-awareness in LLMs is a linear feature that can be easily induced and modulated. <br /> <div>
arXiv:2511.04875v1 Announce Type: new 
Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness: the ability to accurately describe or predict their own learned behaviors without explicit supervision. This capability raises safety concerns as it may, for example, allow models to better conceal their true abilities during evaluation. We attempt to characterize the minimal conditions under which such self-awareness emerges, and the mechanistic processes through which it manifests. Through controlled finetuning experiments on instruction-tuned LLMs with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably induced using a single rank-1 LoRA adapter; (2) that the learned self-aware behavior can be largely captured by a single steering vector in activation space, recovering nearly all of the fine-tune's behavioral effect; and (3) that self-awareness is non-universal and domain-localized, with independent representations across tasks. Together, these findings suggest that behavioral self-awareness emerges as a domain-specific, linear feature that can be easily induced and modulated.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents</title>
<link>https://arxiv.org/abs/2511.04910</link>
<guid>https://arxiv.org/abs/2511.04910</guid>
<content:encoded><![CDATA[
<div> benchmark, visual document retrieval, Korean, public documents, multimodal models <br />
Summary:<br />
The article introduces SDS KoPub VDR, an innovative benchmark focusing on visual document retrieval for Korean public documents. The benchmark includes a corpus of 361 real-world documents with complex visual elements such as tables and charts. It also features 600 query-page-answer triples generated using multimodal models and refined for accuracy. The queries cover six major public domains and are categorized based on reasoning modality. Evaluation tasks include text-only retrieval and multimodal retrieval to assess model performance. Results show significant gaps in performance, particularly in multimodal scenarios requiring cross-modal reasoning. SDS KoPub VDR not only facilitates rigorous evaluation of textual and multimodal retrieval tasks but also serves as a valuable resource for advancing multimodal AI in real-world document intelligence.<br /> <div>
arXiv:2511.04910v1 Announce Type: new 
Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook non-English languages and the structural complexity of official publications. To address this critical gap, we introduce SDS KoPub VDR, the first large-scale, publicly available benchmark for retrieving and understanding Korean public documents. The benchmark is built upon a corpus of 361 real-world documents (40,781 pages), including 256 files under the KOGL Type 1 license and 105 from official legal portals, capturing complex visual elements like tables, charts, and multi-column layouts. To establish a challenging and reliable evaluation set, we constructed 600 query-page-answer triples. These were initially generated using multimodal models (e.g., GPT-4o) and subsequently underwent a rigorous human verification and refinement process to ensure factual accuracy and contextual relevance. The queries span six major public domains and are systematically categorized by the reasoning modality required: text-based, visual-based (e.g., chart interpretation), and cross-modal. We evaluate SDS KoPub VDR on two complementary tasks that reflect distinct retrieval paradigms: (1) text-only retrieval, which measures a model's ability to locate relevant document pages based solely on textual signals, and (2) multimodal retrieval, which assesses retrieval performance when visual features (e.g., tables, charts, and layouts) are jointly leveraged alongside text. This dual-task evaluation reveals substantial performance gaps, particularly in multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art models. As a foundational resource, SDS KoPub VDR not only enables rigorous and fine-grained evaluation across textual and multimodal retrieval tasks but also provides a clear roadmap for advancing multimodal AI in complex, real-world document intelligence.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models</title>
<link>https://arxiv.org/abs/2511.04919</link>
<guid>https://arxiv.org/abs/2511.04919</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, BudgetMem, memory constraints, selective memory policies, document length

Summary:
BudgetMem is a novel memory augmented architecture designed to address the computational and memory challenges faced by Large Language Models when processing long contexts. It focuses on learning what information to remember rather than storing everything, utilizing selective memory policies and feature-based salience scoring. The system combines learned gating mechanisms with BM25 sparse retrieval to efficiently access information while under strict budget constraints. Through comprehensive experiments, BudgetMem demonstrates remarkable results on long documents, achieving minimal F1 score degradation while saving a significant amount of memory compared to baseline models. The approach is validated through budget sensitivity analysis, baseline comparisons, and document length analysis, highlighting its benefits increasing with longer documents. BudgetMem offers a practical solution for deploying advanced language understanding capabilities on modest hardware, making it more accessible to a wider range of users. 

<br /><br />Summary: <div>
arXiv:2511.04919v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context windows to 100K-1M tokens, such approaches incur prohibitive costs for resource constrained deployments. We propose BudgetMem, a novel memory augmented architecture that learns what to remember rather than remembering everything. Our system combines selective memory policies with feature based salience scoring (entity density, TF-IDF, discourse markers, position bias) to decide which information merits storage under strict budget constraints. Unlike existing retrieval augmented generation (RAG) systems that store all chunks, BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval for efficient information access. Through comprehensive experiments on 700 question answer pairs across short (237 tokens) and long (5K-10K tokens) documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves remarkable results on long documents: only 1.0% F1 score degradation while saving 72.4% memory compared to baseline RAG. We validate our approach through budget sensitivity analysis (testing 7 budget ratios), naive baseline comparisons, and document length analysis, showing that BudgetMem's benefits increase with document length. Our work provides a practical pathway for deploying capable long context systems on modest hardware, democratizing access to advanced language understanding capabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent</title>
<link>https://arxiv.org/abs/2511.04921</link>
<guid>https://arxiv.org/abs/2511.04921</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model agents, dataset retrieval, baseline recommendation, experimental design, AI research

Summary:
Large language model agents are being used for tasks like information retrieval and complex reasoning on the web. To facilitate scientific research, automation of experiment design through dataset and baseline retrieval is critical in AI research. Existing methods have limitations in data coverage and bias toward superficial similarity. In this study, a framework for comprehensive baseline and dataset recommendation is proposed by leveraging collective perception from citation networks. An automated data-collection pipeline links accepted papers to the baselines and datasets they actually used. A collective perception enhanced retriever is designed to represent the scholarly network, enabling efficient candidate recall. A reasoning-augmented reranker constructs explicit reasoning chains for interpretable justifications and refined rankings. The method outperforms prior baselines with significant improvements in Recall@20 and HitRate@5 on datasets from top AI conferences. This advancement offers reliable and interpretable automation for experimental design.<br /><br />Summary: <div>
arXiv:2511.04921v1 Announce Type: new 
Abstract: Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy</title>
<link>https://arxiv.org/abs/2511.04926</link>
<guid>https://arxiv.org/abs/2511.04926</guid>
<content:encoded><![CDATA[
<div> entities, open knowledge graph, validation method, taxonomic inconsistency, crowdsourced

Summary:<br />
1. The study focuses on Wikidata, the largest open knowledge graph with over 120 million entities, integrating data from various databases and Wikipedia.
2. A novel validation method is proposed and applied to identify classification errors, over-generalized subclass links, and redundant connections in specific domains within Wikidata.
3. A new evaluation criterion is introduced to determine the significance of these issues for correction.
4. A system is developed for users to inspect taxonomic relationships of Wikidata entities, utilizing the platform's crowdsourced nature for improved accuracy and reliability.
5. The study aims to enhance the quality of taxonomic information on Wikidata and address existing inconsistencies, leveraging the platform's open structure for collaborative improvement. 

<br /><br />Summary: <div>
arXiv:2511.04926v1 Announce Type: new 
Abstract: Wikidata is currently the largest open knowledge graph on the web, encompassing over 120 million entities. It integrates data from various domain-specific databases and imports a substantial amount of content from Wikipedia, while also allowing users to freely edit its content. This openness has positioned Wikidata as a central resource in knowledge graph research and has enabled convenient knowledge access for users worldwide. However, its relatively loose editorial policy has also led to a degree of taxonomic inconsistency. Building on prior work, this study proposes and applies a novel validation method to confirm the presence of classification errors, over-generalized subclass links, and redundant connections in specific domains of Wikidata. We further introduce a new evaluation criterion for determining whether such issues warrant correction and develop a system that allows users to inspect the taxonomic relationships of arbitrary Wikidata entities-leveraging the platform's crowdsourced nature to its full potential.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model</title>
<link>https://arxiv.org/abs/2511.04952</link>
<guid>https://arxiv.org/abs/2511.04952</guid>
<content:encoded><![CDATA[
<div> Tokenization, Long context inference, Computational latency, Lossless Parallel Tokenization, Speedup <br />
Summary: <br />
Long context inference in large language models is important but computationally intensive. Existing methods for parallel tokenization suffer from inconsistent results. To address this issue, a novel framework called LoPT is proposed, ensuring lossless tokenization with significant speedup. LoPT uses character-position-based matching and dynamic chunk length adjustment to accurately align and merge tokenized segments. Extensive experiments on various long-text datasets demonstrate the effectiveness and consistency of LoPT. The theoretical proof and analytical studies further validate the robustness of the approach. <div>
arXiv:2511.04952v1 Announce Type: new 
Abstract: Long context inference scenarios have become increasingly important for large language models, yet they introduce significant computational latency. While prior research has optimized long-sequence inference through operators, model architectures, and system frameworks, tokenization remains an overlooked bottleneck. Existing parallel tokenization methods accelerate processing through text segmentation and multi-process tokenization, but they suffer from inconsistent results due to boundary artifacts that occur after merging. To address this, we propose LoPT, a novel Lossless Parallel Tokenization framework that ensures output identical to standard sequential tokenization. Our approach employs character-position-based matching and dynamic chunk length adjustment to align and merge tokenized segments accurately. Extensive experiments across diverse long-text datasets demonstrate that LoPT achieves significant speedup while guaranteeing lossless tokenization. We also provide theoretical proof of consistency and comprehensive analytical studies to validate the robustness of our method.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title>
<link>https://arxiv.org/abs/2511.04962</link>
<guid>https://arxiv.org/abs/2511.04962</guid>
<content:encoded><![CDATA[
<div> Moral RolePlay benchmark, large language models, creative generation, fictional characters, safety alignment <br />
<br />
Summary: The study explores the ability of Large Language Models (LLMs) to portray morally ambiguous or villainous characters. The researchers introduce the Moral RolePlay benchmark, which evaluates LLMs' role-playing fidelity on a four-level moral alignment scale. Results show a decline in role-playing accuracy as character morality decreases, with models struggling with traits like deceit and manipulation. The study highlights a conflict between model safety and creative fidelity, as highly safety-aligned models perform poorly in playing villainous roles. The findings emphasize the need for more nuanced alignment methods to address this limitation. <div>
arXiv:2511.04962v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Acquiring Common Chinese Emotional Events Using Large Language Model</title>
<link>https://arxiv.org/abs/2511.04989</link>
<guid>https://arxiv.org/abs/2511.04989</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese emotional events, large language model, sentiment polarity, knowledge base, emotion cause extraction

Summary:
Knowledge about emotional events is crucial for improving the effectiveness of various applications. This paper focuses on acquiring common emotional events in Chinese by utilizing a large language model (LLM) and a filter to ensure quality. By collecting emotional event indicators and prompting the LLM, the study generated 102,218 high-quality emotional events with sentiment polarity labels, forming a valuable knowledge base. The approach demonstrated effectiveness in acquiring common Chinese emotional events and showed potential for emotion cause extraction (ECE). The release of related emotional event indicators and emotional events will further contribute to the field. <div>
arXiv:2511.04989v1 Announce Type: new 
Abstract: Knowledge about emotional events is an important kind of knowledge which has been applied to improve the effectiveness of different applications. However, emotional events cannot be easily acquired, especially common or generalized emotional events that are context-independent. The goal of this paper is to obtain common emotional events in Chinese language such as "win a prize" and "be criticized". Our approach begins by collecting a comprehensive list of Chinese emotional event indicators. Then, we generate emotional events by prompting a Chinese large language model (LLM) using these indicators. To ensure the quality of these emotional events, we train a filter to discard invalid generated results. We also classify these emotional events as being positive events and negative events using different techniques. Finally, we harvest a total of 102,218 high-quality common emotional events with sentiment polarity labels, which is the only large-scale commonsense knowledge base of emotional events in Chinese language. Intrinsic evaluation results show that the proposed method in this paper can be effectively used to acquire common Chinese emotional events. An extrinsic use case also demonstrates the strong potential of common emotional events in the field of emotion cause extraction (ECE). Related resources including emotional event indicators and emotional events will be released after the publication of this paper.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title>
<link>https://arxiv.org/abs/2511.05018</link>
<guid>https://arxiv.org/abs/2511.05018</guid>
<content:encoded><![CDATA[
<div> dynamic evaluation suite, LLMs, pluralistic alignment, behavioral policies, adversarial conditions

Summary:
PBSUITE is introduced as a dynamic evaluation suite for assessing LLMs' adherence to pluralistic alignment in multi-turn conversations. It includes a dataset with 300 behavioral policies from various industries and an evaluation framework to test model compliance in adversarial conditions. Leading LLMs show strong adherence to behavioral policies in single-turn interactions but struggle in multi-turn adversarial settings, indicating a gap in enforcing pluralistic behavioral policies. The study highlights the need for improved alignment and safety measures to support diverse user values in real-world LLM interactions. <div>
arXiv:2511.05018v1 Announce Type: new 
Abstract: Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian</title>
<link>https://arxiv.org/abs/2511.05040</link>
<guid>https://arxiv.org/abs/2511.05040</guid>
<content:encoded><![CDATA[
<div> Benchmark, code generation, competitive programming, low-resource languages, language models  
Summary:  
The paper introduces UA-Code-Bench, a new benchmark for evaluating code generation and problem-solving abilities of language models in Ukrainian. The benchmark consists of 500 problems from Eolymp, categorized by complexity levels. Results show that even top models like OpenAI o3 and GPT-5 struggle to solve half of the problems, indicating the difficulty of code generation in low-resource languages. The study analyzes performance across difficulty levels and assesses solution uniqueness and efficiency in terms of time and memory consumption. The research highlights the importance of competitive programming benchmarks in evaluating language models, particularly in underrepresented languages, and sets the stage for future work on multilingual code generation and reasoning-enhanced models. The benchmark and related scripts are available for further exploration.  
Summary:  <div>
arXiv:2511.05040v1 Announce Type: new 
Abstract: Evaluating the real capabilities of large language models in low-resource languages still represents a challenge, as many existing benchmarks focus on widespread tasks translated from English or evaluate only simple language understanding. This paper introduces UA-Code-Bench, a new open-source benchmark established for a thorough evaluation of language models' code generation and competitive programming problem-solving abilities in Ukrainian. The benchmark comprises 500 problems from the Eolymp platform, evenly distributed across five complexity levels from very easy to very hard. A diverse set of 13 leading proprietary and open-source models, generating Python solutions based on a one-shot prompt, was evaluated via the dedicated Eolymp environment against hidden tests, ensuring code correctness. The obtained results reveal that even top-performing models, such as OpenAI o3 and GPT-5, solve only half of the problems, highlighting the challenge of code generation in low-resource natural language. Furthermore, this research presents a comprehensive analysis of performance across various difficulty levels, as well as an assessment of solution uniqueness and computational efficiency, measured by both elapsed time and memory consumption of the generated solutions. In conclusion, this work demonstrates the value of competitive programming benchmarks in evaluating large language models, especially in underrepresented languages. It also paves the way for future research on multilingual code generation and reasoning-enhanced models. The benchmark, data parsing, preparation, code generation, and evaluation scripts are available at https://huggingface.co/datasets/NLPForUA/ua-code-bench.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order-Level Attention Similarity Across Language Models: A Latent Commonality</title>
<link>https://arxiv.org/abs/2511.05064</link>
<guid>https://arxiv.org/abs/2511.05064</guid>
<content:encoded><![CDATA[
<div> attention aggregation, language models, transfer learning, syntactic knowledge, adapter

Summary: 
This paper investigates the commonalities in context aggregation patterns across different language models (LMs). By analyzing the Order-Level Attention (OLA) derived from Attention Rollout, the study reveals significant similarities in OLA among different LMs at the same order, suggesting a implicit mapping to syntactic knowledge. Based on these findings, the Transferable OLA Adapter (TOA) is proposed as a training-free method for cross-LM knowledge transfer. By utilizing OLA as a syntactic feature representation, TOA generalizes effectively to unseen LMs without parameter updates. Extensive experiments demonstrate that the TOA method enhances the performance of unseen LMs through cross-LM generalization. The code implementation of the proposed method is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2511.05064v1 Announce Type: new 
Abstract: In this paper, we explore an important yet previously neglected question: Do context aggregation patterns across Language Models (LMs) share commonalities? While some works have investigated context aggregation or attention weights in LMs, they typically focus on individual models or attention heads, lacking a systematic analysis across multiple LMs to explore their commonalities. In contrast, we focus on the commonalities among LMs, which can deepen our understanding of LMs and even facilitate cross-model knowledge transfer. In this work, we introduce the Order-Level Attention (OLA) derived from the order-wise decomposition of Attention Rollout and reveal that the OLA at the same order across LMs exhibits significant similarities. Furthermore, we discover an implicit mapping between OLA and syntactic knowledge. Based on these two findings, we propose the Transferable OLA Adapter (TOA), a training-free cross-LM adapter transfer method. Specifically, we treat the OLA as a unified syntactic feature representation and train an adapter that takes OLA as input. Due to the similarities in OLA across LMs, the adapter generalizes to unseen LMs without requiring any parameter updates. Extensive experiments demonstrate that TOA's cross-LM generalization effectively enhances the performance of unseen LMs. Code is available at https://github.com/jinglin-liang/OLAS.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts</title>
<link>https://arxiv.org/abs/2511.05078</link>
<guid>https://arxiv.org/abs/2511.05078</guid>
<content:encoded><![CDATA[
<div> Keywords: claim normalization, multilingual misinformation detection, cross-lingual transfer, social media, linguistic structures

Summary:<br /><br />
1. The study focuses on claim normalization for multilingual misinformation detection across 20 languages by utilizing a systematic decomposition approach based on Who, What, Where, When, Why, and How questions.
2. The methodology involves finetuning a model named Qwen3-14B using LoRA and utilizing token-level recall filtering, intra-post deduplication, and retrieval-augmented few-shot learning with contextual examples for robust cross-lingual transfer.
3. Results show significant improvements in METEOR scores across different languages, with a relative improvement of 41.3% over baseline configurations.
4. The approach demonstrates effective cross-lingual generalization for Romance and Germanic languages while maintaining semantic coherence across diverse linguistic structures.
5. The system achieves competitive rankings on the English, Dutch, and Punjabi leaderboards, highlighting its effectiveness in addressing claim normalization in multilingual social media posts. 

Summary: <div>
arXiv:2511.05078v1 Announce Type: new 
Abstract: We address claim normalization for multilingual misinformation detection - transforming noisy social media posts into clear, verifiable statements across 20 languages. The key contribution demonstrates how systematic decomposition of posts using Who, What, Where, When, Why and How questions enables robust cross-lingual transfer despite training exclusively on English data. Our methodology incorporates finetuning Qwen3-14B using LoRA with the provided dataset after intra-post deduplication, token-level recall filtering for semantic alignment and retrieval-augmented few-shot learning with contextual examples during inference. Our system achieves METEOR scores ranging from 41.16 (English) to 15.21 (Marathi), securing third rank on the English leaderboard and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative improvement in METEOR over baseline configurations and substantial gains over existing methods. Results demonstrate effective cross-lingual generalization for Romance and Germanic languages while maintaining semantic coherence across diverse linguistic structures.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class</title>
<link>https://arxiv.org/abs/2511.05080</link>
<guid>https://arxiv.org/abs/2511.05080</guid>
<content:encoded><![CDATA[
<div> health-seeking behavior, digital consumption, text simplification, linguistic capabilities, discourse fidelity 
Summary:<br /><br />The study evaluates the performance of two large language models (LLMs) for text simplification. Mistral 24B shows a balanced approach to simplification, improving readability and maintaining discourse fidelity with a high BERTScore of 0.91. In contrast, QWen2.5 32B struggles to balance readability and accuracy, resulting in a lower BERTScore of 0.89. Mistral's lexical simplification strategy enhances readability metrics and achieves a mean SARI of 42.46. The study also identifies the importance of heuristic metric selection for text simplification tasks. Further analysis reveals functional redundancies among readability indices and highlights lexical support as a crucial domain-adaptation issue for text simplification models. This empirical evidence establishes Mistral as a promising option for text simplification and provides valuable insights for improving the effectiveness of LLMs in this domain.<br /><br /> <div>
arXiv:2511.05080v1 Announce Type: new 
Abstract: The increasing health-seeking behavior and digital consumption of biomedical information by the general public necessitate scalable solutions for automatically adapting complex scientific and technical documents into plain language. Automatic text simplification solutions, including advanced large language models, however, continue to face challenges in reliably arbitrating the tension between optimizing readability performance and ensuring preservation of discourse fidelity. This report empirically assesses the performance of two major classes of general-purpose LLMs, demonstrating their linguistic capabilities and foundational readiness for the task compared to a human benchmark. Using a comparative analysis of the instruction-tuned Mistral 24B and the reasoning-augmented QWen2.5 32B, we identify a potential architectural advantage in the instruction-tuned LLM. Mistral exhibits a tempered lexical simplification strategy that enhances readability across a suite of metrics and the simplification-specific formula SARI (mean 42.46), while preserving human-level discourse with a BERTScore of 0.91. QWen also attains enhanced readability performance, but its operational strategy shows a disconnect in balancing between readability and accuracy, reaching a statistically significantly lower BERTScore of 0.89. Additionally, a comprehensive correlation analysis of 21 metrics spanning readability, discourse fidelity, content safety, and underlying distributional measures for mechanistic insights, confirms strong functional redundancies among five readability indices. This empirical evidence tracks baseline performance of the evolving LLMs for the task of text simplification, identifies the instruction-tuned Mistral 24B for simplification, provides necessary heuristics for metric selection, and points to lexical support as a primary domain-adaptation issue for simplification.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Layer-wise Distillation for Efficient Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2511.05085</link>
<guid>https://arxiv.org/abs/2511.05085</guid>
<content:encoded><![CDATA[
<div> distillation methods, large language models, ShortGPT approach, layer importance, model efficiency <br />
Summary: 
This work explores distillation methods for large language models (LLMs) to create compact models without significant performance loss. By iteratively evaluating layer importance and training with a joint loss function, the proposed method based on ShortGPT reduces the number of layers in the Qwen2.5-3B model with minimal quality loss. Experiment results show that middle transformer layers contribute less to inference, indicating the potential of the approach for efficient model creation. The findings highlight the effectiveness of iterative distillation and fine-tuning, making the method suitable for resource-limited environments. <div>
arXiv:2511.05085v1 Announce Type: new 
Abstract: This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Toolbox for Improving Evolutionary Prompt Search</title>
<link>https://arxiv.org/abs/2511.05120</link>
<guid>https://arxiv.org/abs/2511.05120</guid>
<content:encoded><![CDATA[
<div> Keywords: Evolutionary prompt optimization, LLMs, efficient evaluation, human feedback, code release

Summary:
This work introduces key improvements to evolutionary prompt optimization for refining Language Model prompts. Firstly, it decomposes evolution into distinct steps to enhance control and effectiveness. Secondly, an LLM-based judge is introduced to verify evolutions, ensuring quality. Thirdly, human feedback is integrated to refine evolutionary operators, enhancing performance further. Finally, more efficient evaluation strategies are developed to maintain performance while reducing computational overhead. These improvements result in enhanced optimization quality and efficiency, making prompt optimization more effective. The code has been released, allowing for prompt optimization on new tasks and facilitating future research in this field. 

<br /><br />Summary: <div>
arXiv:2511.05120v1 Announce Type: new 
Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining prompts for LLMs. However, existing approaches lack robust operators and efficient evaluation mechanisms. In this work, we propose several key improvements to evolutionary prompt optimization that can partially generalize to prompt optimization in general: 1) decomposing evolution into distinct steps to enhance the evolution and its control, 2) introducing an LLM-based judge to verify the evolutions, 3) integrating human feedback to refine the evolutionary operator, and 4) developing more efficient evaluation strategies that maintain performance while reducing computational overhead. Our approach improves both optimization quality and efficiency. We release our code, enabling prompt optimization on new tasks and facilitating further research in this area.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ManufactuBERT: Efficient Continual Pretraining for Manufacturing</title>
<link>https://arxiv.org/abs/2511.05135</link>
<guid>https://arxiv.org/abs/2511.05135</guid>
<content:encoded><![CDATA[
<div> Keywords: ManufactuBERT, RoBERTa, deduplication, manufacturing domain, NLP tasks 
 
Summary: <br /><br />ManufactuBERT is a specialized RoBERTa model pretrained on a large-scale corpus specifically curated for the manufacturing domain. The development of ManufactuBERT involved creating a comprehensive data processing pipeline to filter domain-specific terminology and semantics from web data. The multi-stage deduplication process implemented in the pipeline significantly reduced redundancies, leading to improved model performance. ManufactuBERT outperformed strong specialized baselines on manufacturing-related NLP tasks, demonstrating a new state-of-the-art in the field. Additionally, training on the carefully deduplicated corpus accelerated convergence, reducing training time and computational costs by 33%. The proposed pipeline serves as a reproducible example for building high-performing encoders in other specialized domains. The model and curated corpus will be released for public access. <br /><br />Summary: <div>
arXiv:2511.05135v1 Announce Type: new 
Abstract: While large general-purpose Transformer-based encoders excel at general language understanding, their performance diminishes in specialized domains like manufacturing due to a lack of exposure to domain-specific terminology and semantics. In this paper, we address this gap by introducing ManufactuBERT, a RoBERTa model continually pretrained on a large-scale corpus curated for the manufacturing domain. We present a comprehensive data processing pipeline to create this corpus from web data, involving an initial domain-specific filtering step followed by a multi-stage deduplication process that removes redundancies. Our experiments show that ManufactuBERT establishes a new state-of-the-art on a range of manufacturing-related NLP tasks, outperforming strong specialized baselines. More importantly, we demonstrate that training on our carefully deduplicated corpus significantly accelerates convergence, leading to a 33\% reduction in training time and computational cost compared to training on the non-deduplicated dataset. The proposed pipeline offers a reproducible example for developing high-performing encoders in other specialized domains. We will release our model and curated corpus at https://huggingface.co/cea-list-ia.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results</title>
<link>https://arxiv.org/abs/2511.05162</link>
<guid>https://arxiv.org/abs/2511.05162</guid>
<content:encoded><![CDATA[
<div> math, language models, multilingual, performance, dataset

Summary:
- Study on performance of language models across different languages, focusing on math domain.
- Identified translation errors in standard math benchmark dataset (MGSM).
- Lack of standardized answer extraction from language model outputs affecting results.
- Proposed method for automatic quality assurance to address translation errors at scale.
- Recommendations provided to address lack of standardized answer extraction.
- Language gap mostly disappears with proposed approaches, leading to different research conclusions.
- Corrected dataset released to the community for further research. 

<br /><br />Summary: <div>
arXiv:2511.05162v1 Announce Type: new 
Abstract: Most current large language models (LLMs) support a wide variety of languages in addition to English, including high-resource languages (e.g. German, Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In addition they have also shown impressive capabilities in different domains, like coding, science and math. In this short paper, taking math as an example domain, we study the performance of different LLMs across languages. Experimental results show that there exists a non-negligible and consistent gap in the performance of the models across languages. Interestingly, and somewhat against expectations, the gap exists for both high- and low-resource languages. We hope that these results influence further research into cross-lingual capability generalization for next generation LLMs. If it weren't for the fact that they are false! By analyzing one of the standard multilingual math benchmarks (MGSM), we determine that several translation errors are present in the data. Furthermore, the lack of standardized answer extraction from LLM outputs further influences the final results. We propose a method for automatic quality assurance to address the first issue at scale, and give recommendations to address the second one. Combining these two approaches we show that the aforementioned language gap mostly disappears, leading to completely different conclusions from our research. We additionally release the corrected dataset to the community.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models</title>
<link>https://arxiv.org/abs/2511.05184</link>
<guid>https://arxiv.org/abs/2511.05184</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought prompting, Large Language Models, Knowledge Distillation, White-box KD, Natural Language Reasoning<br />
Summary:<br />
Chain-of-Thought (CoT) prompting is utilized to enhance the reasoning capability of Large Language Models (LLMs) and has been applied in Knowledge Distillation (KD) for transferring reasoning skills from larger to smaller LLMs. This study investigates the impact of CoT in white-box KD for reasoning improvement, using LLMs from Qwen and Llama2 families and CoT data from the CoT-Collection dataset. The performance of distilled models is evaluated on challenging tasks from the BIG-Bench-Hard (BBH) benchmark. Results show the effectiveness of CoT in enhancing white-box KD, leading to better performance of distilled models in natural language reasoning and understanding tasks from BBH.<br /><br />Summary: <div>
arXiv:2511.05184v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the reasoning capability of Large Language Models (LLMs). More recently, CoT has been leveraged in Knowledge Distillation (KD) to transfer reasoning capability from a larger LLM to a smaller one. This paper examines the role of CoT in distilling the reasoning capability from larger LLMs to smaller LLMs using white-box KD, analysing its effectiveness in improving the performance of the distilled models for various natural language reasoning and understanding tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural language reasoning and understanding tasks from BBH.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>