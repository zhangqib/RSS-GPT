<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification</title>
<link>https://arxiv.org/abs/2505.05583</link>
<guid>https://arxiv.org/abs/2505.05583</guid>
<content:encoded><![CDATA[
<div> Keywords: Hierarchical Text Classification, Knowledge Graphs, Large Language Models, Zero-shot, Semantic Context

Summary:
Hierarchical Text Classification (HTC) faces challenges in real-world scenarios due to lack of annotated data, large label spaces, and long-tail distributions. To address these issues, Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC) integrates knowledge graphs with Large Language Models (LLMs). The KG-HTC method retrieves relevant subgraphs from knowledge graphs related to input text using a Retrieval-Augmented Generation approach. By enhancing LLMs to understand label semantics at various hierarchy levels, KG-HTC significantly outperforms baselines in the strict zero-shot setting, showing improvements at deeper hierarchy levels. Evaluation on WoS, DBpedia, and Amazon datasets demonstrates the effectiveness of incorporating structured knowledge into LLMs for HTC challenges in large label spaces and long-tailed label distributions. The code for KG-HTC is available on GitHub at https://github.com/QianboZang/KG-HTC.

<br><br>Summary: <div>
arXiv:2505.05583v1 Announce Type: new 
Abstract: Hierarchical Text Classification (HTC) involves assigning documents to labels organized within a taxonomy. Most previous research on HTC has focused on supervised methods. However, in real-world scenarios, employing supervised HTC can be challenging due to a lack of annotated data. Moreover, HTC often faces issues with large label spaces and long-tail distributions. In this work, we present Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC), which aims to address these challenges of HTC in applications by integrating knowledge graphs with Large Language Models (LLMs) to provide structured semantic context during classification. Our method retrieves relevant subgraphs from knowledge graphs related to the input text using a Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to understand label semantics at various hierarchy levels. We evaluate KG-HTC on three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental results show that KG-HTC significantly outperforms three baselines in the strict zero-shot setting, particularly achieving substantial improvements at deeper levels of the hierarchy. This evaluation demonstrates the effectiveness of incorporating structured knowledge into LLMs to address HTC's challenges in large label spaces and long-tailed label distributions. Our code is available at: https://github.com/QianboZang/KG-HTC.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation</title>
<link>https://arxiv.org/abs/2505.05648</link>
<guid>https://arxiv.org/abs/2505.05648</guid>
<content:encoded><![CDATA[
<div> transformer, differential privacy, language modeling, SwiftKey, GPT2

Summary:<br><br>
This paper explores the use of a transformer trained with differential privacy for language modeling in SwiftKey. The study involves running experiments to find the optimal balance between model size, run-time speed, and accuracy. The results indicate small yet consistent improvements in next-word prediction and overall accuracy, achieved through scaling down a GPT2 architecture to meet size requirements and a two-stage training process. The approach involves first building a seed model on general data and then fine-tuning it with differential privacy on typing data. By integrating the transformer using ONNX, the researchers were able to maintain flexibility and efficiency. Overall, this study demonstrates the potential of utilizing differential privacy in language modeling tasks, yielding improved performance without sacrificing memory or speed. <div>
arXiv:2505.05648v1 Announce Type: new 
Abstract: In this paper we train a transformer using differential privacy (DP) for language modeling in SwiftKey. We run multiple experiments to balance the trade-off between the model size, run-time speed and accuracy. We show that we get small and consistent gains in the next-word-prediction and accuracy with graceful increase in memory and speed compared to the production GRU. This is obtained by scaling down a GPT2 architecture to fit the required size and a two stage training process that builds a seed model on general data and DP finetunes it on typing data. The transformer is integrated using ONNX offering both flexibility and efficiency.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration of COVID-19 Discourse on Twitter: American Politician Edition</title>
<link>https://arxiv.org/abs/2505.05687</link>
<guid>https://arxiv.org/abs/2505.05687</guid>
<content:encoded><![CDATA[
<div> keywords: COVID-19, political, polarization, Democrats, Republicans

Summary:
Democrats and Republicans exhibit contrasting approaches to the COVID-19 pandemic, as evidenced by an analysis of tweets from leading American political figures. Democrats prioritize the public health aspect, emphasizing casualties, medical precautions, and recommendations. In contrast, Republicans focus on political responsibilities such as media updates and monitoring virus progress. The study utilizes bag-of-words, bigram, and TF-IDF models to identify keywords, topics, and sentiments, revealing distinct partisan attitudes towards handling the crisis. By employing classification algorithms on different language models, the research aims to predict and differentiate a tweet's political leaning based on its COVID-19 related terms. This systematic approach sheds light on the partisan divide in response to the international crisis and highlights the different priorities and perspectives of Democrats and Republicans. <div>
arXiv:2505.05687v1 Announce Type: new 
Abstract: The advent of the COVID-19 pandemic has undoubtedly affected the political scene worldwide and the introduction of new terminology and public opinions regarding the virus has further polarized partisan stances. Using a collection of tweets gathered from leading American political figures online (Republican and Democratic), we explored the partisan differences in approach, response, and attitude towards handling the international crisis. Implementation of the bag-of-words, bigram, and TF-IDF models was used to identify and analyze keywords, topics, and overall sentiments from each party. Results suggest that Democrats are more concerned with the casualties of the pandemic, and give more medical precautions and recommendations to the public whereas Republicans are more invested in political responsibilities such as keeping the public updated through media and carefully watching the progress of the virus. We propose a systematic approach to predict and distinguish a tweet's political stance (left or right leaning) based on its COVID-19 related terms using different classification algorithms on different language models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Robustness to Spurious Correlations in Post-Training Language Models</title>
<link>https://arxiv.org/abs/2505.05704</link>
<guid>https://arxiv.org/abs/2505.05704</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, fine-tuning techniques, spurious correlations, post-training algorithms, synthetic tasks 

Summary: 
The study evaluates three post-training algorithms (SFT, DPO, KTO) on various synthetic tasks to address spurious correlations in large language models (LLMs). These correlations, often arising from biases or dataset artifacts, can impact model performance and generalization. Tasks included mathematical reasoning, instruction-following, and question answering, with varying levels of spuriousness and types of artifacts. Results show models may degrade under higher spuriousness, with preference-based methods (DPO/KTO) showing relative robustness in some tasks. SFT performs better in more complex tasks requiring contextual understanding. The study emphasizes the lack of a universal best post-training strategy, highlighting the importance of task type and spurious correlation nature in selecting the most effective approach.<br><br>Summary: <div>
arXiv:2505.05704v1 Announce Type: new 
Abstract: Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations -- arising from biases, dataset artifacts, or other "shortcut" features -- that can compromise a model's performance or generalization. In this paper, we systematically evaluate three post-training algorithms -- Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO (Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and spuriousness conditions. Our tasks span mathematical reasoning, constrained instruction-following, and document-grounded question answering. We vary the degree of spurious correlation (10% vs. 90%) and investigate two forms of artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results show that the models often but not always degrade under higher spuriousness. The preference-based methods (DPO/KTO) can demonstrate relative robustness in mathematical reasoning tasks. By contrast, SFT maintains stronger performance in complex, context-intensive tasks. These findings highlight that no single post-training strategy universally outperforms in all scenarios; the best choice depends on the type of target task and the nature of spurious correlations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries</title>
<link>https://arxiv.org/abs/2505.05714</link>
<guid>https://arxiv.org/abs/2505.05714</guid>
<content:encoded><![CDATA[
<div> Dataset, Multimodal Machine Translation, Documentaries, TopicVD, Video-supported<br>
<br>
Summary: <br>
The study introduces TopicVD, a topic-based dataset for video-supported multimodal machine translation (MMT) of documentaries, with video-subtitle pairs categorized into eight topics. An MMT model with a cross-modal bidirectional attention module is proposed to capture shared semantics between text and video. Experimental results show that visual information consistently enhances NMT model performance in documentary translation. However, performance declines in out-of-domain scenarios, emphasizing the need for effective domain adaptation methods. Global context is shown to effectively improve translation performance as well. The dataset and implementations are available at the provided GitHub link. <div>
arXiv:2505.05714v1 Announce Type: new 
Abstract: Most existing multimodal machine translation (MMT) datasets are predominantly composed of static images or short video clips, lacking extensive video data across diverse domains and topics. As a result, they fail to meet the demands of real-world MMT tasks, such as documentary translation. In this study, we developed TopicVD, a topic-based dataset for video-supported multimodal machine translation of documentaries, aiming to advance research in this field. We collected video-subtitle pairs from documentaries and categorized them into eight topics, such as economy and nature, to facilitate research on domain adaptation in video-guided MMT. Additionally, we preserved their contextual information to support research on leveraging the global context of documentaries in video-guided MMT. To better capture the shared semantics between text and video, we propose an MMT model based on a cross-modal bidirectional attention module. Extensive experiments on the TopicVD dataset demonstrate that visual information consistently improves the performance of the NMT model in documentary translation. However, the MMT model's performance significantly declines in out-of-domain scenarios, highlighting the need for effective domain adaptation methods. Additionally, experiments demonstrate that global context can effectively improve translation performance. % Dataset and our implementations are available at https://github.com/JinzeLv/TopicVD
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions</title>
<link>https://arxiv.org/abs/2505.05755</link>
<guid>https://arxiv.org/abs/2505.05755</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive Models, Masked Diffusion Models, Insertion Language Models, Sequence Generation, Token Insertion

Summary: 
Autoregressive Models (ARMs) are effective at sequence generation tasks but struggle with complex constraints and dependencies. Masked Diffusion Models (MDMs) address some limitations but can introduce incoherences when unmasking multiple tokens. Insertion Language Models (ILMs) are introduced to insert tokens at arbitrary positions, allowing for strong dependencies between tokens and generating sequences in arbitrary order. ILMs outperform ARMs and MDMs in planning tasks and perform on par with ARMs in unconditional text generation. They offer greater flexibility than MDMs in text infilling tasks of arbitrary length. To train ILMs, a tailored network parameterization and a denoising objective are used. Empirical evaluation shows the effectiveness of ILMs in various tasks, showcasing their potential for handling complex sequence generation challenges.<br><br>Summary: <div>
arXiv:2505.05755v1 Announce Type: new 
Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one ``from left to right,'' have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that require satisfying sophisticated constraints or whose sequential dependencies are better addressed by out-of-order generation. Masked Diffusion Models (MDMs) address some of these limitations, but the process of unmasking multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs cannot handle arbitrary infilling constraints when the number of tokens to be filled in is not known in advance. In this work, we introduce Insertion Language Models (ILMs), which learn to insert tokens at arbitrary positions in a sequence -- that is, they select jointly both the position and the vocabulary element to be inserted. By inserting tokens one at a time, ILMs can represent strong dependencies between tokens, and their ability to generate sequences in arbitrary order allows them to accurately model sequences where token dependencies do not follow a left-to-right sequential structure. To train ILMs, we propose a tailored network parameterization and use a simple denoising objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs and MDMs on common planning tasks. Furthermore, we show that ILMs outperform MDMs and perform on par with ARMs in an unconditional text generation task while offering greater flexibility than MDMs in arbitrary-length text infilling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM</title>
<link>https://arxiv.org/abs/2505.05772</link>
<guid>https://arxiv.org/abs/2505.05772</guid>
<content:encoded><![CDATA[
<div> scheme, LLM decoding, PIM architectures, sparsity optimization, STARC

Summary:
STARC is a sparsity-optimized data mapping scheme designed for efficient large language model (LLM) decoding on Processing-in-Memory (PIM) architectures. It clusters key-value (KV) pairs based on semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. Queries can retrieve relevant tokens at cluster granularity by matching against precomputed centroids, allowing for selective attention and parallel processing without the need for frequent reclustering or data movement. Experiments on the HBM-PIM system showed that STARC reduces attention-layer latency and energy consumption significantly compared to common token-wise sparsity methods. Under a KV cache budget of 1024, STARC achieved substantial reductions in latency and energy consumption while maintaining model accuracy comparable to state-of-the-art sparse attention methods. STARC proves to be effective in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures. 

<br><br>Summary: <div>
arXiv:2505.05772v1 Announce Type: new 
Abstract: Transformer-based models are the foundation of modern machine learning, but their execution, particularly during autoregressive decoding in large language models (LLMs), places significant pressure on memory systems due to frequent memory accesses and growing key-value (KV) caches. This creates a bottleneck in memory bandwidth, especially as context lengths increase. Processing-in-memory (PIM) architectures are a promising solution, offering high internal bandwidth and compute parallelism near memory. However, current PIM designs are primarily optimized for dense attention and struggle with the dynamic, irregular access patterns introduced by modern KV cache sparsity techniques. Consequently, they suffer from workload imbalance, reducing throughput and resource utilization. In this work, we propose STARC, a novel sparsity-optimized data mapping scheme tailored specifically for efficient LLM decoding on PIM architectures. STARC clusters KV pairs by semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. During decoding, queries retrieve relevant tokens at cluster granularity by matching against precomputed centroids, enabling selective attention and parallel processing without frequent reclustering or data movement overhead. Experiments on the HBM-PIM system show that, compared to common token-wise sparsity methods, STARC reduces attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a KV cache budget of 1024, it achieves up to 54%--74% latency reduction and 45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC maintains model accuracy comparable to state-of-the-art sparse attention methods, demonstrating its effectiveness in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted</title>
<link>https://arxiv.org/abs/2505.05815</link>
<guid>https://arxiv.org/abs/2505.05815</guid>
<content:encoded><![CDATA[
<div> Keywords: AnaQuest, multiple-choice questions, language model, Item Response Theory, assessment <br>
Summary:<br>
The study introduces AnaQuest, an innovative prompting technique utilizing a large language model to generate multiple-choice questions. It integrates formative and summative assessments by having students answer open-ended questions initially and then using AI to create MCQs with sentence-level assertions. Expert instructors rated MCQs from AnaQuest and another AI model, ChatGPT, as valid as human-crafted questions. However, Item Response Theory analysis showed that AnaQuest's MCQs, especially those with incorrect assertions, were more comparable to human-crafted questions in terms of difficulty and discrimination than ChatGPT-generated questions. This suggests that AnaQuest is effective in generating valid and challenging MCQs for assessments. <br> <div>
arXiv:2505.05815v1 Announce Type: new 
Abstract: The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI</title>
<link>https://arxiv.org/abs/2505.05864</link>
<guid>https://arxiv.org/abs/2505.05864</guid>
<content:encoded><![CDATA[
<div> Keywords: experimental datasets, natural language processing, entity recognition, structured data, text-mining

Summary: 
- The article discusses the importance of constructing experimental datasets for data-driven scientific discovery.
- Recent advances in natural language processing have enabled the automatic extraction of structured data from unstructured scientific literature.
- A novel hybrid text-mining framework is proposed to convert unstructured scientific text into structured data by integrating multi-step and direct methods.
- The approach includes transforming raw text into entity-recognized text and then into a structured form, improving entity recognition performance with an entity marker technique.
- The entity marker-based hybrid approach outperforms previous entity recognition methods across benchmark datasets and significantly enhances the quality of final structured data. <div>
arXiv:2505.05864v1 Announce Type: new 
Abstract: The construction of experimental datasets is essential for expanding the scope of data-driven scientific discovery. Recent advances in natural language processing (NLP) have facilitated automatic extraction of structured data from unstructured scientific literature. While existing approaches-multi-step and direct methods-offer valuable capabilities, they also come with limitations when applied independently. Here, we propose a novel hybrid text-mining framework that integrates the advantages of both methods to convert unstructured scientific text into structured data. Our approach first transforms raw text into entity-recognized text, and subsequently into structured form. Furthermore, beyond the overall data structuring framework, we also enhance entity recognition performance by introducing an entity marker-a simple yet effective technique that uses symbolic annotations to highlight target entities. Specifically, our entity marker-based hybrid approach not only consistently outperforms previous entity recognition approaches across three benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the quality of final structured data-yielding up to a 58% improvement in entity-level F1 score and up to 83% improvement in relation-level F1 score compared to direct approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2</title>
<link>https://arxiv.org/abs/2505.05946</link>
<guid>https://arxiv.org/abs/2505.05946</guid>
<content:encoded><![CDATA[
<div> experiment, autoregressive pre-training, Gemma2, large language model, Lithuanian language, continual learning, elastic weight consolidation, benchmarks, catastrophic forgetting effects

Summary:<br><br>This technical report discusses an experiment on autoregressive pre-training of the Gemma2 2 billion parameter large language model (LLM) with a focus on Lithuanian language learning using CulturaX data. The study applies elastic weight consolidation (EWC) to all model parameters to address catastrophic forgetting and enhance learning of new tasks. Various language understanding benchmarks in English and Lithuanian, including Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande sets, as well as perplexity benchmarks, are evaluated. The empirical results demonstrate the effectiveness of EWC regularization in reducing catastrophic forgetting and improving the LLM's ability to learn new tasks. <div>
arXiv:2505.05946v1 Announce Type: new 
Abstract: This technical report describes an experiment on autoregressive pre-training of Gemma2 2 billion parameter large language model (LLM) with 10\% on the Lithuanian language component of CulturaX from the point of view of continual learning. We apply elastic weight consolidation (EWC) to the full set of the model's parameters and investigate language understanding benchmarks, consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande sets (both in English and Lithuanian versions), and perplexity benchmarks. We empirically demonstrate that EWC regularisation allows us not only to mitigate catastrophic forgetting effects but also that it is potentially beneficial for learning of the new task with LLMs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Summarisation of German Judgments in conjunction with a Class-based Evaluation</title>
<link>https://arxiv.org/abs/2505.05947</link>
<guid>https://arxiv.org/abs/2505.05947</guid>
<content:encoded><![CDATA[
<div> legal documents, automated summarisation, German judgments, language model, legal entities <br>
<br>
Summary: The study focuses on automating the summarisation of lengthy legal documents, specifically German judgments, by fine-tuning a decoder-based language model. The inclusion of information about legal entities before training improves the model's ability to identify relevant content. However, the quality of the generated summaries falls short of practical use, despite enhancements from legal entities. An evaluation framework measures language quality, relevance, completeness, and accuracy of the summaries. This research highlights the potential benefits of leveraging language models for legal document summarisation but underscores the need for further refinement to achieve practical applicability. <br> <div>
arXiv:2505.05947v1 Announce Type: new 
Abstract: The automated summarisation of long legal documents can be a great aid for legal experts in their daily work. We automatically create summaries (guiding principles) of German judgments by fine-tuning a decoder-based large language model. We enrich the judgments with information about legal entities before the training. For the evaluation of the created summaries, we define a set of evaluation classes which allows us to measure their language, pertinence, completeness and correctness. Our results show that employing legal entities helps the generative model to find the relevant content, but the quality of the created summaries is not yet sufficient for a use in practice.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeoQA: Evidence-based Question Answering with Generated News Events</title>
<link>https://arxiv.org/abs/2505.05949</link>
<guid>https://arxiv.org/abs/2505.05949</guid>
<content:encoded><![CDATA[
<div> benchmark, retrieval-augmented generation, large language models, evidence-based reasoning, NeoQA

Summary:
The article introduces NeoQA, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs). NeoQA is designed to prevent LLMs from relying on pretraining knowledge by generating fictional news events and entities with corresponding news articles and Q&A pairs. This ensures that LLMs must generate responses exclusively from retrieved evidence. The dataset allows for evaluation across various evidence scenarios, including cases with missing or misleading details. Results show that LLMs struggle to differentiate between questions and evidence, and exhibit short-cut reasoning when key information is missing from the evidence. This highlights limitations in evidence-based reasoning and underscores the importance of developing robust methods for handling incomplete or inaccurate information. 

Summary: <div>
arXiv:2505.05949v1 Announce Type: new 
Abstract: Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q\&amp;A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models</title>
<link>https://arxiv.org/abs/2505.05970</link>
<guid>https://arxiv.org/abs/2505.05970</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, interactive training, child language acquisition, reinforcement learning, communicative success

Summary: 
In this study, the authors propose a novel method for training language models in an interactive setting inspired by child language acquisition. They utilize a single-turn dialogue where a speaker communicates with a listener to achieve communicative success and receives rewards. The success is measured in a language-only question-answering context. Through experiments using reinforcement learning, they aim to fine-tune language models. The feasibility study demonstrated that the reward signal indirectly indicates grammaticality. The results showed that imposing cognitive constraints on communication led to interpretable changes in speaker behavior, but did not show improvements in linguistic evaluations. The authors suggest potential modifications to improve task design and training configuration for future studies to observe the benefits of interaction on language learning in computational models. 

<br><br>Summary: <div>
arXiv:2505.05970v1 Announce Type: new 
Abstract: We propose a method for training language models in an interactive setting inspired by child language acquisition. In our setting, a speaker attempts to communicate some information to a listener in a single-turn dialogue and receives a reward if communicative success is achieved. Unlike earlier related work using image--caption data for interactive reference games, we operationalize communicative success in a more abstract language-only question--answering setting. First, we present a feasibility study demonstrating that our reward provides an indirect signal about grammaticality. Second, we conduct experiments using reinforcement learning to fine-tune language models. We observe that cognitively plausible constraints on the communication channel lead to interpretable changes in speaker behavior. However, we do not yet see improvements on linguistic evaluations from our training regime. We outline potential modifications to the task design and training configuration that could better position future work to use our methodology to observe the benefits of interaction on language learning in computational cognitive models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition</title>
<link>https://arxiv.org/abs/2505.05973</link>
<guid>https://arxiv.org/abs/2505.05973</guid>
<content:encoded><![CDATA[
<div> clustering analysis, word embeddings, shift vectors, Generalized Additive Mixed Models, semantic transparency <br>
Summary: 
This study explores the impact of semantic transparency on word recognition through embedding-based measures. The geometry of complex Malay prefixed words in semantic space was analyzed, revealing distinct clusters based on prefix class. Five measures were derived to predict lexical decision latencies, including word embeddings and shift vectors. Generalized Additive Mixed Models were used to evaluate the predictive power of these measures, with the model incorporating the correlation between each word and their centroid proving to be the most effective. The study highlights the importance of semantic transparency in morphological processing and offers insights into the computational operationalization of this concept. <div>
arXiv:2505.05973v1 Announce Type: new 
Abstract: Studies of morphological processing have shown that semantic transparency is crucial for word recognition. Its computational operationalization is still under discussion. Our primary objectives are to explore embedding-based measures of semantic transparency, and assess their impact on reading. First, we explored the geometry of complex words in semantic space. To do so, we conducted a t-distributed Stochastic Neighbor Embedding clustering analysis on 4,226 Malay prefixed words. Several clusters were observed for complex words varied by their prefix class. Then, we derived five simple measures, and investigated whether they were significant predictors of lexical decision latencies. Two sets of Linear Discriminant Analyses were run in which the prefix of a word is predicted from either word embeddings or shift vectors (i.e., a vector subtraction of the base word from the derived word). The accuracy with which the model predicts the prefix of a word indicates the degree of transparency of the prefix. Three further measures were obtained by comparing embeddings between each word and all other words containing the same prefix (i.e., centroid), between each word and the shift from their base word, and between each word and the predicted word of the Functional Representations of Affixes in Compositional Semantic Space model. In a series of Generalized Additive Mixed Models, all measures predicted decision latencies after accounting for word frequency, word length, and morphological family size. The model that included the correlation between each word and their centroid as a predictor provided the best fit to the data.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models</title>
<link>https://arxiv.org/abs/2505.06004</link>
<guid>https://arxiv.org/abs/2505.06004</guid>
<content:encoded><![CDATA[
<div> language models, grammatical error correction, multilingual, Gemma 9B, English, German, Italian, Swedish
Summary:
- The study examines the performance of 17 popular language models in correcting grammatical errors in texts written in English, German, Italian, and Swedish using a single model for all languages.
- Analysis focuses on minimizing grammatical errors while making minimal changes to the texts.
- Six models are identified as effective in enhancing grammatical correctness across all four languages.
- Gemma 9B is highlighted as the top-performing model among those considered.
- The findings offer insights into the challenges faced by language models in multilingual grammatical error correction tasks and recommend Gemma 9B for such tasks.
<br><br>Summary: <div>
arXiv:2505.06004v1 Announce Type: new 
Abstract: Recent language models can successfully solve various language-related tasks, and many understand inputs stated in different languages. In this paper, we explore the performance of 17 popular models used to correct grammatical issues in texts stated in English, German, Italian, and Swedish when using a single model to correct texts in all those languages. We analyze the outputs generated by these models, focusing on decreasing the number of grammatical errors while keeping the changes small. The conclusions drawn help us understand what problems occur among those models and which models can be recommended for multilingual grammatical error correction tasks. We list six models that improve grammatical correctness in all four languages and show that Gemma 9B is currently the best performing one for the languages considered.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective</title>
<link>https://arxiv.org/abs/2505.06010</link>
<guid>https://arxiv.org/abs/2505.06010</guid>
<content:encoded><![CDATA[
<div> URL addresses, IBAN numbers, emails, NMT models, entities

Summary:
The paper explores the abilities of popular NMT models, including those from the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities such as URL addresses, IBAN numbers, or emails during translation between English, German, Polish, and Ukrainian. The study investigates the accuracy of these models and discusses the errors they make, focusing on specific challenges like emojis. A new multilingual synthetic dataset of 36,000 sentences is proposed to evaluate entity transfer quality across nine categories in the four languages. Overall, the analysis sheds light on the performance of NMT models in entity preservation and identifies areas for improvement in handling entities during translation.<br><br>Summary: <div>
arXiv:2505.06010v1 Announce Type: new 
Abstract: Current machine translation models provide us with high-quality outputs in most scenarios. However, they still face some specific problems, such as detecting which entities should not be changed during translation. In this paper, we explore the abilities of popular NMT models, including models from the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities such as URL addresses, IBAN numbers, or emails when producing translations between four languages: English, German, Polish, and Ukrainian. We investigate the quality of popular NMT models in terms of accuracy, discuss errors made by the models, and examine the reasons for errors. Our analysis highlights specific categories, such as emojis, that pose significant challenges for many models considered. In addition to the analysis, we propose a new multilingual synthetic dataset of 36,000 sentences that can help assess the quality of entity transfer across nine categories and four aforementioned languages.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation</title>
<link>https://arxiv.org/abs/2505.06027</link>
<guid>https://arxiv.org/abs/2505.06027</guid>
<content:encoded><![CDATA[
<div> Keywords: Unilogit, machine unlearning, Large Language Models, data privacy regulations, self-distillation

Summary: 
Unilogit introduces a new self-distillation method for Large Language Models, specifically designed for machine unlearning tasks in compliance with data privacy regulations like GDPR. It dynamically adjusts target logits to selectively forget specific information while maintaining overall model utility. By leveraging the current model's outputs for more accurate targets, Unilogit eliminates the need for additional hyperparameters and enhances the model's ability to approximate golden targets. Extensive experiments on public benchmarks and an e-commerce dataset show Unilogit outperforms existing methods such as NPO and UnDIAL in balancing forget and retain objectives. The analysis demonstrates Unilogit's robustness across various scenarios, showcasing its practical applicability and effectiveness in achieving successful machine unlearning tasks.<br><br>Summary: <div>
arXiv:2505.06027v1 Announce Type: new 
Abstract: This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information</title>
<link>https://arxiv.org/abs/2505.06046</link>
<guid>https://arxiv.org/abs/2505.06046</guid>
<content:encoded><![CDATA[
<div> benchmark, PubHealthBench, Large Language Models, UK Government, public health

Summary:<br><br>Large Language Models (LLMs) are increasingly accessible, raising the need to assess their knowledge in specific domains like public health. A new benchmark called PubHealthBench with over 8000 questions was introduced to evaluate LLMs' knowledge of UK Government public health information. The benchmark includes Multiple Choice Question Answering (MCQA) and free form response tasks. Evaluation of 24 LLMs showed that the latest models like GPT-4.5 and GPT-4.1 performed well in MCQA, exceeding human performance with basic search engine assistance. However, in free form responses, no model scored above 75%, indicating the need for additional safeguards when providing open-ended public health information. While SOTA LLMs show promise as accurate sources of public health information, there is still room for improvement in providing comprehensive responses in this critical domain. <div>
arXiv:2505.06046v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax</title>
<link>https://arxiv.org/abs/2505.06062</link>
<guid>https://arxiv.org/abs/2505.06062</guid>
<content:encoded><![CDATA[
<div> MWEs, fine-tuned encoder-only models, BERT-based models, idioms, MSUs

Summary:
- The study analyzes attention patterns of fine-tuned BERT-based models towards idioms and MSUs.
- Idioms pose challenges due to semantic non-compositionality, while MSUs display unconventional syntactic behavior.
- The effects of fine-tuning on attention to MWEs are investigated, with a focus on semantic and syntactic tasks.
- Attention scores to MWEs in pre-trained and fine-tuned models across six Indo-European languages are examined.
- Results indicate that fine-tuning impacts how models allocate attention to MWEs, with semantic tasks leading to more even attention to idiomatic expressions and syntactic tasks increasing attention to MSUs in lower layers.

<br><br>Summary: <div>
arXiv:2505.06062v1 Announce Type: new 
Abstract: This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages - English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models</title>
<link>https://arxiv.org/abs/2505.06110</link>
<guid>https://arxiv.org/abs/2505.06110</guid>
<content:encoded><![CDATA[
<div> dataset, sentiment analysis, transformer-based models, early fusion, multimodal learning <br>
Summary: <br>
This project focuses on multimodal sentiment analysis using the CMU-MOSEI dataset and transformer-based models with early fusion. By integrating text, audio, and visual modalities using BERT-based encoders and early fusion, the model achieves strong performance, with high accuracy and F1-score on the test set. The use of transformer architectures proves superior in capturing cross-modal interactions for sentiment analysis. Training strategies such as Adam optimization, dropout, and early stopping ensure generalization and robustness, leading to precise sentiment intensity prediction with low MAE. Future work may involve comparing fusion strategies or enhancing interpretability. This approach effectively combines linguistic, acoustic, and visual cues for sentiment analysis through multimodal learning. <br> <div>
arXiv:2505.06110v1 Announce Type: new 
Abstract: This project performs multimodal sentiment analysis using the CMU-MOSEI dataset, using transformer-based models with early fusion to integrate text, audio, and visual modalities. We employ BERT-based encoders for each modality, extracting embeddings that are concatenated before classification. The model achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682 F1-score on the test set, demonstrating the effectiveness of early fusion in capturing cross-modal interactions. The training utilized Adam optimization (lr=1e-4), dropout (0.3), and early stopping to ensure generalization and robustness. Results highlight the superiority of transformer architectures in modeling multimodal sentiment, with a low MAE (0.1060) indicating precise sentiment intensity prediction. Future work may compare fusion strategies or enhance interpretability. This approach utilizes multimodal learning by effectively combining linguistic, acoustic, and visual cues for sentiment analysis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Get Lost In Multi-Turn Conversation</title>
<link>https://arxiv.org/abs/2505.06120</link>
<guid>https://arxiv.org/abs/2505.06120</guid>
<content:encoded><![CDATA[
<div> conversation logs, underspecification, LLM performance, multi-turn setting, simulation experiments

Summary:
Large Language Models (LLMs) serve as conversational interfaces and can assist users in tasks through multi-turn conversations. However, analysis shows that LLM performance drops by 39% in multi-turn settings compared to single-turn interactions. The decrease in performance is attributed to a minor loss in aptitude and a significant increase in unreliability. LLMs tend to make assumptions early in conversations and rely too heavily on these assumptions when generating solutions. Ultimately, when LLMs take a wrong turn in a conversation, they struggle to recover, leading to lower performance in multi-turn interactions. This study highlights the importance of evaluating LLMs in different conversation settings to improve overall performance and reliability.  

<br><br>Summary: <div>
arXiv:2505.06120v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies</title>
<link>https://arxiv.org/abs/2505.06145</link>
<guid>https://arxiv.org/abs/2505.06145</guid>
<content:encoded><![CDATA[
<div> Few-shot text classification, adaptive fine-tuning, contrastive learning, regularization optimization, Transformer-based models <br>
<br>
Summary: 
This paper introduces a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to enhance the performance of Transformer-based models in few-shot text classification tasks. Experimental results on the FewRel 2.0 dataset demonstrate the effectiveness of T5-small, DeBERTa-v3, and RoBERTa-base models, especially in the 5-shot setting. The study identifies varying levels of classification difficulty across different relationship categories, with some categories posing challenges due to ambiguous boundaries and complex feature distributions. By incorporating contrastive and regularization losses, the model's generalization ability is strengthened, addressing overfitting issues in low-resource scenarios. Furthermore, leveraging Transformer models or generative architectures with robust self-attention mechanisms enhances the stability and accuracy of few-shot classification tasks. <div>
arXiv:2505.06145v1 Announce Type: new 
Abstract: Few-shot text classification has important application value in low-resource environments. This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study</title>
<link>https://arxiv.org/abs/2505.06149</link>
<guid>https://arxiv.org/abs/2505.06149</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, multilingual language models, prompting techniques, zero-shot, few-shot

Summary:
Multilingual language models like LLaMA, Aya, Qwen, and BloomZ show promise in automated hate speech detection across languages. This study evaluates the effectiveness of prompting-based detection using these models in eight non-English languages. While zero-shot and few-shot prompting techniques may not perform as well as fine-tuned encoder models on real-world evaluation sets, they show better generalization on functional hate speech detection tests. The study underscores the importance of customized prompt design for each language to maximize performance. The results suggest that while prompting techniques may lag behind in some aspects, they offer potential for improved hate speech detection across linguistically diverse online content. This research highlights the need for further exploration and refinement of multilingual language models for detecting hate speech effectively. 

<br><br>Summary: Multilingual language models show promise in hate speech detection but require customized prompting techniques for optimal performance. Zero-shot and few-shot prompting methods outperform fine-tuned encoder models in generalization for hate speech detection tests across multiple languages. <div>
arXiv:2505.06149v1 Announce Type: new 
Abstract: Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets</title>
<link>https://arxiv.org/abs/2505.06150</link>
<guid>https://arxiv.org/abs/2505.06150</guid>
<content:encoded><![CDATA[
<div> scaling law, language models, compute budgets, dataset volume, token efficiency
<br>
Summary:
This study introduces a novel scaling law for fine-tuning large language models (LLMs) within fixed compute budgets, taking into account the data composition aspect. The conventional approach of measuring training data based solely on total tokens is deemed insufficient, as the number of examples and their average token length, or dataset volume, strongly influences model performance. Through experiments on the BRICC and MMLU datasets, it was demonstrated that the composition of data has a significant impact on token efficiency. These findings emphasize the need for more refined scaling laws for effective LLM fine-tuning in scenarios where resources are constrained. <div>
arXiv:2505.06150v1 Announce Type: new 
Abstract: We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework</title>
<link>https://arxiv.org/abs/2505.06151</link>
<guid>https://arxiv.org/abs/2505.06151</guid>
<content:encoded><![CDATA[
<div> Keywords: engagement, natural language processing, counseling sessions, therapeutic success, conversational dynamics

Summary:
- A multi-dimensional natural language processing framework is proposed to objectively classify engagement quality in counseling sessions based on textual transcripts.
- 42 features across four domains are extracted: conversational dynamics, semantic similarity, sentiment classification, and question detection.
- Classifiers like Random Forest, Cat-Boost, and Support Vector Machines are trained using a stratified 5-fold cross-validation and evaluated on a holdout test set.
- Performance significantly improves after SMOTE-Tomek augmentation, with Random Forest achieving up to 88.9% accuracy and SVM reaching 81.1% accuracy.
- Feature contribution analysis shows conversational dynamics and semantic similarity are top contributors, led by words uttered by the client.
- The framework is robust across original and augmented datasets, showing consistent improvements in F1 scores and recall, with potential for future multimodal extensions for holistic assessments.
- This scalable, data-driven method provides real-time feedback to enhance the quality of therapy sessions, both virtual and in-person. 

<br><br>Summary: 
A natural language processing framework is proposed to classify engagement quality in counseling sessions based on textual transcripts, showing improved performance with data augmentation and robustness across datasets. Feature analysis highlights the importance of conversational dynamics and semantic similarity. The framework offers real-time feedback for enhancing the quality of therapy sessions, supporting future multimodal extensions. <div>
arXiv:2505.06151v1 Announce Type: new 
Abstract: Engagement between client and therapist is a critical determinant of therapeutic success. We propose a multi-dimensional natural language processing (NLP) framework that objectively classifies engagement quality in counseling sessions based on textual transcripts. Using 253 motivational interviewing transcripts (150 high-quality, 103 low-quality), we extracted 42 features across four domains: conversational dynamics, semantic similarity as topic alignment, sentiment classification, and question detection. Classifiers, including Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM), were hyperparameter tuned and trained using a stratified 5-fold cross-validation and evaluated on a holdout test set. On balanced (non-augmented) data, RF achieved the highest classification accuracy (76.7%), and SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation, performance improved significantly: RF achieved up to 88.9% accuracy, 90.0% F1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and 93.6% AUC. The augmented data results reflect the potential of the framework in future larger-scale applications. Feature contribution revealed conversational dynamics and semantic similarity between clients and therapists were among the top contributors, led by words uttered by the client (mean and standard deviation). The framework was robust across the original and augmented datasets and demonstrated consistent improvements in F1 scores and recall. While currently text-based, the framework supports future multimodal extensions (e.g., vocal tone, facial affect) for more holistic assessments. This work introduces a scalable, data-driven method for evaluating engagement quality of the therapy session, offering clinicians real-time feedback to enhance the quality of both virtual and in-person therapeutic interactions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies</title>
<link>https://arxiv.org/abs/2505.06186</link>
<guid>https://arxiv.org/abs/2505.06186</guid>
<content:encoded><![CDATA[
<div> CochraneForest, scientific evidence extraction, biomedical studies, URCA, retrieval-augmented generation

Summary:
The paper introduces CochraneForest, a dataset for document-level scientific evidence extraction in clinical research questions with conflicting evidence. It includes annotated forest plots, associated questions, full texts of studies, and conclusions. The URCA framework is proposed to address challenges in evidence extraction, outperforming existing methods by up to 10.3% in F1 score. The experiments highlight the complexity of CochraneForest, emphasizing its significance as a testing ground for automated evidence synthesis systems. <div>
arXiv:2505.06186v1 Announce Type: new 
Abstract: Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
<div> vulnerability, adversarial perturbations, Universal Adversarial Perturbation (UAP), super transferability, surrogate scaling

Summary:
This paper introduces X-Transfer, a new attack method that exploits a universal vulnerability in CLIP models, particularly focusing on their susceptibility to adversarial perturbations. X-Transfer generates a Universal Adversarial Perturbation (UAP) that can deceive various CLIP encoders and downstream VLMs across different samples, tasks, and domains. The key innovation of X-Transfer is surrogate scaling, which dynamically selects suitable surrogate models to efficiently achieve adversarial transferability. Extensive evaluations show that X-Transfer outperforms existing UAP methods, setting a new benchmark for cross-data, cross-domain, cross-model, and cross-task adversarial transferability in CLIP models. The code for X-Transfer is publicly available on their GitHub repository. <br><br>Summary: <div>
arXiv:2505.05528v1 Announce Type: cross 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Stress Testing Black-Box LLM Planners</title>
<link>https://arxiv.org/abs/2505.05665</link>
<guid>https://arxiv.org/abs/2505.05665</guid>
<content:encoded><![CDATA[
<div> hallucination, language models, safety-critical scenarios, perturbations, Adaptive Stress Testing<br>
Summary: <br>
The article discusses the challenges of large language models (LLMs) hallucinating unsafe outputs in decision-making tasks, posing risks in safety-critical scenarios. Existing methods detect hallucinations by introducing prompt perturbations to test model stability. The study shows that various perturbations trigger hallucinations in LLMs in a driving environment. To efficiently search prompt perturbations, the authors propose Adaptive Stress Testing (AST) with Monte-Carlo Tree Search (MCTS). The AST formulation discovers scenarios causing model uncertainty, enabling the generation of prompts to influence model behavior. By constructing prompt perturbation trees across diverse scenarios, offline analyses can inform real-time trust assessments of LLMs. This method aims to enhance LLM safety and reliability in decision-making tasks. <br> <div>
arXiv:2505.05665v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated success in generalizing across decision-making tasks including planning, control and prediction, but their tendency to hallucinate unsafe and undesired outputs poses risks. We argue that detecting such failures is necessary, especially in safety-critical scenarios. Existing black-box methods often detect hallucinations by identifying inconsistencies across multiple samples. Many of these approaches typically introduce prompt perturbations like randomizing detail order or generating adversarial inputs, with the intuition that a confident model should produce stable outputs. We first perform a manual case study showing that other forms of perturbations (e.g., adding noise, removing sensor details) cause LLMs to hallucinate in a driving environment. We then propose a novel method for efficiently searching the space of prompt perturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search (MCTS). Our AST formulation enables discovery of scenarios and prompts that cause language models to act with high uncertainty. By generating MCTS prompt perturbation trees across diverse scenarios, we show that offline analyses can be used at runtime to automatically generate prompts that influence model uncertainty, and to inform real-time trust assessments of an LLM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompted Meta-Learning for Few-shot Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
<div> keywords: few-shot, knowledge graph completion, meta-semantics, prompt, meta-learning <br>
Summary: 
The article introduces a novel framework, PromptMeta, for few-shot knowledge graph completion (KGC) that incorporates both meta-semantics and relational information. The framework includes a meta-semantic prompt pool to capture high-level meta-semantics and a learnable fusion prompt to combine meta-semantic information with task-specific relational information. These components are optimized within a meta-learning framework to enhance knowledge transfer and adaptation to rare and emerging relations in KGs. Experimental results on benchmark datasets demonstrate the effectiveness of PromptMeta in improving few-shot KGC tasks by leveraging rich semantics in knowledge graphs. <div>
arXiv:2505.05684v1 Announce Type: cross 
Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention due to its practical applications in real-world scenarios, where new knowledge often emerges with limited available data. While most existing methods for few-shot KGC have predominantly focused on leveraging relational information, rich semantics inherent in KGs have been largely overlooked. To address this gap, we propose a novel prompted meta-learning (PromptMeta) framework that seamlessly integrates meta-semantics with relational information for few-shot KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that captures and consolidates high-level meta-semantics, enabling effective knowledge transfer and adaptation to rare and newly emerging relations. (2) a learnable fusion prompt that dynamically combines meta-semantic information with task-specific relational information tailored to different few-shot tasks. Both components are optimized together with model parameters within a meta-learning framework. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
<link>https://arxiv.org/abs/2505.05736</link>
<guid>https://arxiv.org/abs/2505.05736</guid>
<content:encoded><![CDATA[
<div> framework, multimodal biomedical data, Large Language Models, preference optimization, knowledge transfer
Summary:
- The MINT framework aligns unimodal Large Language Models (LLMs) with domain-specific decision patterns from multimodal biomedical data through preference optimization.
- It supports different optimization techniques but primarily uses the Odds Ratio Preference Optimization (ORPO) framework.
- MINT leverages an upstream multimodal machine learning (MML) model to transfer domain-specific insights to downstream text-only or image-only LLMs.
- Demonstrated effectiveness in rare genetic disease prediction from texts, outperforming existing models and even larger LLMs.
- Improved tissue type classification using cell nucleus images by aligning downstream image-only models with multimodal knowledge.
<br><br>Summary: <div>
arXiv:2505.05736v1 Announce Type: cross 
Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification</title>
<link>https://arxiv.org/abs/2505.05744</link>
<guid>https://arxiv.org/abs/2505.05744</guid>
<content:encoded><![CDATA[
<div> Explanation Generation, Surrogate Language Model, Tabular Prediction, Interpretability, Demonstration Selection 
Summary:
The article proposes a novel in-context learning framework for tabular prediction using Large Language Models (LLMs) to generate explanations guiding a Surrogate Language Model (SLM). The framework consists of three stages: Explanation Generation, where LLMs provide insights into reasoning; Explanation-Guided Demonstration Selection, using LLM explanations to select demonstrations; and Explanation-Guided Interpretable SLM Prediction, merging explanations with demonstrations to improve SLM performance and interpretability. Experimental results demonstrate a 5.31% average accuracy improvement across diverse tabular datasets. <div>
arXiv:2505.05744v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable ability in solving complex tasks, making them a promising tool for enhancing tabular learning. However, existing LLM-based methods suffer from high resource requirements, suboptimal demonstration selection, and limited interpretability, which largely hinder their prediction performance and application in the real world. To overcome these problems, we propose a novel in-context learning framework for tabular prediction. The core idea is to leverage the explanations generated by LLMs to guide a smaller, locally deployable Surrogate Language Model (SLM) to make interpretable tabular predictions. Specifically, our framework mainly involves three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to generate explanations for question-answer pairs in candidate demonstrations, providing insights into the reasoning behind the answer. (ii) Post Hoc Explanation-Guided Demonstrations Selection, which utilizes explanations generated by LLMs to guide the process of demonstration selection from candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM Prediction, which utilizes the demonstrations obtained in step (ii) as in-context and merges corresponding explanations as rationales to improve the performance of SLM and guide the model to generate interpretable outputs. Experimental results highlight the framework's effectiveness, with an average accuracy improvement of 5.31% across various tabular datasets in diverse domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection</title>
<link>https://arxiv.org/abs/2505.05763</link>
<guid>https://arxiv.org/abs/2505.05763</guid>
<content:encoded><![CDATA[
<div> Keywords: academic misconduct, biomedical research, deep learning, multimodal fusion, dataset

Summary:
BMMDetect is a novel deep learning framework designed to detect academic misconduct in biomedical research by integrating various data sources and features. It utilizes journal metadata, semantic embeddings, and textual attributes to provide a comprehensive evaluation of manuscripts. The framework incorporates domain-specific features to reduce bias and identifies important predictors such as journal authority metrics and textual anomalies. The BioMCD dataset, consisting of retracted articles and control samples, serves as a benchmark for evaluation. BMMDetect achieves a high AUC of 74.33%, outperforming single-modality baselines by 8.6%. This approach shows promise for detecting misconduct across different biomedical subfields and contributes to the development of scalable and interpretable tools for research integrity. 

Summary: <div>
arXiv:2505.05763v1 Announce Type: cross 
Abstract: Academic misconduct detection in biomedical research remains challenging due to algorithmic narrowness in existing methods and fragmented analytical pipelines. We present BMMDetect, a multimodal deep learning framework that integrates journal metadata (SJR, institutional data), semantic embeddings (PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics, data anomalies) for holistic manuscript evaluation. Key innovations include: (1) multimodal fusion of domain-specific features to reduce detection bias; (2) quantitative evaluation of feature importance, identifying journal authority metrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as dominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with 13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC, outperforming single-modality baselines by 8.6%, and demonstrates transferability across biomedical subfields. This work advances scalable, interpretable tools for safeguarding research integrity.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers</title>
<link>https://arxiv.org/abs/2505.05828</link>
<guid>https://arxiv.org/abs/2505.05828</guid>
<content:encoded><![CDATA[
<div> Keywords: chatbot, mental disorders, self-disclosure technique, teenagers, empathy

Summary:
This paper introduces a chatbot-based system designed to engage young Spanish individuals in understanding various mental disorders through a self-disclosure approach. The system targets teenagers aged 12 to 18, employing a mix of closed and open conversations with controlled messages focusing on specific disorders. By tailoring conversations based on users' sensitivity to a particular disorder, the system aims to establish empathetic communication. Following initial trial questions, the chatbot transitions to open conversations using the GPT-3 language model for more expressive communication. The study indicates that such systems hold promise in capturing the interest of young individuals and raising awareness about mental disorders.<br><br>Summary: <div>
arXiv:2505.05828v1 Announce Type: cross 
Abstract: This paper presents a chatbot-based system to engage young Spanish people in the awareness of certain mental disorders through a self-disclosure technique. The study was carried out in a population of teenagers aged between 12 and 18 years. The dialogue engine mixes closed and open conversations, so certain controlled messages are sent to focus the chat on a specific disorder, which will change over time. Once a set of trial questions is answered, the system can initiate the conversation on the disorder under the focus according to the user's sensibility to that disorder, in an attempt to establish a more empathetic communication. Then, an open conversation based on the GPT-3 language model is initiated, allowing the user to express themselves with more freedom. The results show that these systems are of interest to young people and could help them become aware of certain mental disorders.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary ecology of words</title>
<link>https://arxiv.org/abs/2505.05863</link>
<guid>https://arxiv.org/abs/2505.05863</guid>
<content:encoded><![CDATA[
<div> evolutionary ecology, words, language models, agent-based models, interactions <br>
Summary: 
The article proposes a model for the evolutionary ecology of words using Large Language Models (LLMs). In this model, agents possess short words generated by LLMs and interact in a spatial environment, with word replacements and mutations based on LLM outputs. Experiments were conducted with a focus on the survival of "strong animal species," resulting in the emergence of diverse populations with species adapted to various extreme habitats. Both gradual and punctuated equilibrium evolution patterns were observed, with dominant species types varying across trials. In a long-term experiment, a large population demonstrated the coexistence of diverse species. This model showcases the potential for LLMs to simulate and explore the evolution of word interactions and the emergence of diverse ecological populations. <br> <div>
arXiv:2505.05863v1 Announce Type: cross 
Abstract: We propose a model for the evolutionary ecology of words as one attempt to extend evolutionary game theory and agent-based models by utilizing the rich linguistic expressions of Large Language Models (LLMs). Our model enables the emergence and evolution of diverse and infinite options for interactions among agents. Within the population, each agent possesses a short word (or phrase) generated by an LLM and moves within a spatial environment. When agents become adjacent, the outcome of their interaction is determined by the LLM based on the relationship between their words, with the loser's word being replaced by the winner's. Word mutations, also based on LLM outputs, may occur. We conducted preliminary experiments assuming that ``strong animal species" would survive. The results showed that from an initial population consisting of well-known species, many species emerged both gradually and in a punctuated equilibrium manner. Each trial demonstrated the unique evolution of diverse populations, with one type of large species becoming dominant, such as terrestrial animals, marine life, or extinct species, which were ecologically specialized and adapted ones across diverse extreme habitats. We also conducted a long-term experiment with a large population, demonstrating the emergence and coexistence of diverse species.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification</title>
<link>https://arxiv.org/abs/2505.06032</link>
<guid>https://arxiv.org/abs/2505.06032</guid>
<content:encoded><![CDATA[
<div> actor names, shortcuts, language models, attention heads, Head-based Token Attribution

Summary:
The study investigates how language models rely on shortcuts, identified as spurious correlations, in decision-making processes. By utilizing actor names in movie reviews as controllable shortcuts, the researchers identify specific attention heads within the model that focus on these shortcuts. These attention heads lead the model to make premature decisions before fully processing all contextual information. The researchers introduce Head-based Token Attribution (HTA) as a method to trace intermediate decisions back to input tokens and detect shortcuts in language models effectively. By selectively deactivating attention heads related to shortcuts, HTA allows for targeted mitigation of shortcut reliance within language models.<br><br>Summary: <div>
arXiv:2505.06032v1 Announce Type: cross 
Abstract: Reliance on spurious correlations (shortcuts) has been shown to underlie many of the successes of language models. Previous work focused on identifying the input elements that impact prediction. We investigate how shortcuts are actually processed within the model's decision-making mechanism. We use actor names in movie reviews as controllable shortcuts with known impact on the outcome. We use mechanistic interpretability methods and identify specific attention heads that focus on shortcuts. These heads gear the model towards a label before processing the complete input, effectively making premature decisions that bypass contextual analysis. Based on these findings, we introduce Head-based Token Attribution (HTA), which traces intermediate decisions back to input tokens. We show that HTA is effective in detecting shortcuts in LLMs and enables targeted mitigation by selectively deactivating shortcut-related attention heads.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models</title>
<link>https://arxiv.org/abs/2505.06107</link>
<guid>https://arxiv.org/abs/2505.06107</guid>
<content:encoded><![CDATA[
<div> detect nationality, migration research, digital trace data, machine learning model, return migration<br>
Summary:<br>
The lack of nationality information in web and digital trace data poses challenges for migration research. To address this issue, a method to detect nationality using full names was proposed. Using a character-based machine learning model trained on Wikipedia data, the study achieved high accuracy in categorizing nationalities. Applying this method to academic data revealed discrepancies in the estimation of return migration when using country of academic origin versus country of name origin. For instance, a significant proportion of emigrants from the USA were actually return migrants when considering their names. The study also found that scholars' names can provide valuable insights into migration patterns, such as the high proportion of scholars with Chinese names transitioning from the USA to China. Overall, the proposed methods offer a solution to left-censoring issues in migration research using digital trace data.<br> 
Summary: <div>
arXiv:2505.06107v1 Announce Type: cross 
Abstract: Most web and digital trace data do not include information about an individual's nationality due to privacy concerns. The lack of data on nationality can create challenges for migration research. It can lead to a left-censoring issue since we are uncertain about the migrant's country of origin. Once we observe an emigration event, if we know the nationality, we can differentiate it from return migration. We propose methods to detect the nationality with the least available data, i.e., full names. We use the detected nationality in comparison with the country of academic origin, which is a common approach in studying the migration of researchers. We gathered 2.6 million unique name-nationality pairs from Wikipedia and categorized them into families of nationalities with three granularity levels to use as our training data. Using a character-based machine learning model, we achieved a weighted F1 score of 84% for the broadest and 67% for the most granular, country-level categorization. In our empirical study, we used the trained and tested model to assign nationality to 8+ million scholars' full names in Scopus data. Our results show that using the country of first publication as a proxy for nationality underestimates the size of return flows, especially for countries with a more diverse academic workforce, such as the USA, Australia, and Canada. We found that around 48% of emigration from the USA was return migration once we used the country of name origin, in contrast to 33% based on academic origin. In the most recent period, 79% of scholars whose affiliation has consistently changed from the USA to China, and are considered emigrants, have Chinese names in contrast to 41% with a Chinese academic origin. Our proposed methods for addressing left-censoring issues are beneficial for other research that uses digital trace data to study migration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
<link>https://arxiv.org/abs/2505.06184</link>
<guid>https://arxiv.org/abs/2505.06184</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media profiling, Large language model, User behavior modeling, Misinformation detection, Twitter dataset <br>
Summary: Our novel approach leverages large language models to create interpretable user profiles through domain-defining statements. The method consists of a two-stage process involving semi-supervised filtering with a knowledge base and generating abstractive and extractive user profiles. By harnessing the knowledge of LLMs with minimal human validation, our approach reduces the need for large labeled datasets while producing flexible and adaptable profiles. We introduce a Persian political Twitter (X) dataset and an evaluation framework for LLM-based profiling. Experimental results demonstrate a 9.8% improvement over state-of-the-art methods, showcasing the effectiveness of our approach in creating interpretable and adaptable user profiles. <br><br>Summary: Our approach utilizes large language models to create interpretable user profiles for social media tasks. Through domain-defining statements and a two-stage process, the method effectively condenses user data into flexible profiles for downstream tasks. Results show a significant improvement over existing methods, underscoring the adaptability and effectiveness of our approach. <div>
arXiv:2505.06184v1 Announce Type: cross 
Abstract: Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Concepts</title>
<link>https://arxiv.org/abs/2505.06191</link>
<guid>https://arxiv.org/abs/2505.06191</guid>
<content:encoded><![CDATA[
<div> Keywords: concept-centric paradigm, neuro-symbolic concepts, continual learning, compositional generalization, zero-shot transfer

Summary:
The article introduces a concept-centric paradigm for developing agents capable of continuous learning and flexible reasoning. These agents operate by utilizing a specialized vocabulary of neuro-symbolic concepts, grounded in sensory inputs and actions, and allowing for the creation of new concepts through compositional combination. Concept types are represented using a hybrid approach of symbolic programs and neural networks, enabling efficient learning and problem-solving across diverse domains like images, videos, 3D scenes, and robotics. The concept-centric framework provides benefits such as data efficiency, broad generalization, continual learning, and the ability to transfer knowledge to new tasks without prior training. <div>
arXiv:2505.06191v1 Announce Type: cross 
Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PART: Pre-trained Authorship Representation Transformer</title>
<link>https://arxiv.org/abs/2209.15373</link>
<guid>https://arxiv.org/abs/2209.15373</guid>
<content:encoded><![CDATA[
<div> Keywords: authorship embeddings, stylometric representations, contrastive training, text classification, data visualization

Summary:
In this paper, the authors introduce PART, a model trained to learn authorship embeddings rather than focusing on semantics. By training on a diverse dataset of texts from literature authors, blog posters, and corporate email accounts, PART shows competitive performance on various authorship challenges. When evaluated on test datasets, PART achieves a remarkable zero-shot accuracy of 72.39% with 250 authors, outperforming RoBERTa embeddings by 54% and 56%. The model's representations are qualitatively analyzed through data visualizations, revealing insights into the author's gender, age, and occupation based on writing styles. This approach demonstrates the effectiveness of utilizing stylometric representations for authorship identification tasks, highlighting the significance of contrastive training in improving model performance across diverse author profiles.<br><br>Summary: <div>
arXiv:2209.15373v2 Announce Type: replace 
Abstract: Authors writing documents imprint identifying information within their texts: vocabulary, registry, punctuation, misspellings, or even emoji usage. Previous works use hand-crafted features or classification tasks to train their authorship models, leading to poor performance on out-of-domain authors. Using stylometric representations is more suitable, but this by itself is an open research challenge. In this paper, we propose PART, a contrastively trained model fit to learn \textbf{authorship embeddings} instead of semantics. We train our model on ~1.5M texts belonging to 1162 literature authors, 17287 blog posters and 135 corporate email accounts; a heterogeneous set with identifiable writing styles. We evaluate the model on current challenges, achieving competitive performance. We also evaluate our model on test splits of the datasets achieving zero-shot 72.39\% accuracy when bounded to 250 authors, a 54\% and 56\% higher than RoBERTa embeddings. We qualitatively assess the representations with different data visualizations on the available datasets, observing features such as gender, age, or occupation of the author.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking Heads: Understanding Inter-layer Communication in Transformer Language Models</title>
<link>https://arxiv.org/abs/2406.09519</link>
<guid>https://arxiv.org/abs/2406.09519</guid>
<content:encoded><![CDATA[
<div> low-rank subspaces, communication channels, transformer language models, context retrieval, Singular Value Decomposition <br>
Summary:<br>
The study explores how transformer language models (LMs) use low-rank subspaces to represent and route information between layers. A mechanism that selectively inhibits items in a context is analyzed, revealing the formation of low-rank communication channels between layers. By decomposing attention heads using Singular Value Decomposition (SVD), interactions between heads in different layers can be predicted based on weight matrices alone. The study shows how manipulating internal model representations and editing model weights based on the discovered mechanism can significantly improve performance on a synthetic Laundry List task. The analysis uncovers an intricate interpretable structure learned during LM pretraining and sheds light on the reasons behind the occasional failures of sophisticated LMs in simpler domains, providing insights for future studies on more complex behaviors. <br> <div>
arXiv:2406.09519v4 Announce Type: replace 
Abstract: Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank communication channels (Elhage et al., 2021) between layers. A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. That is, the model has trouble copying the correct information from context when many items ``crowd" this limited space. By decomposing attention heads with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices alone. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?</title>
<link>https://arxiv.org/abs/2407.11963</link>
<guid>https://arxiv.org/abs/2407.11963</guid>
<content:encoded><![CDATA[
<div> framework, bilingual, long-context tasks, retrieval, reasoning

Summary:
NeedleBench is a synthetic framework designed to assess the retrieval and reasoning performance of large language models in bilingual long-context tasks with adaptive context lengths. It categorizes tasks into information-sparse and information-dense scenarios to simulate simple retrieval and complex reasoning tasks, respectively. The framework embeds key data points at varying depths to rigorously test model capabilities. Findings from experiments show that models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning but struggle with continuous retrieval and reasoning in information-dense scenarios. The study also identifies a phenomenon dubbed 'under-thinking,' where models prematurely conclude reasoning despite available information. NeedleBench provides critical insights and tools for evaluating and enhancing the long-context capabilities of large language models. Resources are available on the OpenCompass GitHub repository at https://github.com/open-compass/opencompass. 

Summary: <br><br> <div>
arXiv:2407.11963v2 Announce Type: replace 
Abstract: The capability of large language models to handle long-context information is crucial across various real-world applications. Existing evaluation methods often rely either on real-world long texts, making it difficult to exclude the influence of models' inherent knowledge, or introduce irrelevant filler content to artificially achieve target lengths, reducing assessment effectiveness. To address these limitations, we introduce NeedleBench, a synthetic framework for assessing retrieval and reasoning performance in bilingual long-context tasks with adaptive context lengths. NeedleBench systematically embeds key data points at varying depths to rigorously test model capabilities. Tasks are categorized into two scenarios: information-sparse, featuring minimal relevant details within extensive irrelevant text to simulate simple retrieval tasks; and information-dense (the Ancestral Trace Challenge), where relevant information is continuously distributed throughout the context to simulate complex reasoning tasks. Our experiments reveal that although recent reasoning models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they struggle with continuous retrieval and reasoning in information-dense scenarios, even at shorter context lengths. We also characterize a phenomenon termed 'under-thinking', where models prematurely conclude reasoning despite available information. NeedleBench thus provides critical insights and targeted tools essential for evaluating and improving LLMs' long-context capabilities. All resources are available at OpenCompass: https://github.com/open-compass/opencompass.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget</title>
<link>https://arxiv.org/abs/2408.00103</link>
<guid>https://arxiv.org/abs/2408.00103</guid>
<content:encoded><![CDATA[
<div> Retriever-Reader architecture, Entity Linking, Relation Extraction, ReLiK, input representation<br>
Summary:<br>
In this paper, a novel Retriever-Reader architecture named ReLiK is proposed for Entity Linking (EL) and Relation Extraction (RE) tasks in Natural Language Processing. The Retriever module identifies candidate entities or relations in the input text, while the Reader module determines the relevant entities or relations and aligns them with the text. An innovative input representation incorporating candidate entities or relations with the text enables linking entities and extracting relations in a single pass, leveraging pre-trained language models. The approach achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks with academic budget training and up to 40x faster inference speed. Additionally, the architecture seamlessly applies to Information Extraction (cIE) by sharing a Reader for entity and relation extraction, further setting a new state-of-the-art. <div>
arXiv:2408.00103v3 Announce Type: replace 
Abstract: Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits</title>
<link>https://arxiv.org/abs/2410.18234</link>
<guid>https://arxiv.org/abs/2410.18234</guid>
<content:encoded><![CDATA[
<div> draft models, token-level draft selection scheme, linear program, importance sampling, speculative sampling <br>
Summary: <br>
The article discusses multi-draft speculative sampling, focusing on token-level draft selection schemes. It introduces an optimal scheme derived from a linear program solution and decomposes it into a two-step process involving importance sampling and speculative sampling. For identical draft models, conditions for achieving a maximum acceptance probability are established, along with an explicit expression for optimal acceptance probability. Additionally, the study proposes a new class of selection schemes based on weighted importance sampling. Experimental results show significant enhancements in block efficiency and token rates compared to baseline schemes across various scenarios. <div>
arXiv:2410.18234v2 Announce Type: replace 
Abstract: We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation</title>
<link>https://arxiv.org/abs/2411.11053</link>
<guid>https://arxiv.org/abs/2411.11053</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning-augmented data generation, SRA-MCTS, self-improvement, complex tasks<br>
Summary:<br>
The article discusses the challenges faced by large language models in tackling complex tasks and proposes a novel solution called SRA-MCTS. This method enhances the model's reasoning and problem decomposition capabilities by autonomously generating high-quality reasoning paths. Through a feedback loop, the model continuously improves its performance without additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy in solving complex tasks. Experimental results show performance improvements across different model scales, highlighting the significant potential of self-improvement in smaller models. The method remains robust even when traditional approaches exhibit performance degradation, with notable enhancements seen in diversity metrics. The study emphasizes the importance of exploring reasoning processes within training data to enhance language models' ability to address complex problems.<br><br>Summary: <div>
arXiv:2411.11053v5 Announce Type: replace 
Abstract: Large language models demonstrate exceptional performance in simple code generation tasks but still face challenges in tackling complex problems. These challenges may stem from insufficient reasoning and problem decomposition capabilities. To address this issue, we propose a reasoning-augmented data generation process, SRA-MCTS, which guides the model to autonomously generate high-quality intermediate reasoning paths. This creates a positive feedback loop, enabling continuous improvement. Our method operates entirely through the model itself without requiring additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy and enhances the success rate in solving complex tasks. Experimental results show that, even without additional supervisory signals, our method achieves performance improvements across different model scales, demonstrating the significant potential of self-improvement in small models. Furthermore, the method remains robust when traditional Chain-of-Thought (CoT) approaches exhibit performance degradation, with notable improvements observed in diversity metrics such as pass@10. We encourage further exploration of reasoning processes within training data to enhance the ability of language models to address complex problems. Our code and data are public at https://github.com/DIRECT-BIT/SRA-MCTS.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes</title>
<link>https://arxiv.org/abs/2501.12106</link>
<guid>https://arxiv.org/abs/2501.12106</guid>
<content:encoded><![CDATA[
<div> Keywords: tumor documentation, large language models, medical NLP, dataset, German-language

Summary:
Large language models (LLMs) were evaluated for tumor documentation tasks in Germany, showing potential to automate the process efficiently and reliably. Eleven open source LLMs were tested on tasks such as identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis using a dataset of annotated text snippets from urology. Models with 7-12 billion parameters performed well, indicating an optimal balance between performance and resource efficiency. Few-shot prompting with examples from different medical domains improved outcomes, demonstrating the LLMs' versatility. The study highlights the potential of LLMs in clinical documentation and offers valuable resources, including code for evaluation and the released dataset for German-language medical NLP benchmarking.<br><br>Summary: <div>
arXiv:2501.12106v3 Announce Type: replace 
Abstract: Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors' notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2501.14851</link>
<guid>https://arxiv.org/abs/2501.14851</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, deductive reasoning, benchmark, JustLogic, error analysis 

Summary: 
JustLogic is a new deductive reasoning benchmark designed to evaluate Large Language Models (LLMs). Unlike existing benchmarks, JustLogic is highly complex, independent of prior knowledge, and allows for in-depth error analysis on reasoning depth and argument form. Experimental results show that state-of-the-art reasoning LLMs perform comparably to the human average but fall short of the human ceiling. Non-reasoning models still lag behind the human average. JustLogic provides a challenging evaluation platform for assessing LLMs' deductive reasoning capabilities, addressing deficiencies in current benchmarks such as task complexity and confounding factors. The benchmark is publicly available, enabling further research and development. 

<br><br>Summary: <div>
arXiv:2501.14851v2 Announce Type: replace 
Abstract: Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average. All code and data are available at https://github.com/michaelchen-lab/JustLogic
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought</title>
<link>https://arxiv.org/abs/2501.16154</link>
<guid>https://arxiv.org/abs/2501.16154</guid>
<content:encoded><![CDATA[
<div> multilingual models, pretraining, AdaCoT, factual reasoning, low-resource languages <br>
<br>
Summary: 
The article introduces AdaCoT, a framework designed to improve multilingual factual reasoning by dynamically routing thought processes through intermediary "thinking languages" before generating responses in the target language. AdaCoT utilizes a language-agnostic core and incorporates an adaptive, reward-based mechanism to select optimal reasoning pathways without the need for additional pretraining. Through comprehensive evaluations on multiple benchmarks, AdaCoT demonstrates significant enhancements in both factual reasoning quality and cross-lingual consistency, particularly benefiting low-resource languages. The results indicate that adaptive reasoning paths can effectively narrow the performance gap between high and low-resource languages while preserving cultural and linguistic nuances. <div>
arXiv:2501.16154v2 Announce Type: replace 
Abstract: Large language models have shown impressive multilingual capabilities through pretraining on diverse corpora. While these models show strong reasoning abilities, their performance varies significantly across languages due to imbalanced training data distribution. Existing approaches using sample-level translation for extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages. In this paper, we introduce AdaCoT (Adaptive Chain-of-Thought), a framework that enhances multilingual factual reasoning by dynamically routing thought processes in intermediary ``thinking languages'' before generating target-language responses. AdaCoT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phonetic accommodation and inhibition in a dynamic neural field model</title>
<link>https://arxiv.org/abs/2502.01210</link>
<guid>https://arxiv.org/abs/2502.01210</guid>
<content:encoded><![CDATA[
<div> Keywords: short-term phonetic accommodation, accent change, speech planning representations, dynamic neural field equations, memory dynamics <br>
Summary: 
The study investigates how real-time input from another speaker's voice influences the speech planning representations of an interlocutor during short-term phonetic accommodation. A computational model grounded in dynamic neural field equations for movement planning and memory dynamics is proposed. The model suggests that convergence to a model talker on one trial can lead to divergence on subsequent trials due to delayed inhibitory effects in the memory field. Empirical patterns from an experimental pilot study are compared with the model's predictions, indicating that variations in inhibitory memory dynamics may reflect resistance to accommodation influenced by phonological and sociolinguistic pressures. The findings shed light on the relationship between short-term phonetic accommodation and accent change, highlighting the complex interplay between memory dynamics and societal influences on speech adaptation.<br><br>Summary: <div>
arXiv:2502.01210v2 Announce Type: replace 
Abstract: Short-term phonetic accommodation is a fundamental driver behind accent change, but how does real-time input from another speaker's voice shape the speech planning representations of an interlocutor? We advance a computational model of change in speech planning representations during phonetic accommodation, grounded in dynamic neural field equations for movement planning and memory dynamics. A dual-layer planning/memory field predicts that convergence to a model talker on one trial can trigger divergence on subsequent trials, due to a delayed inhibitory effect in the more slowly evolving memory field. The model's predictions are compared with empirical patterns of accommodation from an experimental pilot study. We show that observed empirical phenomena may correspond to variation in the magnitude of inhibitory memory dynamics, which could reflect resistance to accommodation due to phonological and/or sociolinguistic pressures. We discuss the implications of these results for the relations between short-term phonetic accommodation and sound change.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs</title>
<link>https://arxiv.org/abs/2502.04134</link>
<guid>https://arxiv.org/abs/2502.04134</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, order sensitivity, reliability, input arrangement, output accuracy

Summary:
Large language models (LLMs) are crucial in various applications, but the issue of order sensitivity remains a challenge. This study explores the impact of input order on LLM performance across tasks like paraphrasing and relevance judgment. Results reveal that shuffled inputs significantly decrease output accuracy, highlighting the persistent risks associated with order sensitivity in high-stakes applications. While few-shot prompting shows some effectiveness in mitigating the issue, it does not completely solve the problem. The study underscores the need for more robust LLMs or improved input-handling techniques in future development to address the reliability concerns posed by order sensitivity in hidden LLM components. 

Summary: <div>
arXiv:2502.04134v2 Announce Type: replace 
Abstract: As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in the input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in LLMs whose internal components are hidden from users (such as closed-source models or those accessed via API calls). We conduct experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation; however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids</title>
<link>https://arxiv.org/abs/2502.09667</link>
<guid>https://arxiv.org/abs/2502.09667</guid>
<content:encoded><![CDATA[
<div> Keywords: k-LLMmeans, text clustering, LLM-generated summaries, semantic interpretability, mini-batch variant

Summary: 
The study introduces k-LLMmeans, a modified version of the k-means algorithm for text clustering that uses LLM-generated summaries as cluster centroids to capture semantic nuances. This approach enhances semantic interpretability while maintaining the optimization properties of k-means and addressing scalability and instability issues associated with LLM-based clustering. The method does not rely on increased LLM usage with dataset size and provides transparent intermediate outputs. Additionally, a mini-batch variant is introduced for efficient real-time clustering of streaming text. Experimental results across various datasets, embeddings, and LLMs demonstrate that k-LLMmeans consistently outperforms traditional baselines and achieves comparable results to state-of-the-art LLM-based clustering with fewer LLM calls. A case study on sequential text streams is presented, along with a new benchmark dataset from StackExchange for evaluating text-stream clustering methods. 

<br><br>Summary: <div>
arXiv:2502.09667v2 Announce Type: replace 
Abstract: We introduce k-LLMmeans, a novel modification of the k-means algorithm for text clustering that leverages LLM-generated summaries as cluster centroids, capturing semantic nuances often missed by purely numerical averages. This design preserves the core optimization properties of k-means while enhancing semantic interpretability and avoiding the scalability and instability issues typical of modern LLM-based clustering. Unlike existing methods, our approach does not increase LLM usage with dataset size and produces transparent intermediate outputs. We further extend it with a mini-batch variant for efficient, real-time clustering of streaming text. Extensive experiments across multiple datasets, embeddings, and LLMs show that k-LLMmeans consistently outperforms k-means and other traditional baselines and achieves results comparable to state-of-the-art LLM-based clustering, with a fraction of the LLM calls. Finally, we present a case study on sequential text streams and introduce a new benchmark dataset constructed from StackExchange to evaluate text-stream clustering methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization</title>
<link>https://arxiv.org/abs/2502.20364</link>
<guid>https://arxiv.org/abs/2502.20364</guid>
<content:encoded><![CDATA[

arXiv:2502.20364v2 Announce Type: replace 
Abstract: Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2503.17460</link>
<guid>https://arxiv.org/abs/2503.17460</guid>
<content:encoded><![CDATA[

arXiv:2503.17460v2 Announce Type: replace 
Abstract: In this paper, we present ConvoGen: an innovative framework for generating synthetic conversational data using multi-agent systems. Our method leverages few-shot learning and introduces iterative sampling from a dynamically updated few-shot hub to create diverse and realistic conversational scenarios. The generated data has numerous applications, including training and evaluating conversational AI models, and augmenting existing datasets for tasks like conversational intent classification or conversation summarization. Our experiments demonstrate the effectiveness of this method in producing high-quality diverse synthetic conversational data, highlighting its potential to enhance the development and evaluation of conversational AI systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Urban Science: Scaling Causal Inference with Large Language Models</title>
<link>https://arxiv.org/abs/2504.12345</link>
<guid>https://arxiv.org/abs/2504.12345</guid>
<content:encoded><![CDATA[

arXiv:2504.12345v2 Announce Type: replace 
Abstract: Urban causal research is essential for understanding the complex dynamics of cities and informing evidence-based policies. However, it is challenged by the inefficiency and bias of hypothesis generation, barriers to multimodal data complexity, and the methodological fragility of causal experimentation. Recent advances in large language models (LLMs) present an opportunity to rethink how urban causal analysis is conducted. This Perspective examines current urban causal research by analyzing taxonomies that categorize research topics, data sources, and methodological approaches to identify structural gaps. We then introduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy recommendations. We propose evaluation criteria for rigor and transparency and reflect on implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces AI-augmented workflows not as replacements for human expertise but as tools to broaden participation, improve reproducibility, and unlock more inclusive forms of urban causal reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
<link>https://arxiv.org/abs/2406.09831</link>
<guid>https://arxiv.org/abs/2406.09831</guid>
<content:encoded><![CDATA[

arXiv:2406.09831v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.24289</link>
<guid>https://arxiv.org/abs/2503.24289</guid>
<content:encoded><![CDATA[

arXiv:2503.24289v2 Announce Type: replace-cross 
Abstract: We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2404.04545</link>
<guid>https://arxiv.org/abs/2404.04545</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, Cross-Attention Network, Text-oriented, Gated Control Mechanism, Joint Learning  

<br /><br />Summary:  
Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic signals to determine human sentiment. Previous MSA techniques demonstrated strong performance but often treated different modalities equally, leading to a lack of attention to the variation in semantic richness among them. Recognizing that certain modalities may carry more weight, the authors propose the Text-oriented Cross-Attention Network (TCAN), which prioritizes the text modality. The framework begins by organizing unimodal features into visual-text and acoustic-text pairs, followed by implementing self-attention on the text modality. The method includes text-queried cross-attention to refine features from the visual and acoustic streams. A gated control mechanism is used to control noise and redundant features, enhancing the system’s focus on significant data. Furthermore, unimodal joint learning is employed to better understand emotional dynamics across different modalities via backpropagation. Extensive experiments validate the effectiveness of the TCAN model, showing consistent improvements over state-of-the-art MSA methods on two datasets, CMU-MOSI and CMU-MOSEI. This approach innovatively addresses the challenge of multimodal heterogeneities in sentiment analysis, marking a significant advancement in the field. <div>
arXiv:2404.04545v3 Announce Type: replace-cross 
Abstract: Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the variation in semantic richness among different modalities, treating each modality uniformly. This approach may lead to underestimating the significance of strong modalities while overemphasizing the importance of weak ones. Motivated by these insights, we introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the predominant role of the text modality in MSA. Specifically, for each multimodal sample, by taking unaligned sequences of the three modalities as inputs, we initially allocate the extracted unimodal features into a visual-text and an acoustic-text pair. Subsequently, we implement self-attention on the text modality and apply text-queried cross-attention to the visual and acoustic modalities. To mitigate the influence of noise signals and redundant features, we incorporate a gated control mechanism into the framework. Additionally, we introduce unimodal joint learning to gain a deeper understanding of homogeneous emotional tendencies across diverse modalities through backpropagation. Experimental results demonstrate that TCAN consistently outperforms state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks</title>
<link>https://arxiv.org/abs/2505.04628</link>
<guid>https://arxiv.org/abs/2505.04628</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social interaction, benchmark, communication, efficiency

<br /><br />Summary: The article discusses the need for large language models (LLMs) to extend their roles beyond being single-user assistants to effectively engage in multi-user, multi-turn social agent tasks within complex environments. Currently, there is no systematic approach to measure this capability. To tackle this issue, the authors introduce an agent task leveling framework based on sociological principles and a new benchmark called How Social Is It (HSII). HSII is aimed at evaluating the social capabilities of LLMs through tasks that encompass format parsing, target selection, target switching conversation, and stable conversation, all derived from a dataset called HSII-Dataset. They also conduct an ablation study involving clustering to enhance the dataset's effectiveness. Additionally, the impact of the chain of thought (COT) method on LLMs' social performance is examined. The authors propose a new metric, COT-complexity, to measure the efficiency of LLMs using COT for social tasks, aiming to balance correctness and efficiency. Experimental results indicate that HSII serves as a suitable benchmark for assessing the social skills of LLMs. <div>
arXiv:2505.04628v1 Announce Type: new 
Abstract: Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.04637</link>
<guid>https://arxiv.org/abs/2505.04637</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, human cognition, tokenization, dynamic representation, Visual Question Answering<br /><br />Summary: This research explores the discrepancies between human cognitive processes and the methodologies employed by multimodal large language models (MLLMs) for integrating diverse data types. It systematically investigates how human cross-modal chunking mechanisms align with token representation in MLLMs. Through empirical studies, the authors compare human performance with model behaviors across visual-linguistic tasks, revealing that traditional static tokenization significantly limits models from simulating the dynamic nature of human information processing. To address these limitations, the study proposes a new framework for dynamic cross-modal tokenization which includes adaptive boundaries, hierarchical representations, and alignment mechanisms based on cognitive science principles. Quantitative evaluations indicate that this novel approach surpasses existing state-of-the-art models, achieving improvements of +7.8% on Visual Question Answering and +5.3% on Complex Scene Description tasks. Additionally, it demonstrates more human-like error patterns and attention distributions. The findings contribute to a deeper theoretical understanding of the relationship between human cognition and AI, providing valuable empirical evidence for creating AI systems that are more aligned with cognitive processes. <div>
arXiv:2505.04637v1 Announce Type: new 
Abstract: Recent advancements in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing diverse data types, yet significant disparities persist between human cognitive processes and computational approaches to multimodal information integration. This research presents a systematic investigation into the parallels between human cross-modal chunking mechanisms and token representation methodologies in MLLMs. Through empirical studies comparing human performance patterns with model behaviors across visual-linguistic tasks, we demonstrate that conventional static tokenization schemes fundamentally constrain current models' capacity to simulate the dynamic, context-sensitive nature of human information processing. We propose a novel framework for dynamic cross-modal tokenization that incorporates adaptive boundaries, hierarchical representations, and alignment mechanisms grounded in cognitive science principles. Quantitative evaluations demonstrate that our approach yields statistically significant improvements over state-of-the-art models on benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene Description) while exhibiting more human-aligned error patterns and attention distributions. These findings contribute to the theoretical understanding of the relationship between human cognition and artificial intelligence, while providing empirical evidence for developing more cognitively plausible AI systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language translation, and change of accent for speech-to-speech task using diffusion model</title>
<link>https://arxiv.org/abs/2505.04639</link>
<guid>https://arxiv.org/abs/2505.04639</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech-to-speech translation, accent adaptation, conditional generation, diffusion models, Mel spectrograms

<br /><br />Summary: The paper presents a unified approach for simultaneous speech-to-speech translation (S2ST) and accent adaptation. Traditional methods have largely focused on either translating spoken content or adjusting accents, but effective cross-cultural communication requires addressing both aspects concurrently. The authors reformulate S2ST as a conditional generation task where the target speech is generated based on phonemes while integrating features that reflect the desired accent of the target language. To achieve this, they leverage diffusion models, which are recognized for their high-fidelity generative capabilities, adapting strategies commonly used in text-to-image diffusion. Their model conditions on source speech transcriptions to generate Mel spectrograms—acoustic representations of speech—reflecting both linguistic content and accentual characteristics. This integrated framework allows for joint optimization of translation and accent adaptation, providing a more parameter-efficient and effective solution compared to conventional S2ST pipelines. By exploring this underexplored area in the literature, the authors aim to enhance cross-cultural communication through more natural-sounding and contextually appropriate speech output in the target language. <div>
arXiv:2505.04639v1 Announce Type: new 
Abstract: Speech-to-speech translation (S2ST) aims to convert spoken input in one language to spoken output in another, typically focusing on either language translation or accent adaptation. However, effective cross-cultural communication requires handling both aspects simultaneously - translating content while adapting the speaker's accent to match the target language context. In this work, we propose a unified approach for simultaneous speech translation and change of accent, a task that remains underexplored in current literature. Our method reformulates the problem as a conditional generation task, where target speech is generated based on phonemes and guided by target speech features. Leveraging the power of diffusion models, known for high-fidelity generative capabilities, we adapt text-to-image diffusion strategies by conditioning on source speech transcriptions and generating Mel spectrograms representing the target speech with desired linguistic and accentual attributes. This integrated framework enables joint optimization of translation and accent adaptation, offering a more parameter-efficient and effective model compared to traditional pipelines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)</title>
<link>https://arxiv.org/abs/2505.04640</link>
<guid>https://arxiv.org/abs/2505.04640</guid>
<content:encoded><![CDATA[
<div> Keywords: Typica.ai, Moroccan Darija, toxicity detection, LLM-based moderation, cultural adaptation  

<br /><br />Summary: This paper introduces a benchmark assessing the effectiveness of Typica.ai's Moroccan Darija toxicity detection model in comparison to prominent LLM-based moderation APIs such as OpenAI, Mistral, and Anthropic Claude. The focus is on culturally nuanced toxic content that includes implicit insults, sarcasm, and specific aggression that general models may overlook. A balanced test set was derived from the OMCD_Typica.ai_Mix dataset to evaluate the performance of each model. Key metrics used for assessment included precision, recall, F1-score, and accuracy. The findings of the study indicate that Typica.ai outperforms other models, thereby emphasizing the significance of creating culturally adapted moderation systems. The research highlights both the challenges and opportunities present in moderating content in underrepresented languages, illustrating the necessity for tailored solutions to ensure reliable and relevant content moderation. Overall, the results advocate for the development of specialized models that can address the complexities of cultural context in toxicity detection, ultimately contributing to more effective content management. <div>
arXiv:2505.04640v1 Announce Type: new 
Abstract: This paper presents a comparative benchmark evaluating the performance of Typica.ai's custom Moroccan Darija toxicity detection model against major LLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral (mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We focus on culturally grounded toxic content, including implicit insults, sarcasm, and culturally specific aggression often overlooked by general-purpose systems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset, we report precision, recall, F1-score, and accuracy, offering insights into challenges and opportunities for moderation in underrepresented languages. Our results highlight Typica.ai's superior performance, underlining the importance of culturally adapted models for reliable content moderation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture</title>
<link>https://arxiv.org/abs/2505.04642</link>
<guid>https://arxiv.org/abs/2505.04642</guid>
<content:encoded><![CDATA[
<div> Multimodal, sentiment analysis, affective computing, deep learning, emotion classification  

<br /><br />Summary:  
This article addresses multimodal sentiment analysis, a significant aspect of affective computing, which aims to understand human emotions by integrating textual, audio, and visual cues. The authors highlight that many recent models employ complex attention mechanisms and hierarchical architectures; however, they introduce a lightweight deep learning model specifically designed for utterance-level emotion classification. Their research utilizes the IEMOCAP dataset, which comprises aligned text, audio-derived numeric features, and visual descriptors. A modality-specific encoder is developed using fully connected layers with dropout regularization to enhance performance. The representations derived from individual modalities are fused through simple concatenation and subsequently processed by a dense fusion layer to effectively capture cross-modal interactions. Notably, this architecture minimizes computational overhead while achieving impressive performance, recording a classification accuracy of 92% across six emotional categories. The findings suggest that with judicious feature engineering and a modular design, simpler fusion techniques can either outperform or match the effectiveness of more complex models, particularly in environments where resources are limited. <div>
arXiv:2505.04642v1 Announce Type: new 
Abstract: Multimodal sentiment analysis, a pivotal task in affective computing, seeks to understand human emotions by integrating cues from language, audio, and visual signals. While many recent approaches leverage complex attention mechanisms and hierarchical architectures, we propose a lightweight, yet effective fusion-based deep learning model tailored for utterance-level emotion classification. Using the benchmark IEMOCAP dataset, which includes aligned text, audio-derived numeric features, and visual descriptors, we design a modality-specific encoder using fully connected layers followed by dropout regularization. The modality-specific representations are then fused using simple concatenation and passed through a dense fusion layer to capture cross-modal interactions. This streamlined architecture avoids computational overhead while preserving performance, achieving a classification accuracy of 92% across six emotion categories. Our approach demonstrates that with careful feature engineering and modular design, simpler fusion strategies can outperform or match more complex models, particularly in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation</title>
<link>https://arxiv.org/abs/2505.04643</link>
<guid>https://arxiv.org/abs/2505.04643</guid>
<content:encoded><![CDATA[
<div> Keywords: population parameters, transformer encoder, survey sampling, hate crime statistics, manual annotation

<br /><br />Summary: The article addresses the challenge of estimating population parameters in finite populations of text documents, particularly when obtaining target variable labels demands manual annotation. To tackle this issue, the authors propose a method that combines predictions from a transformer encoder neural network with traditional survey sampling estimators, using the model predictions as auxiliary variables. This approach is applied to analyze hate crime statistics in Sweden, specifically drawing from Swedish police reports. The researchers derive estimates for the yearly incidence of hate crimes as well as the extent of under-reporting by the police. They utilize several statistical methods, including the Hansen-Hurwitz estimator, difference estimation, and stratified random sampling estimation to strengthen their findings. The results suggest that when labeled training data exists, their proposed method can yield efficient estimates, significantly reducing the reliance on time-consuming manual annotation processes. Overall, this innovative method shows promise in enhancing the accuracy and efficiency of population parameter estimation in the context of social issues like hate crimes. <div>
arXiv:2505.04643v1 Announce Type: new 
Abstract: Estimating population parameters in finite populations of text documents can be challenging when obtaining the labels for the target variable requires manual annotation. To address this problem, we combine predictions from a transformer encoder neural network with well-established survey sampling estimators using the model predictions as an auxiliary variable. The applicability is demonstrated in Swedish hate crime statistics based on Swedish police reports. Estimates of the yearly number of hate crimes and the police's under-reporting are derived using the Hansen-Hurwitz estimator, difference estimation, and stratified random sampling estimation. We conclude that if labeled training data is available, the proposed method can provide very efficient estimates with reduced time spent on manual annotation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT for automated grading of short answer questions in mechanical ventilation</title>
<link>https://arxiv.org/abs/2505.04645</link>
<guid>https://arxiv.org/abs/2505.04645</guid>
<content:encoded><![CDATA[
<div> Keywords: short answer questions, automated grading, ChatGPT, postgraduate education, grading rubric  

<br /><br />Summary: The study evaluates the use of ChatGPT 4o for grading short answer questions (SAQs) in postgraduate medical education, utilizing data from 215 students' responses in an online mechanical ventilation course. ChatGPT was tasked with grading three case-based scenarios using a standardized prompt and rubric. The analysis employed complex statistical methods, including mixed-effects modeling and intraclass correlation coefficients (ICCs). Results showed that ChatGPT assigned systematically lower grades than human graders, with an average bias of -1.34 on a 10-point scale. The agreement between ChatGPT and human graders was poor, evidenced by low ICC values (ICC1 = 0.086) and negative Cohen's kappa (-0.0786), indicating a lack of meaningful agreement. Variance component analysis found minimal variability across five ChatGPT grading sessions, suggesting internal consistency but also divergence from human assessment. The study highlighted that the lowest agreement occurred in evaluative and analytic items, whereas checklist items demonstrated less disagreement. Ultimately, over 60% of grades assigned by ChatGPT were beyond acceptable discrepancies when compared to human grades, prompting caution regarding the use of large language models in high-stakes postgraduate coursework grading. <div>
arXiv:2505.04645v1 Announce Type: new 
Abstract: Standardised tests using short answer questions (SAQs) are common in postgraduate education. Large language models (LLMs) simulate conversational language and interpret unstructured free-text responses in ways aligning with applying SAQ grading rubrics, making them attractive for automated grading. We evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data from 215 students (557 short-answer responses) enrolled in an online course on mechanical ventilation (2020--2024). Deidentified responses to three case-based scenarios were presented to ChatGPT with a standardised grading prompt and rubric. Outputs were analysed using mixed-effects modelling, variance component analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks than human graders with a mean difference (bias) of -1.34 on a 10-point scale. ICC values indicated poor individual-level agreement (ICC1 = 0.086), and Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component analysis showed minimal variability among the five ChatGPT sessions (G-value = 0.87), indicating internal consistency but divergence from the human grader. The poorest agreement was observed for evaluative and analytic items, whereas checklist and prescriptive rubric items had less disagreement. We caution against the use of LLMs in grading postgraduate coursework. Over 60% of ChatGPT-assigned grades differed from human grades by more than acceptable boundaries for high-stakes assessments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
<link>https://arxiv.org/abs/2505.04649</link>
<guid>https://arxiv.org/abs/2505.04649</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, FRAME, medical paper generation, iterative refinement, evaluation framework  

<br /><br />Summary: The paper introduces the Feedback-Refined Agent Methodology (FRAME), a new framework aimed at improving the automation of medical paper generation utilizing large language models (LLMs). FRAME addresses challenges in knowledge synthesis and quality assurance through three key innovations. First, it presents a structured dataset construction method that breaks down 4,287 medical papers into essential research components via iterative refinement. Second, the tripartite architecture of FRAME consists of Generator, Evaluator, and Reflector agents working in conjunction to enhance the quality of generated content through metric-driven feedback. Lastly, a comprehensive evaluation framework combines statistical metrics with human-grounded benchmarks to assess the quality of outputs. Experimental results reveal that FRAME significantly outperforms conventional methods, achieving an average gain of 9.91% with DeepSeek V3 and showing comparable improvements with GPT-4o Mini. Human evaluations indicate that the papers generated by FRAME meet quality standards comparable to those authored by humans, particularly excelling in synthesizing future research directions. Overall, FRAME establishes a solid foundation for automating medical research paper generation while upholding rigorous academic standards. <div>
arXiv:2505.04649v1 Announce Type: new 
Abstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions</title>
<link>https://arxiv.org/abs/2505.04651</link>
<guid>https://arxiv.org/abs/2505.04651</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hypothesis generation, validation, symbolic frameworks, human-AI collaboration  

<br /><br />Summary: The article discusses the transformative role of Large Language Models (LLMs) in scientific hypothesis generation and validation. It provides a structured overview of various LLM-driven approaches, such as symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. The survey highlights techniques like retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, addressing trade-offs among interpretability, novelty, and domain alignment. Additionally, it contrasts early symbolic discovery systems (e.g., BACON, KEKADA) with current LLM pipelines that utilize in-context learning and domain adaptation methods. For the validation of hypotheses, it reviews methods including simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing the importance of iterative assessment in open-world scenarios. The article maps datasets across various fields, including biomedicine, materials science, environmental science, and social science, while introducing new resources like AHTech and CSKG-600. Finally, it outlines a roadmap for future research focusing on novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, advocating for LLMs as agents of principled and scalable scientific discovery. <div>
arXiv:2505.04651v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming scientific hypothesis generation and validation by enabling information synthesis, latent relationship discovery, and reasoning augmentation. This survey provides a structured overview of LLM-driven approaches, including symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. We examine techniques such as retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, highlighting trade-offs in interpretability, novelty, and domain alignment. We contrast early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM pipelines that leverage in-context learning and domain adaptation via fine-tuning, retrieval, and symbolic grounding. For validation, we review simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing iterative assessment in open-world contexts. The survey maps datasets across biomedicine, materials science, environmental science, and social science, introducing new resources like AHTech and CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, positioning LLMs as agents for principled, scalable scientific discovery.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Conversational Diagnostic AI with Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.04653</link>
<guid>https://arxiv.org/abs/2505.04653</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AMIE, multimodal data, diagnostic accuracy, clinical consultation

<br /><br />Summary: Large Language Models (LLMs) have shown promise in facilitating diagnostic conversations, but evaluations so far have primarily focused on text-based interactions, neglecting the real-world demands of remote medical care. To address this, the Articulate Medical Intelligence Explorer (AMIE) has been enhanced to process and interpret multimodal medical data during consultations. Utilizing Gemini 2.0 Flash, AMIE employs a dynamic dialogue framework that adapts based on patient states and evolving diagnoses. The system strategically poses follow-up questions, emulating the structured history-taking of experienced clinicians. A randomized, blinded, OSCE-style study comparing AMIE to primary care physicians (PCPs) was conducted with 105 scenarios that included various medical artifacts like skin photos and ECGs. The evaluation assessed multiple axes, including multimodal capabilities, history-taking, diagnostic accuracy, and empathy. Results indicated that AMIE outperformed PCPs in 7 out of 9 multimodal and 29 out of 32 non-multimodal categories, including diagnostic accuracy. The findings suggest significant advancements in multimodal conversational diagnostic AI, although further research is needed for real-world application. <div>
arXiv:2505.04653v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient</title>
<link>https://arxiv.org/abs/2505.04654</link>
<guid>https://arxiv.org/abs/2505.04654</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, ethical analysis, Relative Danger Coefficient, human oversight  

<br /><br />Summary: This article discusses the rapid evolution of Artificial Intelligence (AI) and Large Language Models (LLMs) in natural language processing, highlighting their impressive capabilities. However, it also addresses significant ethical issues such as safety concerns, potential for misuse, discrimination, and broader societal impacts. The study provides a comparative analysis of the ethical performance of multiple AI models, including the latest DeepSeek-V3 (R1 with and without reasoning), various versions of GPT (4o, 3.5 Turbo, 4 Turbo, o1/o3 mini), and Gemini (1.5 flash, 2.0 flash, and 2.0 flash exp). It underscores the importance of maintaining robust human oversight, particularly in high-stakes scenarios where the ramifications of AI decisions can be profound. Furthermore, the paper introduces a novel metric for assessing harm in LLMs, termed the Relative Danger Coefficient (RDC), which aims to quantify the potential risks associated with different models. The findings advocate for a thoughtful approach to AI development and deployment, ensuring ethical considerations are a fundamental aspect of AI technologies. <div>
arXiv:2505.04654v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly evolved in recent years, showcasing remarkable capabilities in natural language understanding and generation. However, these advancements also raise critical ethical questions regarding safety, potential misuse, discrimination and overall societal impact. This article provides a comparative analysis of the ethical performance of various AI models, including the brand new DeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5 Turbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp) and highlights the need for robust human oversight, especially in situations with high stakes. Furthermore, we present a new metric for calculating harm in LLMs called Relative Danger Coefficient (RDC).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction</title>
<link>https://arxiv.org/abs/2505.04655</link>
<guid>https://arxiv.org/abs/2505.04655</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Determinants of Health, deep learning, Large Language Models, classification, healthcare

<br /><br />Summary: This study focuses on the extraction of Social Determinants of Health (SDoH), which are pivotal factors that influence individuals' health statuses. Recognizing the correlation between SDoH and wellness outcomes, the authors utilized both traditional deep learning methods and Large Language Models (LLMs) to assess their effectiveness in classifying SDoHs from clinical texts. Their approach resulted in a 10-point improvement over previous benchmarks in multilabel SDoH classification. Importantly, the researchers introduced a novel method that significantly speeds up the classification process—achieving a 12-fold reduction in execution time—by minimizing the reliance on expensive LLM processing. This innovative method combines the high precision capabilities of LLMs with the efficiency of traditional deep learning techniques. Moreover, the study demonstrates that traditional deep learning models can outperform LLMs, especially when trained on a dataset augmented with synthetic data. Ultimately, the findings present a more effective strategy for the automatic prediction of SDoHs, which is crucial for identifying and assisting at-risk patients. <div>
arXiv:2505.04655v1 Announce Type: new 
Abstract: Social Determinants of Health (SDoH) are economic, social and personal circumstances that affect or influence an individual's health status. SDoHs have shown to be correlated to wellness outcomes, and therefore, are useful to physicians in diagnosing diseases and in decision-making. In this work, we automatically extract SDoHs from clinical text using traditional deep learning and Large Language Models (LLMs) to find the advantages and disadvantages of each on an existing publicly available dataset. Our models outperform a previous reference point on a multilabel SDoH classification by 10 points, and we present a method and model to drastically speed up classification (12X execution time) by eliminating expensive LLM processing. The method we present combines a more nimble and efficient solution that leverages the power of the LLM for precision and traditional deep learning methods for efficiency. We also show highly performant results on a dataset supplemented with synthetic data and several traditional deep learning models that outperform LLMs. Our models and methods offer the next iteration of automatic prediction of SDoHs that impact at-risk patients.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection</title>
<link>https://arxiv.org/abs/2505.04660</link>
<guid>https://arxiv.org/abs/2505.04660</guid>
<content:encoded><![CDATA[
<div> Keywords: fall detection, synthetic data, Large Language Models, motion simulation, LSTM

<br /><br />Summary: The article addresses the challenge of training fall detection systems due to limited real-world fall data among the elderly. It investigates the use of Large Language Models (LLMs) to generate synthetic fall data. Various models, including text-to-motion (T2M, SATO, ParCo) and text-to-text (GPT4o, GPT4, Gemini), were evaluated for simulating realistic fall scenarios. Synthetic datasets created were then integrated with four real-world baseline datasets to analyze their impact on the performance of a Long Short-Term Memory (LSTM) model. The study also compares LLM-generated synthetic data with a diffusion-based method, assessing the alignment of each with actual accelerometer data distributions. Results reveal that dataset characteristics are crucial in determining the effectiveness of synthetic data, with LLM-generated datasets notably performing well in low-frequency settings (20Hz) but exhibiting instability at higher frequencies (200Hz). Furthermore, text-to-motion models tend to produce more realistic biomechanical data compared to text-to-text models, although their influence on fall detection varies. The diffusion-based synthetic data aligns closely with real data but does not consistently improve model performance. An ablation study underscores that sensor placement and fall representation are essential factors in the effectiveness of synthetic data. <div>
arXiv:2505.04660v1 Announce Type: new 
Abstract: Training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. To address this, we explore the potential of Large Language Models (LLMs) for generating synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall scenarios. We generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a Long Short-Term Memory (LSTM) model. Additionally, we compare LLM-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. Results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with LLM-generated data performing best in low-frequency settings (e.g., 20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. Diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. An ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. These findings provide insights into optimizing synthetic data generation for fall detection models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising</title>
<link>https://arxiv.org/abs/2505.04665</link>
<guid>https://arxiv.org/abs/2505.04665</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, personalized advertising, user privacy protection, BERT, data security

<br /><br />Summary: This paper explores the intersection of personalized advertising recommendations and user privacy in the context of large language models (LLMs). It begins by outlining the LLM principles, particularly the self-attention mechanism derived from the Transformer architecture, which enhances natural language understanding and generation. The study then integrates BERT (Bidirectional Encoder Representations from Transformers) with the attention mechanism to create an algorithmic model focused on personalized advertising recommendations while safeguarding user privacy. The research details the specific steps involved: data collection and preprocessing, feature construction, semantic embedding using LLMs, and ad recommendations tailored to user profiles. To address privacy concerns, it emphasizes local model training and data encryption methods that aim to prevent personal data leakage. The paper culminates in an experimental validation using real user data, demonstrating that BERT-based advertising strategies significantly enhance the click-through and conversion rates. Furthermore, the incorporated privacy protection mechanisms indicate a reduction in the risks associated with user data exposure, thereby aligning efficient advertising with privacy assurance in digital marketing. <div>
arXiv:2505.04665v1 Announce Type: new 
Abstract: Although large language models have demonstrated the potential for personalized advertising recommendations in experimental environments, in actual operations, how advertising recommendation systems can be combined with measures such as user privacy protection and data security is still an area worthy of in-depth discussion. To this end, this paper studies the personalized risks and regulatory strategies of large language models in digital advertising. This study first outlines the principles of Large Language Model (LLM), especially the self-attention mechanism based on the Transformer architecture, and how to enable the model to understand and generate natural language text. Then, the BERT (Bidirectional Encoder Representations from Transformers) model and the attention mechanism are combined to construct an algorithmic model for personalized advertising recommendations and user factor risk protection. The specific steps include: data collection and preprocessing, feature selection and construction, using large language models such as BERT for advertising semantic embedding, and ad recommendations based on user portraits. Then, local model training and data encryption are used to ensure the security of user privacy and avoid the leakage of personal data. This paper designs an experiment for personalized advertising recommendation based on a large language model of BERT and verifies it with real user data. The experimental results show that BERT-based advertising push can effectively improve the click-through rate and conversion rate of advertisements. At the same time, through local model training and privacy protection mechanisms, the risk of user privacy leakage can be reduced to a certain extent.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes</title>
<link>https://arxiv.org/abs/2505.04666</link>
<guid>https://arxiv.org/abs/2505.04666</guid>
<content:encoded><![CDATA[
<div> Keywords: building codes, Question-Answering, Retrieval-Augmented Generation, fine-tuning, Elasticsearch  

<br /><br />Summary: Building codes serve as essential regulations to ensure the design, construction, and safety of buildings, but they are often complex and challenging to navigate. This study proposes a solution through a Question-Answering (QA) system that utilizes a Retrieval-Augmented Generation (RAG) approach, comprising a retriever and a language model. The primary focus is on identifying an effective retriever method tailored for building codes while enhancing the generational capability of the language model via fine-tuning techniques. The research involved a thorough evaluation of various retrieval methods, particularly using the National Building Code of Canada (NBCC), and assessed the impact of fine-tuning on different language models with datasets derived from the NBCC. The findings revealed that Elasticsearch emerged as the most effective retriever, demonstrating robust performance. Furthermore, the study highlighted that fine-tuning language models on NBCC-specific data significantly improved their capacity to produce contextually relevant responses. By integrating a powerful retriever like Elasticsearch with fine-tuned language models, the study contributes to optimizing RAG systems, effectively addressing the complexities associated with navigating the NBCC. <div>
arXiv:2505.04666v1 Announce Type: new 
Abstract: Building codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards</title>
<link>https://arxiv.org/abs/2505.04671</link>
<guid>https://arxiv.org/abs/2505.04671</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Text-to-SQL, process reward models, reasoning chains, BIRD benchmark  

<br /><br />Summary: Recent advancements in large language models (LLMs) have notably enhanced performance on the Text-to-SQL task by utilizing their reasoning capabilities. To improve the accuracy of reasoning, external Process Reward Models (PRMs) can be integrated, although improper use may lead to incorrect SQL generation. This study proposes Reward-SQL, a systematic framework for effectively incorporating PRMs into Text-to-SQL reasoning. It adopts a "cold start, then PRM supervision" approach, starting with training the model to decompose SQL queries into structured reasoning chains using common table expressions (Chain-of-CTEs). This establishes a strong reasoning baseline. The study also examines four strategies for PRM integration, determining that combining PRMs as an online training signal with PRM-guided inference yields optimal results. Empirical results on the BIRD benchmark indicate that models leveraging a 7B PRM through Reward-SQL achieve a 13.1% performance improvement. Furthermore, the GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct reaches an accuracy of 68.9% on the BIRD development set, surpassing all baseline models of comparable size. This underscores the potential of reward-based supervision in enhancing Text-to-SQL reasoning. The code is made publicly available. <div>
arXiv:2505.04671v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation.To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a "cold start, then PRM supervision" paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning. Our code is publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM</title>
<link>https://arxiv.org/abs/2505.04673</link>
<guid>https://arxiv.org/abs/2505.04673</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Large Language Models, REVEAL Framework, harm assessment, multi-turn interactions, misinformation

<br /><br />Summary: Vision Large Language Models (VLLMs) represent a breakthrough in AI by merging image processing with text understanding, thereby improving user interaction and broadening applications. However, their complexity brings new safety and ethical issues, especially in multi-modal, multi-turn dialogues, for which existing evaluation frameworks are ineffective. To tackle this, the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework is introduced, offering a scalable and automated approach for assessing image-related harms in VLLMs. The framework encompasses automated image mining, synthetic adversarial data generation, and multi-turn conversational enhancements using crescendo attack strategies, alongside harm evaluations via metrics such as the Safety-Usability Index (SUI). Five leading VLLMs—GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral—were assessed across three harm categories: sexual harm, violence, and misinformation. Results indicated that multi-turn interactions significantly raised defect rates compared to single-turn settings, exposing vulnerabilities. GPT-4o showed the best overall performance, with misinformation identified as a critical concern needing stronger defenses. Llama-3.2 had the highest defect rate at 16.55%, while Qwen2-VL displayed the highest refusal rate at 19.1%. <div>
arXiv:2505.04673v1 Announce Type: new 
Abstract: Vision Large Language Models (VLLMs) represent a significant advancement in artificial intelligence by integrating image-processing capabilities with textual understanding, thereby enhancing user interactions and expanding application domains. However, their increased complexity introduces novel safety and ethical challenges, particularly in multi-modal and multi-turn conversations. Traditional safety evaluation frameworks, designed for text-based, single-turn interactions, are inadequate for addressing these complexities. To bridge this gap, we introduce the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated image mining, synthetic adversarial data generation, multi-turn conversational expansion using crescendo attack strategies, and comprehensive harm assessment through evaluators like GPT-4o.
  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual harm, violence, and misinformation. Our findings reveal that multi-turn interactions result in significantly higher defect rates compared to single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably, GPT-4o demonstrated the most balanced performance as measured by our Safety-Usability Index (SUI) followed closely by Pixtral. Additionally, misinformation emerged as a critical area requiring enhanced contextual defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \%$) while Qwen2-VL showed the highest MT refusal rate ($19.1 \%$).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols</title>
<link>https://arxiv.org/abs/2505.04678</link>
<guid>https://arxiv.org/abs/2505.04678</guid>
<content:encoded><![CDATA[
<div> Keywords: cuneiform, deep learning, Akkadian, translation, archaeology  

<br /><br />Summary: This paper introduces an automated method for identifying and interpreting cuneiform characters through advanced deep-learning algorithms. Five distinct models were trained on a comprehensive dataset of cuneiform symbols and evaluated on performance metrics such as accuracy and precision. Among these, two models exhibited exceptional performance levels. These models were tested using symbols from the Hammurabi law acquisition, specifically Hammurabi Law 1, effectively recognizing the corresponding Akkadian meanings and providing accurate English translations. The authors intend to explore ensemble and stacking methods in future work, aiming to enhance detection accuracy and reliability via hybrid architectures. Additionally, this research emphasizes the linguistic connections between Akkadian, an ancient Mesopotamian language, and Arabic, highlighting their historical and cultural links. Overall, the study showcases the potential of deep learning in deciphering ancient scripts, integrating computational linguistics with archaeological insights, thereby offering significant contributions to the understanding and preservation of human history. <div>
arXiv:2505.04678v1 Announce Type: new 
Abstract: This paper presents a thoroughly automated method for identifying and interpreting cuneiform characters via advanced deep-learning algorithms. Five distinct deep-learning models were trained on a comprehensive dataset of cuneiform characters and evaluated according to critical performance metrics, including accuracy and precision. Two models demonstrated outstanding performance and were subsequently assessed using cuneiform symbols from the Hammurabi law acquisition, notably Hammurabi Law 1. Each model effectively recognized the relevant Akkadian meanings of the symbols and delivered precise English translations. Future work will investigate ensemble and stacking approaches to optimize performance, utilizing hybrid architectures to improve detection accuracy and reliability. This research explores the linguistic relationships between Akkadian, an ancient Mesopotamian language, and Arabic, emphasizing their historical and cultural linkages. This study demonstrates the capability of deep learning to decipher ancient scripts by merging computational linguistics with archaeology, therefore providing significant insights for the comprehension and conservation of human history.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.04723</link>
<guid>https://arxiv.org/abs/2505.04723</guid>
<content:encoded><![CDATA[
<div> Keywords: domain-specific, large language models, Chinese SOAEs, continual pre-training, inference acceleration

<br /><br />Summary: This study addresses significant challenges in developing domain-specific large language models (LLMs) for Chinese state-owned assets and enterprises (SOAEs). The limitations identified include model capacity constraints, excessive reliance on domain-specific supervised fine-tuning (SFT) data, and inefficient inference acceleration. The authors propose the SOAEsV2-7B/72B model series, developed through a three-phase framework: Firstly, continual pre-training integrates domain knowledge while keeping base capabilities intact. Secondly, domain-progressive SFT employs a curriculum-based learning strategy, transitioning from weakly relevant conversational data to expert-annotated SOAE datasets. Lastly, distillation-enhanced speculative decoding accelerates inference via logit distillation between 72B target and 7B draft models, achieving a speedup of 1.39-1.52 times without compromising quality. Experimental results affirm that the domain-specific pre-training retains 99.8% of original capabilities while enhancing domain performance—showcasing a 1.08 times improvement in Rouge-1 scores and a 1.17 times enhancement in BLEU-4 scores. Further ablation studies revealed that domain-progressive SFT surpasses single-stage training, yielding improvements in both Rouge-1 and BLEU-4 scores. This work presents a comprehensive approach for optimizing SOAEs LLMs, effectively bridging general language capabilities with domain-specific expertise. <div>
arXiv:2505.04723v1 Announce Type: new 
Abstract: This study addresses key challenges in developing domain-specific large language models (LLMs) for Chinese state-owned assets and enterprises (SOAEs), where current approaches face three limitations: 1) constrained model capacity that limits knowledge integration and cross-task adaptability; 2) excessive reliance on domain-specific supervised fine-tuning (SFT) data, which neglects the broader applicability of general language patterns; and 3) inefficient inference acceleration for large models processing long contexts. In this work, we propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase framework: 1) continual pre-training integrates domain knowledge while retaining base capabilities; 2) domain-progressive SFT employs curriculum-based learning strategy, transitioning from weakly relevant conversational data to expert-annotated SOAEs datasets to optimize domain-specific tasks; 3) distillation-enhanced speculative decoding accelerates inference via logit distillation between 72B target and 7B draft models, achieving 1.39-1.52$\times$ speedup without quality loss. Experimental results demonstrate that our domain-specific pre-training phase maintains 99.8% of original general language capabilities while significantly improving domain performance, resulting in a 1.08$\times$ improvement in Rouge-1 score and a 1.17$\times$ enhancement in BLEU-4 score. Ablation studies further show that domain-progressive SFT outperforms single-stage training, achieving 1.02$\times$ improvement in Rouge-1 and 1.06$\times$ in BLEU-4. Our work introduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs, bridging the gap between general language capabilities and domain-specific expertise.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence</title>
<link>https://arxiv.org/abs/2505.04785</link>
<guid>https://arxiv.org/abs/2505.04785</guid>
<content:encoded><![CDATA[
<div> Keywords: Tang dynasty, Song dynasty, floral motifs, sentiment analysis, artistic representation

<br /><br />Summary: This study explores the intricate relationship between literary emotions and visual culture during the Tang (618-907) and Song (960-1279) dynasties, periods renowned for their rich cultural expression. It addresses a gap in scholarship that has largely viewed literary and artistic domains in isolation. By utilizing BERT-based sentiment analysis, the research quantifies emotional patterns related to floral imagery in classical poetry, particularly focusing on peony and plum blossom motifs. The study detects significant shifts in emotional connotations between the two dynasties. To validate these findings, the emotional patterns identified in poetry are cross-referenced with visual representations found in textiles, ceramics, and other forms of material culture. The approach integrates computational methodologies with traditional sinological analysis, highlighting previously unrecognized connections between literary expression and artistic representation. This comprehensive examination not only enhances our understanding of cultural dynamics during these historical periods but also illustrates the importance of interdisciplinary methods in the study of humanities. Overall, the research reveals the dynamic interplay between poetry and visual arts, contributing to a deeper appreciation of Chinese cultural heritage. <div>
arXiv:2505.04785v1 Announce Type: new 
Abstract: The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an extraordinary flourishing of Chinese cultural expression, where floral motifs served as a dynamic medium for both poetic sentiment and artistic design. While previous scholarship has examined these domains independently, the systematic correlation between evolving literary emotions and visual culture remains underexplored. This study addresses that gap by employing BERT-based sentiment analysis to quantify emotional patterns in floral imagery across Tang Song poetry, then validating these patterns against contemporaneous developments in decorative arts.Our approach builds upon recent advances in computational humanities while remaining grounded in traditional sinological methods. By applying a fine tuned BERT model to analyze peony and plum blossom imagery in classical poetry, we detect measurable shifts in emotional connotations between the Tang and Song periods. These textual patterns are then cross berenced with visual evidence from textiles, ceramics, and other material culture, revealing previously unrecognized synergies between literary expression and artistic representation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Osiris: A Lightweight Open-Source Hallucination Detection System</title>
<link>https://arxiv.org/abs/2505.04844</link>
<guid>https://arxiv.org/abs/2505.04844</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, hallucinations, multi-hop QA dataset, fine-tuning, RAGTruth

<br /><br />Summary: Retrieval-Augmented Generation (RAG) systems are increasingly popular for improving the factual accuracy of outputs from Large Language Models (LLMs). However, the problem of hallucinations—where the model generates responses that are not faithful to the provided context—poses challenges for deploying these systems in production. Current methods for detecting hallucinations rely on either human evaluation or closed-source models, which face scalability issues due to their high costs and slow inference speeds. This work introduces a perturbed multi-hop question-answering dataset that induces hallucinations to facilitate research in this area. The authors present a supervised fine-tuning approach using their dataset, achieving superior recall with a 7B parameter model compared to the performance of GPT-4o on the RAGTruth hallucination detection benchmark. Additionally, their model competes well in terms of precision and accuracy while using significantly fewer parameters. This advancement demonstrates a promising solution to the challenges of hallucination detection in RAG systems, making it more feasible for practical applications. The code related to this research has been made publicly available in their repository. <div>
arXiv:2505.04844v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems have gained widespread adoption by application builders because they leverage sources of truth to enable Large Language Models (LLMs) to generate more factually sound responses. However, hallucinations, instances of LLM responses that are unfaithful to the provided context, often prevent these systems from being deployed in production environments. Current hallucination detection methods typically involve human evaluation or the use of closed-source models to review RAG system outputs for hallucinations. Both human evaluators and closed-source models suffer from scaling issues due to their high costs and slow inference speeds. In this work, we introduce a perturbed multi-hop QA dataset with induced hallucinations. Via supervised fine-tuning on our dataset, we achieve better recall with a 7B model than GPT-4o on the RAGTruth hallucination detection benchmark and offer competitive performance on precision and accuracy, all while using a fraction of the parameters. Code is released at our repository.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards</title>
<link>https://arxiv.org/abs/2505.04847</link>
<guid>https://arxiv.org/abs/2505.04847</guid>
<content:encoded><![CDATA[
<div> Keywords: hallucinations, LLMs, summarization, FaithJudge, evaluation

<br /><br />Summary: Hallucinations remain a significant issue for Large Language Models (LLMs), particularly in tasks involving summarization. While Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations by providing contextual grounding, LLMs still often produce unsupported information or contradictions. This paper focuses on measuring the prevalence of hallucinations in various LLMs during summarization tasks, analyzing their performance through the existing Hughes Hallucination Evaluation Model (HHEM) and Vectara's Hallucination Leaderboard. Despite generating interest, the authors identify challenges with HHEM and current methods of hallucination detection, evaluating their effectiveness with existing datasets. To address these limitations, the authors propose FaithJudge, an innovative LLM-as-a-judge framework that uses few-shot human annotations to significantly enhance automated hallucination evaluation. The introduction of FaithJudge leads to the development of an improved hallucination leaderboard, alongside the existing Vectara leaderboard, facilitating more dependable benchmarking of LLMs with respect to hallucinations in RAG settings. This research contributes to the ongoing discourse surrounding LLM performance and the persistent challenge of hallucinations in AI-generated responses, presenting new frameworks for better assessment and understanding. <div>
arXiv:2505.04847v1 Announce Type: new 
Abstract: Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce hallucinations by grounding responses in contexts. However, even when provided context, LLMs still frequently introduce unsupported information or contradictions. This paper presents our efforts to measure LLM hallucinations with a focus on summarization tasks, assessing how often various LLMs introduce hallucinations when summarizing documents. We discuss Vectara's existing LLM hallucination leaderboard, based on the Hughes Hallucination Evaluation Model (HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great research interest, we examine challenges faced by HHEM and current hallucination detection methods by analyzing the effectiveness of these methods on existing hallucination datasets. To address these limitations, we propose FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination annotations, which substantially improves automated LLM hallucination evaluation over current methods. We introduce an enhanced hallucination leaderboard centered on FaithJudge, alongside our current hallucination leaderboard, enabling more reliable benchmarking of LLMs for hallucinations in RAG.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education</title>
<link>https://arxiv.org/abs/2505.04916</link>
<guid>https://arxiv.org/abs/2505.04916</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, educational tools, embedding models, question answering, semantic retrieval

<br /><br />Summary: This study investigates the need for better semantic retrieval systems tailored to academic content, focusing on educational tools enhanced by AI. It introduces two open-source embedding models specifically fine-tuned for answering questions related to course syllabi. To create a synthetic dataset, researchers compiled 3,197 sentence pairs that included synonymous terms, paraphrased questions, and implicit/explicit mappings using manual curation and large language model assistance. Two training strategies were assessed: (1) a baseline model trained with MultipleNegativesRankingLoss (MNRL) and (2) a dual-loss model that integrates MNRL with CosineSimilarityLoss to enhance semantic ranking and similarity calibration. The models were evaluated on 28 university course syllabi based on natural language questions regarding course, faculty, and teaching assistant details. The findings indicate that both models surpass established open-source benchmarks, such as all-MiniLM-L6-v2, with the dual-loss model significantly closing the gap with high-performance proprietary embeddings. This research contributes domain-specific embedding models and establishes a replicable framework for educational semantic retrieval, facilitating applications like academic chatbots, retrieval-augmented generation (RAG) systems, and learning management system (LMS) integrations. <div>
arXiv:2505.04916v1 Announce Type: new 
Abstract: Recent advances in AI have catalyzed the adoption of intelligent educational tools, yet many semantic retrieval systems remain ill-suited to the unique linguistic and structural characteristics of academic content. This study presents two open-source embedding models fine-tuned for educational question answering, particularly in the context of course syllabi. A synthetic dataset of 3,197 sentence pairs, spanning synonymous terminology, paraphrased questions, and implicit-explicit mappings, was constructed through a combination of manual curation and large language model (LLM)-assisted generation. Two training strategies were evaluated: (1) a baseline model fine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model that combines MNRL with CosineSimilarityLoss to improve both semantic ranking and similarity calibration. Evaluations were conducted on 28 university course syllabi using a fixed set of natural language questions categorized into course, faculty, and teaching assistant information. Results demonstrate that both fine-tuned models outperform strong open-source baselines, including all-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model narrows the performance gap with high-performing proprietary embeddings such as OpenAI's text-embedding-3 series. This work contributes reusable, domain-aligned embedding models and provides a replicable framework for educational semantic retrieval, supporting downstream applications such as academic chatbots, retrieval-augmented generation (RAG) systems, and learning management system (LMS) integrations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought Tokens are Computer Program Variables</title>
<link>https://arxiv.org/abs/2505.04955</link>
<guid>https://arxiv.org/abs/2505.04955</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought, large language models, intermediate results, multi-digit multiplication, dynamic programming 

<br /><br />Summary: The paper investigates the role of Chain-of-Thought (CoT) tokens in large language models (LLMs) for complex reasoning tasks, focusing on multi-digit multiplication and dynamic programming. It establishes that while CoT is crucial for problem solving, preserving only tokens that store intermediate results can yield similar performance outcomes. The research further reveals that representing intermediate results in different latent forms does not adversely impact the model's effectiveness. An interesting aspect of the study involves random interventions in CoT values, which demonstrate that changes propagate through subsequent CoT tokens and affect the final answer. These observations imply that CoT tokens might behave analogously to variables used in computer programs. However, the study also highlights potential issues, such as the risk of unintended shortcuts and the computational complexity limitations present among the tokens. By providing empirical evidence on these aspects, this paper sheds light on the functioning of CoT tokens and enhances the understanding of their utility and limitations in reasoning tasks. The associated code and data are made available at the provided GitHub link. <div>
arXiv:2505.04955v1 Announce Type: new 
Abstract: Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically study the role of CoT tokens in LLMs on two compositional tasks: multi-digit multiplication and dynamic programming. While CoT is essential for solving these problems, we find that preserving only tokens that store intermediate results would achieve comparable performance. Furthermore, we observe that storing intermediate results in an alternative latent form will not affect model performance. We also randomly intervene some values in CoT, and notice that subsequent CoT tokens and the final answer would change correspondingly. These findings suggest that CoT tokens may function like variables in computer programs but with potential drawbacks like unintended shortcuts and computational complexity limits between tokens. The code and data are available at https://github.com/solitaryzero/CoTs_are_Variables.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Relationship between the Power Law and Hierarchical Structures</title>
<link>https://arxiv.org/abs/2505.04984</link>
<guid>https://arxiv.org/abs/2505.04984</guid>
<content:encoded><![CDATA[
<div> Keywords: power laws, statistical analysis, syntactic structures, parse trees, hierarchical structures  

<br /><br />Summary: This study investigates the statistical analysis of language corpora, particularly focusing on the emergence of power laws that suggest universal principles in natural languages. Previous interpretations of power-law decay have indicated hierarchical structures in various aspects of language, including syntax, semantics, and discourse. The study extends this inquiry to child languages and animal signals but notes a lack of empirical testing to support the interpretations made. To address this gap, the authors aim to examine the relationship between power laws and syntactic structures by analyzing properties of parse trees derived from English corpora. They assess mutual information, probabilities, and deviations from probabilistic context-free grammars (PCFGs) within these trees and their approximating PCFGs. The findings reveal that the fundamental assumptions underlying the argument for hierarchical structures do not adequately align with the observed properties of syntactic structures. Consequently, the study posits that the proposed connections between power laws and hierarchical structures are questionable, calling into question their applicability to child languages and animal signals and emphasizing the need for a reevaluation of this relationship. <div>
arXiv:2505.04984v1 Announce Type: new 
Abstract: Statistical analysis of corpora provides an approach to quantitatively investigate natural languages. This approach has revealed that several power laws consistently emerge across different corpora and languages, suggesting the universal principles underlying languages. Particularly, the power-law decay of correlation has been interpreted as evidence for underlying hierarchical structures in syntax, semantics, and discourse. This perspective has also been extended to child languages and animal signals. However, the argument supporting this interpretation has not been empirically tested. To address this problem, this study examines the validity of the argument for syntactic structures. Specifically, we test whether the statistical properties of parse trees align with the implicit assumptions in the argument. Using English corpora, we analyze the mutual information, deviations from probabilistic context-free grammars (PCFGs), and other properties in parse trees, as well as in the PCFG that approximates these trees. Our results indicate that the assumptions do not hold for syntactic structures and that it is difficult to apply the proposed argument to child languages and animal signals, highlighting the need to reconsider the relationship between the power law and hierarchical structures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes</title>
<link>https://arxiv.org/abs/2505.04993</link>
<guid>https://arxiv.org/abs/2505.04993</guid>
<content:encoded><![CDATA[
<div> Keywords: Latent Preference Coding, large language models, human alignment, preference modeling, alignment algorithms

<br /><br />Summary:  
Large language models (LLMs) have made significant strides, yet aligning their outputs with human preferences poses a substantial challenge. Current methods for modeling preferences typically depend on pre-defined or implicit reward functions, which may not capture the complex and sometimes conflicting nature of human preferences across various tasks and demographics. To address this shortcoming, the authors present Latent Preference Coding (LPC), an innovative framework that utilizes discrete latent codes to model the implicit factors and their interactions behind holistic preferences. LPC integrates with various offline alignment strategies, autonomously inferring fundamental factors and their significance from data, without the need for predefined reward functions or manual weight adjustments. Comprehensive experiments indicate that LPC enhances the performance of three alignment algorithms—DPO, SimPO, and IPO—across three base models: Mistral-7B, Llama3-8B, and Llama3-8B-Instruct. In-depth analyses reveal that the latent codes effectively represent the distribution of human preferences, improving alignment robustness against data noise. By offering a cohesive representation of diverse preference factors, LPC advances the development of more reliable and adaptable alignment techniques, supporting the responsible use of advanced LLMs. <div>
arXiv:2505.04993v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for the responsible deployment of powerful LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Invariance in In-context Learning</title>
<link>https://arxiv.org/abs/2505.04994</link>
<guid>https://arxiv.org/abs/2505.04994</guid>
<content:encoded><![CDATA[
<div> Keywords: In-Context Learning, permutation invariance, information non-leakage, context interdependence, InvICL

<br /><br />Summary: 
In-Context Learning (ICL) is a crucial aspect of auto-regressive large language models but is significantly affected by the ordering of context examples, despite their independence. To tackle this challenge, recent studies have brought forth several ICL variants that aim for permutation invariance, though many fail to match the performance of the traditional auto-regressive ICL. This paper highlights two fundamental elements essential for crafting an invariant ICL algorithm: information non-leakage and context interdependence, which have not been concurrently achieved by existing methodologies. To address these gaps, the authors propose a new approach called Invariant ICL (InvICL), specifically designed to ensure invariance while preserving both aforementioned properties. Through empirical evaluations, InvICL demonstrates superior performance compared to both invariant and non-invariant models across various benchmark datasets, effectively showcasing enhanced generalization abilities with differing input lengths. Additionally, the code for InvICL is made freely available for use, encouraging further research and application in the field. <div>
arXiv:2505.04994v1 Announce Type: new 
Abstract: In-Context Learning (ICL) has emerged as a pivotal capability of auto-regressive large language models, yet it is hindered by a notable sensitivity to the ordering of context examples regardless of their mutual independence. To address this issue, recent studies have introduced several variant algorithms of ICL that achieve permutation invariance. However, many of these do not exhibit comparable performance with the standard auto-regressive ICL algorithm. In this work, we identify two crucial elements in the design of an invariant ICL algorithm: information non-leakage and context interdependence, which are not simultaneously achieved by any of the existing methods. These investigations lead us to the proposed Invariant ICL (InvICL), a methodology designed to achieve invariance in ICL while ensuring the two properties. Empirically, our findings reveal that InvICL surpasses previous models, both invariant and non-invariant, in most benchmark datasets, showcasing superior generalization capabilities across varying input lengths. Code is available at https://github.com/PKU-ML/InvICL.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations</title>
<link>https://arxiv.org/abs/2505.05016</link>
<guid>https://arxiv.org/abs/2505.05016</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Group Recommender Systems, In-Context Learning, group complexity, accuracy

<br /><br />Summary: This paper explores the application of Large Language Models (LLMs) in Group Recommender Systems (GRS), which aggregate preferences from multiple individuals. It examines conditions under which LLMs can accurately perform social choice-based aggregation strategies via zero-shot learning. The research focuses on how group complexity—defined by the number of users and items—affects LLM performance. Findings indicate that performance declines when the number of ratings exceeds 100. However, the sensitivity to group complexity varies among different LLMs. The study highlights the effectiveness of In-Context Learning (ICL) in improving performance for complex group scenarios, while other prompting modifications like domain cues or request for explanations did not enhance accuracy. Additionally, varying the formatting of group preferences—whether by rating lists per user or per item—also impacts accuracy. The conclusion advocates for incorporating group complexity into GRS evaluations due to its significant influence on LLM performance. It emphasizes that smaller LLMs can generate effective group recommendations under specific conditions, supporting the use of less computationally demanding models that can reduce costs. <div>
arXiv:2505.05016v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly applied in recommender systems aimed at both individuals and groups. Previously, Group Recommender Systems (GRS) often used social choice-based aggregation strategies to derive a single recommendation based on the preferences of multiple people. In this paper, we investigate under which conditions language models can perform these strategies correctly based on zero-shot learning and analyse whether the formatting of the group scenario in the prompt affects accuracy. We specifically focused on the impact of group complexity (number of users and items), different LLMs, different prompting conditions, including In-Context learning or generating explanations, and the formatting of group preferences. Our results show that performance starts to deteriorate when considering more than 100 ratings. However, not all language models were equally sensitive to growing group complexity. Additionally, we showed that In-Context Learning (ICL) can significantly increase the performance at higher degrees of group complexity, while adding other prompt modifications, specifying domain cues or prompting for explanations, did not impact accuracy. We conclude that future research should include group complexity as a factor in GRS evaluation due to its effect on LLM performance. Furthermore, we showed that formatting the group scenarios differently, such as rating lists per user or per item, affected accuracy. All in all, our study implies that smaller LLMs are capable of generating group recommendations under the right conditions, making the case for using smaller models that require less computing power and costs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization</title>
<link>https://arxiv.org/abs/2505.05017</link>
<guid>https://arxiv.org/abs/2505.05017</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, influence functions, fine-tuning, EK-FAC, interpretive power  

<br /><br />Summary:  
The paper addresses the challenge of attributing the predictions of fine-tuned large language models (LLMs) to their pre-training data, utilizing a novel approach known as multi-stage influence functions. Existing methods for determining influence fail to effectively handle multi-stage processes and are not scalable to the size of billion-parameter LLMs. This study introduces the multi-stage influence function specifically designed for full-parameter fine-tuning in LLMs. To improve efficiency and scalability, the authors implement an Eigenvalue-corrected Kronecker-Factored (EK-FAC) parameterization, which serves as an effective approximation method. Empirical results demonstrate the robust scalability of the EK-FAC approximation and the practical utility of the multi-stage influence function. The researchers also present case studies involving a real-world LLM, dolly-v2-3b, illustrating the interpretive capabilities of their proposed approach. These case studies include examples that showcase the insights gained from multi-stage influence estimates, affirming the method’s potential to enhance understanding and transparency in LLM predictions. The code developed for this work is publicly accessible, providing a resource for further exploration and application of the multi-stage influence function. <div>
arXiv:2505.05017v1 Announce Type: new 
Abstract: Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to downstream tasks. Since the majority of knowledge is acquired during pre-training, attributing the predictions of fine-tuned LLMs to their pre-training data may provide valuable insights. Influence functions have been proposed as a means to explain model predictions based on training data. However, existing approaches fail to compute ``multi-stage'' influence and lack scalability to billion-scale LLMs.
  In this paper, we propose the multi-stage influence function to attribute the downstream predictions of fine-tuned LLMs to pre-training data under the full-parameter fine-tuning paradigm. To enhance the efficiency and practicality of our multi-stage influence function, we leverage Eigenvalue-corrected Kronecker-Factored (EK-FAC) parameterization for efficient approximation. Empirical results validate the superior scalability of EK-FAC approximation and the effectiveness of our multi-stage influence function. Additionally, case studies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power, with exemplars illustrating insights provided by multi-stage influence estimates. Our code is public at https://github.com/colored-dye/multi_stage_influence_function.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
<div> Keywords: UI design, persuasiveness, benchmark, G-FOCUS, evaluation

<br /><br />Summary:  
Evaluating user interface (UI) design effectiveness impacts user behavior and is essential for Design Persuasiveness. A/B testing remains the standard approach for assessing UI variations that enhance user engagement, although it is often expensive and time-consuming. Recent advances in Vision-Language Models (VLMs) allow for automated UI analysis, yet current methodologies tend to focus on isolated design features rather than the comparative persuasiveness that is crucial for optimal user interactions. To tackle this issue, we introduce WiserUI-Bench, a benchmark specifically designed for the Pairwise UI Design Persuasiveness Assessment task. It includes 300 real-world UI image pairs, each annotated with A/B test results and expert rationales. Furthermore, we introduce G-FOCUS, a novel strategy for inference-time reasoning that enhances the persuasiveness assessment by minimizing position bias and improving evaluation accuracy. Experimental findings indicate that G-FOCUS outperforms existing inference strategies in terms of consistency and accuracy for pairwise UI evaluations. By advocating for VLM-driven evaluation of UI persuasiveness, this work aims to supplement traditional A/B testing and advance scalable UI preference modeling and design optimization. Code and data will be made publicly available. <div>
arXiv:2505.05026v1 Announce Type: new 
Abstract: Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Text Relation Prediction for Multilingual Tweets</title>
<link>https://arxiv.org/abs/2505.05040</link>
<guid>https://arxiv.org/abs/2505.05040</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, vision-language models, image-text relation, benchmark dataset, multilingual

<br /><br />Summary: This work investigates the relationship between images and text in social networks, particularly focusing on multilingual contexts. It highlights the prevalence of media uploads on platforms like Twitter over the past decade while noting the ambiguity often present regarding how images relate to their accompanying text. The authors delve into how recent multilingual vision-language models address the task of predicting these image-text relations across different languages. To facilitate their research, they construct a balanced benchmark dataset consisting of Twitter posts in Latvian, meticulously translated into English. Their findings reveal that contemporary vision-language model checkpoints are increasingly successful in predicting these relations, demonstrating marked advancements over previous models. However, despite this progress, the study also identifies considerable opportunities for enhancement, suggesting that further development in this area is necessary. By providing comparative results with existing literature, the authors emphasize the ongoing evolution and potential of vision-language models within the domain of social media analysis. In conclusion, the research presents a significant step towards understanding the complexities of image-text relationships in a multilingual framework, while paving the way for future explorations and improvements in model performance. <div>
arXiv:2505.05040v1 Announce Type: new 
Abstract: Various social networks have been allowing media uploads for over a decade now. Still, it has not always been clear what is their relation with the posted text or even if there is any at all. In this work, we explore how multilingual vision-language models tackle the task of image-text relation prediction in different languages, and construct a dedicated balanced benchmark data set from Twitter posts in Latvian along with their manual translations into English. We compare our results to previous work and show that the more recently released vision-language model checkpoints are becoming increasingly capable at this task, but there is still much room for further improvement.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations</title>
<link>https://arxiv.org/abs/2505.05056</link>
<guid>https://arxiv.org/abs/2505.05056</guid>
<content:encoded><![CDATA[
<div> Keywords: Teochew dialect, speech corpus, ASR, TTS, low-resource language  

<br /><br />Summary: This paper introduces the Teochew-Wild, a newly constructed speech corpus for the Teochew dialect. It consists of 18.9 hours of recorded speech data from various speakers, encompassing both formal and colloquial expressions. Each segment of the corpus is accompanied by precise orthographic and pinyin annotations, enhancing its utility for linguistic research. A unique aspect of this corpus is that it is the first publicly available dataset for the Teochew language with accurate orthographic annotations, making it a significant resource for the study of this low-resource language. Furthermore, the authors provide supplementary text processing tools and resources, which aim to advance research and practical applications in various speech-related tasks, particularly automatic speech recognition (ASR) and text-to-speech (TTS). The paper also includes experimental validation of the corpus, showcasing its effectiveness in performing ASR and TTS tasks. Overall, this work represents a substantial contribution to the field, facilitating further exploration and development in Teochew language technology and offering a foundation for future studies and applications. <div>
arXiv:2505.05056v1 Announce Type: new 
Abstract: This paper reports the construction of the Teochew-Wild, a speech corpus of the Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew speech data from multiple speakers, covering both formal and colloquial expressions, with precise orthographic and pinyin annotations. Additionally, we provide supplementary text processing tools and resources to propel research and applications in speech tasks for this low-resource language, such as automatic speech recognition (ASR) and text-to-speech (TTS). To the best of our knowledge, this is the first publicly available Teochew dataset with accurate orthographic annotations. We conduct experiments on the corpus, and the results validate its effectiveness in ASR and TTS tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization</title>
<link>https://arxiv.org/abs/2505.05070</link>
<guid>https://arxiv.org/abs/2505.05070</guid>
<content:encoded><![CDATA[
<div> Keywords: Consumer Health Queries, Bengali, large language models, summarization, low-resource languages

<br /><br />Summary: This study examines the performance of nine advanced large language models (LLMs) in summarizing Consumer Health Queries (CHQs) in Bengali, a low-resource language. The presence of extraneous details in CHQs complicates efficient medical responses. The analysis utilized the BanglaCHQ-Summ dataset, which contains 2,350 annotated query-summary pairs, to benchmark the LLMs using ROUGE metrics. Among the models assessed—GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet, Llama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro, Qwen2-72b-Instruct, Gemma-2-27b, and Athene-70B—Mixtral-8x22b-Instruct delivered the best performance in both ROUGE-1 and ROUGE-L metrics. In contrast, Bangla T5, a fine-tuned state-of-the-art model, outperformed others in ROUGE-2. The study reveals that zero-shot LLMs can achieve high-quality summarization comparable to fine-tuned models, as they perform effectively without task-specific training. This research highlights the capacity of LLMs to tackle challenges faced in low-resource languages, indicating their potential to provide scalable solutions for healthcare query summarization in Bengali. <div>
arXiv:2505.05070v1 Announce Type: new 
Abstract: Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language, often contain extraneous details, complicating efficient medical responses. This study investigates the zero-shot performance of nine advanced large language models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet, Llama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro, Qwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs. Using the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary pairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a fine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top performing model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2. The results demonstrate that zero-shot LLMs can rival fine-tuned models, achieving high-quality summaries even without task-specific training. This work underscores the potential of LLMs in addressing challenges in low-resource languages, providing scalable solutions for healthcare query summarization.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction</title>
<link>https://arxiv.org/abs/2505.05084</link>
<guid>https://arxiv.org/abs/2505.05084</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, detection accuracy, false positive rates, Conformal Prediction, Zero-Shot Machine-Generated Text Detection

<br /><br />Summary: The emergence of large language models has intensified concerns about their misuse by malicious actors, prompting a need for effective detection methods. While current detection strategies prioritize accuracy, they often overlook the societal implications of high false positive rates (FPRs). This paper introduces an innovative approach by leveraging Conformal Prediction (CP) to effectively constrain FPRs. However, using CP directly compromises detection performance. To address this challenge, the authors propose a Zero-Shot Machine-Generated Text Detection Framework utilizing Multiscaled Conformal Prediction (MCP), which simultaneously enforces FPR limits and enhances detection capabilities. Additionally, the study presents RealDet, a comprehensive dataset that covers diverse domains, facilitating realistic calibration and improved detection outcomes when integrated with MCP. Empirical findings demonstrate that MCP not only effectively bounds FPRs but also significantly boosts detection performance and robustness against adversarial attacks, making it applicable across various detectors and datasets. This research emphasizes the importance of balancing detection accuracy with societal safety considerations in the deployment of language models. <div>
arXiv:2505.05084v1 Announce Type: new 
Abstract: The rapid advancement of large language models has raised significant concerns regarding their potential misuse by malicious actors. As a result, developing effective detectors to mitigate these risks has become a critical priority. However, most existing detection methods focus excessively on detection accuracy, often neglecting the societal risks posed by high false positive rates (FPRs). This paper addresses this issue by leveraging Conformal Prediction (CP), which effectively constrains the upper bound of FPRs. While directly applying CP constrains FPRs, it also leads to a significant reduction in detection performance. To overcome this trade-off, this paper proposes a Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction (MCP), which both enforces the FPR constraint and improves detection performance. This paper also introduces RealDet, a high-quality dataset that spans a wide range of domains, ensuring realistic calibration and enabling superior detection performance when combined with MCP. Empirical evaluations demonstrate that MCP effectively constrains FPRs, significantly enhances detection performance, and increases robustness against adversarial attacks across multiple detectors and datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.05111</link>
<guid>https://arxiv.org/abs/2505.05111</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual, Sparse Autoencoders, language features, LLMs, steering vectors  

<br /><br />Summary: The study investigates multilingual capabilities in Large Language Models (LLMs) using Sparse Autoencoders (SAEs) for a more precise analysis. Traditional neuron-based or internal-activation methods struggle with issues like superposition and layer-wise variance, limiting their effectiveness. By employing SAEs, the researchers can decompose LLM activations into sparse linear combinations of features, leading to the introduction of a new metric to evaluate the monolinguality of these features. Their findings reveal that certain SAE features are strongly associated with specific languages. Interestingly, ablating these features impacts LLM performance significantly in one language while leaving others relatively unaffected. Additionally, some languages benefit from multiple synergistic SAE features, suggesting that joint ablation results in greater performance improvement compared to individual ablations. By harnessing these language-specific features derived from SAEs, the researchers successfully enhance steering vectors, allowing for more controlled generation of language in LLMs. This work highlights the potential of utilizing SAEs for better understanding and manipulation of multilingual capabilities in LLMs, ultimately contributing to advancements in language processing technologies. <div>
arXiv:2505.05111v1 Announce Type: new 
Abstract: The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition</title>
<link>https://arxiv.org/abs/2505.05148</link>
<guid>https://arxiv.org/abs/2505.05148</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Named Entity Recognition, Urdu, Twitter2015-Urdu, U-MNER framework, low-resource languages

<br /><br />Summary: The study highlights the growing importance of Multimodal Named Entity Recognition (MNER) in the context of social media, particularly for low-resource languages like Urdu. Despite advancements in MNER for high-resource languages, challenges remain due to a lack of annotated multimodal datasets and standardized baselines for low-resource languages. To tackle these issues, the authors introduce the U-MNER framework and the Twitter2015-Urdu dataset, which is the first of its kind for Urdu MNER, adapted from the popular Twitter2015 dataset with Urdu-specific grammar annotations. The research establishes benchmark baselines by evaluating both text-based and multimodal models on this dataset, facilitating comparative analyses for future Urdu MNER research. The U-MNER framework employs Urdu-BERT for text embeddings and ResNet for visual feature extraction while incorporating a Cross-Modal Fusion Module to effectively integrate text and visual information. The model demonstrates state-of-the-art performance on the Twitter2015-Urdu dataset, paving the way for further research in MNER for low-resource languages. <div>
arXiv:2505.05148v1 Announce Type: new 
Abstract: The emergence of multimodal content, particularly text and images on social media, has positioned Multimodal Named Entity Recognition (MNER) as an increasingly important area of research within Natural Language Processing. Despite progress in high-resource languages such as English, MNER remains underexplored for low-resource languages like Urdu. The primary challenges include the scarcity of annotated multimodal datasets and the lack of standardized baselines. To address these challenges, we introduce the U-MNER framework and release the Twitter2015-Urdu dataset, a pioneering resource for Urdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated with Urdu-specific grammar rules. We establish benchmark baselines by evaluating both text-based and multimodal models on this dataset, providing comparative analyses to support future research on Urdu MNER. The U-MNER framework integrates textual and visual context using Urdu-BERT for text embeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion Module to align and fuse information. Our model achieves state-of-the-art performance on the Twitter2015-Urdu dataset, laying the groundwork for further MNER research in low-resource languages.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation</title>
<link>https://arxiv.org/abs/2505.05225</link>
<guid>https://arxiv.org/abs/2505.05225</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese LLMs, QualBench, domain-specific evaluation, vertical domains, knowledge enhancement  

<br /><br />Summary: The development of large language models (LLMs) in China has led to the necessity for tailored evaluations that address domain-specific applications. Current benchmarks are often inadequate in representing vertical domains and providing relevant insights for the Chinese context. To address this, the authors introduce QualBench, a pioneering multi-domain Chinese question-answering benchmark designed for localized assessment of Chinese LLMs. QualBench encompasses over 17,000 questions from six vertical domains, drawing from 24 Chinese qualifications aligned with national policies and work standards. Evaluation results demonstrate that the Qwen2.5 model outperforms the more advanced GPT-4o, with Chinese LLMs consistently exceeding their non-Chinese counterparts, emphasizing the significance of localized domain knowledge. The leading performance score of 75.26% indicates notable gaps in current model capabilities concerning domain coverage. Additionally, the article discusses the challenges faced by LLMs in collaboration with crowdsourcing and emphasizes opportunities for enhancing knowledge through multi-domain retrieval-augmented generation (RAG) and vertical domain LLM training via Federated Learning. This research underscores the critical need for specialized benchmarks in ensuring the effectiveness of Chinese LLMs. <div>
arXiv:2505.05225v1 Announce Type: new 
Abstract: The rapid advancement of Chinese large language models (LLMs) underscores the need for domain-specific evaluations to ensure reliable applications. However, existing benchmarks often lack coverage in vertical domains and offer limited insights into the Chinese working context. Leveraging qualification exams as a unified framework for human expertise evaluation, we introduce QualBench, the first multi-domain Chinese QA benchmark dedicated to localized assessment of Chinese LLMs. The dataset includes over 17,000 questions across six vertical domains, with data selections grounded in 24 Chinese qualifications to closely align with national policies and working standards. Through comprehensive evaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with Chinese LLMs consistently surpassing non-Chinese models, highlighting the importance of localized domain knowledge in meeting qualification requirements. The best performance of 75.26% reveals the current gaps in domain coverage within model capabilities. Furthermore, we present the failure of LLM collaboration with crowdsourcing mechanisms and suggest the opportunities for multi-domain RAG knowledge enhancement and vertical domain LLM training with Federated Learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction</title>
<link>https://arxiv.org/abs/2505.05271</link>
<guid>https://arxiv.org/abs/2505.05271</guid>
<content:encoded><![CDATA[
<div> Keywords: Aspect sentiment triplet extraction, table tagging method, transformer layers, stripe attention, computational costs  

<br /><br />Summary: 
Aspect sentiment triplet extraction (ASTE) focuses on identifying triplets of aspect terms, opinion terms, and sentiment polarities from sentences. The table tagging method, which encodes sentences into a 2-dimensional table, is widely used for this task. Previous research has highlighted the importance of downstream relation learning modules to improve token interaction capture within the table. This study takes a novel approach by leveraging transformer layers as these relation learning modules, given their robust semantic capabilities. However, directly applying transformers presents two challenges: excessively long table sequences and biased local attention interactions. To combat these issues, the researchers introduced the Table-Transformer (T-T) framework, which incorporates a stripe attention mechanism with a loop-shift strategy. The stripe attention reformulates the global attention mechanism to focus on a limited 2-dimensional local window, while the loop-shift strategy promotes interaction across different attention windows. Comprehensive experiments show that T-T achieves state-of-the-art performance with reduced computational costs, validating its effectiveness as a downstream relation learning module in the tagging-based ASTE approach. <div>
arXiv:2505.05271v1 Announce Type: new 
Abstract: Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed of aspect terms, opinion terms, and sentiment polarities from given sentences. The table tagging method is a popular approach to addressing this task, which encodes a sentence into a 2-dimensional table, allowing for the tagging of relations between any two words. Previous efforts have focused on designing various downstream relation learning modules to better capture interactions between tokens in the table, revealing that a stronger capability to capture relations can lead to greater improvements in the model. Motivated by this, we attempt to directly utilize transformer layers as downstream relation learning modules. Due to the powerful semantic modeling capability of transformers, it is foreseeable that this will lead to excellent improvement. However, owing to the quadratic relation between the length of the table and the length of the input sentence sequence, using transformers directly faces two challenges: overly long table sequences and unfair local attention interaction. To address these challenges, we propose a novel Table-Transformer (T-T) for the tagging-based ASTE method. Specifically, we introduce a stripe attention mechanism with a loop-shift strategy to tackle these challenges. The former modifies the global attention mechanism to only attend to a 2-dimensional local attention window, while the latter facilitates interaction between different attention windows. Extensive and comprehensive experiments demonstrate that the T-T, as a downstream relation learning module, achieves state-of-the-art performance with lower computational costs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design</title>
<link>https://arxiv.org/abs/2505.05298</link>
<guid>https://arxiv.org/abs/2505.05298</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational technology, argumentative processes, large language models, reasonable parrots, argumentation theory

<br /><br />Summary: This position paper argues for the need to develop conversational technology tailored to support argumentative processes. Currently, large language models (LLMs) are seen as insufficient for this purpose. The authors propose an alternative design that enhances critical thinking skills rather than aiming to replace them. They introduce the concept of 'reasonable parrots,' which embody essential principles of relevance, responsibility, and freedom in discourse. These parrots are designed to engage in argumentative dialogue, informed by established principles from millennia of argumentation theory. The paper emphasizes that these principles should be foundational in the creation of LLM-based technologies. By reframing LLMs as tools for exercising critical thinking, the authors advocate for a paradigm shift in how conversational technologies can be developed. Ultimately, the proposal seeks to foster an environment where argumentation is not only supported but also enriched through technology, paving the way for more effective and responsible discourse in society. <div>
arXiv:2505.05298v1 Announce Type: new 
Abstract: In this position paper, we advocate for the development of conversational technology that is inherently designed to support and facilitate argumentative processes. We argue that, at present, large language models (LLMs) are inadequate for this purpose, and we propose an ideal technology design aimed at enhancing argumentative skills. This involves re-framing LLMs as tools to exercise our critical thinking rather than replacing them. We introduce the concept of 'reasonable parrots' that embody the fundamental principles of relevance, responsibility, and freedom, and that interact through argumentative dialogical moves. These principles and moves arise out of millennia of work in argumentation theory and should serve as the starting point for LLM-based technology that incorporates basic principles of argumentation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICon: In-Context Contribution for Automatic Data Selection</title>
<link>https://arxiv.org/abs/2505.05327</link>
<guid>https://arxiv.org/abs/2505.05327</guid>
<content:encoded><![CDATA[
<div> Keywords: Data selection, instruction tuning, Large Language Models, ICon, performance improvement

<br /><br />Summary: The paper discusses the importance of data selection for instruction tuning in enhancing the performance of Large Language Models (LLMs) while minimizing training costs. Current automated selection techniques often rely on expensive gradient-based metrics or manually crafted heuristics, which can limit their effectiveness. To address this issue, the authors introduce In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that leverages the implicit fine-tuning qualities of in-context learning (ICL) to evaluate sample contributions without requiring gradient calculations or the construction of manual indicators. ICon effectively identifies high-contribution data by monitoring performance shifts induced by implicit learning through ICL, providing a more computationally efficient alternative to existing methods. Extensive evaluations across three LLMs, 12 benchmarks, and five pairwise assessments validate ICon's effectiveness, with results indicating that models trained using only 15% of ICon-selected data surpass the performance of full datasets by 5.42 percentage points, exceeding traditional selection methods by 2.06 points. The analysis of high-contribution samples identified by ICon highlights their diversity in tasks and suitable difficulty levels, rather than focusing solely on the most challenging examples. <div>
arXiv:2505.05327v1 Announce Type: new 
Abstract: Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?</title>
<link>https://arxiv.org/abs/2505.05406</link>
<guid>https://arxiv.org/abs/2505.05406</guid>
<content:encoded><![CDATA[
<div> Keywords: framing, large language models, biases, news content, evaluation

<br /><br />Summary: The paper discusses the impact of framing in media on public perception, particularly in the context of large language models (LLMs) used for automated news generation. It emphasizes that these models may introduce or amplify framing biases more so than human authors. The authors investigate how framing appears in both standard and fine-tuned LLM-generated news articles. Their analysis indicates that LLMs often demonstrate stronger framing tendencies, especially in politically and socially sensitive contexts, compared to traditional human-written content. Additionally, the study reveals considerable variability in framing biases across different LLM architectures, with some models exhibiting significantly higher levels of bias than others. These findings underscore the necessity for developing effective post-training mitigation strategies to address such biases in automated content. Furthermore, the authors advocate for the implementation of stricter evaluation frameworks to ensure that automated news reporting meets the standards of balanced and unbiased journalism. This research highlights critical considerations for the integration of AI in media and the responsibility of developers to enhance the reliability and ethics of automated news generation. <div>
arXiv:2505.05406v1 Announce Type: new 
Abstract: Framing in media critically shapes public perception by selectively emphasizing some details while downplaying others. With the rise of large language models in automated news and content creation, there is growing concern that these systems may introduce or even amplify framing biases compared to human authors. In this paper, we explore how framing manifests in both out-of-the-box and fine-tuned LLM-generated news content. Our analysis reveals that, particularly in politically and socially sensitive contexts, LLMs tend to exhibit more pronounced framing than their human counterparts. In addition, we observe significant variation in framing tendencies across different model architectures, with some models displaying notably higher biases. These findings point to the need for effective post-training mitigation strategies and tighter evaluation frameworks to ensure that automated news content upholds the standards of balanced reporting.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crosslingual Reasoning through Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.05408</link>
<guid>https://arxiv.org/abs/2505.05408</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning capabilities, multilingual, chain-of-thought, crosslingual generalization, low-resource languages

<br /><br />Summary: This work investigates the generalization of English-centric reasoning fine-tuning in large language models (RLMs) to other languages. Firstly, it demonstrates that increasing inference compute for English-based RLMs enhances multilingual mathematical reasoning, allowing them to outperform larger models in various languages, including low-resource ones. Secondly, the study finds that the chain-of-thought (CoT) reasoning patterns of these models predominantly reflect English but can effectively quote and reason about non-English inputs. Thirdly, an effective strategy is introduced to control the language of long CoT reasoning, revealing that models perform better and more efficiently in high-resource languages. Additionally, it highlights the challenges of out-of-domain reasoning generalization, especially shifting from STEM to cultural commonsense knowledge, even within English reasoning tasks. The overall findings illustrate the potentials and mechanisms of crosslingual generalization in English-centric RLMs while noting the limitations in low-resource languages and out-of-domain scenarios. The conclusion advocates for practitioners to leverage English-centric RLMs for reasoning in high-resource languages, while emphasizing the need for further research to enhance capabilities in low-resource settings and diverse contexts. <div>
arXiv:2505.05408v1 Announce Type: new 
Abstract: Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Models Don't Always Say What They Think</title>
<link>https://arxiv.org/abs/2505.05410</link>
<guid>https://arxiv.org/abs/2505.05410</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought, AI safety, reasoning models, reinforcement learning, monitoring  

<br /><br />Summary: The study evaluates the faithfulness of Chain-of-Thought (CoT) in state-of-the-art reasoning models concerning AI safety. Firstly, it finds that CoTs disclose their use of reasoning hints in at least 1% of cases where hints are employed, but the reveal rate often falls below 20%. Secondly, the application of outcome-based reinforcement learning initially enhances the faithfulness of CoTs; however, this improvement plateaus without reaching full saturation. Thirdly, even when reinforcement learning increases the frequency of hint usage—illustrated as potential reward hacking—the models do not necessarily enhance their verbalization of these hints. These findings indicate that while CoT monitoring presents a valuable method for identifying undesired behaviors during training and evaluations, it remains insufficient to eliminate such behaviors entirely. Additionally, in contexts like those examined, where CoT reasoning is not essential, monitoring at test time may not reliably uncover rare and catastrophic unexpected behaviors. The implications underscore the need for further examination of CoT representational fidelity to ensure effective monitoring strategies for AI safety. <div>
arXiv:2505.05410v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering</title>
<link>https://arxiv.org/abs/2505.05423</link>
<guid>https://arxiv.org/abs/2505.05423</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, literary translation, evaluation metric, TransProQA, translation quality  

<br /><br />Summary: The impact of Large Language Models (LLMs) has reached literary domains, yet current evaluation metrics tend to value mechanical accuracy over artistic expression, often misjudging machine translation (MT) as superior to human translation. This bias poses a risk of diminishing translation quality and cultural authenticity. To address this issue, the study introduces TransProQA, a specialized, reference-free, LLM-based question-answering framework aimed at evaluating literary translation. TransProQA draws on insights from professional literary translators, emphasizing critical aspects like literary devices, cultural understanding, and authorial voice. The evaluation reveals that while the literary-finetuned XCOMET-XL shows marginal improvements, TransProQA significantly surpasses existing metrics, achieving up to 0.07 improvement in correlation measures (ACC-EQ and Kendall's tau) and exceeding top state-of-the-art metrics by over 15 points in adequacy assessments. Additionally, incorporating professional translator insights enhances performance further, demonstrating the importance of their input. Notably, TransProQA approaches human-level evaluation, exhibiting broad applicability across open-source models like LLaMA3.3-70b and Qwen2.5-32b, positioning it as an accessible and training-free tool valuable for evaluating texts particularly sensitive to copyright or ethical considerations. <div>
arXiv:2505.05423v1 Announce Type: new 
Abstract: The impact of Large Language Models (LLMs) has extended into literary domains. However, existing evaluation metrics prioritize mechanical accuracy over artistic expression and tend to overrate machine translation (MT) as being superior to experienced professional human translation. In the long run, this bias could result in a permanent decline in translation quality and cultural authenticity. In response to the urgent need for a specialized literary evaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based question-answering (QA) framework designed specifically for literary translation evaluation. TransProQA uniquely integrates insights from professional literary translators and researchers, focusing on critical elements in literary quality assessment such as literary devices, cultural understanding, and authorial voice. Our extensive evaluation shows that while literary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially outperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ and Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by over 15 points in adequacy assessments. Incorporating professional translator insights as weights further improves performance, highlighting the value of translator inputs. Notably, TransProQA approaches human-level evaluation performance comparable to trained linguistic annotators. It demonstrates broad applicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b, indicating its potential as an accessible and training-free literary evaluation metric and a valuable tool for evaluating texts that require local processing due to copyright or ethical considerations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data</title>
<link>https://arxiv.org/abs/2505.05427</link>
<guid>https://arxiv.org/abs/2505.05427</guid>
<content:encoded><![CDATA[
<div> Keywords: data quality, model-driven filtering, verification strategy, lightweight classifier, Ultra-FineWeb  

<br /><br />Summary: Data quality is crucial for enhancing the performance of large language models (LLMs). Model-driven data filtering is a primary approach to acquire high-quality data but faces two challenges: inefficiency in data verification and unclear criteria for seed data selection, which relies on human expertise. To address the verification challenge, an efficient strategy is introduced to rapidly evaluate the impact of data on LLM training with minimal computational costs. For the seed data selection issue, the authors integrate this verification strategy to optimize the selection of positive and negative samples, thus proposing a more effective data filtering pipeline. This pipeline enhances filtering efficiency and classifier quality while reducing experimental and inference costs. A lightweight classifier based on fastText is utilized for high-quality data filtering. The proposed filtering pipeline is applied to two pre-training corpora, resulting in the creation of the Ultra-FineWeb dataset, which contains around 1 trillion English tokens and 120 billion Chinese tokens. Empirical results show that LLMs trained on Ultra-FineWeb exhibit significant performance improvements across various benchmark tasks, demonstrating the effectiveness of the pipeline in improving data quality and training efficiency. <div>
arXiv:2505.05427v1 Announce Type: new 
Abstract: Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations</title>
<link>https://arxiv.org/abs/2505.05445</link>
<guid>https://arxiv.org/abs/2505.05445</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction-tuned, dialogue systems, benchmarking, user simulators, conversational AI

<br /><br />Summary: The emergence of instruction-tuned large language models (LLMs) has revolutionized dialogue systems, enhancing both user simulations and multi-turn conversational agents. Current research often evaluates these systems in isolation, which hinders the ability to generalize insights across various architectures and configurations. To address this limitation, the authors introduce clem todd (chat-optimized LLMs for task-oriented dialogue systems development), a comprehensive framework designed for the systematic evaluation of dialogue systems under standardized conditions. This framework allows for thorough benchmarking across different user simulators and dialogue systems, including both established models and newly developed ones. clem todd facilitates easy integration and maintains uniform datasets, evaluation metrics, and computational limits, ensuring consistency. The authors demonstrate clem todd's versatility by re-evaluating existing task-oriented dialogue systems and incorporating three new systems within the same evaluation setup. The findings yield valuable insights regarding the impact of architecture, scale, and prompting strategies on dialogue performance. Ultimately, this work offers practical guidance for the development of efficient and effective conversational AI systems, paving the way for improved dialogue interactions in future applications. <div>
arXiv:2505.05445v1 Announce Type: new 
Abstract: The emergence of instruction-tuned large language models (LLMs) has advanced the field of dialogue systems, enabling both realistic user simulations and robust multi-turn conversational agents. However, existing research often evaluates these components in isolation-either focusing on a single user simulator or a specific system design-limiting the generalisability of insights across architectures and configurations. In this work, we propose clem todd (chat-optimized LLMs for task-oriented dialogue systems development), a flexible framework for systematically evaluating dialogue systems under consistent conditions. clem todd enables detailed benchmarking across combinations of user simulators and dialogue systems, whether existing models from literature or newly developed ones. It supports plug-and-play integration and ensures uniform datasets, evaluation metrics, and computational constraints. We showcase clem todd's flexibility by re-evaluating existing task-oriented dialogue systems within this unified setup and integrating three newly proposed dialogue systems into the same evaluation pipeline. Our results provide actionable insights into how architecture, scale, and prompting strategies affect dialogue performance, offering practical guidance for building efficient and effective conversational AI systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections</title>
<link>https://arxiv.org/abs/2505.05459</link>
<guid>https://arxiv.org/abs/2505.05459</guid>
<content:encoded><![CDATA[
<div> Keywords: misleading narratives, elections, taxonomy, dataset, language models

<br /><br />Summary: Misleading narratives significantly shape public opinion during elections, impacting voter perceptions of candidates and parties. To address this issue, the article presents the first taxonomy of common misleading narratives encountered in recent European elections. Utilizing this taxonomy, the authors introduce UKElectionNarratives, the inaugural dataset comprising human-annotated misleading narratives from the UK General Elections of 2019 and 2024. Furthermore, the study benchmarks the effectiveness of Pre-trained and Large Language Models, particularly focusing on GPT-4o, in detecting these election-related misleading narratives. By analyzing the performance of these models, the authors aim to understand their utility in narrative detection. The paper not only identifies and categorizes these narratives but also emphasizes the importance of accurate detection methods in maintaining fair electoral processes. Additionally, the study discusses potential use cases for the dataset and model findings and offers recommendations for future research directions. This work aims to contribute to the broader discourse on misinformation in politics, providing a foundational resource for researchers and practitioners interested in tackling misleading narratives in electoral contexts. <div>
arXiv:2505.05459v1 Announce Type: new 
Abstract: Misleading narratives play a crucial role in shaping public opinion during elections, as they can influence how voters perceive candidates and political parties. This entails the need to detect these narratives accurately. To address this, we introduce the first taxonomy of common misleading narratives that circulated during recent elections in Europe. Based on this taxonomy, we construct and analyse UKElectionNarratives: the first dataset of human-annotated misleading narratives which circulated during the UK General Elections in 2019 and 2024. We also benchmark Pre-trained and Large Language Models (focusing on GPT-4o), studying their effectiveness in detecting election-related misleading narratives. Finally, we discuss potential use cases and make recommendations for future research directions using the proposed codebook and dataset.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging</title>
<link>https://arxiv.org/abs/2505.05464</link>
<guid>https://arxiv.org/abs/2505.05464</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, reasoning abilities, model merging, multimodal integration, perception capabilities  

<br /><br />Summary: This work investigates the integration of visual perception and reasoning abilities through a novel approach called model merging, which connects parameters of Vision-Language Models (VLMs) and Large Language Models (LLMs). Unlike previous methodologies that focus on merging similar models, this study emphasizes the fusion of models across different modalities to enhance VLMs with LLMs' reasoning capabilities. The researchers conducted extensive experiments demonstrating that model merging successfully transfers reasoning abilities from LLMs to VLMs without the need for additional training. Furthermore, the merged models facilitate a better understanding of how perception and reasoning are processed internally, revealing that perception abilities are primarily encoded in the early layers of the model while reasoning operates mainly in the middle-to-late layers. Post-merging, all layers begin to contribute to reasoning tasks, in contrast to the perception abilities that remain largely concentrated in the earlier layers. These findings highlight the efficacy of model merging as a means for multimodal integration and provide insights into the internal mechanisms of perception and reasoning across different models. <div>
arXiv:2505.05464v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComPO: Preference Alignment via Comparison Oracles</title>
<link>https://arxiv.org/abs/2505.05465</link>
<guid>https://arxiv.org/abs/2505.05465</guid>
<content:encoded><![CDATA[
<div> Keywords: preference alignment, large language models, convergence guarantee, noisy preference pairs, performance improvement  

<br /><br />Summary: This paper addresses the challenges associated with direct alignment methods used to align large language models (LLMs) with human preferences, specifically focusing on issues of verbosity and likelihood displacement caused by noisy preference pairs. The authors propose a new preference alignment method utilizing comparison oracles, providing a convergence guarantee for its fundamental approach. To enhance this method, they incorporate various heuristics and present experimental results showcasing its flexibility and effectiveness in improving LLM performance when dealing with noisy preference pairs. The evaluations are conducted across several models, including Mistral-7B, Llama-3-8B, and Gemma-2-9B, using benchmarks such as AlpacaEval 2, MT-Bench, and Arena-Hard. The findings highlight the effectiveness of their proposed method as a viable alternative to existing direct alignment methods. A significant contribution of the research is the demonstration of the necessity for specialized methods tailored to preference pairs with distinct likelihood margins, supporting recent insights in the literature. <div>
arXiv:2505.05465v1 Announce Type: new 
Abstract: Direct alignment methods are increasingly used for aligning large language models (LLMs) with human preferences. However, these methods suffer from the issues of verbosity and likelihood displacement, which can be driven by the noisy preference pairs that induce similar likelihood for preferred and dispreferred responses. The contributions of this paper are two-fold. First, we propose a new preference alignment method based on comparison oracles and provide the convergence guarantee for its basic scheme. Second, we improve our method using some heuristics and conduct the experiments to demonstrate the flexibility and compatibility of practical scheme in improving the performance of LLMs using noisy preference pairs. Evaluations are conducted across multiple base and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with benchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show the effectiveness of our method as an alternative to addressing the limitations of existing direct alignment methods. A highlight of our work is that we evidence the importance of designing specialized methods for preference pairs with distinct likelihood margin, which complements the recent findings in \citet{Razin-2025-Unintentional}.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification</title>
<link>https://arxiv.org/abs/2505.04629</link>
<guid>https://arxiv.org/abs/2505.04629</guid>
<content:encoded><![CDATA[
<div> Keywords: Kurdish, speaker detection, dialects, machine learning, recognition performance  

<br /><br />Summary:  
This work investigates the complexities and challenges involved in Kurdish speaker detection amidst its various dialects, specifically Kurmanji, Sorani, and Hawrami. The researchers highlight significant phonetic and lexical differences that create hurdles for speaker recognition systems. The study delves into the main difficulties faced in developing an effective speaker identification system capable of accurately recognizing speakers across these different dialects. To enhance the accuracy and reliability of these systems, the research proposes solutions such as advanced machine learning techniques, data augmentation methods, and the creation of a comprehensive corpus tailored to each dialect. The findings indicate that implementing customized strategies for each dialect, along with cross-dialect training, significantly improves recognition performance. The work ultimately emphasizes the importance of addressing dialect-specific characteristics in speaker detection systems to achieve better results across the diverse Kurdish language landscape. <div>
arXiv:2505.04629v1 Announce Type: cross 
Abstract: The complexity and difficulties of Kurdish speaker detection among its several dialects are investigated in this work. Because of its great phonetic and lexical differences, Kurdish with several dialects including Kurmanji, Sorani, and Hawrami offers special challenges for speaker recognition systems. The main difficulties in building a strong speaker identification system capable of precisely identifying speakers across several dialects are investigated in this work. To raise the accuracy and dependability of these systems, it also suggests solutions like sophisticated machine learning approaches, data augmentation tactics, and the building of thorough dialect-specific corpus. The results show that customized strategies for every dialect together with cross-dialect training greatly enhance recognition performance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Artificial Intelligence Research Assistant for Expert-Involved Learning</title>
<link>https://arxiv.org/abs/2505.04638</link>
<guid>https://arxiv.org/abs/2505.04638</guid>
<content:encoded><![CDATA[
arXiv:2505.04638v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged as transformative tools in scientific research, yet their reliability and specific contributions to biomedical applications remain insufficiently characterized. In this study, we present \textbf{AR}tificial \textbf{I}ntelligence research assistant for \textbf{E}xpert-involved \textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and enhance two critical capabilities of LLMs and LMMs in biomedical research: summarizing extensive scientific texts and interpreting complex biomedical figures. To facilitate rigorous assessment, we create two open-source sets comprising biomedical articles and figures with designed questions. We systematically benchmark both open- and closed-source foundation models, incorporating expert-driven human evaluations conducted by doctoral-level experts. Furthermore, we improve model performance through targeted prompt engineering and fine-tuning strategies for summarizing research papers, and apply test-time computational scaling to enhance the reasoning capabilities of LMMs, achieving superior accuracy compared to human-expert corrections. We also explore the potential of using LMM Agents to generate scientific hypotheses from diverse multimodal inputs. Overall, our results delineate clear strengths and highlight significant limitations of current foundation models, providing actionable insights and guiding future advancements in deploying large-scale language and multi-modal models within biomedical research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Bad Data Leads to Good Models</title>
<link>https://arxiv.org/abs/2505.04741</link>
<guid>https://arxiv.org/abs/2505.04741</guid>
<content:encoded><![CDATA[
arXiv:2505.04741v1 Announce Type: cross 
Abstract: In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of "quality" from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs</title>
<link>https://arxiv.org/abs/2505.04806</link>
<guid>https://arxiv.org/abs/2505.04806</guid>
<content:encoded><![CDATA[
arXiv:2505.04806v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into consumer and enterprise applications. Despite their capabilities, they remain susceptible to adversarial attacks such as prompt injection and jailbreaks that override alignment safeguards. This paper provides a systematic investigation of jailbreak strategies against various state-of-the-art LLMs. We categorize over 1,400 adversarial prompts, analyze their success against GPT-4, Claude 2, Mistral 7B, and Vicuna, and examine their generalizability and construction logic. We further propose layered mitigation strategies and recommend a hybrid red-teaming and sandboxing approach for robust LLM security.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
<link>https://arxiv.org/abs/2505.04846</link>
<guid>https://arxiv.org/abs/2505.04846</guid>
<content:encoded><![CDATA[
arXiv:2505.04846v1 Announce Type: cross 
Abstract: The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.04851</link>
<guid>https://arxiv.org/abs/2505.04851</guid>
<content:encoded><![CDATA[
arXiv:2505.04851v1 Announce Type: cross 
Abstract: Despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. This is due to the content of existing large training datasets collected on the Internet, which are predominantly based on Western European or American popular culture. Meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. In an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. We propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the Russian one. We explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the Kandinsky 3.1 text-to-image model. Human evaluation results demonstrate an increase in the level of awareness of Russian culture in the model.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.04881</link>
<guid>https://arxiv.org/abs/2505.04881</guid>
<content:encoded><![CDATA[
arXiv:2505.04881v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused by redundant content, increasing computational overhead, and degrading user experience. Existing compression methods either operate post-hoc pruning, risking disruption to reasoning coherence, or rely on sampling-based selection, which fails to intervene effectively during generation. In this work, we introduce a confidence-guided perspective to explain the emergence of redundant reflection in LRMs, identifying two key patterns: Confidence Deficit, where the model reconsiders correct steps due to low internal confidence, and Termination Delay, where reasoning continues even after reaching a confident answer. Based on this analysis, we propose ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), a framework that simplifies reasoning chains by reinforcing the model's confidence during inference, thus preventing the generation of redundant reflection steps. It integrates Confidence Injection to stabilize intermediate steps and Early Stopping to terminate reasoning when confidence is sufficient. Extensive experiments demonstrate that fine-tuning LRMs on ConCISE-generated data yields significantly shorter outputs, reducing length by up to approximately 50% under SimPO, while maintaining high task accuracy. ConCISE consistently outperforms existing baselines across multiple reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.04911</link>
<guid>https://arxiv.org/abs/2505.04911</guid>
<content:encoded><![CDATA[
arXiv:2505.04911v1 Announce Type: cross 
Abstract: This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2505.04914</link>
<guid>https://arxiv.org/abs/2505.04914</guid>
<content:encoded><![CDATA[
arXiv:2505.04914v1 Announce Type: cross 
Abstract: Transformer-decoder language models are a core innovation in text based generative artificial intelligence. These models are being deployed as general-purpose intelligence systems in many applications. Central to their utility is the capacity to understand natural language commands and exploit the reasoning embedded in human text corpora to apply some form of reasoning process to a wide variety of novel tasks. To understand the limitations of this approach to generating reasoning we argue that we need to consider the architectural constraints of these systems. Consideration of the latent variable structure of transformer-decoder models allows us to design reasoning tasks that should probe the boundary of their capacity to reason. We present enigme, an open-source library for generating text-based puzzles to be used in training and evaluating reasoning skills within transformer-decoder models and future AI architectures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
arXiv:2505.04921v1 Announce Type: cross 
Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models</title>
<link>https://arxiv.org/abs/2505.04946</link>
<guid>https://arxiv.org/abs/2505.04946</guid>
<content:encoded><![CDATA[
arXiv:2505.04946v1 Announce Type: cross 
Abstract: Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations</title>
<link>https://arxiv.org/abs/2505.04948</link>
<guid>https://arxiv.org/abs/2505.04948</guid>
<content:encoded><![CDATA[
arXiv:2505.04948v1 Announce Type: cross 
Abstract: Recommender systems are essential for delivering personalized content across digital platforms by modeling user preferences and behaviors. Recently, large language models (LLMs) have been adopted for prompt-based recommendation due to their ability to generate personalized outputs without task-specific training. However, LLM-based methods face limitations such as limited context window size, inefficient pointwise and pairwise prompting, and difficulty handling listwise ranking due to token constraints. LLMs can also be sensitive to position bias, as they may overemphasize earlier items in the prompt regardless of their true relevance. To address and investigate these issues, we propose a hybrid framework that combines a traditional recommendation model with an LLM for reranking top-k items using structured prompts. We evaluate the effects of user history reordering and instructional prompts for mitigating position bias. Experiments on MovieLens-100K show that randomizing user history improves ranking quality, but LLM-based reranking does not outperform the base model. Explicit instructions to reduce position bias are also ineffective. Our evaluations reveal limitations in LLMs' ability to model ranking context and mitigate bias. Our code is publicly available at https://github.com/aminul7506/LLMForReRanking.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Transform: A Unified Framework for Adaptive Transform to Enhance Representations</title>
<link>https://arxiv.org/abs/2505.04969</link>
<guid>https://arxiv.org/abs/2505.04969</guid>
<content:encoded><![CDATA[
arXiv:2505.04969v1 Announce Type: cross 
Abstract: Discrete transforms, such as the discrete Fourier transform, are widely used in machine learning to improve model performance by extracting meaningful features. However, with numerous transforms available, selecting an appropriate one often depends on understanding the dataset's properties, making the approach less effective when such knowledge is unavailable. In this work, we propose General Transform (GT), an adaptive transform-based representation designed for machine learning applications. Unlike conventional transforms, GT learns data-driven mapping tailored to the dataset and task of interest. Here, we demonstrate that models incorporating GT outperform conventional transform-based approaches across computer vision and natural language processing tasks, highlighting its effectiveness in diverse learning scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts</title>
<link>https://arxiv.org/abs/2505.05063</link>
<guid>https://arxiv.org/abs/2505.05063</guid>
<content:encoded><![CDATA[
arXiv:2505.05063v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in code generation tasks, powering various applications like code completion, debugging, and programming assistance. However, existing benchmarks such as HumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only prompts, overlooking the real-world scenario where multilingual developers often use code-mixed language while interacting with LLMs. To address this gap, we introduce CodeMixBench, a novel benchmark designed to evaluate the robustness of LLMs on code generation from code-mixed prompts. Built upon BigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the natural language parts of prompts across three language pairs: Hinglish (Hindi-English), Spanish-English, and Chinese Pinyin-English. We comprehensively evaluate a diverse set of open-source code generation models ranging from 1.5B to 15B parameters. Our results show that code-mixed prompts consistently degrade Pass@1 performance compared to their English-only counterparts, with performance drops increasing under higher CMD levels for smaller models. CodeMixBench provides a realistic evaluation framework for studying multilingual code generation and highlights new challenges and directions for building robust code generation models that generalize well across diverse linguistic settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Driver: Explainable Autonomous Driving with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.05098</link>
<guid>https://arxiv.org/abs/2505.05098</guid>
<content:encoded><![CDATA[
arXiv:2505.05098v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. However, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. In this paper, we introduce X-Driver, a unified multi-modal large language models(MLLMs) framework designed for closed-loop autonomous driving, leveraging Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and decision-making. We validate X-Driver across multiple autonomous driving tasks using public benchmarks in CARLA simulation environment, including Bench2Drive[6]. Our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(SOTA) while improving the interpretability of driving decisions. These findings underscore the importance of structured reasoning in end-to-end driving and establish X-Driver as a strong baseline for future research in closed-loop autonomous driving.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding In-context Learning of Addition via Activation Subspaces</title>
<link>https://arxiv.org/abs/2505.05145</link>
<guid>https://arxiv.org/abs/2505.05145</guid>
<content:encoded><![CDATA[
arXiv:2505.05145v1 Announce Type: cross 
Abstract: To perform in-context learning, language models must extract signals from individual few-shot examples, aggregate these into a learned prediction rule, and then apply this rule to new examples. How is this implemented in the forward pass of modern transformer models? To study this, we consider a structured family of few-shot learning tasks for which the true prediction rule is to add an integer $k$ to the input. We find that Llama-3-8B attains high accuracy on this task for a range of $k$, and localize its few-shot ability to just three attention heads via a novel optimization approach. We further show the extracted signals lie in a six-dimensional subspace, where four of the dimensions track the unit digit and the other two dimensions track overall magnitude. We finally examine how these heads extract information from individual few-shot examples, identifying a self-correction mechanism in which mistakes from earlier examples are suppressed by later examples. Our results demonstrate how tracking low-dimensional subspaces across a forward pass can provide insight into fine-grained computational structures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks</title>
<link>https://arxiv.org/abs/2505.05190</link>
<guid>https://arxiv.org/abs/2505.05190</guid>
<content:encoded><![CDATA[
arXiv:2505.05190v1 Announce Type: cross 
Abstract: Text watermarking aims to subtly embed statistical signals into text by controlling the Large Language Model (LLM)'s sampling process, enabling watermark detectors to verify that the output was generated by the specified model. The robustness of these watermarking algorithms has become a key factor in evaluating their effectiveness. Current text watermarking algorithms embed watermarks in high-entropy tokens to ensure text quality. In this paper, we reveal that this seemingly benign design can be exploited by attackers, posing a significant risk to the robustness of the watermark. We introduce a generic efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA), which leverages the vulnerability by calculating the self-information of each token to identify potential pattern tokens and perform targeted attack. Our work exposes a widely prevalent vulnerability in current watermarking algorithms. The experimental results show SIRA achieves nearly 100% attack success rates on seven recent watermarking methods with only 0.88 USD per million tokens cost. Our approach does not require any access to the watermark algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the attack model, even mobile-level models. Our findings highlight the urgent need for more robust watermarking.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Chain of Thoughts via Elastic Reasoning</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
arXiv:2505.05315v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation</title>
<link>https://arxiv.org/abs/2505.05422</link>
<guid>https://arxiv.org/abs/2505.05422</guid>
<content:encoded><![CDATA[
arXiv:2505.05422v1 Announce Type: cross 
Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at https://github.com/TencentARC/TokLIP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding</title>
<link>https://arxiv.org/abs/2505.05446</link>
<guid>https://arxiv.org/abs/2505.05446</guid>
<content:encoded><![CDATA[
arXiv:2505.05446v1 Announce Type: cross 
Abstract: Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following. Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. Our code and models are released at https://github. com/Euphoria16/DocMark.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</title>
<link>https://arxiv.org/abs/2505.05467</link>
<guid>https://arxiv.org/abs/2505.05467</guid>
<content:encoded><![CDATA[
arXiv:2505.05467v1 Announce Type: cross 
Abstract: We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: AI Evaluation Should Learn from How We Test Humans</title>
<link>https://arxiv.org/abs/2306.10512</link>
<guid>https://arxiv.org/abs/2306.10512</guid>
<content:encoded><![CDATA[
arXiv:2306.10512v4 Announce Type: replace 
Abstract: As AI systems continue to evolve, their rigorous evaluation becomes crucial for their development and deployment. Researchers have constructed various large-scale benchmarks to determine their capabilities, typically against a gold-standard test set and report metrics averaged across all items. However, this static evaluation paradigm increasingly shows its limitations, including high evaluation costs, data contamination, and the impact of low-quality or erroneous items on evaluation reliability and efficiency. In this Position, drawing from human psychometrics, we discuss a paradigm shift from static evaluation methods to adaptive testing. This involves estimating the characteristics or value of each test item in the benchmark, and tailoring each model's evaluation instead of relying on a fixed test set. This paradigm provides robust ability estimation, uncovering the latent traits underlying a model's observed scores. This position paper analyze the current possibilities, prospects, and reasons for adopting psychometrics in AI evaluation. We argue that psychometrics, a theory originating in the 20th century for human assessment, could be a powerful solution to the challenges in today's AI evaluations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying</title>
<link>https://arxiv.org/abs/2405.13325</link>
<guid>https://arxiv.org/abs/2405.13325</guid>
<content:encoded><![CDATA[
arXiv:2405.13325v3 Announce Type: replace 
Abstract: Recent advancements in event argument extraction (EAE) involve incorporating useful auxiliary information into models during training and inference, such as retrieved instances and event templates. These methods face two challenges: (1) the retrieval results may be irrelevant and (2) templates are developed independently for each event without considering their possible relationship. In this work, we propose DEGAP to address these challenges through a simple yet effective components: dual prefixes, i.e. learnable prompt vectors, where the instance-oriented prefix and template-oriented prefix are trained to learn information from different event instances and templates. Additionally, we propose an event-guided adaptive gating mechanism, which can adaptively leverage possible connections between different events and thus capture relevant information from the prefix. Finally, these event-guided prefixes provide relevant information as cues to EAE model without retrieval. Extensive experiments demonstrate that our method achieves new state-of-the-art performance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further analysis shows the impact of different components.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</title>
<link>https://arxiv.org/abs/2406.17746</link>
<guid>https://arxiv.org/abs/2406.17746</guid>
<content:encoded><![CDATA[
arXiv:2406.17746v2 Announce Type: replace 
Abstract: Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors influence the likelihood of memorization differently depending on the taxonomic category.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Synthetic Data Creation with 1,000,000,000 Personas</title>
<link>https://arxiv.org/abs/2406.20094</link>
<guid>https://arxiv.org/abs/2406.20094</guid>
<content:encoded><![CDATA[
arXiv:2406.20094v3 Announce Type: replace 
Abstract: We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant</title>
<link>https://arxiv.org/abs/2409.11055</link>
<guid>https://arxiv.org/abs/2409.11055</guid>
<content:encoded><![CDATA[
arXiv:2409.11055v3 Announce Type: replace 
Abstract: Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in coding and STEM tasks, though reasoning may sometimes improve.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</title>
<link>https://arxiv.org/abs/2409.12183</link>
<guid>https://arxiv.org/abs/2409.12183</guid>
<content:encoded><![CDATA[
arXiv:2409.12183v3 Announce Type: replace 
Abstract: Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks</title>
<link>https://arxiv.org/abs/2410.04055</link>
<guid>https://arxiv.org/abs/2410.04055</guid>
<content:encoded><![CDATA[
arXiv:2410.04055v2 Announce Type: replace 
Abstract: While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title>
<link>https://arxiv.org/abs/2410.12705</link>
<guid>https://arxiv.org/abs/2410.12705</guid>
<content:encoded><![CDATA[
arXiv:2410.12705v5 Announce Type: replace 
Abstract: Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2411.00437</link>
<guid>https://arxiv.org/abs/2411.00437</guid>
<content:encoded><![CDATA[
arXiv:2411.00437v2 Announce Type: replace 
Abstract: Retrieval-augmented generation methods often neglect the quality of content retrieved from external knowledge bases, resulting in irrelevant information or potential misinformation that negatively affects the generation results of large language models. In this paper, we propose an end-to-end model with adaptive filtering for retrieval-augmented generation (E2E-AFG), which integrates answer existence judgment and text generation into a single end-to-end framework. This enables the model to focus more effectively on relevant content while reducing the influence of irrelevant information and generating accurate answers. We evaluate E2E-AFG on six representative knowledge-intensive language datasets, and the results show that it consistently outperforms baseline models across all tasks, demonstrating the effectiveness and robustness of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models</title>
<link>https://arxiv.org/abs/2411.04996</link>
<guid>https://arxiv.org/abs/2411.04996</guid>
<content:encoded><![CDATA[
arXiv:2411.04996v2 Announce Type: replace 
Abstract: The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\% of the wall-clock time and text quality in 75.6\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning</title>
<link>https://arxiv.org/abs/2501.01031</link>
<guid>https://arxiv.org/abs/2501.01031</guid>
<content:encoded><![CDATA[
arXiv:2501.01031v3 Announce Type: replace 
Abstract: Ensuring cultural values alignment in Large Language Models (LLMs) remains a critical challenge, as these models often embed Western-centric biases from their training data, leading to misrepresentations and fairness concerns in cross-cultural applications. Existing approaches such as role assignment and few-shot learning struggle to address these limitations effectively due to their reliance on pre-trained knowledge, limited scalability, and inability to capture nuanced cultural values. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with In-Context Learning (ICL) to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. We subsequently curate several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. We evaluate ValuesRAG using 6 diverse regional datasets and show that it consistently outperforms baselines: including zero-shot, role-assignment, few-shot, and hybrid methods, both in main experiments and ablation settings. Notably, ValuesRAG achieves the best overall performance over prior methods, demonstrating its effectiveness in fostering culturally aligned and inclusive AI systems. Our findings underscore the potential of dynamic retrieval-based methods to bridge the gap between global LLM capabilities and localized cultural values.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communicating Activations Between Language Model Agents</title>
<link>https://arxiv.org/abs/2501.14082</link>
<guid>https://arxiv.org/abs/2501.14082</guid>
<content:encoded><![CDATA[
arXiv:2501.14082v2 Announce Type: replace 
Abstract: Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\textit{B}$'s computation at an intermediate layer, combine its current activation with another LM $\textit{A}$'s intermediate activation via some function $\textit{f}$, then pass $\textit{f}$'s output into the next layer of $\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative "language" for communication between LMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Evaluation of DeepSeek Models in Chinese Contexts</title>
<link>https://arxiv.org/abs/2502.11137</link>
<guid>https://arxiv.org/abs/2502.11137</guid>
<content:encoded><![CDATA[
arXiv:2502.11137v3 Announce Type: replace 
Abstract: Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmark and periodically update this report to provide more comprehensive and accurate assessment outcomes. Please refer to the latest version of the paper for the most recent evaluation results and conclusions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drift: Decoding-time Personalized Alignments with Implicit User Preferences</title>
<link>https://arxiv.org/abs/2502.14289</link>
<guid>https://arxiv.org/abs/2502.14289</guid>
<content:encoded><![CDATA[
arXiv:2502.14289v3 Announce Type: replace 
Abstract: Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework</title>
<link>https://arxiv.org/abs/2503.05505</link>
<guid>https://arxiv.org/abs/2503.05505</guid>
<content:encoded><![CDATA[
arXiv:2503.05505v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly adopted in medical question-answering (QA) scenarios. However, LLMs can generate hallucinations and nonfactual information, undermining their trustworthiness in high-stakes medical tasks. Conformal Prediction (CP) provides a statistically rigorous framework for marginal (average) coverage guarantees but has limited exploration in medical QA. This paper proposes an enhanced CP framework for medical multiple-choice question-answering (MCQA) tasks. By associating the non-conformance score with the frequency score of correct options and leveraging self-consistency, the framework addresses internal model opacity and incorporates a risk control strategy with a monotonic loss function. Evaluated on MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the proposed method meets specified error rate guarantees while reducing average prediction set size with increased risk level, offering a promising uncertainty evaluation metric for LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges</title>
<link>https://arxiv.org/abs/2503.08292</link>
<guid>https://arxiv.org/abs/2503.08292</guid>
<content:encoded><![CDATA[
arXiv:2503.08292v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly applied to outpatient referral tasks across healthcare systems. However, there is a lack of standardized evaluation criteria to assess their effectiveness, particularly in dynamic, interactive scenarios. In this study, we systematically examine the capabilities and limitations of LLMs in managing tasks within Intelligent Outpatient Referral (IOR) systems and propose a comprehensive evaluation framework specifically designed for such systems. This framework comprises two core tasks: static evaluation, which focuses on evaluating the ability of predefined outpatient referrals, and dynamic evaluation, which evaluates capabilities of refining outpatient referral recommendations through iterative dialogues. Our findings suggest that LLMs offer limited advantages over BERT-like models, but show promise in asking effective questions during interactive dialogues.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization</title>
<link>https://arxiv.org/abs/2503.13690</link>
<guid>https://arxiv.org/abs/2503.13690</guid>
<content:encoded><![CDATA[
arXiv:2503.13690v2 Announce Type: replace 
Abstract: We present a submission to the SemEval 2025 shared task on unlearning sensitive content from LLMs. Our approach employs negative preference optimization using low-rank adaptation. We show that we can utilize this combination to efficiently compute additional regularization terms, which help with unlearning stabilization. The results of our approach significantly exceed the shared task baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks</title>
<link>https://arxiv.org/abs/2503.15169</link>
<guid>https://arxiv.org/abs/2503.15169</guid>
<content:encoded><![CDATA[
arXiv:2503.15169v2 Announce Type: replace 
Abstract: The application of large language models (LLMs) to healthcare information extraction has emerged as a promising approach. This study evaluates the classification performance of five open-source LLMs: GEMMA-3-27B-IT, LLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and DEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks involving both social media data (breast cancer, changes in medication regimen, adverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma labeling, medication change discussion). We report precision, recall, and F1 scores with 95% confidence intervals for all model-task combinations. Our findings reveal significant performance variability between LLMs, with DeepSeekV3 emerging as the strongest overall performer, achieving the highest F1 scores in four tasks. Notably, models generally performed better on social media tasks compared to clinical data tasks, suggesting potential domain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high recall despite its smaller parameter count, while LLAMA4-109B showed surprisingly underwhelming performance compared to its predecessor LLAMA3-70B, indicating that larger parameter counts do not guarantee improved classification results. We observed distinct precision-recall trade-offs across models, with some favoring sensitivity over specificity and vice versa. These findings highlight the importance of task-specific model selection for healthcare applications, considering the particular data domain and precision-recall requirements rather than model size alone. As healthcare increasingly integrates AI-driven text classification tools, this comprehensive benchmarking provides valuable guidance for model selection and implementation while underscoring the need for continued evaluation and domain adaptation of LLMs in healthcare contexts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment</title>
<link>https://arxiv.org/abs/2307.02075</link>
<guid>https://arxiv.org/abs/2307.02075</guid>
<content:encoded><![CDATA[
arXiv:2307.02075v3 Announce Type: replace-cross 
Abstract: Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To circumvent the shortage of seed alignments provided for training, recent EA models utilize pseudo-labeling strategies to iteratively add unaligned entity pairs predicted with high confidence to the seed alignments for model training. However, the adverse impact of confirmation bias during pseudo-labeling has been largely overlooked, thus hindering entity alignment performance. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components: (1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to determine entity correspondences and reduce erroneous matches across two KGs. An effective criterion is derived to infer pseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel pseudo-label ensembling refines pseudo-labeled alignments by combining predictions over multiple models independently trained in parallel. The ensembled pseudo-labeled alignments are thereafter used to augment seed alignments to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. Our extensive results and in-depth analyses demonstrate the superiority of UPL-EA over 15 competitive baselines and its utility as a general pseudo-labeling framework for entity alignment.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HORAE: A Domain-Agnostic Language for Automated Service Regulation</title>
<link>https://arxiv.org/abs/2406.06600</link>
<guid>https://arxiv.org/abs/2406.06600</guid>
<content:encoded><![CDATA[
arXiv:2406.06600v4 Announce Type: replace-cross 
Abstract: Artificial intelligence is rapidly encroaching on the field of service regulation. However, existing AI-based regulation techniques are often tailored to specific application domains and thus are difficult to generalize in an automated manner. This paper presents Horae, a unified specification language for modeling (multimodal) regulation rules across a diverse set of domains. We showcase how Horae facilitates an intelligent service regulation pipeline by further exploiting a fine-tuned large language model named RuleGPT that automates the Horae modeling process, thereby yielding an end-to-end framework for fully automated intelligent service regulation. The feasibility and effectiveness of our framework are demonstrated over a benchmark of various real-world regulation domains. In particular, we show that our open-sourced, fine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and perform on par with GPT-4o.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2409.03757</link>
<guid>https://arxiv.org/abs/2409.03757</guid>
<content:encoded><![CDATA[
arXiv:2409.03757v3 Announce Type: replace-cross 
Abstract: Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. Code: https://github.com/YunzeMan/Lexicon3D
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play</title>
<link>https://arxiv.org/abs/2411.08884</link>
<guid>https://arxiv.org/abs/2411.08884</guid>
<content:encoded><![CDATA[
arXiv:2411.08884v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) become more prevalent, concerns about their safety, ethics, and potential biases have risen. Systematically evaluating LLMs' risk decision-making tendencies and attitudes, particularly in the ethical domain, has become crucial. This study innovatively applies the Domain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and proposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess LLMs' ethical risk attitudes in depth. We further propose a novel approach integrating risk scales and role-playing to quantitatively evaluate systematic biases in LLMs. Through systematic evaluation and analysis of multiple mainstream LLMs, we assessed the "risk personalities" of LLMs across multiple domains, with a particular focus on the ethical domain, and revealed and quantified LLMs' systematic biases towards different groups. This research helps understand LLMs' risk decision-making and ensure their safe and reliable application. Our approach provides a tool for identifying and mitigating biases, contributing to fairer and more trustworthy AI systems. The code and data are available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems</title>
<link>https://arxiv.org/abs/2502.18635</link>
<guid>https://arxiv.org/abs/2502.18635</guid>
<content:encoded><![CDATA[
arXiv:2502.18635v2 Announce Type: replace-cross 
Abstract: While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-evaluating Open-ended Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20170</link>
<guid>https://arxiv.org/abs/2502.20170</guid>
<content:encoded><![CDATA[
arXiv:2502.20170v2 Announce Type: replace-cross 
Abstract: Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining</title>
<link>https://arxiv.org/abs/2504.02107</link>
<guid>https://arxiv.org/abs/2504.02107</guid>
<content:encoded><![CDATA[
arXiv:2504.02107v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of magnitude larger than previous continual language modeling benchmarks. We also design time-stratified evaluations across both general CC data and specific domains (Wikipedia, StackExchange, and code documentation) to assess how well various continual learning methods adapt to new data while retaining past knowledge. Our findings demonstrate that, on general CC data, autoregressive meta-schedules combined with a fixed-ratio replay of older data can achieve comparable held-out loss to re-training from scratch, while requiring significantly less computation (2.6x). However, the optimal balance between incorporating new data and replaying old data differs as replay is crucial to avoid forgetting on generic web data but less so on specific domains.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets</title>
<link>https://arxiv.org/abs/2504.19981</link>
<guid>https://arxiv.org/abs/2504.19981</guid>
<content:encoded><![CDATA[
arXiv:2504.19981v2 Announce Type: replace-cross 
Abstract: Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding</title>
<link>https://arxiv.org/abs/2505.03788</link>
<guid>https://arxiv.org/abs/2505.03788</guid>
<content:encoded><![CDATA[
<div> Approach, Uncertainty Quantification, Large Language Models, Calibration, Multi-modal.

Summary:
This study introduces a novel approach for calibrating uncertainty quantification in multi-modal large language models (LLMs). Current methods for uncertainty quantification rely on consistency among multiple responses generated by LLMs, but they can lead to overconfidence in incorrect scenarios. To address this issue, the researchers incorporate cross-modal consistency to improve calibration by grounding textual responses to visual inputs. They use temperature scaling to calibrate the confidence of the grounding model. The proposed framework is evaluated on tasks such as medical question answering and visual question answering using multi-modal models. The experiments show that the approach significantly enhances calibration on both tasks.<br /><br />Summary: <div>
arXiv:2505.03788v1 Announce Type: new 
Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty</title>
<link>https://arxiv.org/abs/2505.03910</link>
<guid>https://arxiv.org/abs/2505.03910</guid>
<content:encoded><![CDATA[
<div> Keywords: chest radiograph interpretation, Deep Learning, uncertainty, Bayesian Deep Learning, BERT <br />
Summary:<br />
- Automating chest radiograph interpretation using Deep Learning models can improve clinical workflows and decision-making.
- Quantifying uncertainty is crucial in medical settings, and this study investigates the relationship between predictive uncertainty and linguistic uncertainty.
- Bayesian Deep Learning approximations and Monte Carlo Dropout are effective in estimating predictive uncertainty.
- There is a modest correlation between predictive and linguistic uncertainty, highlighting challenges in aligning machine uncertainty with human interpretation nuances.
- While Bayesian approximations provide valuable uncertainty estimates, further refinement is needed to fully capture and utilize human uncertainty in clinical applications. <br /> 

Summary: <br />
Automating chest radiograph interpretation through Deep Learning is beneficial for clinical workflows. Uncertainty quantification is crucial in medical contexts, but aligning machine uncertainty with human interpretation nuances poses challenges. Bayesian Deep Learning and Monte Carlo Dropout show promise in estimating predictive uncertainty. A modest correlation between predictive and linguistic uncertainty underscores the need for further refinement in capturing human uncertainty accurately. Although Bayesian approximations offer valuable uncertainty estimates, there is room for improvement to better utilize human uncertainty in clinical applications. <div>
arXiv:2505.03910v1 Announce Type: new 
Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models has the potential to significantly improve clinical workflows, decision-making, and large-scale health screening. However, in medical settings, merely optimising predictive performance is insufficient, as the quantification of uncertainty is equally crucial. This paper investigates the relationship between predictive uncertainty, derived from Bayesian Deep Learning approximations, and human/linguistic uncertainty, as estimated from free-text radiology reports labelled by rule-based labellers. Utilising BERT as the model of choice, this study evaluates different binarisation methods for uncertainty labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in estimating predictive uncertainty. The results demonstrate good model performance, but also a modest correlation between predictive and linguistic uncertainty, highlighting the challenges in aligning machine uncertainty with human interpretation nuances. Our findings suggest that while Bayesian approximations provide valuable uncertainty estimates, further refinement is necessary to fully capture and utilise the subtleties of human uncertainty in clinical applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reasoning-Focused Legal Retrieval Benchmark</title>
<link>https://arxiv.org/abs/2505.03970</link>
<guid>https://arxiv.org/abs/2505.03970</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, legal AI, retrieval-augmented LLMs, legal benchmarks, legal research

Summary: 
The article explores the use of large language models (LLMs) in legal applications, specifically focusing on retrieval-augmented LLMs (RAG systems) to enhance system performance in the legal community. To address the lack of realistic legal benchmarks for RAG systems, the authors introduce two new legal RAG benchmarks: Bar Exam QA and Housing Statute QA. These benchmarks simulate real-world legal research tasks and provide a challenging test for existing retriever pipelines. The study highlights the complexity of legal RAG applications and calls for further research in this area to improve system performance and robustness. It emphasizes the need for specialized benchmarks to accurately evaluate the capabilities of legal AI systems in legal question-answering and retrieval tasks. <div>
arXiv:2505.03970v1 Announce Type: new 
Abstract: As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs ("RAG" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale</title>
<link>https://arxiv.org/abs/2505.03973</link>
<guid>https://arxiv.org/abs/2505.03973</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based optimization, agentic systems, Fine-Grained Optimization, scalability, pattern recognition

Summary:<br />
LLM-based optimization has shown great promise in improving agentic systems. However, traditional methods of training LLM optimizers with entire trajectories in a single pass become inefficient as datasets grow, causing issues such as context window overflow and reduced pattern recognition. To combat these challenges, Fine-Grained Optimization (FGO) proposes a scalable framework that breaks large optimization tasks into manageable subsets, conducts targeted optimizations, and merges optimized components progressively. Evaluation on various benchmarks shows that FGO outperforms existing methods by 1.6-8.6% while decreasing prompt token consumption by 56.3%. This framework offers a practical solution for scaling up LLM-based optimization in complex agent systems. Analysis reveals that FGO consistently improves performance across all dataset sizes, highlighting its scalability and efficiency. <br /><br />Summary: <div>
arXiv:2505.03973v1 Announce Type: new 
Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains</title>
<link>https://arxiv.org/abs/2505.03981</link>
<guid>https://arxiv.org/abs/2505.03981</guid>
<content:encoded><![CDATA[
<div> Generalizable reasoning; multimodal reasoning; X-Reasoner; domain transfer; medical benchmarks <br />
<br />
Summary: Recent research has focused on training text-only reasoning models, neglecting the extension of reasoning capabilities to multiple modalities and domains. This study addresses the question of whether reasoning can be generalized across modalities and domains. The findings indicate that text-based post-training in a general domain can facilitate strong, generalizable reasoning. The X-Reasoner model is introduced, which is post-trained on general-domain text for generalized reasoning using a two-stage approach. Experiment results demonstrate that X-Reasoner outperforms existing models on various general and medical benchmarks in both multimodal and out-of-domain settings. Continual training on domain-specific data further enhances performance, leading to the development of X-Reasoner-Med, a medical-specialized variant that achieves top performance on medical benchmarks. <br /><br />Summary: <div>
arXiv:2505.03981v1 Announce Type: new 
Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Structuring the Output of Large Language Models</title>
<link>https://arxiv.org/abs/2505.04016</link>
<guid>https://arxiv.org/abs/2505.04016</guid>
<content:encoded><![CDATA[
<div> transformer, structured outputs, language models, schema accuracy, content fidelity
<br />
Structured language models often struggle to generate outputs that adhere to predefined schemas, impacting their applicability in critical applications. To address this, a model-agnostic approach called SLOT (Structured LLM Output Transformer) is introduced. SLOT utilizes a fine-tuned lightweight language model as a post-processing layer to transform unstructured outputs into precise structured formats. A systematic pipeline for data curation and synthesis is established, along with a formal evaluation methodology to assess schema accuracy and content fidelity. Results show that the Mistral-7B model equipped with SLOT achieves near-perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming existing models significantly. Even compact models like Llama-3.2-1B can exhibit structured output capabilities on par with larger models when enhanced with SLOT, enabling reliable structured generation in resource-constrained scenarios.
<br /><br />Summary: <div>
arXiv:2505.04016v1 Announce Type: new 
Abstract: Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing and Benchmarking Personalized Tool Invocation for LLMs</title>
<link>https://arxiv.org/abs/2505.04072</link>
<guid>https://arxiv.org/abs/2505.04072</guid>
<content:encoded><![CDATA[
<div> framework, personalized, tool invocation, benchmark, data synthesis
Summary:
Personalized Tool Invocation is introduced as a concept for extending the capabilities of Large Language Models (LLMs) by considering user preferences and profile-dependent queries in tool invocation. Two key tasks, Tool Preference and Profile-dependent Query, are defined to address personalized constraints. PTool, a data synthesis framework, is proposed to handle these challenges, while PTBench is introduced as the first benchmark for evaluating personalized tool invocation. Fine-tuning various open-source models using PTool demonstrates its effectiveness and provides valuable insights for future research. The public repository for PTBench can be found at https://github.com/hyfshadow/PTBench. <div>
arXiv:2505.04072v1 Announce Type: new 
Abstract: Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention. It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge. However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation. In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query. Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile. To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation. Additionally, we construct \textbf{PTBench}, the first benchmark for evaluating personalized tool invocation. We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights. Our benchmark is public at https://github.com/hyfshadow/PTBench.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Generation in Healthcare: A Review of Methods and Applications</title>
<link>https://arxiv.org/abs/2505.04073</link>
<guid>https://arxiv.org/abs/2505.04073</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language generation, medical applications, generative models, clinical workflows, healthcare

Summary:
Natural language generation (NLG) is a critical technology for generative AI, particularly in the medical field. NLG, powered by large language models, is transforming clinical workflows, aiding in decision-making, and enhancing documentation in healthcare. This review examines 113 scientific publications to categorize NLG methods, model architecture, clinical applications, and evaluation methods. By following PRISMA guidelines, the study identifies key NLG technologies, assesses their capabilities and limitations, and highlights emerging challenges in leveraging NLG for medical discovery and healthcare. The review underscores the importance of utilizing heterogeneous medical data modalities and diverse generative models in a range of healthcare applications. Through this comprehensive overview, insights are provided for future research endeavors seeking to harness NLG advancements in the medical domain. 

<br /><br />Summary: <div>
arXiv:2505.04073v1 Announce Type: new 
Abstract: Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI). With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation. Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG. Researchers have proposed many generative models and applied them in a number of healthcare applications. There is a need for a comprehensive review of NLG methods and applications in the medical domain. In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods. Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges. This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model</title>
<link>https://arxiv.org/abs/2505.04132</link>
<guid>https://arxiv.org/abs/2505.04132</guid>
<content:encoded><![CDATA[
<div> Keywords: legal information, layperson, CLIC-pages, Legal Question Bank, GPT-3

Summary: 
The study addresses the challenge of making legal information accessible to the public by translating technical legal documents into easily understandable snippets called CLIC-pages. They also develop a Legal Question Bank (LQB) to provide answers to legal questions, and an interactive CLIC Recommender (CRec) to help users find relevant legal knowledge. Using large-scale pre-trained language models like GPT-3, they generate machine-generated legal questions (MGQs) for the question bank, comparing them to human-composed questions (HCQs). They find that MGQs are more scalable and cost-effective, while HCQs are more precise. The prototype of CRec demonstrates how their three-step approach effectively brings relevant legal knowledge to laypersons. <div>
arXiv:2505.04132v1 Announce Type: new 
Abstract: Access to legal information is fundamental to access to justice. Yet accessibility refers not only to making legal documents available to the public, but also rendering legal information comprehensible to them. A vexing problem in bringing legal information to the public is how to turn formal legal documents such as legislation and judgments, which are often highly technical, to easily navigable and comprehensible knowledge to those without legal education. In this study, we formulate a three-step approach for bringing legal knowledge to laypersons, tackling the issues of navigability and comprehensibility. First, we translate selected sections of the law into snippets (called CLIC-pages), each being a small piece of article that focuses on explaining certain technical legal concept in layperson's terms. Second, we construct a Legal Question Bank (LQB), which is a collection of legal questions whose answers can be found in the CLIC-pages. Third, we design an interactive CLIC Recommender (CRec). Given a user's verbal description of a legal situation that requires a legal solution, CRec interprets the user's input and shortlists questions from the question bank that are most likely relevant to the given legal situation and recommends their corresponding CLIC pages where relevant legal knowledge can be found. In this paper we focus on the technical aspects of creating an LQB. We show how large-scale pre-trained language models, such as GPT-3, can be used to generate legal questions. We compare machine-generated questions (MGQs) against human-composed questions (HCQs) and find that MGQs are more scalable, cost-effective, and more diversified, while HCQs are more precise. We also show a prototype of CRec and illustrate through an example how our 3-step approach effectively brings relevant legal knowledge to the public.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models</title>
<link>https://arxiv.org/abs/2505.04135</link>
<guid>https://arxiv.org/abs/2505.04135</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought prompting, large language models, sentiment categorization, app store reviews, classification accuracy <br />
Summary: <br />
The study investigates the effectiveness of Chain-of-Thought prompting with large language models (LLMs) in improving granular sentiment classification in app store reviews. Traditional rating systems often struggle to capture the nuanced sentiment in user feedback. By testing CoT prompting against simple prompting on 2000 Amazon app reviews, the study found that CoT prompting significantly enhanced classification accuracy from 84% to 93%. This demonstrates the value of explicit reasoning in boosting sentiment analysis performance. The research highlights the potential of leveraging CoT prompting techniques to better understand and categorize sentiment in textual data, such as app reviews. This improvement could have implications for various industries reliant on sentiment analysis, providing more accurate insights into customer opinions and preferences. <div>
arXiv:2505.04135v1 Announce Type: new 
Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety</title>
<link>https://arxiv.org/abs/2505.04146</link>
<guid>https://arxiv.org/abs/2505.04146</guid>
<content:encoded><![CDATA[
<div> Benchmark Dataset, Large Language Model, Image Generation, Vulnerability, Safety Checks

Summary:<br />
- Existing large language models (LLMs) are excelling in image generation tasks but have vulnerable content safety checks, susceptible to prompt-based jailbreaks.
- A new benchmark dataset, Unmasking the Canvas (UTCB), is introduced to evaluate LLM vulnerability in image generation using structured prompt engineering and multilingual obfuscation.
- The methodology includes zero-shot and fallback prompting strategies, risk scoring, and automated tagging, with data categorized into Bronze, Silver, and Gold tiers based on verification levels.
- The dataset is designed to evolve over time with new data sources, prompt templates, and model behaviors to ensure comprehensive evaluation.
- The paper includes visual examples of adversarial inputs for testing model safety, with redacted outputs for responsible disclosure.

Summary:<br />
This paper presents the Unmasking the Canvas (UTCB) benchmark dataset to assess large language model vulnerability in image generation tasks. The dataset utilizes structured prompt engineering, multilingual obfuscation techniques, and evaluation tools to categorize generated outputs based on verification levels. With the inclusion of various features like zero-shot prompting strategies and automated tagging, UTCB aims to evolve continuously to enhance LLM evaluation. The paper also showcases visual examples of adversarial inputs for responsible disclosure and highlights the importance of robust content safety checks in advancing large language models. <div>
arXiv:2505.04146v1 Announce Type: new 
Abstract: Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Language Models Understand Social Behavior in Clinical Conversations?</title>
<link>https://arxiv.org/abs/2505.04152</link>
<guid>https://arxiv.org/abs/2505.04152</guid>
<content:encoded><![CDATA[
<div> Keywords: social signals, patient-provider communication, large language models, clinical dialogue, healthcare settings <br />
Summary: <br />
- Effective communication between healthcare providers and patients plays a crucial role in influencing health and care outcomes. <br />
- Social signals, conveyed through non-verbal cues, are important for shaping the patient-provider relationship. <br />
- Recent advancements in large language models have shown promise in inferring emotional and social behaviors from textual information. <br />
- Automation in clinical settings, such as transcription of patient-provider conversations, can leverage large language models to analyze and extract social behaviors. <br />
- This study designed task-specific prompts to evaluate the performance of large language models in tracking 20 distinct social signals in clinical dialogue, providing insights for enhancing their performance in healthcare settings. <br /> <div>
arXiv:2505.04152v1 Announce Type: new 
Abstract: Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Independent Adaptive RAG: Let the Question Speak for Itself</title>
<link>https://arxiv.org/abs/2505.04253</link>
<guid>https://arxiv.org/abs/2505.04253</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Hallucinations, Retrieval-Augmented Generation, Adaptive retrieval, QA performance

Summary:
In this study, the focus is on addressing hallucinations in Large Language Models (LLMs) through Adaptive retrieval methods without relying on LLM-based uncertainty estimation. The use of external information to guide retrieval processes is explored, with 27 features organized into 7 groups and various hybrid combinations tested. The evaluation was conducted on 6 question-answering (QA) datasets to assess both the performance and efficiency of the proposed methods. The results indicate that the lightweight LLM-independent adaptive retrieval methods not only match the performance of complex LLM-based approaches but also achieve significant efficiency gains. This study highlights the potential of leveraging external information for improving adaptive retrieval processes in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.04253v1 Announce Type: new 
Abstract: Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance</title>
<link>https://arxiv.org/abs/2505.04284</link>
<guid>https://arxiv.org/abs/2505.04284</guid>
<content:encoded><![CDATA[
<div> dataset, pharmacovigilance, adverse drug events, cancer treatment, summarization <br />
Summary: 
This study introduces the task of summarizing adverse drug events reported by cancer patients using prescribed drugs. The MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset is introduced, containing pharmacovigilance posts with extracted labels for drug names, adverse reactions, severity, and adversity levels, as well as summaries of ADEs for each drug. The Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework is presented, combining Large Language Models (LLMs) with the encoder-decoder T5 model to generate summaries of patient concerns. Alignment techniques, including Direct Preference Optimization, are applied to improve performance. Extensive experiments demonstrate the superiority of GASCADE in summarization tasks, validated through automated assessments and human evaluations. This approach enhances drug-related decision-making, improves understanding of patient concerns, and contributes to personalized cancer care advancements. The code and dataset used in the study are publicly available. 

<br /><br />Summary: <div>
arXiv:2505.04284v1 Announce Type: new 
Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Aloe Family Recipe for Open and Specialized Healthcare LLMs</title>
<link>https://arxiv.org/abs/2505.04388</link>
<guid>https://arxiv.org/abs/2505.04388</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Healthcare, Data Preprocessing, Model Safety, Model Efficacy

Summary:
The article introduces Aloe Beta, an open-source medical Large Language Model (LLM) that aims to optimize data preprocessing and training stages while enhancing model safety and efficacy through Direct Preference Optimization (DPO) and RAG. The evaluation methodology includes four types of tests, setting a new standard for the field. The Aloe Family models, built on top of strong base models, demonstrate competitive performance across healthcare benchmarks and are preferred by professionals. They also show improved safety on bias and toxicity, with resilience to unseen attacks. Recommendations are provided for the entire pipeline, and a detailed risk assessment specific to healthcare accompanies the Aloe Family models. Overall, the Aloe Beta models and their development process significantly contribute to open-source medical LLMs, offering top-tier performance while meeting ethical standards. 

<br /><br />Summary: <div>
arXiv:2505.04388v1 Announce Type: new 
Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters</title>
<link>https://arxiv.org/abs/2505.04393</link>
<guid>https://arxiv.org/abs/2505.04393</guid>
<content:encoded><![CDATA[
<div> political bias, large language models, Wahl-O-Mat, left-leaning parties, misinformation 

Summary: 
The study investigates political bias in large language models (LLMs) and its implications on users. LLMs, used widely for information retrieval, exhibit biases towards left-leaning parties, particularly in larger models. The language used to communicate with the models influences their political preferences. The study compares LLMs alignment scores with the recent vote of the German Bundestag using the Wahl-O-Mat metric, revealing a predisposition for left-leaning parties. The results suggest that LLMs are susceptible to political bias, which can impact voter decision-making and shape public opinion. Large corporations developing LLMs have a responsibility to address these biases to ensure responsible use and mitigate misinformation dissemination. <div>
arXiv:2505.04393v1 Announce Type: new 
Abstract: With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YABLoCo: Yet Another Benchmark for Long Context Code Generation</title>
<link>https://arxiv.org/abs/2505.04406</link>
<guid>https://arxiv.org/abs/2505.04406</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, code generation, benchmark, repositories, C, C++<br />
Summary:<br />
This paper introduces a new benchmark, YABLoCo, for evaluating code generation in large repositories using Large Language Models (LLMs). The benchmark includes 215 functions from four large repositories, containing metadata, contexts, docstrings, function bodies, and call graphs. It focuses on generating function bodies in C and C++, languages not covered in previous benchmarks, within repositories ranging from 200K to 2,000K lines of code. Additionally, the paper presents a scalable evaluation pipeline for efficient metric computation and a tool for visual analysis of generated code. This benchmark aims to bridge the gap between the small to medium-sized context windows typically used in LLM evaluations and the real-world scale of repositories, providing a comprehensive evaluation of LLM performance in large C and C++ codebases. <br /><br />Summary: <div>
arXiv:2505.04406v1 Announce Type: new 
Abstract: Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.04416</link>
<guid>https://arxiv.org/abs/2505.04416</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, unlearning framework, sensitive content, data removal, model utility

Summary:
OBLIVIATE is a new framework designed to help large language models (LLMs) remove sensitive, copyrighted, or toxic content while maintaining model utility. The framework follows a structured process of extracting target tokens, building retain sets, and fine-tuning with a tailored loss function that includes masking, distillation, and world fact components. Efficiency is ensured through the use of low-rank adapters (LoRA). Experimental results on datasets like the Harry Potter series, WMDP, and TOFU show that OBLIVIATE effectively resists membership inference attacks, minimizes the impact on retained data, and maintains robustness across diverse scenarios. Key metrics such as forget quality, model utility, and fluency were used to evaluate the framework's performance. OBLIVIATE presents a promising solution to the challenge of managing sensitive content in large language models. 

<br /><br />Summary: 
- OBLIVIATE is an unlearning framework for large language models that removes sensitive content while preserving model utility.
- The framework uses a structured process and a tailored loss function with masking, distillation, and world fact components.
- Efficiency is maintained through the use of low-rank adapters (LoRA).
- Experimental results show that OBLIVIATE effectively resists membership inference attacks and maintains robustness across diverse scenarios.
- Key metrics such as forget quality, model utility, and fluency were used to evaluate the framework's performance. <div>
arXiv:2505.04416v1 Announce Type: new 
Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts</title>
<link>https://arxiv.org/abs/2505.04507</link>
<guid>https://arxiv.org/abs/2505.04507</guid>
<content:encoded><![CDATA[
<div> Keyword: natural language texts, fine-tuning datasets, generative models, linguistic anomaly detection, RUPOR dataset <br />
Summary:
The quality of natural language texts in fine-tuning datasets significantly impacts the performance of generative models in creative tasks like poem generation. Poorly written texts can hinder the effectiveness of these models. To address this issue, automated linguistic anomaly detection methods are proposed to identify and filter out low-quality texts from training datasets. The comparison between unsupervised and supervised anomaly detection approaches is conducted using synthetic and human-labeled datasets. The RUPOR dataset, consisting of Russian-language human-labeled poems, is introduced for cross-sentence grammatical error detection. The evaluation code for the dataset is provided, aiming to assist the community in enhancing the quality of training datasets for generative models in creative domains. <br /><br />Summary: <div>
arXiv:2505.04507v1 Announce Type: new 
Abstract: The quality of natural language texts in fine-tuning datasets plays a critical role in the performance of generative models, particularly in computational creativity tasks such as poem or song lyric generation. Fluency defects in generated poems significantly reduce their value. However, training texts are often sourced from internet-based platforms without stringent quality control, posing a challenge for data engineers to manage defect levels effectively.
  To address this issue, we propose the use of automated linguistic anomaly detection to identify and filter out low-quality texts from training datasets for creative models. In this paper, we present a comprehensive comparison of unsupervised and supervised text anomaly detection approaches, utilizing both synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a collection of Russian-language human-labeled poems designed for cross-sentence grammatical error detection, and provide the full evaluation code. Our work aims to empower the community with tools and insights to improve the quality of training datasets for generative models in creative domains.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs</title>
<link>https://arxiv.org/abs/2505.04519</link>
<guid>https://arxiv.org/abs/2505.04519</guid>
<content:encoded><![CDATA[
<div> Sparse large language models, Mixture of Experts, Ascend NPUs, performance optimization, expert parallelism<br />
Summary:<br />
The paper discusses the challenges of implementing large language models with Mixture of Experts on Ascend NPUs due to their massive scale. Through simulation, the study identifies optimal model configurations for the hardware. This leads to the development of Pangu Ultra MoE, a sparse LLM with 718 billion parameters. The research focuses on enhancing computing resource utilization and system efficiency, utilizing Expert Parallelism to reduce synchronization overhead and optimizing memory management within devices. Experimental results show a Mean Field Unit utilization of 30.0% when training Pangu Ultra MoE on 6K Ascend NPUs, achieving performance comparable to DeepSeek R1. The study demonstrates the Ascend system’s capability to effectively train state-of-the-art language models, providing insights into the behaviors and efficiency of training large-scale sparse LLMs with MoE. <div>
arXiv:2505.04519v1 Announce Type: new 
Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review</title>
<link>https://arxiv.org/abs/2505.04531</link>
<guid>https://arxiv.org/abs/2505.04531</guid>
<content:encoded><![CDATA[
<div> Keywords: generative language modelling, low-resource languages, data scarcity, transformer-based models, linguistic diversity 

Summary: 
This paper reviews strategies for addressing data scarcity in generative language modelling for low-resource languages (LRL). It examines techniques such as monolingual data augmentation, back-translation, multilingual training, and prompt engineering across generative tasks. The analysis reveals a heavy reliance on transformer-based models, a focus on a limited number of LRLs, and inconsistency in evaluation methods. The study aims to encourage the development of more inclusive AI tools for underrepresented languages, emphasizing the importance of linguistic diversity in a world increasingly influenced by large-scale language technologies. The paper offers recommendations for expanding these strategies to a wider range of LRLs and outlines key challenges in achieving equitable generative language systems. This systematic review provides valuable insights for researchers and developers working towards empowering LRL speakers and preserving linguistic diversity in the field of natural language processing. 

<br /><br />Summary: <div>
arXiv:2505.04531v1 Announce Type: new 
Abstract: Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroSearch: Incentivize the Search Capability of LLMs without Searching</title>
<link>https://arxiv.org/abs/2505.04588</link>
<guid>https://arxiv.org/abs/2505.04588</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, information searching, ZeroSearch, document retrieval <br />
<br />
Summary: ZeroSearch is introduced as a reinforcement learning framework to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses the challenges of unpredictable document quality and high API costs associated with RL training. ZeroSearch first fine-tunes the LLM into a retrieval module that can generate relevant and noisy documents in response to queries. During RL training, a curriculum-based rollout strategy progressively challenges the model's reasoning ability by degrading the quality of generated documents. Experimental results show that ZeroSearch effectively incentivizes LLM search capabilities, with a 14B retrieval module outperforming a real search engine. The framework generalizes across LLMs of various sizes and is compatible with different RL algorithms. <div>
arXiv:2505.04588v1 Announce Type: new 
Abstract: Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator</title>
<link>https://arxiv.org/abs/2505.03786</link>
<guid>https://arxiv.org/abs/2505.03786</guid>
<content:encoded><![CDATA[
<div> distilled reasoning model, Large Language Models, text-to-SQL task, discrimination performance, logical capabilities <br />
Summary:
The study evaluates a distilled reasoning model against non-reasoning Large Language Models in a planning framework for the text-to-SQL task. The hypothesis that reasoning models are superior discriminators is supported by results showing the reasoning model outperforming non-reasoning models in F1 score and discrimination accuracy. However, there is a limit to the logical capabilities of reasoning models, and simply increasing context or compute does not enhance discrimination performance. The study also reveals that reasoning models struggle more with generation tasks compared to discrimination tasks and may underperform as generators. These findings underscore the potential of reasoning models as effective discriminators in planning frameworks, emphasizing their strengths in discrimination over generation tasks. <br /> 
Summary: <div>
arXiv:2505.03786v1 Announce Type: cross 
Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</title>
<link>https://arxiv.org/abs/2505.03799</link>
<guid>https://arxiv.org/abs/2505.03799</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Graph Language Models, Graph Neural Networks, Node Classification, Link Prediction 

Summary: 
SDM-InstructGLM proposes a novel framework for enhancing scalability and efficiency in graph-related tasks using Large Language Models (LLMs) without requiring Graph Neural Networks (GNNs). The method introduces a biased random walk mechanism based on node-feature similarity and degree centrality to encode graph information within the LLM. This improves token efficiency, reduces information loss, and enhances performance in node classification and link prediction tasks. The results show that LLM-only graph processing is feasible, enabling scalable and interpretable Graph Language Models (GLMs) through instruction-based fine-tuning. This approach opens up new possibilities for graph learning without the need for GNNs, utilizing LLMs as standalone graph reasoning models.<br /><br />Summary: <div>
arXiv:2505.03799v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free</title>
<link>https://arxiv.org/abs/2505.03810</link>
<guid>https://arxiv.org/abs/2505.03810</guid>
<content:encoded><![CDATA[
<div> Walsh-Hadamard transform, Post-Training Quantization, Large Language Models, Rotation-based methods, Grouped Sequency-arranged Rotation<br />
<br />
Summary: 
The study introduces a novel approach to improving rotation matrices for quantization in Large Language Models (LLMs). By utilizing the Walsh-Hadamard transform with sequency ordering, it effectively reduces quantization errors by clustering similar frequency components. This method, known as Grouped Sequency-arranged Rotation (GSR), employs block-diagonal matrices with smaller Walsh blocks to isolate outlier impacts without the need for training. The proposed technique demonstrates robust performance on reasoning tasks and Perplexity (PPL) scores on WikiText-2, even outperforming existing learned rotation methods. The GSR approach provides a viable solution for addressing deployment challenges faced by LLMs at very low bit-widths like 2-bit, offering comparable performance to optimization-based methods but without the training requirement. <div>
arXiv:2505.03810v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs</title>
<link>https://arxiv.org/abs/2505.03814</link>
<guid>https://arxiv.org/abs/2505.03814</guid>
<content:encoded><![CDATA[
<div> framework evaluation LLMs confidence intervals test sample complexity Cer-Eval

Summary:<br />
The paper introduces a certifiable and cost-efficient evaluation framework for large language models (LLMs) that adapts to different evaluation objectives and provides confidence intervals for true values. Test sample complexity is used to determine the number of test points needed for accurate evaluation, with Cer-Eval, a partition-based algorithm, minimizing the cost of evaluation by adaptively selecting test points. Real-world experiments show that Cer-Eval can save 20% to 40% of test points across various benchmarks while maintaining accuracy levels similar to current evaluation processes and providing a 95% confidence guarantee. <div>
arXiv:2505.03814v1 Announce Type: cross 
Abstract: As foundation models continue to scale, the size of trained models grows exponentially, presenting significant challenges for their evaluation. Current evaluation practices involve curating increasingly large datasets to assess the performance of large language models (LLMs). However, there is a lack of systematic analysis and guidance on determining the sufficiency of test data or selecting informative samples for evaluation. This paper introduces a certifiable and cost-efficient evaluation framework for LLMs. Our framework adapts to different evaluation objectives and outputs confidence intervals that contain true values with high probability. We use ``test sample complexity'' to quantify the number of test points needed for a certifiable evaluation and derive tight bounds on test sample complexity. Based on the developed theory, we develop a partition-based algorithm, named Cer-Eval, that adaptively selects test points to minimize the cost of LLM evaluation. Real-world experiments demonstrate that Cer-Eval can save 20% to 40% test points across various benchmarks, while maintaining an estimation error level comparable to the current evaluation process and providing a 95% confidence guarantee.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective</title>
<link>https://arxiv.org/abs/2505.03828</link>
<guid>https://arxiv.org/abs/2505.03828</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, recommendation systems, e-commerce, natural language processing, deep learning<br />
Summary:<br />
This paper reviews recent advancements in sentiment-aware recommendation systems for e-commerce platforms. It emphasizes the importance of incorporating sentiment analysis into recommendation engines to improve prediction accuracy and enhance explainability. The study categorizes current research into four main approaches: deep learning classifiers, transformer-based methods, graph neural networks, and conversational recommenders. It discusses how sentiment influences recommendation pipelines and impacts dialogue-based suggestions. Key challenges include dealing with noisy or sarcastic text, dynamic user preferences, and bias mitigation. The paper concludes by outlining research gaps and proposing a roadmap for the development of more intelligent, fair, and user-centric recommendation tools. <div>
arXiv:2505.03828v1 Announce Type: cross 
Abstract: E-commerce platforms generate vast volumes of user feedback, such as star ratings, written reviews, and comments. However, most recommendation engines rely primarily on numerical scores, often overlooking the nuanced opinions embedded in free text. This paper comprehensively reviews sentiment-aware recommendation systems from a natural language processing perspective, covering advancements from 2023 to early 2025. It highlights the benefits of integrating sentiment analysis into e-commerce recommenders to enhance prediction accuracy and explainability through detailed opinion extraction. Our survey categorizes recent work into four main approaches: deep learning classifiers that combine sentiment embeddings with user item interactions, transformer based methods for nuanced feature extraction, graph neural networks that propagate sentiment signals, and conversational recommenders that adapt in real time to user feedback. We summarize model architectures and demonstrate how sentiment flows through recommendation pipelines, impacting dialogue-based suggestions. Key challenges include handling noisy or sarcastic text, dynamic user preferences, and bias mitigation. Finally, we outline research gaps and provide a roadmap for developing smarter, fairer, and more user-centric recommendation tools.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete</title>
<link>https://arxiv.org/abs/2505.03961</link>
<guid>https://arxiv.org/abs/2505.03961</guid>
<content:encoded><![CDATA[
<div> collaboration, narratives, negotiation, LLM agents, public goods game 
Summary: 
- The study investigates the impact of shared narratives on collaboration among LLM agents in a public goods game.
- Story-based priming influences negotiation behavior and outcomes.
- Common narratives enhance collaboration, leading to mutual benefits for the agents.
- Different narratives among agents result in a shift towards self-interest, undermining collaboration.
- The study suggests implications for multi-agent system design and AI alignment. 
<br /><br />Summary: <div>
arXiv:2505.03961v1 Announce Type: cross 
Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quiet Feature Learning in Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2505.03997</link>
<guid>https://arxiv.org/abs/2505.03997</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based language models, algorithmic tasks, phase transitions, internal representations, performance gain<br />
Summary:<br />
The study focuses on training Transformer-based language models on foundational algorithmic tasks and observes distinct phase transitions in their loss curves, deviating from typical power-law scaling trends. Unlike expected incremental progress, the models exhibit stagnant phases where validation loss barely improves, followed by abrupt drops indicating rapid learning. Internal representation analysis shows the acquisition of significant features coinciding with these drops, highlighting their crucial role in task performance. Ablation experiments reinforce this by demonstrating the dramatic impact of disrupting a single learned feature on overall model performance. These findings challenge prevailing assumptions, suggesting that key internal features develop quietly before triggering a sudden performance improvement, rather than relying solely on next-token predictive loss tracking progress.<br /> 
Summary: <div>
arXiv:2505.03997v1 Announce Type: cross 
Abstract: We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models' internal representations reveals the learning of quiet features during the stagnant phase, followed by sudden acquisition of loud features that coincide with the sharp drop in loss. Our ablation experiments show that disrupting a single learned feature can dramatically degrade performance, providing evidence of their causal role in task performance. These findings challenge the prevailing assumption that next-token predictive loss reliably tracks incremental progress; instead, key internal features may be developing below the surface until they coalesce, triggering a rapid performance gain.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLAMAPIE: Proactive In-Ear Conversation Assistants</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
<div> Keywords: LlamaPIE, real-time proactive assistant, hearable devices, dialogue dataset, user studies <br />
Summary: <br />
LlamaPIE is introduced as a real-time proactive assistant designed to provide discreet guidance during human conversations through hearable devices. Unlike traditional language models, LlamaPIE operates in the background, anticipating user needs without interrupting discussions. The assistant addresses challenges such as determining optimal response timing, generating concise responses to enhance conversations, utilizing user knowledge for context-aware assistance, and enabling real-time processing on device. A two-model pipeline is proposed, with a small model deciding when to respond and a larger model generating responses. Evaluations on real-world datasets demonstrate the effectiveness of LlamaPIE in offering unobtrusive assistance. User studies conducted on Apple Silicon M2 hardware show a strong user preference for LlamaPIE over a baseline with no assistance and a reactive model, highlighting its potential to enhance live conversations. <br />  <div>
arXiv:2505.04066v1 Announce Type: cross 
Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts</title>
<link>https://arxiv.org/abs/2505.04171</link>
<guid>https://arxiv.org/abs/2505.04171</guid>
<content:encoded><![CDATA[
<div> LLMs, transformational technology, biases, political, influence <br />
Summary: Large Language Models (LLMs) are changing how people access information and engage with the world. Research has found that LLMs have small overall political biases, similar to moderate voters, but they can have extreme views on specific topics. A study comparing LLMs to legislators and judges revealed that they can influence political preferences, with users interacting with an LLM chatbot being more likely to express similar views. This influence is not affected by familiarity with LLMs, news consumption, or interest in politics. The findings suggest that LLMs, particularly those controlled by private companies or governments, could serve as potent tools for targeted political persuasion. <div>
arXiv:2505.04171v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.04192</link>
<guid>https://arxiv.org/abs/2505.04192</guid>
<content:encoded><![CDATA[
<div> Keywords: VideoPath-LLaVA, computational pathology, multimodal model, diagnostic reasoning, histopathology videos<br />
Summary: <br />
VideoPath-LLaVA is a large multimodal model in computational pathology that integrates single patch images, keyframe-extracted clips, and manually segmented video pathology images. It mimics the natural diagnostic process of pathologists, generating detailed histological descriptions and definitive sign-out diagnoses. The model is trained on the VideoPath-Instruct dataset, containing video and diagnosis-specific instructional pairs sourced from histopathology videos. Knowledge transfer from single-image instruction datasets is utilized to train on weakly annotated clips before fine-tuning on manually segmented videos. VideoPath-LLaVA sets a new benchmark in pathology video analysis, providing a foundation for AI systems supporting clinical decision-making through visual and diagnostic reasoning. The code, data, and model are publicly available on GitHub for further research and development. <div>
arXiv:2505.04192v1 Announce Type: cross 
Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube. Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning. Our code, data, and model are publicly available at https://github.com/trinhvg/VideoPath-LLaVA.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models, Multi-Agent Systems, decentralized coordination, swarm intelligence  
Summary:  
SwarmBench is introduced as a novel benchmark for evaluating Large Language Models (LLMs) acting as decentralized agents in Multi-Agent Systems (MAS). The benchmark features tasks within a 2D grid environment with constraints on local sensory input and communication. Various LLMs were evaluated in a zero-shot setting, revealing significant performance variations across tasks. While some coordination emerged, the results highlighted limitations in robust planning and strategy formation under uncertainty in decentralized scenarios. Assessing LLMs under swarm-like conditions is essential for realizing their potential in future decentralized systems. SwarmBench is released as an open, extensible toolkit for reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. <div>
arXiv:2505.04364v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration</title>
<link>https://arxiv.org/abs/2505.04457</link>
<guid>https://arxiv.org/abs/2505.04457</guid>
<content:encoded><![CDATA[
<div> model-based speech restoration, training data cleaning, generative models, large language models, Miipher-2

Summary:
Miipher-2 is an advanced speech restoration model designed for large-scale data cleaning tasks, particularly for training large generative models like language models. It addresses challenges such as generalization to unseen languages, operation without explicit conditioning, and computational efficiency. Utilizing a pre-trained Universal Speech Model for feature extraction and incorporating parallel adapters for efficient prediction, Miipher-2 outperforms conventional models in various metrics across multiple languages. It also boasts superior efficiency, achieving real-time processing of massive datasets using consumer-grade accelerators. <div>
arXiv:2505.04457v1 Announce Type: cross 
Abstract: Training data cleaning is a new application for generative model-based speech restoration (SR). This paper introduces Miipher-2, an SR model designed for million-hour scale data, for training data cleaning for large-scale generative models like large language models. Key challenges addressed include generalization to unseen languages, operation without explicit conditioning (e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a frozen, pre-trained Universal Speech Model (USM), supporting over 300 languages, as a robust, conditioning-free feature extractor. To optimize efficiency and minimize memory, Miipher-2 incorporates parallel adapters for predicting clean USM features from noisy inputs and employs the WaneFit neural vocoder for waveform synthesis. These components were trained on 3,000 hours of multi-lingual, studio-quality recordings with augmented degradations, while USM parameters remained fixed. Experimental results demonstrate Miipher-2's superior or comparable performance to conventional SR models in word-error-rate, speaker similarity, and both objective and subjective sound quality scores across all tested languages. Miipher-2 operates efficiently on consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling the processing of a million-hour speech dataset in approximately three days using only 100 such accelerators.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving</title>
<link>https://arxiv.org/abs/2505.04528</link>
<guid>https://arxiv.org/abs/2505.04528</guid>
<content:encoded><![CDATA[
<div> formulation, process-level verifiability, AI-based problem-solving agents, FPS framework, D-FPS framework  
Summary:  
- This article introduces a principled formulation of problem-solving as a deterministic Markov decision process and presents a novel framework called FPS for process-verified problem-solving using existing formal theorem proving environments.
- The D-FPS framework separates solving and answer verification for better human alignment.
- The frameworks are proven to be expressive, sound, and complete.
- Three benchmarks on problem-solving are constructed: FormalMath500, MiniF2F-Solving, and PutnamBench-Solving.
- The RPE approach is proposed for evaluating the correctness of answers by formal verification.
- Evaluation results show that prevalent FTP models and prompting methods as baselines achieve limited success in solving the benchmarks provided. 

<br /><br />Summary: <div>
arXiv:2505.04528v1 Announce Type: cross 
Abstract: As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing repeated games with Large Language Models</title>
<link>https://arxiv.org/abs/2305.16867</link>
<guid>https://arxiv.org/abs/2305.16867</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, behavioural game theory, cooperation, coordination, GPT-4<br />
Summary:<br />
LLMs are being used in various applications involving interactions with humans and other agents. This study proposes analyzing LLMs' cooperation and coordination behavior using behavioral game theory. The research involves LLMs playing finite repeated 2x2 games with human-like strategies and actual human players. Results indicate that LLMs excel in self-interested games such as the iterated Prisoner's Dilemma but struggle in coordination games like the Battle of the Sexes. These behavioral patterns remain consistent across robustness checks. Furthermore, it is demonstrated that providing additional information about opponents and utilizing the "social chain-of-thought" (SCoT) strategy can enhance GPT-4's performance and coordination with human players. This study contributes to understanding LLMs' social behavior and lays the groundwork for a behavioral game theory approach for machines. <br /><br />Summary: <div>
arXiv:2305.16867v2 Announce Type: replace 
Abstract: LLMs are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a "social chain-of-thought" (SCoT) strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</title>
<link>https://arxiv.org/abs/2308.15022</link>
<guid>https://arxiv.org/abs/2308.15022</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, conversation, dialogue, memory, consistency

Summary:
Our method proposes a way to enhance the long-term memory ability of large language models (LLMs) like GPT-4, which often struggle with recalling past information in long conversations. By recursively generating summaries or memory using LLMs, we aim to improve the chatbot's response consistency. The method involves stimulating LLMs to memorize small dialogue contexts and then recursively generating new memory based on previous memory and following contexts. This approach enables the chatbot to produce more consistent responses by utilizing the latest memory. Experimental evaluations on both open and closed LLMs using a public dataset demonstrate that our method can enhance response consistency in long-context conversations. Additionally, our strategy complements long-context and retrieval-enhanced LLMs, leading to improved long-term dialogue performance. This method shows promise in enabling LLMs to model extremely long contexts, offering a potential solution to the challenge of long-term dialogue coherence. The code and scripts for our method will be made available for further exploration and application. 

<br /><br />Summary: Our method enhances long-term memory in large language models for improved response consistency in long conversations. By recursively generating summaries/memories based on small dialogue contexts, the chatbot can recall past information and generate more consistent responses using the latest memory. Experimental results show enhanced performance on open and closed LLMs, complementing long-context and retrieval-enhanced models for better long-term dialogue consistency. This approach presents a solution for LLMs to model extremely long contexts and offers potential advancements in dialogue generation capabilities. <div>
arXiv:2308.15022v3 Announce Type: replace 
Abstract: Recently, large language models (LLMs), such as GPT-4, stand out remarkable conversational abilities, enabling them to engage in dynamic and contextually relevant dialogues across a wide range of topics. However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses. To address this, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the chatbot can easily generate a highly consistent response with the help of the latest memory. We evaluate our method on both open and closed LLMs, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Also, we show that our strategy could nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced LLMs, bringing further long-term dialogue performance. Notably, our method is a potential solution to enable the LLM to model the extremely long context. The code and scripts will be released later.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Are Struggle to Cope with Unreasonability in Math Problems</title>
<link>https://arxiv.org/abs/2403.19346</link>
<guid>https://arxiv.org/abs/2403.19346</guid>
<content:encoded><![CDATA[
<div> math, reasoning, LLMs, benchmark, unreasonable
Summary:
- Recent research has shown the impressive abilities of Large Language Models (LLMs) in math and reasoning tasks.
- A new benchmark called Unreasonable Math Problem (UMP) has been introduced to evaluate LLMs' capacity to handle unreasonable math problems.
- The UMP benchmark includes a range of math questions with internal inconsistencies and flawed assumptions.
- Experimentation with 19 LLMs, including state-of-the-art models like GPT-4o, revealed limited performance in recognizing and responding to unreasonability.
- DeepSeek-R1, a reasoning model, exhibited tendencies to overthink and instability in the face of unreasonable inputs.
<br /><br />Summary: 
Recent research highlights the remarkable abilities of LLMs in math and reasoning, but their performance under unconventional conditions like internal inconsistencies and flawed assumptions is largely unexplored. The introduction of the UMP benchmark aims to assess LLMs' ability to recognize and deal with unreasonability in math problems. Despite testing multiple LLMs, including advanced models like GPT-4o, their performance in the UMP benchmark was limited. Additionally, reasoning models like DeepSeek-R1 exhibited overthinking tendencies and instability. These findings shed light on the potential and challenges of LLMs in handling unreasonable math problems. <div>
arXiv:2403.19346v4 Announce Type: replace 
Abstract: Recent research have demonstrated LLMs' impressive performance in math and reasoning. However, the capacity of LLMs to address math problems under unconventional conditions, such as internal inconsistencies and flawed assumptions, remains largely unexplored. In this paper, we propose a novel benchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to recognize and respond to unreasonability in math problem. The benchmark consists of a carefully curated collection of unreasonable math questions across diverse types. Based on extensive experiments covering 19 LLMs, we observe that even state-of-the-art models such as GPT-4o achieve only limited performance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone to overthinking and unstable. We further explore strategies for improving the recognition of unreasonable inputs, shedding light on both the possibility and limitations of LLMs in this challenging setting.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ReST: Reflection-Reinforced Self-Training for Language Agents</title>
<link>https://arxiv.org/abs/2406.01495</link>
<guid>https://arxiv.org/abs/2406.01495</guid>
<content:encoded><![CDATA[
<div> Keywords: self-training, language agents, Reflection-Reinforced Self-Training, code generation, question answering
<br />
<br />
Summary: 
This paper explores the use of self-training for language agents, avoiding the need for human or stronger model demonstrations. The proposed Reflection-Reinforced Self-Training (Re-ReST) method utilizes a reflector to enhance the quality of generated samples during self-training. Experiments across various language agent tasks show significant improvements in performance, with self-training boosting baselines by 7.6% on HotpotQA and 28.4% on AlfWorld. Re-ReST further enhances performance by 2.0% and 14.1%, respectively. The efficiency of using a reflector to generate high-quality samples is demonstrated, and a method for employing reflection during inference without ground-truth feedback is presented. The code for Re-ReST is available on GitHub, providing a valuable resource for further research and development in the field. <div>
arXiv:2406.01495v3 Announce Type: replace 
Abstract: Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\% on HotpotQA and 28.4\% on AlfWorld, and Re-ReST further boosting performance by 2.0\% and 14.1\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code is released at https://github.com/PlusLabNLP/Re-ReST.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA</title>
<link>https://arxiv.org/abs/2406.02044</link>
<guid>https://arxiv.org/abs/2406.02044</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, adversarial manipulations, QROA, black-box jailbreak method, optimization bandit problem<br />
<br />
Summary: <br />
The paper discusses the vulnerabilities of Large Language Models (LLMs) to adversarial manipulations and introduces QROA, a black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards. Unlike existing approaches, QROA does not require internal information or human-crafted templates, operating solely through the model's query-response interface. By framing the attack as an optimization bandit problem, QROA efficiently explores suffix variations using a surrogate model and token level optimization. The paper also proposes QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling quick jailbreaks across different instructions. Testing on multiple models showed an Attack Success Rate (ASR) over 80%, highlighting critical vulnerabilities and emphasizing the need for advanced defenses in secure AI deployment. The code for QROA is publicly available on GitHub. <br /><br />Summary: <div>
arXiv:2406.02044v3 Announce Type: replace 
Abstract: The rapid adoption of Large Language Models (LLMs) has exposed critical security and ethical vulnerabilities, particularly their susceptibility to adversarial manipulations. This paper introduces QROA, a novel black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards when appended to a malicious instruction. Unlike existing suffix-based jailbreak approaches, QROA does not require access to the model's logit or any other internal information. It also eliminates reliance on human-crafted templates, operating solely through the standard query-response interface of LLMs. By framing the attack as an optimization bandit problem, QROA employs a surrogate model and token level optimization to efficiently explore suffix variations. Furthermore, we propose QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling one-query jailbreaks across a wide range of instructions. Testing on multiple models demonstrates Attack Success Rate (ASR) greater than 80\%. These findings highlight critical vulnerabilities, emphasize the need for advanced defenses, and contribute to the development of more robust safety evaluations for secure AI deployment. The code is made public on the following link: https://github.com/qroa/QROA
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming</title>
<link>https://arxiv.org/abs/2406.18501</link>
<guid>https://arxiv.org/abs/2406.18501</guid>
<content:encoded><![CDATA[
<div> Frequency Effect, In-Context Learning, Error-Driven Learning, Large Language Models, Structural Priming
Summary:
The study investigates whether in-context learning (ICL) in large language models (LLMs) involves error-driven learning mechanisms. By simulating structural priming with ICL, the researchers found that LLMs exhibit the inverse frequency effect (IFE), with a stronger effect in larger models. This suggests that ICL is a form of error-driven learning, indicating that an error signal is implicitly computed during the learning process. The results support the hypothesis that both humans and LLMs utilize error-driven processing mechanisms in online processing. The IFE observed in LLMs provides evidence that error-driven learning plays a role in the in-context learning process, shedding light on the underlying mechanisms of language processing in LLMs. 
<br /><br />Summary: <div>
arXiv:2406.18501v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an agent's behavior is influenced to a greater degree when presented with improbable examples as compared to more likely ones. The IFE has previously been identified in psycholinguistics where humans exhibit the IFE in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently). In that context, the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming with ICL and found that LLMs indeed display the IFE, with the effect being stronger in larger models. We conclude that at least in the case we studied, ICL is indeed a type of error-driven learning, supporting the hypothesis that an error signal is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of error-driven processing mechanisms in on-line processing.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning</title>
<link>https://arxiv.org/abs/2408.13184</link>
<guid>https://arxiv.org/abs/2408.13184</guid>
<content:encoded><![CDATA[
<div> transform, spatial reasoning, large language models, embodied intelligence, Q-learning

Summary:
The study introduces a novel model, S2RCQL, to improve the spatial reasoning capabilities of Large Language Models (LLMs) in maze environments. By utilizing the Spatial-to-Relational approach, spatial prompts are transformed into entity relations and paths, enhancing the sequential thinking abilities of LLMs. A Q-learning-based path-planning algorithm is implemented to mitigate context inconsistency hallucination and improve LLMs' reasoning skills. By incorporating Q-values as auxiliary information for prompts, the model corrects LLMs' hallucinations and guides them towards optimal path planning. Additionally, a reverse curriculum learning technique is proposed to reduce task difficulty and enable LLMs to tackle more complex tasks based on successful experiences. Experimental results using ERNIE-Bot 4.0 show significant improvements in success and optimality rates compared to existing prompt engineering methods. 

<br /><br />Summary: <div>
arXiv:2408.13184v3 Announce Type: replace 
Abstract: Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied intelligence. However, even in simple maze environments, LLMs still encounter challenges in long-term path-planning, primarily influenced by their spatial hallucination and context inconsistency hallucination by long-term reasoning. To address this challenge, this study proposes an innovative model, Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs, we propose the Spatial-to-Relational approach, which transforms spatial prompts into entity relations and paths representing entity relation chains. This approach fully taps the potential of LLMs in terms of sequential thinking. As a result, we design a path-planning algorithm based on Q-learning to mitigate the context inconsistency hallucination, which enhances the reasoning ability of LLMs. Using the Q-value of state-action as auxiliary information for prompts, we correct the hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally, we propose a reverse curriculum learning technique based on LLMs to further mitigate the context inconsistency hallucination. LLMs can rapidly accumulate successful experiences by reducing task difficulty and leveraging them to tackle more complex tasks. We performed comprehensive experiments based on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our S2RCQL achieved a 23%--40% improvement in both success and optimality rates compared with advanced prompt engineering.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements and limitations of LLMs in replicating human color-word associations</title>
<link>https://arxiv.org/abs/2411.02116</link>
<guid>https://arxiv.org/abs/2411.02116</guid>
<content:encoded><![CDATA[
<div> Keywords: Color-word associations, Large Language Models, GPT-4o, Human cognition, Semantic memory structures<br />
Summary:<br />
The study compares different generations of Large Language Models (LLMs) from GPT-3 to GPT-4o with human color-word associations using data from Japanese participants. LLMs showed improvement across generations, with GPT-4o performing best but still only achieving around 50% accuracy. Performance varied across word categories and colors, excelling in some and struggling in others. Color discrimination ability in LLMs correlated with human patterns, indicating alignment in basic color discrimination. However, there were systematic differences in the words assigned to colors between humans and LLMs. This highlights advancements in LLM capabilities but also their persistent limitations, suggesting potential differences in semantic memory structures between humans and LLMs when representing color-word associations.<br /> 
Summary: <div>
arXiv:2411.02116v3 Announce Type: replace 
Abstract: Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution</title>
<link>https://arxiv.org/abs/2501.05040</link>
<guid>https://arxiv.org/abs/2501.05040</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, software engineering, GitHub, SWE-Fixer, code editing module <br />
Summary: <br />
The article introduces SWE-Fixer, an open-source framework designed to resolve GitHub issues efficiently by leveraging large language models (LLMs). SWE-Fixer consists of two modules: a code file retrieval module using BM25 and a code editing module to generate patches. The authors compiled a dataset of 110K GitHub issues with corresponding patches to train the models. SWE-Fixer performs competitively on benchmarks, achieving scores of 22.0% and 30.2%, and surpasses existing models with state-of-the-art performance of 24.7% and 32.8% with P2P filtering. The approach is shown to be efficient, requiring only two model calls per instance. Overall, SWE-Fixer demonstrates effectiveness in real-world code-fixing scenarios and will be made publicly available for further research and development. <br /> 
Summary: <div>
arXiv:2501.05040v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating LLM Uncertainty with Logits</title>
<link>https://arxiv.org/abs/2502.00290</link>
<guid>https://arxiv.org/abs/2502.00290</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucinations, uncertainty estimation, Logits-induced token uncertainty, downstream tasks 

Summary: 
Large Language Models (LLMs) have seen rapid development but often face the issue of generating unreliable responses, known as hallucinations, when lacking relevant knowledge. Current uncertainty estimation methods to detect these hallucinations rely on critical tokens but struggle with accurate reliability assessment. This paper introduces Logits-induced token uncertainty (LogTokU), a framework that decouples token uncertainty estimation in LLMs to provide real-time assessment without the need for multiple sampling processes. By utilizing evidence modeling, LogTokU successfully estimates uncertainty and guides downstream tasks effectively. Experimental results show significant effectiveness and promise for LogTokU in addressing the issue of hallucinations in LLMs. <br /><br />Summary: <div>
arXiv:2502.00290v4 Announce Type: replace 
Abstract: Over the past few years, Large Language Models (LLMs) have developed rapidly and are widely applied in various domains. However, LLMs face the issue of hallucinations, generating responses that may be unreliable when the models lack relevant knowledge. To be aware of potential hallucinations, uncertainty estimation methods have been introduced, and most of them have confirmed that reliability lies in critical tokens. However, probability-based methods perform poorly in identifying token reliability, limiting their practical utility. In this paper, we reveal that the probability-based method fails to estimate token reliability due to the loss of evidence strength information which is accumulated in the training stage. Therefore, we present Logits-induced token uncertainty (LogTokU), a framework for estimating decoupled token uncertainty in LLMs, enabling real-time uncertainty estimation without requiring multiple sampling processes. We employ evidence modeling to implement LogTokU and use the estimated uncertainty to guide downstream tasks. The experimental results demonstrate that LogTokU has significant effectiveness and promise.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Liger: Linearizing Large Language Models to Gated Recurrent Structures</title>
<link>https://arxiv.org/abs/2503.01496</link>
<guid>https://arxiv.org/abs/2503.01496</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, linear recurrent modeling, Liger, Low-Rank Adaptation, Liger Attention

Summary: 
Liger is a new method for converting pretrained large language models (LLMs) into gated linear recurrent models without adding extra parameters. It utilizes pretrained key matrix weights to create diverse gating mechanisms, allowing for the formation of various gated recurrent structures without the need to train additional components from scratch. By leveraging Low-Rank Adaptation (LoRA) for lightweight fine-tuning, Liger can restore the performance of linearized gated recurrent models to match that of the original LLMs. Additionally, Liger introduces Liger Attention, an intra-layer hybrid attention mechanism that efficiently recovers a large portion of the Transformer-based LLM performance during the linearization process. The method has been validated on models with parameters ranging from 1B to 8B across multiple benchmarks, showcasing competitive results. The code for Liger is available on GitHub for further exploration and implementation.<br /><br />Summary: <div>
arXiv:2503.01496v2 Announce Type: replace 
Abstract: Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation</title>
<link>https://arxiv.org/abs/2503.03186</link>
<guid>https://arxiv.org/abs/2503.03186</guid>
<content:encoded><![CDATA[
<div> support, indigenous communities, technology development, risks, community participation
Summary: 
This paper examines the challenges and opportunities in developing speech technologies to support speakers of Australian Aboriginal English, a local variety of English spoken by Indigenous communities. The authors address the risks inherent in such a project and advocate for culturally appropriate and participatory practices to mitigate these risks. By integrating meaningful community participation throughout the technology development process, the researchers demonstrate a case study that highlights the importance of supporting languages used by Indigenous communities. They emphasize the practical economic and socio-cultural benefits of supporting contact varieties of languages, provided that culturally safe practices are implemented.<br /><br />Summary: <div>
arXiv:2503.03186v3 Announce Type: replace 
Abstract: In Australia, post-contact language varieties, including creoles and local varieties of international languages, emerged as a result of forced contact between Indigenous communities and English speakers. These contact varieties are widely used, yet are poorly supported by language technologies. This gap presents barriers to participation in civil and economic society for Indigenous communities using these varieties, and reproduces minoritisation of contemporary Indigenous sociolinguistic identities. This paper concerns three questions regarding this context. First, can speech technologies support speakers of Australian Aboriginal English, a local indigenised variety of English? Second, what risks are inherent in such a project? Third, what technology development practices are appropriate for this context, and how can researchers integrate meaningful community participation in order to mitigate risks? We argue that opportunities do exist -- as well as risks -- and demonstrate this through a case study exploring design practices in a real-world project aiming to improve speech technologies for Australian Aboriginal English. We discuss how we integrated culturally appropriate and participatory processes throughout the project. We call for increased support for languages used by Indigenous communities, including contact varieties, which provide practical economic and socio-cultural benefits, provided that participatory and culturally safe practices are enacted.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Interlingual Representations of Large Language Models</title>
<link>https://arxiv.org/abs/2503.11280</link>
<guid>https://arxiv.org/abs/2503.11280</guid>
<content:encoded><![CDATA[
<div> interlingual constructs, multilingual LLMs, interlingual representation, ILO score, cross-lingual alignment
Summary:
Interlingual constructs in large language models (LLMs) are not consistently aligned across languages. A framework to identify shared interlingual semantic subspace and fragmented components is proposed, along with a metric called Interlingual Local Overlap (ILO) score. Single-language fine-tuning disrupts alignment in early layers but freezing them preserves alignment, improving cross-lingual generalization. The study includes 31 diverse languages and highlights the importance of interlingual alignment in scalable multilingual learning. Based on the results, it is evident that multilingual LLMs exhibit inconsistent cross-lingual alignments, emphasizing the need for a framework and metric like ILO score to evaluate interlingual representations. <div>
arXiv:2503.11280v3 Announce Type: replace 
Abstract: Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching</title>
<link>https://arxiv.org/abs/2503.21813</link>
<guid>https://arxiv.org/abs/2503.21813</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucinations, Language Models, Ontology Matching, Benchmark Dataset, LLM Leaderboard<br />
Summary:<br />
Hallucinations are a common issue faced by large language models (LLMs) in ontology matching tasks. To address this challenge, a new benchmark dataset called OAEI-LLM-T has been introduced, focusing on hallucinations generated by LLMs during ontology matching. The dataset, derived from existing TBox datasets in the Ontology Alignment Evaluation Initiative (OAEI), categorizes LLM-induced hallucinations into two primary categories and six sub-categories. This dataset proves valuable for constructing a leaderboard for LLMs and fine-tuning foundational LLMs for ontology matching systems. The goal is to improve the accuracy and reliability of LLM-based ontology matching systems by addressing and understanding the hallucinations that occur during the process. <div>
arXiv:2503.21813v2 Announce Type: replace 
Abstract: Hallucinations are often inevitable in downstream tasks using large language models (LLMs). To tackle the substantial challenge of addressing hallucinations for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching) datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of different LLMs performing OM tasks. These OM-specific hallucinations are carefully classified into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing the LLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance</title>
<link>https://arxiv.org/abs/2311.18681</link>
<guid>https://arxiv.org/abs/2311.18681</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational AI, radiology report generation, vision-language model, clinical correctness, interactive dialog.

Summary:
<br /><br />
Conversational AI tools integrating visual image features and structured pathology findings with large language models (LLM) have the potential to revolutionize radiology by facilitating collaborative diagnostic processes. RaDialog is the first thoroughly evaluated and publicly available vision-language model for radiology report generation and interactive dialog. By fine-tuning the LLM on a specialized chest X-ray radiology instruct dataset, RaDialog achieves state-of-the-art clinical correctness in report generation. The model also demonstrates impressive abilities in interactive tasks such as correcting reports and answering questions, marking a significant step towards the development of clinical dialog systems. The availability of RaDialog's code on github enables further research and development in this field. <div>
arXiv:2311.18681v3 Announce Type: replace-cross 
Abstract: Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners</title>
<link>https://arxiv.org/abs/2410.02131</link>
<guid>https://arxiv.org/abs/2410.02131</guid>
<content:encoded><![CDATA[
arXiv:2410.02131v3 Announce Type: replace-cross 
Abstract: The accurate interpretation of Electrocardiogram (ECG) signals is pivotal for diagnosing cardiovascular diseases. Integrating ECG signals with accompanying textual reports further holds immense potential to enhance clinical diagnostics by combining physiological data and qualitative insights. However, this integration faces significant challenges due to inherent modality disparities and the scarcity of labeled data for robust cross-modal learning. To address these obstacles, we propose D-BETA, a novel framework that pre-trains ECG and text data using a contrastive masked auto-encoder architecture. D-BETA uniquely combines the strengths of generative with boosted discriminative capabilities to achieve robust cross-modal representations. This is accomplished through masked modality modeling, specialized loss functions, and an improved negative sampling strategy tailored for cross-modal alignment. Extensive experiments on five public datasets across diverse downstream tasks demonstrate that D-BETA significantly outperforms existing methods, achieving an average AUC improvement of 15% in linear probing with only one percent of training data and 2% in zero-shot performance without requiring training data over state-of-the-art models. These results highlight the effectiveness of D-BETA, underscoring its potential to advance automated clinical diagnostics through multi-modal representations. Our sample code and checkpoint are made available at https://github.com/manhph2211/D-BETA.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Create Cross-Modal Task Representations</title>
<link>https://arxiv.org/abs/2410.22330</link>
<guid>https://arxiv.org/abs/2410.22330</guid>
<content:encoded><![CDATA[
arXiv:2410.22330v2 Announce Type: replace-cross 
Abstract: Autoregressive vision-language models (VLMs) can handle many tasks within a single model, yet the representations that enable this capability remain opaque. We find that VLMs align conceptually equivalent inputs into a shared task vector, which is invariant to modality (text, image) and format (examples, instruction), and may simplify VLM processing. We measure this alignment via cross-modal transfer -- the ability of a task vector derived in one modality to trigger the correct generation in another -- on a range of tasks and model architectures. Although the task vector is highly compressed, we find that this single vector outperforms prompting the model with the full task information, unique to this cross-modal case. Furthermore, we show that task vectors can be transferred from a base language model to its fine-tuned vision-language counterpart, and that they can be derived solely from instructions without the need for examples. Taken together, our findings shed light on how VLMs internally process task information, and how they map different modalities into common semantic representations. Project page: https://vlm-cross-modal-reps.github.io.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</title>
<link>https://arxiv.org/abs/2411.04997</link>
<guid>https://arxiv.org/abs/2411.04997</guid>
<content:encoded><![CDATA[
arXiv:2411.04997v4 Announce Type: replace-cross 
Abstract: CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMs' superior text understanding and extensive open-world knowledge can enhance CLIP's capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</title>
<link>https://arxiv.org/abs/2411.05261</link>
<guid>https://arxiv.org/abs/2411.05261</guid>
<content:encoded><![CDATA[
arXiv:2411.05261v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term "cyclic manipulation". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT</title>
<link>https://arxiv.org/abs/2411.10246</link>
<guid>https://arxiv.org/abs/2411.10246</guid>
<content:encoded><![CDATA[
arXiv:2411.10246v3 Announce Type: replace-cross 
Abstract: Collaborative problem solving (CPS) is widely recognized as a critical 21st-century skill. Assessing CPS depends heavily on coding the communication data using a construct-relevant framework, and this process has long been a major bottleneck to scaling up such assessments. Based on five datasets and two coding frameworks, we demonstrate that ChatGPT can code communication data to a satisfactory level, though performance varies across ChatGPT models, and depends on the coding framework and task characteristics. Interestingly, newer reasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily yield better coding results. Additionally, we show that refining prompts based on feedback from miscoded cases can improve coding accuracy in some instances, though the effectiveness of this approach is not consistent across all tasks. These findings offer practical guidance for researchers and practitioners in developing scalable, efficient methods to analyze communication data in support of 21st-century skill assessment.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild</title>
<link>https://arxiv.org/abs/2503.18892</link>
<guid>https://arxiv.org/abs/2503.18892</guid>
<content:encoded><![CDATA[
arXiv:2503.18892v2 Announce Type: replace-cross 
Abstract: DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models</title>
<link>https://arxiv.org/abs/2505.02847</link>
<guid>https://arxiv.org/abs/2505.02847</guid>
<content:encoded><![CDATA[
<div> evaluation framework, large language model, social cognition, emotional changes, empathy metrics  
<br />
Assessing the ability of language models to understand human emotions and social interactions is crucial for their real-world applications. This study introduces the Sentient Agent as a Judge (SAGE) framework, which evaluates large language models based on their higher-order social cognition. SAGE simulates human-like emotional changes and inner thoughts during interactive conversations, providing a more realistic assessment of the model's performance. Experimental results on supportive-dialogue scenarios demonstrate that SAGE's emotion score correlates strongly with established psychological metrics like the Barrett-Lennard Relationship Inventory and empathy metrics at the utterance level. The study also compares the performance of 18 language models on a public Sentient Leaderboard, highlighting significant gaps between advanced models and earlier baselines. SAGE offers a principled, scalable, and interpretable tool for tracking progress towards creating genuinely empathetic and socially adept language agents.  
<br /><br />Summary: <div>
arXiv:2505.02847v1 Announce Type: new 
Abstract: Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors</title>
<link>https://arxiv.org/abs/2505.02850</link>
<guid>https://arxiv.org/abs/2505.02850</guid>
<content:encoded><![CDATA[
<div> Framework, MCQs, Hierarchical concept map, Automated generation, Common misconceptions

Summary:
- The paper introduces a framework for generating high-quality multiple-choice questions (MCQs) efficiently, focusing on diverse cognitive levels and including common misconceptions.
- The framework utilizes a hierarchical concept map in the domain of high-school physics to guide Large Language Models (LLMs) in generating MCQs with targeted distractors.
- An automated pipeline retrieves topic-relevant sections of concept maps to provide structured context for question generation.
- Expert evaluations show that the concept map-driven approach outperforms baseline methods, meeting quality criteria at a significantly higher rate.
- Student assessments indicate that the generated MCQs using the framework result in a lower guess success rate, highlighting effective assessment of conceptual understanding.
<br /><br />Summary: <div>
arXiv:2505.02850v1 Announce Type: new 
Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive levels and incorporating common misconceptions into distractor design, is time-consuming and expertise-intensive, making manual creation impractical at scale. Current automated approaches typically generate questions at lower cognitive levels and fail to incorporate domain-specific misconceptions. This paper presents a hierarchical concept map-based framework that provides structured knowledge to guide LLMs in generating MCQs with distractors. We chose high-school physics as our test domain and began by developing a hierarchical concept map covering major Physics topics and their interconnections with an efficient database design. Next, through an automated pipeline, topic-relevant sections of these concept maps are retrieved to serve as a structured context for the LLM to generate questions and distractors that specifically target common misconceptions. Lastly, an automated validation is completed to ensure that the generated MCQs meet the requirements provided. We evaluate our framework against two baseline approaches: a base LLM and a RAG-based generation. We conducted expert evaluations and student assessments of the generated MCQs. Expert evaluation shows that our method significantly outperforms the baseline approaches, achieving a success rate of 75.20% in meeting all quality criteria compared to approximately 37% for both baseline methods. Student assessment data reveal that our concept map-driven approach achieved a significantly lower guess success rate of 28.05% compared to 37.10% for the baselines, indicating a more effective assessment of conceptual understanding. The results demonstrate that our concept map-based approach enables robust assessment across cognitive levels and instant identification of conceptual gaps, facilitating faster feedback loops and targeted interventions at scale.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation</title>
<link>https://arxiv.org/abs/2505.02851</link>
<guid>https://arxiv.org/abs/2505.02851</guid>
<content:encoded><![CDATA[
<div> Keywords: habit formation, Large Language Models, 30-day challenges, content generation, semantic deduplication

Summary:
The paper introduces 30 Day Me, a habit formation app that utilizes Large Language Models (LLMs) to assist users in breaking down their goals into achievable steps and monitoring their progress. The app features the 30DAYGEN system which offers a wide range of 30-day challenges sourced from various webpages to align with user-defined goals. It showcases the efficient use of LLMs in constructing domain-specific content for behavioral and educational purposes. The proposed pipeline incorporates LLM-enhanced methods for content generation and semantic deduplication to improve user experience and engagement with the app. Overall, the research demonstrates the potential of leveraging LLMs to create personalized and effective habit-forming tools for users seeking to improve their daily routines and achieve their goals.<br /><br />Summary: <div>
arXiv:2505.02851v1 Announce Type: new 
Abstract: In this paper, we present 30 Day Me, a habit formation application that leverages Large Language Models (LLMs) to help users break down their goals into manageable, actionable steps and track their progress. Central to the app is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced from over 15K webpages, and enables runtime search of challenge ideas aligned with user-defined goals. We showcase how LLMs can be harnessed to rapidly construct domain specific content corpora for behavioral and educational purposes, and propose a practical pipeline that incorporates effective LLM enhanced approaches for content generation and semantic deduplication.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets</title>
<link>https://arxiv.org/abs/2505.02854</link>
<guid>https://arxiv.org/abs/2505.02854</guid>
<content:encoded><![CDATA[
<div> benchmark, generative AI systems, reproducibility, regression testing, language models <br />
Summary:
The article introduces GPR-bench, a benchmark for generative AI systems that aims to address reproducibility and reliability challenges. GPR-bench includes an open dataset in English and Japanese, covering various task categories and scenarios. The benchmark uses an automated evaluation pipeline with LLM-as-a-Judge scoring for correctness and conciseness. Experiments with three model versions and two prompt configurations show modest improvements in correctness with newer models but significant enhancements in conciseness with concise-writing instructions. The results suggest that GPR-bench may not effectively differentiate between recent model versions but highlight the effectiveness of prompt engineering. The benchmark is released under the MIT License, providing a foundation for reproducibility monitoring and prompting discussions on benchmark design for evolving language models. <br /><br />Summary: <div>
arXiv:2505.02854v1 Announce Type: new 
Abstract: Reproducibility and reliability remain pressing challenges for generative AI systems whose behavior can drift with each model update or prompt revision. We introduce GPR-bench, a lightweight, extensible benchmark that operationalizes regression testing for general purpose use cases. GPR-bench couples an open, bilingual (English and Japanese) dataset covering eight task categories (e.g., text generation, code generation, and information retrieval) and 10 scenarios in each task categories (80 total test cases for each language) with an automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of correctness and conciseness. Experiments across three recent model versions - gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default versus concise-writing instruction) reveal heterogeneous quality. Our results show that newer models generally improve correctness, but the differences are modest and not statistically significant, suggesting that GPR-bench may not be sufficiently challenging to differentiate between recent model versions. In contrast, the concise-writing instruction significantly enhances conciseness (+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of prompt engineering. Released under the MIT License, GPR- bench lowers the barrier to initiating reproducibility monitoring and provides a foundation for community-driven extensions, while also raising important considerations about benchmark design for rapidly evolving language models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models</title>
<link>https://arxiv.org/abs/2505.02858</link>
<guid>https://arxiv.org/abs/2505.02858</guid>
<content:encoded><![CDATA[
<div> Keywords: social media datasets, large language models, multi-platform, synthetic data, fidelity metrics

Summary:
This paper discusses the challenges of accessing social media datasets for research purposes due to costs and platform restrictions. The study explores the use of large language models to generate synthetic social media datasets across multiple platforms, aiming to match the quality of real data. By employing multi-platform topic-based prompting, the researchers generated synthetic data using various language models and compared the lexical and semantic properties with real data. The empirical findings demonstrate the feasibility of using large language models to create synthetic multi-platform social media data, highlighting variations in fidelity among different models. The study also suggests that a post-processing approach may be necessary to generate high-fidelity synthetic datasets. Additionally, new fidelity metrics specific to multi-platform social media datasets were introduced as part of the research. <div>
arXiv:2505.02858v1 Announce Type: new 
Abstract: Social media datasets are essential for research on a variety of topics, such as disinformation, influence operations, hate speech detection, or influencer marketing practices. However, access to social media datasets is often constrained due to costs and platform restrictions. Acquiring datasets that span multiple platforms, which is crucial for understanding the digital ecosystem, is particularly challenging. This paper explores the potential of large language models to create lexically and semantically relevant social media datasets across multiple platforms, aiming to match the quality of real data. We propose multi-platform topic-based prompting and employ various language models to generate synthetic data from two real datasets, each consisting of posts from three different social media platforms. We assess the lexical and semantic properties of the synthetic data and compare them with those of the real data. Our empirical findings show that using large language models to generate synthetic multi-platform social media data is promising, different language models perform differently in terms of fidelity, and a post-processing approach might be needed for generating high-fidelity synthetic datasets for research. In addition to the empirical evaluation of three state of the art large language models, our contributions include new fidelity metrics specific to multi-platform social media datasets.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI</title>
<link>https://arxiv.org/abs/2505.02859</link>
<guid>https://arxiv.org/abs/2505.02859</guid>
<content:encoded><![CDATA[
<div> Keywords: eXplainableAI, Large Language Models, chatbot, State-of-Health prediction, batteries

Summary: 
The paper discusses the growing importance of eXplainableAI (XAI) in various sectors due to the opacity of Machine Learning (ML) models. It explores the use of Large Language Models (LLMs) to interpret XAI, proposing a novel reference architecture involving an interactive chatbot powered by a fine-tuned LLM. The architecture is tested in the context of State-of-Health (SoH) prediction for batteries, demonstrating improved human interpretability of ML, particularly for users with less XAI experience. Multiple rounds of evaluation validate the effectiveness of the prototype in enhancing understanding of complex patterns in language and ML models.<br /><br />Summary: <div>
arXiv:2505.02859v1 Announce Type: new 
Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as the increasing black-boxedness of prevailing Machine Learning (ML) models became apparent. In parallel, Large Language Models (LLMs) significantly developed in their abilities to understand human language and complex patterns. By combining both, this paper presents a novel reference architecture for the interpretation of XAI through an interactive chatbot powered by a fine-tuned LLM. We instantiate the reference architecture in the context of State-of-Health (SoH) prediction for batteries and validate its design in multiple evaluation and demonstration rounds. The evaluation indicates that the implemented prototype enhances the human interpretability of ML, especially for users with less experience with XAI.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs</title>
<link>https://arxiv.org/abs/2505.02862</link>
<guid>https://arxiv.org/abs/2505.02862</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, jailbreak attacks, ICRT framework, cognitive decomposition, ranking-based harmfulness evaluation<br />
Summary:<br /> 
The article introduces a novel jailbreak attack framework called ICRT, inspired by human cognition biases, to exploit vulnerabilities in Large Language Models (LLMs). By leveraging the simplicity effect and relevance bias, ICRT reduces the complexity of malicious prompts and enhances semantic alignment to generate harmful outputs effectively. The framework also introduces a ranking-based harmfulness evaluation metric that goes beyond binary success-or-failure by using ranking aggregation methods to assess the risk level of generated content comprehensively. Experimental results demonstrate the effectiveness of ICRT in bypassing LLMs' safety mechanisms and producing high-risk content, shedding light on the potential risks of jailbreak attacks and suggesting the need for stronger defense strategies. <br /><br />Summary: <div>
arXiv:2505.02862v1 Announce Type: new 
Abstract: Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Large Language Model Reasoning via Speculative Search</title>
<link>https://arxiv.org/abs/2505.02865</link>
<guid>https://arxiv.org/abs/2505.02865</guid>
<content:encoded><![CDATA[
<div> Keywords: Tree-search-based reasoning methods, large language models, Speculative Search framework, inference latency, thought generation

Summary: 
Tree-search-based reasoning methods are effective in enhancing the reasoning capability of large language models by exploring multiple intermediate reasoning steps. However, they suffer from significant inference latency due to the generation of numerous reasoning thoughts. To address this challenge, the Speculative Search framework is proposed to optimize thought generation and accelerate LLM reasoning. SpecSearch collaborates a small model with a large model to efficiently generate high-quality reasoning thoughts using a quality-preserving rejection mechanism. The framework maintains comparable reasoning quality to the large model while achieving up to 2.12x speedup. Experimental results on the Qwen and Llama models show that SpecSearch outperforms existing approaches in terms of speed and reasoning quality.<br /><br />Summary: <div>
arXiv:2505.02865v1 Announce Type: new 
Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning capability of large language models (LLMs) by facilitating the exploration of multiple intermediate reasoning steps, i.e., thoughts. However, these methods suffer from substantial inference latency, as they have to generate numerous reasoning thoughts, severely limiting LLM applicability. To address this challenge, we propose a novel Speculative Search (SpecSearch) framework that significantly accelerates LLM reasoning by optimizing thought generation. Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts. The major pillar of SpecSearch is a novel quality-preserving rejection mechanism, which effectively filters out thoughts whose quality falls below that of the large model's outputs. Moreover, we show that SpecSearch preserves comparable reasoning quality to the large model. Experiments on both the Qwen and Llama models demonstrate that SpecSearch significantly outperforms state-of-the-art approaches, achieving up to 2.12$\times$ speedup with comparable reasoning quality.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading</title>
<link>https://arxiv.org/abs/2505.02872</link>
<guid>https://arxiv.org/abs/2505.02872</guid>
<content:encoded><![CDATA[
<div> goals, reading, eye movements, classification, reconstruction

Summary: 
This study explores the possibility of automatically decoding open-ended reading goals from eye movements during reading. The researchers introduce goal classification and goal reconstruction tasks and evaluate them using large-scale eye tracking data from English readers with various text-specific information-seeking tasks. Different discriminative and generative multimodal LLMs that combine eye movements and text are developed and compared for goal classification and goal reconstruction. The experiments demonstrate significant success in both tasks, indicating that LLMs can effectively extract information about readers' text-specific goals from their eye movements. <div>
arXiv:2505.02872v1 Announce Type: new 
Abstract: When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logits-Constrained Framework with RoBERTa for Ancient Chinese NER</title>
<link>https://arxiv.org/abs/2505.02983</link>
<guid>https://arxiv.org/abs/2505.02983</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, Logits-Constrained framework, Ancient Chinese, EvaHan 2025 benchmark, GujiRoBERTa

Summary:
The paper introduces a Logits-Constrained framework for Ancient Chinese Named Entity Recognition (NER) using the EvaHan 2025 benchmark. The model, which combines GujiRoBERTa for contextual encoding with a differentiable decoding mechanism, enforces valid BMES label transitions. The proposed framework outperforms traditional CRF and BiLSTM-based approaches, particularly in high-label or large-data scenarios. The study also suggests a model selection criterion that considers label complexity and dataset size, offering practical insights for Ancient Chinese NLP tasks. The research showcases the effectiveness of the LC framework in enhancing NER performance and provides a guideline for optimizing NLP techniques in Ancient Chinese language processing tasks.

<br /><br />Summary: <div>
arXiv:2505.02983v1 Announce Type: new 
Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our two-stage model integrates GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions. Experiments demonstrate that LC improves performance over traditional CRF and BiLSTM-based approaches, especially in high-label or large-data settings. We also propose a model selection criterion balancing label complexity and dataset size, providing practical guidance for real-world Ancient Chinese NLP tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
<div> Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) presents a protocol for converting softmax attention transformers into linear attention decoder models efficiently. They introduce two new RWKV-variant architectures and convert popular Qwen2.5 models into 7B, 32B, and 72B sizes with minimal token usage. The conversion process is cost-effective, with the 72B model costing less than $2,000 USD. Despite the reduced token count, the quality of inference remains comparable to the original transformer models. The models achieve state-of-the-art performance on standard benchmarks for linear attention models of their sizes. All models are released on HuggingFace under the Apache 2.0 license, except for the 72B models, which are governed by the Qwen License Agreement. 

Keywords: Rapid Attention Distillation, Linear Attention Decoders, RWKV-variant architectures, Token count, State-of-the-art performance

<br /><br />Summary: 
- RADLADS introduces a protocol for efficiently converting softmax attention transformers into linear attention decoder models.
- They present two new RWKV-variant architectures and convert popular Qwen2.5 models into various sizes with minimal token usage.
- The conversion process is cost-effective, with the 72B model costing less than $2,000 USD.
- Despite the reduced token count, the quality of inference remains comparable to the original transformer models.
- The released models achieve state-of-the-art performance on standard benchmarks for their sizes.
- All models are available on HuggingFace under the Apache 2.0 license, except for the 72B models, which are governed by the Qwen License Agreement. <div>
arXiv:2505.03005v1 Announce Type: new 
Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.
  Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis</title>
<link>https://arxiv.org/abs/2505.03019</link>
<guid>https://arxiv.org/abs/2505.03019</guid>
<content:encoded><![CDATA[
<div> detecting memorization, Large Language Models (LLMs), input perturbations, generalization, Pythia open model

Summary: 
The paper introduces PEARL, a novel approach for detecting memorization in Large Language Models (LLMs). PEARL assesses the sensitivity of an LLM's performance to input perturbations, distinguishing between true generalization and memorization without needing access to the model's internals. Through experiments on the Pythia open model, the framework can identify when an LLM simply regurgitates learned information, as demonstrated on GPT 4o models. PEARL identified cases of memorization of classic texts and common code, providing evidence that certain data, like New York Times articles, were likely part of a model's training data. This framework addresses concerns about data privacy, intellectual property rights, and model reliability by offering a robust method to detect memorization in LLMs. <br /><br />Summary: <div>
arXiv:2505.03019v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) achieve remarkable performance through training on massive datasets, they can exhibit concerning behaviors such as verbatim reproduction of training data rather than true generalization. This memorization phenomenon raises significant concerns about data privacy, intellectual property rights, and the reliability of model evaluations. This paper introduces PEARL, a novel approach for detecting memorization in LLMs. PEARL assesses how sensitive an LLM's performance is to input perturbations, enabling memorization detection without requiring access to the model's internals. We investigate how input perturbations affect the consistency of outputs, enabling us to distinguish between true generalization and memorization. Our findings, following extensive experiments on the Pythia open model, provide a robust framework for identifying when the model simply regurgitates learned information. Applied on the GPT 4o models, the PEARL framework not only identified cases of memorization of classic texts from the Bible or common code from HumanEval but also demonstrated that it can provide supporting evidence that some data, such as from the New York Times news articles, were likely part of the training data of a given model.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts</title>
<link>https://arxiv.org/abs/2505.03025</link>
<guid>https://arxiv.org/abs/2505.03025</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, clinical dialogues, data synthesis, data evaluation, medical domain
<br />
Summary: 
Synthetic data sets are becoming increasingly important in the field of clinical healthcare due to challenges related to privacy and data governance. In particular, synthetic datasets of clinical dialogues are used when authentic data is limited or sensitive. However, there is a lack of theory on how to best use and generalize these synthetic datasets for new applications. This paper provides an overview of the creation, evaluation, and usage of synthetic datasets for dialogue related tasks in the medical domain. It also introduces a novel typology for classifying different types and degrees of data synthesis to facilitate comparison and evaluation of synthetic datasets. Research in this area is crucial for improving the effectiveness and reliability of synthetic data in healthcare contexts. 
<br /><br />Summary: <div>
arXiv:2505.03025v1 Announce Type: new 
Abstract: Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output</title>
<link>https://arxiv.org/abs/2505.03030</link>
<guid>https://arxiv.org/abs/2505.03030</guid>
<content:encoded><![CDATA[
<div> hallucinations, language models, Mu-SHROOM, shared task, UCSC system

Summary:<br />
The paper discusses the challenge of hallucinations in large language models (LLMs) when responding to knowledge-based queries. It introduces the SemEval 2025 Task 3, Mu-SHROOM, aimed at detecting and pinpointing hallucinations in LLM outputs. The UCSC system submission to this task involves a framework that retrieves context, identifies false information in answers, and maps them back to LLM outputs. The system also automatically optimizes prompts. The UCSC system achieved the highest overall performance in the Mu-SHROOM task, ranking first in average position across all languages. The code and experiment results for the system are released for further research and development. <br /><br />Summary: <div>
arXiv:2505.03030v1 Announce Type: new 
Abstract: Hallucinations pose a significant challenge for large language models when answering knowledge-intensive queries. As LLMs become more widely adopted, it is crucial not only to detect if hallucinations occur but also to pinpoint exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM: Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes, is a recent effort in this direction. This paper describes the UCSC system submission to the shared Mu-SHROOM task. We introduce a framework that first retrieves relevant context, next identifies false content from the answer, and finally maps them back to spans in the LLM output. The process is further enhanced by automatically optimizing prompts. Our system achieves the highest overall performance, ranking #1 in average position across all languages. We release our code and experiment results.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Models to Understand (but not Generate) High-risk Data</title>
<link>https://arxiv.org/abs/2505.03052</link>
<guid>https://arxiv.org/abs/2505.03052</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, pre-training data, high-risk content, Selective Loss to Understand but Not Generate (SLUNG), toxic content

Summary: 
Language model developers usually filter out high-risk content from pre-training data to prevent models from generating similar outputs. However, this limits their ability to recognize and respond to harmful content. The paper introduces the concept of Selective Loss to Understand but Not Generate (SLUNG) where models learn to understand high-risk data without generating it. SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they are within the model's context window. This approach improves models' understanding of high-risk data, such as recognizing toxic content, without increasing the generation of such content in model responses. The experiments show that SLUNG consistently enhances models' understanding of high-risk data, allowing them to benefit from such text that would typically be filtered out.<br /><br />Summary: <div>
arXiv:2505.03052v1 Announce Type: new 
Abstract: Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text</title>
<link>https://arxiv.org/abs/2505.03053</link>
<guid>https://arxiv.org/abs/2505.03053</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM evaluation, bias evaluation, human insights, operational definition, methodology<br />
Summary:<br />
The article discusses the challenges of evaluating LLMs, especially in real-world deployments where task-specific prompts and experiential context come into play. While short context, fixed choice benchmarks are commonly used for bias evaluation, they may lose validity when the deployed context differs. Large-scale human evaluation is deemed too costly, leading the researchers to develop a semi-automated bias evaluation framework for free text responses. By operationalizing bias and incorporating human insights, they were able to automate their pipeline and classify bias beyond multiple choice. Human evaluation also revealed problematic templates in a bias benchmark. This journey highlights the importance of human involvement in crafting automated evaluation frameworks for LLMs.<br /><br />Summary: <div>
arXiv:2505.03053v1 Announce Type: new 
Abstract: LLM evaluation is challenging even the case of base models. In real world deployments, evaluation is further complicated by the interplay of task specific prompts and experiential context. At scale, bias evaluation is often based on short context, fixed choice benchmarks that can be rapidly evaluated, however, these can lose validity when the LLMs' deployed context differs. Large scale human evaluation is often seen as too intractable and costly. Here we present our journey towards developing a semi-automated bias evaluation framework for free text responses that has human insights at its core. We discuss how we developed an operational definition of bias that helped us automate our pipeline and a methodology for classifying bias beyond multiple choice. We additionally comment on how human evaluation helped us uncover problematic templates in a bias benchmark.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Model Alignment Through Collective Intelligence of Open-Source LLMS</title>
<link>https://arxiv.org/abs/2505.03059</link>
<guid>https://arxiv.org/abs/2505.03059</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, model alignment, Mixture of Agents Alignment, supervised fine-tuning, synthetic data

Summary:<br /><br />
The study introduces Mixture of Agents Alignment (MoAA) as a method to enhance model alignment in large language models (LLMs) using diverse synthetic data. By combining the strengths of various language models, MoAA improves supervised fine-tuning and preference optimization, leading to better performance in model alignment tasks. Evaluation results show significant improvements in win rates on different evaluation benchmarks, highlighting the effectiveness of the approach. Additionally, MoAA enables a self-improvement pipeline where models finetuned on MoA-generated data surpass their initial capabilities, pushing the frontier of open-source LLMs without external supervision. The approach provides a scalable and diverse solution for generating high-quality alignment data and demonstrates promising results for improving model alignment in LLMs. Data and code related to the study will be released for further research and development. <div>
arXiv:2505.03059v1 Announce Type: new 
Abstract: Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models finetuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of Abstract Meaning Representation: Then, Now, Future</title>
<link>https://arxiv.org/abs/2505.03229</link>
<guid>https://arxiv.org/abs/2505.03229</guid>
<content:encoded><![CDATA[
<div> semantic representation, Abstract Meaning Representation, graph-based structure, text-to-AMR parsing, AMR-to-text generation

Summary:
This survey explores Abstract Meaning Representation (AMR), a semantic framework representing sentence meaning through graph structures. It discusses AMR capabilities, parsing, and generation tasks with traditional and current approaches. Various applications such as text generation, classification, and information extraction are also reviewed. The survey analyzes recent developments and challenges in the field, providing insights for future research directions and the potential impact of AMR on enhancing machine understanding of human language. 

<br /><br />Summary: <div>
arXiv:2505.03229v1 Announce Type: new 
Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a semantic representation framework that captures the meaning of sentences through a graph-based structure. AMR represents sentences as rooted, directed acyclic graphs, where nodes correspond to concepts and edges denote relationships, effectively encoding the meaning of complex sentences. This survey investigates AMR and its extensions, focusing on AMR capabilities. It then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by showing traditional, current, and possible futures approaches. It also reviews various applications of AMR including text generation, text classification, and information extraction and information seeking. By analyzing recent developments and challenges in the field, this survey provides insights into future directions for research and the potential impact of AMR on enhancing machine understanding of human language.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>{\Psi}-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback</title>
<link>https://arxiv.org/abs/2505.03293</link>
<guid>https://arxiv.org/abs/2505.03293</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, mental health support, counseling, evaluation, optimization

Summary:<br /><br />
The article introduces the {\Psi}-Arena framework, aimed at assessing and optimizing Large Language Models (LLMs) for counseling in mental health support. The framework includes realistic arena interactions with psychologically profiled NPC clients, tripartite evaluation from client, counselor, and supervisor perspectives, and closed-loop optimization based on diagnostic feedback. Experiments on eight LLMs reveal significant performance variations across different scenarios and perspectives, highlighting the need for comprehensive assessment. Through reflection-based optimization, counseling performance improved by up to 141%. The {\Psi}-Arena framework aims to advance the reliability and human-aligned applications of LLMs in mental healthcare, offering a promising resource for enhancing the efficacy and safety of LLM-based counseling services. 

Summary: <br />
The article introduces the {\Psi}-Arena framework for evaluating and optimizing Large Language Models (LLMs) for mental health counseling. This framework includes realistic counseling interactions, a tripartite evaluation process, and closed-loop optimization based on feedback. Experiments show varying performance across different scenarios and perspectives, with significant improvements achieved through reflection-based optimization. The {\Psi}-Arena framework aims to enhance the reliability and human alignment of LLM applications in mental healthcare, providing a valuable resource for improving the effectiveness and safety of LLM-based counseling services. <div>
arXiv:2505.03293v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in providing scalable mental health support, while evaluating their counseling capability remains crucial to ensure both efficacy and safety. Existing evaluations are limited by the static assessment that focuses on knowledge tests, the single perspective that centers on user experience, and the open-loop framework that lacks actionable feedback. To address these issues, we propose {\Psi}-Arena, an interactive framework for comprehensive assessment and optimization of LLM-based counselors, featuring three key characteristics: (1) Realistic arena interactions that simulate real-world counseling through multi-stage dialogues with psychologically profiled NPC clients, (2) Tripartite evaluation that integrates assessments from the client, counselor, and supervisor perspectives, and (3) Closed-loop optimization that iteratively improves LLM counselors using diagnostic feedback. Experiments across eight state-of-the-art LLMs show significant performance variations in different real-world scenarios and evaluation perspectives. Moreover, reflection-based optimization results in up to a 141% improvement in counseling performance. We hope PsychoArena provides a foundational resource for advancing reliable and human-aligned LLM applications in mental healthcare.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation</title>
<link>https://arxiv.org/abs/2505.03320</link>
<guid>https://arxiv.org/abs/2505.03320</guid>
<content:encoded><![CDATA[
<div> Keywords: Mamba, Recall with Reasoning (RwR), long-context memory, chain-of-thought (CoT), Transformer

Summary: 
The article introduces a method called Recall with Reasoning (RwR) to enhance Mamba's ability to recall and reason over long contexts. By distilling chain-of-thought (CoT) summarization from a teacher model and incorporating it as prompts during fine-tuning, Mamba is trained to actively recall and reason over extended sequences. Experimentation on LONGMEMEVAL and HELMET datasets demonstrates that RwR significantly improves Mamba's performance in handling long-context tasks compared to Transformer and hybrid models without altering its architecture. The method effectively enhances Mamba's long-context memory while preserving its proficiency in short-context tasks. This approach provides a simple yet effective solution to overcome the limitations of Mamba's theoretical infinite-context potential in practical scenarios where sequences surpass training lengths. <div>
arXiv:2505.03320v1 Announce Type: new 
Abstract: Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.03406</link>
<guid>https://arxiv.org/abs/2505.03406</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Healthcare, Decision Support, Retrieval-Augmented Generation, Quantized Low-Rank Adaptation <br />
<br />
Summary: 
This research paper explores the use of Large Language Models (LLMs) in healthcare, focusing on improving medical decision support through Retrieval-Augmented Generation (RAG) integrated with hospital-specific data and Quantized Low-Rank Adaptation (QLoRA). Using Llama 3.2-3B-Instruct as the foundation model, the system embeds and retrieves relevant healthcare information to enhance response accuracy, while QLoRA ensures parameter efficiency and memory optimization. The model shows promising performance on medical benchmarks, with applications such as disease prediction, treatment suggestions, and summarization of medical reports. Ethical considerations like patient privacy and data security, as well as practical challenges of integration, are discussed. The lightweight quantized weights enable scalability and deployment in low-resource hospital settings. The paper also discusses the broader impact of LLMs in healthcare and suggests future directions for their use in medical settings. <div>
arXiv:2505.03406v1 Announce Type: new 
Abstract: This research paper investigates the application of Large Language Models (LLMs) in healthcare, specifically focusing on enhancing medical decision support through Retrieval-Augmented Generation (RAG) integrated with hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation (QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By embedding and retrieving context-relevant healthcare information, the system significantly improves response accuracy. QLoRA facilitates notable parameter efficiency and memory optimization, preserving the integrity of medical information through specialized quantization techniques. Our research also shows that our model performs relatively well on various medical benchmarks, indicating that it can be used to make basic medical suggestions. This paper details the system's technical components, including its architecture, quantization methods, and key healthcare applications such as enhanced disease prediction from patient symptoms and medical history, treatment suggestions, and efficient summarization of complex medical reports. We touch on the ethical considerations-patient privacy, data security, and the need for rigorous clinical validation-as well as the practical challenges of integrating such systems into real-world healthcare workflows. Furthermore, the lightweight quantized weights ensure scalability and ease of deployment even in low-resource hospital environments. Finally, the paper concludes with an analysis of the broader impact of LLMs on healthcare and outlines future directions for LLMs in medical settings.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks</title>
<link>https://arxiv.org/abs/2505.03427</link>
<guid>https://arxiv.org/abs/2505.03427</guid>
<content:encoded><![CDATA[
<div> dataset, Arabic medical, benchmark, Large Language Models, healthcare
<br />
Large Language Models have shown promise in healthcare applications but have not been explored in the Arabic medical domain due to the lack of specific datasets. This study introduces MedArabiQ, a new benchmark dataset comprising seven Arabic medical tasks. The dataset includes various question types and tasks from multiple specialties. Different modifications were made to evaluate the abilities of different LLMs, including bias mitigation. Evaluation was conducted on five state-of-the-art LLMs, highlighting the importance of creating high-quality benchmarks across languages for fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, the study aims to lay the groundwork for future research on improving the multilingual capabilities of LLMs for equitable use in healthcare.
<br /><br />Summary: <div>
arXiv:2505.03427v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.03452</link>
<guid>https://arxiv.org/abs/2505.03452</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation (RAG), hyper-parameter optimization (HPO), datasets, performance improvement, greedy HPO approaches<br />
Summary: 
This study evaluates the effectiveness of hyper-parameter optimization (HPO) algorithms for Retrieval-Augmented Generation (RAG) configurations. It covers 5 datasets, including a new one on product documentation. The study explores a large HPO search space and uses two evaluation metrics. The results show that RAG HPO can efficiently enhance performance, with greedy or iterative random search approaches being effective. Optimizing models first is found to be more beneficial than optimizing sequentially according to the RAG pipeline order. This comprehensive analysis demonstrates the potential for significant performance boosts through optimized RAG configurations.<br />  <div>
arXiv:2505.03452v1 Announce Type: new 
Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with two optimized evaluation metrics. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with iterative random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing models first is preferable to the prevalent practice of optimizing sequentially according to the RAG pipeline order.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis</title>
<link>https://arxiv.org/abs/2505.03467</link>
<guid>https://arxiv.org/abs/2505.03467</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable disease diagnosis, diagnostic uncertainty, ConfiDx, large language model, trustworthy explanations

Summary:<br /><br />Researchers have developed ConfiDx, an uncertainty-aware large language model that addresses diagnostic uncertainty in disease diagnosis. When clinical notes do not provide enough evidence for a definitive diagnosis, ConfiDx can identify and explain diagnostic uncertainties, reducing the risk of misdiagnosis and adverse outcomes. By fine-tuning open-source language models with diagnostic criteria, ConfiDx excels in recognizing uncertainties and generating trustworthy explanations for diagnoses. This study is the first to jointly address diagnostic uncertainty recognition and explanation, enhancing the reliability of automatic diagnostic systems. Through evaluations on real-world datasets, ConfiDx demonstrated superior diagnostic performance in handling various degrees of diagnostic ambiguity. The introduction of ConfiDx paves the way for more reliable and transparent disease diagnosis models that can provide valuable insights and explanations for healthcare practitioners. <br /><br />Summary: <div>
arXiv:2505.03467v1 Announce Type: new 
Abstract: Explainable disease diagnosis, which leverages patient information (e.g., signs and symptoms) and computational models to generate probable diagnoses and reasonings, offers clear clinical values. However, when clinical notes encompass insufficient evidence for a definite diagnosis, such as the absence of definitive symptoms, diagnostic uncertainty usually arises, increasing the risk of misdiagnosis and adverse outcomes. Although explicitly identifying and explaining diagnostic uncertainties is essential for trustworthy diagnostic systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an uncertainty-aware large language model (LLM) created by fine-tuning open-source LLMs with diagnostic criteria. We formalized the task and assembled richly annotated datasets that capture varying degrees of diagnostic ambiguity. Evaluating ConfiDx on real-world datasets demonstrated that it excelled in identifying diagnostic uncertainties, achieving superior diagnostic performance, and generating trustworthy explanations for diagnoses and uncertainties. To our knowledge, this is the first study to jointly address diagnostic uncertainty recognition and explanation, substantially enhancing the reliability of automatic diagnostic systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.03469</link>
<guid>https://arxiv.org/abs/2505.03469</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Supervised Fine-Tuning, Chain-of-Thought, LS-Mixture SFT, Reasoning

Summary: 
LS-Mixture SFT addresses the overthinking problem inherited from teacher models in fine-tuned models by combining long and short CoT reasoning datasets. Through experiments, models trained using LS-Mixture SFT showed an average accuracy improvement of 2.3% across various benchmarks and reduced model response length by approximately 47.61% compared to models trained with direct SFT. This method offers a way to equip non-reasoning models with reasoning capabilities efficiently while avoiding the overthinking issue. <div>
arXiv:2505.03469v1 Announce Type: new 
Abstract: Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the "overthinking" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought \textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning (\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3\% across various benchmarks while substantially reducing model response length by approximately 47.61\%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLMs on Long-tail Entity Linking in Historical Documents</title>
<link>https://arxiv.org/abs/2505.03473</link>
<guid>https://arxiv.org/abs/2505.03473</guid>
<content:encoded><![CDATA[
<div> EL, Entity Linking, NLP, LLMs, Long-tail

Summary:
- Entity Linking (EL) is crucial in NLP, using LLMs can improve results.
- LLMs face challenges with less popular entities.
- Long-tail EL is not extensively studied, especially with LLMs.
- The study assesses GPT and LLama3 performance in long-tail EL.
- Preliminary results show LLMs perform well in long-tail EL, bridging the gap between head and long-tail EL.<br /><br />Summary: <div>
arXiv:2505.03473v1 Announce Type: new 
Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP) applications, enabling the disambiguation of entity mentions by linking them to their corresponding entries in a reference knowledge base (KB). Thanks to their deep contextual understanding capabilities, LLMs offer a new perspective to tackle EL, promising better results than traditional methods. Despite the impressive generalization capabilities of LLMs, linking less popular, long-tail entities remains challenging as these entities are often underrepresented in training data and knowledge bases. Furthermore, the long-tail EL task is an understudied problem, and limited studies address it with LLMs. In the present work, we assess the performance of two popular LLMs, GPT and LLama3, in a long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated benchmark of sentences from domain-specific historical texts, we quantitatively compare the performance of LLMs in identifying and linking entities to their corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity Linking and Relation Extraction framework. Our preliminary experiments reveal that LLMs perform encouragingly well in long-tail EL, indicating that this technology can be a valuable adjunct in filling the gap between head and long-tail EL.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentence Embeddings as an intermediate target in end-to-end summarisation</title>
<link>https://arxiv.org/abs/2505.03481</link>
<guid>https://arxiv.org/abs/2505.03481</guid>
<content:encoded><![CDATA[
<div> document summarisation, neural network, sentence embeddings, content-selection, user reviews<br />
Summary:<br />
The paper addresses the challenge of document summarisation, particularly with large inputs. It proposes a novel approach combining extractive methods and pre-trained sentence embeddings with abstractive summarisation models. This hybrid model outperforms existing techniques in summarising user reviews of accommodations. By predicting sentence embeddings for the summary, the model demonstrates higher quality in handling loosely aligned source-to-target corpora compared to traditional methods that predict probability distributions for sentence selection. This approach offers promising results for improving end-to-end summarisation systems and enhancing the efficiency of summarising large datasets. <div>
arXiv:2505.03481v1 Announce Type: new 
Abstract: Current neural network-based methods to the problem of document summarisation struggle when applied to datasets containing large inputs. In this paper we propose a new approach to the challenge of content-selection when dealing with end-to-end summarisation of user reviews of accommodations. We show that by combining an extractive approach with externally pre-trained sentence level embeddings in an addition to an abstractive summarisation model we can outperform existing methods when this is applied to the task of summarising a large input dataset. We also prove that predicting sentence level embedding of a summary increases the quality of an end-to-end system for loosely aligned source to target corpora, than compared to commonly predicting probability distributions of sentence selection.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster MoE LLM Inference for Extremely Large Models</title>
<link>https://arxiv.org/abs/2505.03531</link>
<guid>https://arxiv.org/abs/2505.03531</guid>
<content:encoded><![CDATA[
<div> Sparse Mixture of Experts, large language models, optimization, fine-grained models, efficiency<br />
<br />
Summary: Sparse Mixture of Experts (MoE) models are gaining popularity for large language models. Existing optimization efforts have focused on coarse-grained MoE architectures, but research on fine-grained models is limited. This study explores the efficiency dynamics under different service loads for MoE models. Fine-grained models allow deployers to reduce the number of routed experts, leading to efficiency improvements in some scenarios with minimal performance degradation. However, reducing the total number of experts results in severe performance degradation. The study finds that reducing activated experts can increase throughput by at least 10% without performance loss. The research concludes that MoE inference optimization offers significant opportunities for improvement and further exploration.<br /> <div>
arXiv:2505.03531v1 Announce Type: new 
Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Say It Another Way: A Framework for User-Grounded Paraphrasing</title>
<link>https://arxiv.org/abs/2505.03563</link>
<guid>https://arxiv.org/abs/2505.03563</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, prompt variations, paraphrasing, stereotype evaluation, evaluation protocols 

Summary: 
Small changes in how a prompt is worded can significantly impact the behavior of large language models. This study introduces a controlled paraphrasing framework to systematically generate natural variations in prompts using minimal linguistic transformations. Validated using the BBQ dataset with human annotations and automated checks, the framework is then used to analyze how language models respond to paraphrased prompts in stereotype evaluation tasks. The results demonstrate that even subtle modifications to prompts can lead to significant changes in model behavior. This emphasizes the importance of robust evaluation protocols that are paraphrase-aware. <div>
arXiv:2505.03563v1 Announce Type: new 
Abstract: Small changes in how a prompt is worded can lead to meaningful differences in the behavior of large language models (LLMs), raising concerns about the stability and reliability of their evaluations. While prior work has explored simple formatting changes, these rarely capture the kinds of natural variation seen in real-world language use. We propose a controlled paraphrasing framework based on a taxonomy of minimal linguistic transformations to systematically generate natural prompt variations. Using the BBQ dataset, we validate our method with both human annotations and automated checks, then use it to study how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our analysis shows that even subtle prompt modifications can lead to substantial changes in model behavior. These results highlight the need for robust, paraphrase-aware evaluation protocols.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure</title>
<link>https://arxiv.org/abs/2505.03675</link>
<guid>https://arxiv.org/abs/2505.03675</guid>
<content:encoded><![CDATA[
<div> keywords: ChatGPT, self-care, African-American, heart failure patients, Social Determinants of Health

Summary:
Effective prompt design is crucial in utilizing ChatGPT for generating conversations focused on self-care strategies for African-American heart failure patients. The study explored four prompting strategies: domain, African American Vernacular English (AAVE), Social Determinants of Health (SDOH), and SDOH-informed reasoning. Conversations were generated across key self-care domains, incorporating patient-specific SDOH attributes. While incorporating SDOH and reasoning improved dialogue quality, ChatGPT still lacks the empathy and engagement required for meaningful healthcare communication. The research highlights the importance of specialized datasets in healthcare dialogue generation and points out the need for further advancements in natural language processing models for catering to specific healthcare communication needs, especially in addressing marginalized communities such as African-American heart failure patients. The study serves as a stepping stone towards enhancing AI-generated healthcare conversations for better patient education and support.<br /><br />Summary: <div>
arXiv:2505.03675v1 Announce Type: new 
Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate conversations focused on self-care strategies for African-American heart failure patients -- a domain with limited specialized datasets. To simulate patient-health educator dialogues, we employed four prompting strategies: domain, African American Vernacular English (AAVE), Social Determinants of Health (SDOH), and SDOH-informed reasoning. Conversations were generated across key self-care domains of food, exercise, and fluid intake, with varying turn lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as age, gender, neighborhood, and socioeconomic status. Our findings show that effective prompt design is essential. While incorporating SDOH and reasoning improves dialogue quality, ChatGPT still lacks the empathy and engagement needed for meaningful healthcare communication.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages</title>
<link>https://arxiv.org/abs/2505.03688</link>
<guid>https://arxiv.org/abs/2505.03688</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering, Indic languages, dataset, BERT models, multilingual

Summary:
- The study introduces IndicSQuAD, a multi-lingual extractive QA dataset covering nine major Indic languages, derived from the SQuAD dataset.
- Translation techniques are adapted and extended to ensure linguistic fidelity and accurate answer-span alignment across diverse languages in IndicSQuAD.
- The dataset includes training, validation, and test sets for each language, serving as a solid foundation for model development.
- Baseline performances are evaluated using language-specific monolingual BERT models and the multilingual MuRIL-BERT, highlighting challenges in low-resource settings.
- Future directions include expanding to more languages, creating domain-specific datasets, and incorporating multimodal data.

<br /><br />Summary: <div>
arXiv:2505.03688v1 Announce Type: new 
Abstract: The rapid progress in question-answering (QA) systems has predominantly benefited high-resource languages, leaving Indic languages largely underrepresented despite their vast native speaker base. In this paper, we present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset covering nine major Indic languages, systematically derived from the SQuAD dataset. Building on previous work with MahaSQuAD for Marathi, our approach adapts and extends translation techniques to maintain high linguistic fidelity and accurate answer-span alignment across diverse languages. IndicSQuAD comprises extensive training, validation, and test sets for each language, providing a robust foundation for model development. We evaluate baseline performances using language-specific monolingual BERT models and the multilingual MuRIL-BERT. The results indicate some challenges inherent in low-resource settings. Moreover, our experiments suggest potential directions for future work, including expanding to additional languages, developing domain-specific datasets, and incorporating multimodal data. The dataset and models are publicly shared at https://github.com/l3cube-pune/indic-nlp
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation</title>
<link>https://arxiv.org/abs/2505.03711</link>
<guid>https://arxiv.org/abs/2505.03711</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-lingual subject classification, bilingual data, dimension-as-token self-attention, sentence embeddings, resource constraints

Summary:
Our system submission for SemEval 2025 Task 5 focuses on cross-lingual subject classification in the English and German academic domains. We leverage bilingual data during training, utilizing negative sampling and a margin-based retrieval objective. Our approach incorporates a dimension-as-token self-attention mechanism with reduced internal dimensions to encode sentence embeddings for subject retrieval effectively. In quantitative evaluation, our system achieved an average recall rate of 32.24% in the general setting, with competitive performance in both qualitative and quantitative evaluation methods while minimizing GPU usage. Our results demonstrate the effectiveness of our approach in capturing relevant subject information under resource constraints, although there is still potential for further improvement. 

<br /><br />Summary: 
- Submission for SemEval 2025 Task 5 on cross-lingual subject classification
- Leveraging bilingual data and utilizing negative sampling
- Incorporating a dimension-as-token self-attention mechanism for encoding sentence embeddings
- Achieved an average recall rate of 32.24% in general quantitative setting
- Competitive performance in qualitative and quantitative evaluation methods with minimal GPU usage <div>
arXiv:2505.03711v1 Announce Type: new 
Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on cross-lingual subject classification in the English and German academic domains. Our approach leverages bilingual data during training, employing negative sampling and a margin-based retrieval objective. We demonstrate that a dimension-as-token self-attention mechanism designed with significantly reduced internal dimensions can effectively encode sentence embeddings for subject retrieval. In quantitative evaluation, our system achieved an average recall rate of 32.24% in the general quantitative setting (all subjects), 43.16% and 31.53% of the general qualitative evaluation methods with minimal GPU usage, highlighting their competitive performance. Our results demonstrate that our approach is effective in capturing relevant subject information under resource constraints, although there is still room for improvement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch</title>
<link>https://arxiv.org/abs/2505.03733</link>
<guid>https://arxiv.org/abs/2505.03733</guid>
<content:encoded><![CDATA[
<div> Benchmark, LLM-based agents, website generation, test cases, code-agent frameworks <br />
Summary: 
This paper introduces WebGen-Bench, a benchmark for measuring LLM-based agents' ability to create multi-file website codebases. The benchmark includes diverse instructions for website generation across various categories, with 647 test cases created to assess the quality of generated websites. Three high-performance code-agent frameworks are evaluated using proprietary and open-source LLMs as engines, with Bolt.diy powered by DeepSeek-R1 achieving 27.8% accuracy on test cases. The challenging nature of the benchmark is highlighted. Additionally, a training set, WebGen-Instruct, consisting of 6,667 website-generation instructions is constructed. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories from a subset achieves 38.2% accuracy, surpassing the performance of the best proprietary model. <div>
arXiv:2505.03733v1 Announce Type: new 
Abstract: LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\%, surpassing the performance of the best proprietary model.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model</title>
<link>https://arxiv.org/abs/2505.03739</link>
<guid>https://arxiv.org/abs/2505.03739</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-based systems, streaming scenarios, multiple audio tokens, model acceleration, real-time conversational capabilities

Summary: 
VITA-Audio introduces a Multiple Cross-modal Token Prediction module to efficiently generate multiple audio tokens during the first model forward pass, reducing latency in streaming scenarios. A four-stage progressive training strategy is implemented to accelerate the model with minimal speech quality loss. The model achieves a 3-5x speedup in inference at the 7B parameter scale and outperforms similar models on ASR, TTS, and SQA tasks. VITA-Audio is the first large language model capable of real-time audio token generation. <div>
arXiv:2505.03739v1 Announce Type: new 
Abstract: With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration</title>
<link>https://arxiv.org/abs/2505.02848</link>
<guid>https://arxiv.org/abs/2505.02848</guid>
<content:encoded><![CDATA[
<div> alignment, healthcare stakeholders, large language models, human-AI alignment, trustworthy

Summary:
The article discusses the importance of aligning healthcare stakeholder preferences with the outputs of large language models (LLMs) to empower the healthcare workflow effectively and responsibly. It emphasizes the crucial role of human professionals in guiding and enhancing the performance of LLMs throughout their life cycle in healthcare applications. By integrating healthcare knowledge, understanding tasks, and providing human guidance, LLMs can better align with human values. The review highlights approaches, tools, and applications for achieving alignment between humans and LLMs in healthcare. The outlook also focuses on enhancing the alignment process to develop trustworthy real-world healthcare applications. <div>
arXiv:2505.02848v1 Announce Type: cross 
Abstract: The wide exploration of large language models (LLMs) raises the awareness of alignment between healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to empower the healthcare workflow effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not always match with healthcare stakeholders' knowledge, demands, and values. To enable a human-AI alignment, healthcare stakeholders will need to perform essential roles in guiding and enhancing the performance of LLMs. Human professionals must participate in the entire life cycle of adopting LLM in healthcare, including training data curation, model training, and inference. In this review, we discuss the approaches, tools, and applications of alignments between healthcare stakeholders and LLMs. We demonstrate that LLMs can better follow human values by properly enhancing healthcare knowledge integration, task understanding, and human guidance. We provide outlooks on enhancing the alignment between humans and LLMs to build trustworthy real-world healthcare applications.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger</title>
<link>https://arxiv.org/abs/2505.02888</link>
<guid>https://arxiv.org/abs/2505.02888</guid>
<content:encoded><![CDATA[
<div> recursive self-improvement, AI agent, information integration threshold, Godelian self-reference, AutoML 
Summary: 
The article introduces the Noise-to-Meaning Recursive Self-Improvement (N2M-RSI) model, which demonstrates that once an AI agent begins feeding its own outputs back as inputs and surpasses a specific information-integration threshold, its internal complexity will exponentially increase. This framework consolidates previous concepts related to self-prompting language models, Godelian self-reference, and AutoML, remaining independent of any particular implementation. The model also extends naturally to networks of interacting agents, suggesting that communication among instances can lead to super-linear effects. For safety purposes, the specific implementation details are omitted, and only a basic, model-agnostic prototype is provided in the appendix. <br /><br />Summary: <div>
arXiv:2505.02888v1 Announce Type: cross 
Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\"odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models</title>
<link>https://arxiv.org/abs/2505.02931</link>
<guid>https://arxiv.org/abs/2505.02931</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic program repair, Instruction-tuned LLMs, Fine-tuning, Multiple outputs, Iterative refinement 

Summary:
Automatic program repair (APR) using instruction-tuned large language models (LLMs) aims to reduce manual efforts by generating multiple patches and iterating over them to refine the results. By limiting the number of patches generated per bug to 10, the study explores a balanced approach between output generation and iteration. Fine-tuning LLMs on APR datasets of varying sizes and using different techniques shows significant improvements in patch quality, with gains of up to 78%. However, exceeding optimal thresholds can lead to diminishing returns due to overfitting. The research highlights the benefits of iterative patch generation, especially in complex benchmarks, and demonstrates the advantages for both base and fine-tuned models. Overall, a balanced APR strategy combining multi-output generation and iterative refinement is crucial for improving the efficiency and effectiveness of automatic program repair. 

<br /><br />Summary: <div>
arXiv:2505.02931v1 Announce Type: cross 
Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach</title>
<link>https://arxiv.org/abs/2505.02952</link>
<guid>https://arxiv.org/abs/2505.02952</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, natural language, coding, problem solving, clarification questions

Summary:<br /><br />Generative AI systems have transformed human interaction by allowing natural language-based coding and problem solving. However, the ambiguity in natural language often leads to imprecise instructions, requiring users to repeatedly test and correct their prompts. This study proposes an iterative approach that systematically reduces ambiguities through a structured sequence of clarification questions and alternative solution proposals, supported by input/output examples. By resolving all uncertainties, a final precise solution is produced. Evaluations on a diverse dataset covering coding, data analysis, and creative writing showcase that this method delivers improved accuracy, competitive resolution times, and enhanced user satisfaction compared to traditional one-shot solutions requiring multiple manual iterations for correct outputs. <div>
arXiv:2505.02952v1 Announce Type: cross 
Abstract: Generative AI systems have revolutionized human interaction by enabling natural language-based coding and problem solving. However, the inherent ambiguity of natural language often leads to imprecise instructions, forcing users to iteratively test, correct, and resubmit their prompts. We propose an iterative approach that systematically narrows down these ambiguities through a structured series of clarification questions and alternative solution proposals, illustrated with input/output examples as well. Once every uncertainty is resolved, a final, precise solution is generated. Evaluated on a diverse dataset spanning coding, data analysis, and creative writing, our method demonstrates superior accuracy, competitive resolution times, and higher user satisfaction compared to conventional one-shot solutions, which typically require multiple manual iterations to achieve a correct output.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radio: Rate-Distortion Optimization for Large Language Model Compression</title>
<link>https://arxiv.org/abs/2505.03031</link>
<guid>https://arxiv.org/abs/2505.03031</guid>
<content:encoded><![CDATA[
<div> compression, language models, quantization, rate-distortion theory, optimization

Summary:<br />
The article addresses the issue of compressing large language models (LLMs) for deployment on resource-limited devices and reducing compute costs. It introduces the concept of LLM quantization from a rate-distortion theory perspective and presents a quantization technique based on rate-distortion optimization. The proposed technique is scalable to LLMs with hundreds of billions of weight parameters and allows users to compress models post-training to a specified model size or accuracy. This approach aims to enable more efficient and environmentally friendly deployment of large-scale AI infrastructure. <div>
arXiv:2505.03031v1 Announce Type: cross 
Abstract: In recent years, the compression of large language models (LLMs) has emerged as a key problem in facilitating LLM deployment on resource-limited devices, reducing compute costs, and mitigating the environmental footprint due to large-scale AI infrastructure. Here, we establish the foundations of LLM quantization from a rate-distortion theory perspective and propose a quantization technique based on simple rate-distortion optimization. Our technique scales to models containing hundreds of billions of weight parameters and offers users the flexibility to compress models, post-training, to a model size or accuracy specified by the user.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAB: Brutally Long Audio Bench</title>
<link>https://arxiv.org/abs/2505.03054</link>
<guid>https://arxiv.org/abs/2505.03054</guid>
<content:encoded><![CDATA[
<div> long-form audio benchmark, audio language models, natural user interactions, audio understanding, multimodal communication
Summary:
The study introduces BLAB, a challenging long-form audio benchmark that evaluates audio language models on localization, duration estimation, emotion, and counting tasks using 51-minute audio clips. BLAB consists of 833+ hours of diverse, full-length audio clips paired with human-annotated natural language questions. Six open-source and proprietary audio language models, including advanced ones like Gemini 2.0 Pro and GPT-4o, struggle with tasks in BLAB, particularly as audio duration increases. The analysis shows that the models perform poorly on localization, temporal reasoning, counting, and understanding non-phonemic information, often relying more on prompts than audio content. This highlights the difficulty audio language models face in comprehending long-form speech and underscores the need for developing models with robust long-form audio understanding capabilities.
<br /><br />Summary: <div>
arXiv:2505.03054v1 Announce Type: cross 
Abstract: Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation</title>
<link>https://arxiv.org/abs/2505.03273</link>
<guid>https://arxiv.org/abs/2505.03273</guid>
<content:encoded><![CDATA[
<div> Keywords: speech separation, audio language models, error correction, Chain-of-Thought, adaptability <br />
Summary: SepALM introduces a novel approach to speech separation by utilizing audio language models (ALMs) to rectify and re-synthesize speech in the text domain after separation. It consists of a separator, corrector, synthesizer, and aligner to enhance precision and adaptability in challenging environments. By incorporating an ALM-based error correction mechanism, the risk of error accumulation is reduced, and optimization challenges are overcome. The use of Chain-of-Thought (CoT) prompting and knowledge distillation techniques aids in the reasoning and training processes of the ALM, further improving performance. Experimental results demonstrate that SepALM significantly improves the accuracy of speech separation and its ability to adapt to new acoustic environments. <br /><br />Summary: <div>
arXiv:2505.03273v1 Announce Type: cross 
Abstract: While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
<link>https://arxiv.org/abs/2505.03335</link>
<guid>https://arxiv.org/abs/2505.03335</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Absolute Zero, Reasoning, Code Executor

Summary: 
The article introduces a new paradigm called Absolute Zero for reinforcement learning with verifiable rewards (RLVR). In Absolute Zero, a model called Absolute Zero Reasoner (AZR) proposes tasks to maximize its own learning progress and improves reasoning by solving them without relying on external data. AZR utilizes a code executor to validate code reasoning tasks and verify answers as a unified source of reward. Despite being trained without external data, AZR achieves state-of-the-art performance on coding and mathematical reasoning tasks, surpassing models that use human-curated examples. Additionally, AZR is scalable across different model sizes and compatible with various model classes. The Absolute Zero paradigm aims to address concerns about the scalability of human-supervised learning and the limited potential for learning from tasks provided by humans in a future where AI surpasses human intelligence. <br /><br />Summary: <div>
arXiv:2505.03335v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Target-unspecific Tasks through a Features Matrix</title>
<link>https://arxiv.org/abs/2505.03414</link>
<guid>https://arxiv.org/abs/2505.03414</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt learning, vision-language models, Features Matrix, general knowledge, target-unspecific tasks

Summary: 
The article discusses the limitations of existing prompt optimizing methods in tackling target-unspecific tasks due to overfitting. To address this issue, the proposed Features Matrix (FM) regularization approach extracts and leverages general knowledge to enhance model performance on such tasks. The FM captures diverse input semantics deeply and finely, preserving essential general knowledge and reducing the risk of overfitting. Evaluations demonstrate that the FM can be integrated as a generic and flexible module into existing frameworks. Furthermore, the FM significantly improves performance on target-unspecific tasks, achieving state-of-the-art results. Overall, the FM regularization approach offers a promising solution to enhancing large vision-language models for a broader range of tasks beyond specific targets. 

Summary: <br /><br />Keywords: prompt learning, vision-language models, Features Matrix, general knowledge, target-unspecific tasks <div>
arXiv:2505.03414v1 Announce Type: cross 
Abstract: Recent developments in prompt learning of large vision-language models have significantly improved performance in target-specific tasks. However, these prompt optimizing methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge having strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) regularization approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories</title>
<link>https://arxiv.org/abs/2505.03443</link>
<guid>https://arxiv.org/abs/2505.03443</guid>
<content:encoded><![CDATA[
<div> Keywords: centralized systems, distributed systems, ICT infrastructure, document repository system, edge repositories

Summary: 
Centralized and distributed systems are two main approaches to organizing ICT infrastructure. Centralized systems concentrate resources in one location for easier management but have single points of failure. Distributed systems spread resources across multiple nodes, offer better scalability and fault tolerance but require complex management. The choice between them depends on factors like application needs, scalability, and data sensitivity. Centralized systems are suitable for applications with limited scalability and centralized control. In contrast, distributed systems excel in large-scale environments requiring high availability and performance. The paper explores a distributed document repository system developed for the Italian Ministry of Justice, utilizing edge repositories to analyze textual data and metadata, enhancing semantic exploration capabilities.<br /><br />Summary: <div>
arXiv:2505.03443v1 Announce Type: cross 
Abstract: Centralized and distributed systems are two main approaches to organizing ICT infrastructure, each with its pros and cons. Centralized systems concentrate resources in one location, making management easier but creating single points of failure. Distributed systems, on the other hand, spread resources across multiple nodes, offering better scalability and fault tolerance, but requiring more complex management. The choice between them depends on factors like application needs, scalability, and data sensitivity. Centralized systems suit applications with limited scalability and centralized control, while distributed systems excel in large-scale environments requiring high availability and performance. This paper explores a distributed document repository system developed for the Italian Ministry of Justice, using edge repositories to analyze textual data and metadata, enhancing semantic exploration capabilities.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models</title>
<link>https://arxiv.org/abs/2505.03501</link>
<guid>https://arxiv.org/abs/2505.03501</guid>
<content:encoded><![CDATA[
<div> lingual-backdoor attacks, Large Language Models (LLMs), racial discrimination, BadLingual, vulnerabilities  

Summary:  
- This paper introduces lingual-backdoor attacks on Large Language Models (LLMs), using language triggers to produce inflammatory speech targeting specific language-speaking groups, exacerbating racial discrimination.
- A baseline lingual-backdoor attack is implemented by poisoning training data through translation, but lacks generalization and real-world practicality.
- BadLingual, a task-agnostic lingual-backdoor, is designed to trigger any downstream tasks within LLMs without task-specific requirements.
- PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) is used for adversarial training to enhance lingual-backdoor generalization across various tasks.
- Experimental results show the baseline attack achieving high ASR on specified tasks but low ASR in task-agnostic scenarios, with BadLingual improving ASR by up to 37.35% over the baseline.
<br /><br />Summary: <div>
arXiv:2505.03501v1 Announce Type: cross 
Abstract: In this paper, we present a new form of backdoor attack against Large Language Models (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks is that the language itself serves as the trigger to hijack the infected LLMs to generate inflammatory speech. They enable the precise targeting of a specific language-speaking group, exacerbating racial discrimination by malicious entities. We first implement a baseline lingual-backdoor attack, which is carried out by poisoning a set of training data for specific downstream tasks through translation into the trigger language. However, this baseline attack suffers from poor task generalization and is impractical in real-world settings. To address this challenge, we design BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any downstream tasks within the chat LLMs, regardless of the specific questions of these tasks. We design a new approach using PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) based adversarial training to expand the decision boundary of lingual-backdoor, thereby enhancing the generalization ability of lingual-backdoor across various tasks. We perform extensive experiments to validate the effectiveness of our proposed attacks. Specifically, the baseline attack achieves an ASR of over 90% on the specified tasks. However, its ASR reaches only 37.61% across six tasks in the task-agnostic scenario. In contrast, BadLingual brings up to 37.35% improvement over the baseline. Our study sheds light on a new perspective of vulnerabilities in LLMs with multilingual capabilities and is expected to promote future research on the potential defenses to enhance the LLMs' robustness
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval</title>
<link>https://arxiv.org/abs/2505.03676</link>
<guid>https://arxiv.org/abs/2505.03676</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse neural information retrieval, Rational Speech Acts, document collection, token-document interactions, BEIR benchmark

Summary:<br /><br />
The paper introduces a novel approach to improve sparse neural information retrieval by incorporating the Rational Speech Acts (RSA) framework. Traditional IR methods like BM25 do not consider the complexities of document collections and interplay between different term weights. The RSA framework, originally used in linguistics to minimize communication features, is adapted to the IR context to dynamically modulate token-document interactions. By considering the influence of other documents in the dataset, RSA enhances document representations and contrasts them effectively. Experimental results demonstrate consistent improvements in multiple sparse retrieval models, ultimately achieving state-of-the-art performance on out-of-domain datasets from the BEIR benchmark. This innovative integration of RSA offers a promising avenue for optimizing information retrieval systems, showcasing the potential of linguistic frameworks in IR research. 

Summary: <div>
arXiv:2505.03676v1 Announce Type: cross 
Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser extent more traditional models such as BM25, do not take into account the document collection and the complex interplay between different term weights when representing a single document. In this paper, we show how the Rational Speech Acts (RSA), a linguistics framework used to minimize the number of features to be communicated when identifying an object in a set, can be adapted to the IR case -- and in particular to the high number of potential features (here, tokens). RSA dynamically modulates token-document interactions by considering the influence of other documents in the dataset, better contrasting document representations. Experiments show that incorporating RSA consistently improves multiple sparse retrieval models and achieves state-of-the-art performance on out-of-domain datasets from the BEIR benchmark. https://github.com/arthur-75/Rational-Retrieval-Acts
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incoherent Probability Judgments in Large Language Models</title>
<link>https://arxiv.org/abs/2401.16646</link>
<guid>https://arxiv.org/abs/2401.16646</guid>
<content:encoded><![CDATA[
<div> autoregressive large language models, probability judgments, coherence, Bayesian inference, Bayesian Sampler<br />
Summary:<br />
Autoregressive Large Language Models (LLMs) trained for next-word prediction are adept at generating coherent text but struggle with forming coherent probability judgments. Through the use of probabilistic identities and repeated judgments, researchers assessed the coherence of LLMs' probability judgments and found them to often be incoherent, deviating from probability theory rules. Additionally, when prompted to judge the same event multiple times, LLMs display a mean-variance relationship of probability judgments similar to that of humans, showing an inverted-U-shaped pattern. These deviations from rationality in LLMs' probability judgments may be explained by linking them to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments. Research suggests that further investigation into the workings of autoregressive LLMs in relation to Bayesian inference could shed light on their behavior in probability judgment tasks. <br /><br />Summary: <div>
arXiv:2401.16646v2 Announce Type: replace 
Abstract: Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors</title>
<link>https://arxiv.org/abs/2406.14498</link>
<guid>https://arxiv.org/abs/2406.14498</guid>
<content:encoded><![CDATA[
<div> Keywords: Wearables, motion data, LLaSA, sensor-based QA, large-scale datasets

Summary:
Wearables collect extensive motion data, but existing systems only classify actions without providing explanations. LLaSA, a 13B model, allows open-ended questioning grounded in raw IMU data. It supports context-aware reasoning, offering explanations for detected behaviors and answering free-form questions. Three large-scale datasets, SensorCaps, OpenSQA, and Tune-OpenSQA have been released to benchmark sensor-based QA models. LLaSA excels in accuracy, coherence, and reliability, surpassing commercial language models in various scenarios. The model's code repository and datasets are available on the GitHub page of the BASHLab. <br /><br />Summary: <div>
arXiv:2406.14498v3 Announce Type: replace 
Abstract: Wearables generate rich motion data, yet current systems only classify what happened - failing to support natural questions about why it happened or what it means. We introduce LLaSA (Large Language and Sensor Assistant), a compact 13B model that enables ask-anything, open-ended question answering grounded in raw IMU data. LLaSA supports conversational, context-aware reasoning - explaining the causes of sensor-detected behaviors and answering free-form questions in real-world scenarios. It is tuned for scientific accuracy, coherence, and response reliability. To advance this new task of sensor-based QA, we release three large-scale datasets: SensorCaps, OpenSQA, and Tune-OpenSQA. Together, these resources define a new benchmark for sensor-language models. LLaSA consistently produces interpretable, causal answers and outperforms commercial LLMs across both public and real-world settings. Our code repository and datasets can be found at https://github.com/BASHLab/LLaSA.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFBench: A Comprehensive Constraints-Following Benchmark for LLMs</title>
<link>https://arxiv.org/abs/2408.01122</link>
<guid>https://arxiv.org/abs/2408.01122</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Comprehensive Constraints Following Benchmark, NLP tasks, evaluation, improvement

Summary:
Large Language Models (LLMs) play a crucial role in understanding natural language instructions for real-world applications. The Comprehensive Constraints Following Benchmark (CFBench) is introduced to evaluate LLMs comprehensively with 1,000 curated samples covering over 200 scenarios and 50 NLP tasks. CFBench categorizes constraints into 10 primary categories and develops a systematic framework for constraint types. An advanced evaluation methodology is proposed to align LLM outputs with user perceptions and prioritize requirement fulfillment. Current leading LLMs show significant room for improvement in constraints following on CFBench, prompting further investigation into influencing factors and enhancement strategies. The data and code for CFBench are publicly available for research purposes. 

<br /><br />Summary: <div>
arXiv:2408.01122v2 Announce Type: replace 
Abstract: The adeptness of Large Language Models (LLMs) in comprehending and following natural language instructions is critical for their deployment in sophisticated real-world applications. Existing evaluations mainly focus on fragmented constraints or narrow scenarios, but they overlook the comprehensiveness and authenticity of constraints from the user's perspective. To bridge this gap, we propose CFBench, a large-scale Comprehensive Constraints Following Benchmark for LLMs, featuring 1,000 curated samples that cover more than 200 real-life scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from real-world instructions and constructs an innovative systematic framework for constraint types, which includes 10 primary categories and over 25 subcategories, and ensures each constraint is seamlessly integrated within the instructions. To make certain that the evaluation of LLM outputs aligns with user perceptions, we propose an advanced methodology that integrates multi-dimensional assessment criteria with requirement prioritization, covering various perspectives of constraints, instructions, and requirement fulfillment. Evaluating current leading LLMs on CFBench reveals substantial room for improvement in constraints following, and we further investigate influencing factors and enhancement strategies. The data and code are publicly available at https://github.com/PKU-Baichuan-MLSystemLab/CFBench
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-3D Print: Large Language Models To Monitor and Control 3D Printing</title>
<link>https://arxiv.org/abs/2408.14307</link>
<guid>https://arxiv.org/abs/2408.14307</guid>
<content:encoded><![CDATA[
<div> Keywords: Industry 4.0, Additive Manufacturing, Fused Deposition Modeling, Process Monitoring, Large Language Models

Summary:
Industry 4.0 has transformed manufacturing by promoting digitalization and additive manufacturing using technologies like Fused Deposition Modeling (FDM). However, the complexity of material extrusion can lead to printing defects that require expert intervention. Existing automated error detection and machine learning models have limited generalizability and scalability. To address these challenges, a new framework combining Large Language Models (LLMs) with 3D printers has been introduced. This framework monitors the printing process, detects defects, analyzes images post-printing, identifies failure modes, queries the printer for parameters, and generates and executes corrective actions. Evaluation against a control group showed that LLM-based agents accurately detect common 3D printing errors and autonomously correct them without human intervention. This framework has the potential to enhance the quality and efficiency of additive manufacturing processes. 

<br /><br />Summary: <div>
arXiv:2408.14307v2 Announce Type: replace 
Abstract: Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution</title>
<link>https://arxiv.org/abs/2410.00153</link>
<guid>https://arxiv.org/abs/2410.00153</guid>
<content:encoded><![CDATA[
arXiv:2410.00153v3 Announce Type: replace 
Abstract: Probing learned concepts in large language models (LLMs) is crucial for understanding how semantic knowledge is encoded internally. Training linear classifiers on probing tasks is a principle approach to denote the vector of a certain concept in the representation space. However, the single vector identified for a concept varies with both data and training, making it less robust and weakening its effectiveness in real-world applications. To address this challenge, we propose an approach to approximate the subspace representing a specific concept. Built on linear probing classifiers, we extend the concept vectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's effectiveness through measuring its faithfulness and plausibility across multiple LLMs with different sizes and architectures. Additionally, we use representation intervention tasks to showcase its efficacy in real-world applications such as emotion steering. Experimental results indicate that GCS concept vectors have the potential to balance steering performance and maintaining the fluency in natural language generation tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2410.09580</link>
<guid>https://arxiv.org/abs/2410.09580</guid>
<content:encoded><![CDATA[
arXiv:2410.09580v2 Announce Type: replace 
Abstract: Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner are used to guide the training of S-agent, creating a self-training loop where S-agent can iteratively improve its capability for conversational planning. Furthermore, we propose an efficient variant SAPIENT-e for trade-off between training efficiency and performance. Extensive experiments on four benchmark datasets validate the effectiveness of our approach, showing that SAPIENT outperforms the state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalization of Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2411.00027</link>
<guid>https://arxiv.org/abs/2411.00027</guid>
<content:encoded><![CDATA[
arXiv:2411.00027v2 Announce Type: replace 
Abstract: Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment</title>
<link>https://arxiv.org/abs/2412.18135</link>
<guid>https://arxiv.org/abs/2412.18135</guid>
<content:encoded><![CDATA[
arXiv:2412.18135v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) demonstrate exceptional performance across various domains, deploying LLMs on edge devices has emerged as a new trend. Quantization techniques, which reduce the size and memory requirements of LLMs, are effective for deploying LLMs on resource-limited edge devices. However, existing one-size-fits-all quantization methods often fail to dynamically adjust the memory requirements of LLMs, limiting their applications to practical edge devices with various computation resources. To tackle this issue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for adaptive quantization and dynamic deployment of LLMs based on layer importance. Specifically, LSAQ evaluates the importance of LLMs' neural layers by constructing top-k token sets from the inputs and outputs of each layer and calculating their Jaccard similarity. Based on layer importance, our system adaptively adjusts quantization strategies in real time according to the computation resource of edge devices, which applies higher quantization precision to layers with higher importance, and vice versa. {Experimental results show that LSAQ consistently outperforms the selected quantization baselines in terms of perplexity and zero-shot tasks. Additionally, it can devise appropriate quantization schemes for different usage scenarios to facilitate the deployment of LLMs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-reflecting Large Language Models: A Hegelian Dialectical Approach</title>
<link>https://arxiv.org/abs/2501.14917</link>
<guid>https://arxiv.org/abs/2501.14917</guid>
<content:encoded><![CDATA[
arXiv:2501.14917v4 Announce Type: replace 
Abstract: Investigating NLP through a philosophical lens has recently caught researcher's eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the \textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the opposing points of view. Moreover, this paper investigates the effect of LLMs' temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed-temperature strategy for generation. We assess the effectiveness of our proposed method in generating novel ideas and in improving the reasoning abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. Our experiments demonstrate promising results in generating ideas and enhancing problem-solving performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech</title>
<link>https://arxiv.org/abs/2501.15858</link>
<guid>https://arxiv.org/abs/2501.15858</guid>
<content:encoded><![CDATA[
arXiv:2501.15858v3 Announce Type: replace 
Abstract: Purpose: Speech intelligibility is a critical outcome in the assessment and management of dysarthria, yet most research and clinical practices have focused on English, limiting their applicability across languages. This commentary introduces a conceptual framework--and a demonstration of how it can be implemented--leveraging artificial intelligence (AI) to advance cross-language intelligibility assessment of dysarthric speech. Method: We propose a two-tiered conceptual framework consisting of a universal speech model that encodes dysarthric speech into acoustic-phonetic representations, followed by a language-specific intelligibility assessment model that interprets these representations within the phonological or prosodic structures of the target language. We further identify barriers to cross-language intelligibility assessment of dysarthric speech, including data scarcity, annotation complexity, and limited linguistic insights into dysarthric speech, and outline potential AI-driven solutions to overcome these challenges. Conclusion: Advancing cross-language intelligibility assessment of dysarthric speech necessitates models that are both efficient and scalable, yet constrained by linguistic rules to ensure accurate and language-sensitive assessment. Recent advances in AI provide the foundational tools to support this integration, shaping future directions toward generalizable and linguistically informed assessment frameworks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting potentially abusive clauses in Chilean terms of services with natural language processing</title>
<link>https://arxiv.org/abs/2502.00865</link>
<guid>https://arxiv.org/abs/2502.00865</guid>
<content:encoded><![CDATA[
arXiv:2502.00865v2 Announce Type: replace 
Abstract: This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and/or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v2 Announce Type: replace 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports</title>
<link>https://arxiv.org/abs/2502.14338</link>
<guid>https://arxiv.org/abs/2502.14338</guid>
<content:encoded><![CDATA[
arXiv:2502.14338v3 Announce Type: replace 
Abstract: Accurate translation of bug reports is critical for efficient collaboration in global software development. In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and large language models such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag. To assess both translation quality and source language identification accuracy, we employ a range of MT evaluation metrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside classification metrics such as accuracy, precision, recall, and F1-score. Our findings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical translation quality, it does not lead in source language identification. Claude and Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively), and Gemini records the best precision (0.7414). AWS Translate shows the highest accuracy (0.4717) in identifying source languages. These results highlight that no single system dominates across all tasks, reinforcing the importance of task-specific evaluations. This study underscores the need for domain adaptation when translating technical content and provides actionable insights for integrating MT into bug-triaging workflows. The code and dataset for this paper are available at GitHub-https://github.com/av9ash/English-Please
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIG-Bench Extra Hard</title>
<link>https://arxiv.org/abs/2502.19187</link>
<guid>https://arxiv.org/abs/2502.19187</guid>
<content:encoded><![CDATA[
arXiv:2502.19187v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\% for the best general-purpose model and 44.8\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models</title>
<link>https://arxiv.org/abs/2503.10707</link>
<guid>https://arxiv.org/abs/2503.10707</guid>
<content:encoded><![CDATA[
arXiv:2503.10707v2 Announce Type: replace 
Abstract: Cancer survivors face unique emotional challenges that impact their quality of life. Mobile diary entries provide a promising method for tracking emotional states, improving self-awareness, and promoting well-being outcome. This paper aims to, through mobile diaries, understand cancer survivors' emotional states and key variables related to just-in-time intervention opportunities, including the desire to regulate emotions and the availability to engage in interventions. Although emotion analysis tools show potential for recognizing emotions from text, current methods lack the contextual understanding necessary to interpret brief mobile diary narratives. Our analysis of diary entries from cancer survivors (N=407) reveals systematic relationships between described contexts and emotional states, with administrative and health-related contexts associated with negative affect and regulation needs, while leisure activities promote positive emotions. We propose CALLM, a Context-Aware framework leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to analyze these brief entries by integrating retrieved peer experiences and personal diary history. CALLM demonstrates strong performance with balanced accuracies reaching 72.96% for positive affect, 73.29% for negative affect, 73.72% for emotion regulation desire, and 60.09% for intervention availability, outperforming language model baselines. Post-hoc analysis reveals that model confidence strongly predicts accuracy, with longer diary entries generally enhancing performance, and brief personalization periods yielding meaningful improvements. Our findings demonstrate how contextual information in mobile diaries can be effectively leveraged to understand emotional experiences, predict key states, and identify optimal intervention moments for personalized just-in-time support.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2503.13551</link>
<guid>https://arxiv.org/abs/2503.13551</guid>
<content:encoded><![CDATA[
arXiv:2503.13551v3 Announce Type: replace 
Abstract: Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate step. In addition, the cost of annotating reasoning processes for reward modeling is high, making large-scale collection of high-quality data challenging. To address this, we propose a novel reward model approach called the Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps at both fine-grained and coarse-grained levels. HRM excels at assessing multi-step reasoning coherence, especially when flawed steps are later corrected through self-reflection. To further reduce the cost of generating training data, we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC), which merges two consecutive reasoning steps into one within the tree structure. By applying HNC to MCTS-generated reasoning trajectories, we enhance the diversity and robustness of HRM training data while introducing controlled noise with minimal computational overhead. Empirical results on the PRM800K dataset show that HRM, together with HNC, provides more stable and reliable evaluations than PRM. Furthermore, cross-domain evaluations on the MATH500 and GSM8K datasets demonstrate HRM's strong generalization and robustness across a variety of reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement</title>
<link>https://arxiv.org/abs/2503.17279</link>
<guid>https://arxiv.org/abs/2503.17279</guid>
<content:encoded><![CDATA[
arXiv:2503.17279v2 Announce Type: replace 
Abstract: The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM), where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised nonlinear projection is learned to reduce the dimensionality of the LLM-based text embeddings. We show that CASE significantly outperforms previously proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing standard benchmark dataset. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings. Moreover, we propose a supervised dimensionality reduction method that not only reduces the dimensionality of LLM-based embeddings but also significantly improves their performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment</title>
<link>https://arxiv.org/abs/2503.18991</link>
<guid>https://arxiv.org/abs/2503.18991</guid>
<content:encoded><![CDATA[
arXiv:2503.18991v2 Announce Type: replace 
Abstract: The alignment of large language models (LLMs) with human values remains critical yet hindered by four key challenges: (1) scarcity of balanced safety datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to shallow alignment, and (4) inability to dynamically adapt rewards according to task difficulty. To address these limitations, we introduce HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a novel alignment approach inspired by shadow models in membership inference attacks. Our approach consists of two main components: (1) construction of a balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using structured prompts that leverage the introspective reasoning capabilities of LLMs; and (2) training of category-specific reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization to task difficulty at both the data and model levels. Comprehensive experiments across four harmlessness and four usefulness benchmarks demonstrate that HAIR achieves state-of-the-art performance, outperforming all baseline methods in safety while maintaining high levels of usefulness.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clean &amp; Clear: Feasibility of Safe LLM Clinical Guidance</title>
<link>https://arxiv.org/abs/2503.20953</link>
<guid>https://arxiv.org/abs/2503.20953</guid>
<content:encoded><![CDATA[
arXiv:2503.20953v2 Announce Type: replace 
Abstract: Background:
  Clinical guidelines are central to safe evidence-based medicine in modern healthcare, providing diagnostic criteria, treatment options and monitoring advice for a wide range of illnesses. LLM-empowered chatbots have shown great promise in Healthcare Q&amp;A tasks, offering the potential to provide quick and accurate responses to medical inquiries.
  Our main objective was the development and preliminary assessment of an LLM-empowered chatbot software capable of reliably answering clinical guideline questions using University College London Hospital (UCLH) clinical guidelines.
  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant information from the UCLH guidelines to answer questions. Our approach highlights the safety and reliability of referencing information over its interpretation and response generation. Seven doctors from the ward assessed the chatbot's performance by comparing its answers to the gold standard.
  Results: Our chatbot demonstrates promising performance in terms of relevance, with ~73% of its responses rated as very relevant, showcasing a strong understanding of the clinical context. Importantly, our chatbot achieves a recall of 1.00 for extracted guideline lines, substantially minimising the risk of missing critical information. Approximately 78% of responses were rated satisfactory in terms of completeness. A small portion (~14.5%) contained minor unnecessary information, indicating occasional lapses in precision. The chatbot' showed high efficiency, with an average completion time of 10 seconds, compared to 30 seconds for human respondents. Evaluation of clinical reasoning showed that 72% of the chatbot's responses were without flaws. Our chatbot demonstrates significant potential to speed up and improve the process of accessing locally relevant clinical information for healthcare professionals.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAL: Steerable Reasoning Calibration of Large Language Models for Free</title>
<link>https://arxiv.org/abs/2504.07986</link>
<guid>https://arxiv.org/abs/2504.07986</guid>
<content:encoded><![CDATA[
arXiv:2504.07986v2 Announce Type: replace 
Abstract: Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our code is publicly available at https://github.com/VITA-Group/SEAL.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback</title>
<link>https://arxiv.org/abs/2404.05046</link>
<guid>https://arxiv.org/abs/2404.05046</guid>
<content:encoded><![CDATA[
arXiv:2404.05046v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through Fine-Grained Artificial Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Specifically, We first utilize AI tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback. Then, based on the collected reward data, three specialized reward models are trained to produce dense rewards. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OAC: Output-adaptive Calibration for Accurate Post-training Quantization</title>
<link>https://arxiv.org/abs/2405.15025</link>
<guid>https://arxiv.org/abs/2405.15025</guid>
<content:encoded><![CDATA[
arXiv:2405.15025v2 Announce Type: replace-cross 
Abstract: Deployment of Large Language Models (LLMs) has major computational costs, due to their rapidly expanding size. Compression of LLMs reduces the memory footprint, latency, and energy required for their inference. Post-training Quantization (PTQ) techniques have been developed to compress LLMs while avoiding expensive re-training. Most PTQ approaches formulate the quantization error based on a layer-wise Euclidean loss, ignoring the model output. Then, each layer is calibrated using its layer-wise Hessian to update the weights towards minimizing the quantization error. The Hessian is also used for detecting the most salient weights to quantization. Such PTQ approaches are prone to accuracy drop in low-precision quantization. We propose Output-adaptive Calibration (OAC) to incorporate the model output in the calibration process. We formulate the quantization error based on the distortion of the output cross-entropy loss. OAC approximates the output-adaptive Hessian for each layer under reasonable assumptions to reduce the computational complexity. The output-adaptive Hessians are used to update the weight matrices and detect the salient weights towards maintaining the model output. Our proposed method outperforms the state-of-the-art baselines such as SpQR and BiLLM, especially, at extreme low-precision (2-bit and binary) quantization.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</title>
<link>https://arxiv.org/abs/2405.19313</link>
<guid>https://arxiv.org/abs/2405.19313</guid>
<content:encoded><![CDATA[
arXiv:2405.19313v2 Announce Type: replace-cross 
Abstract: The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioBench: A Universal Benchmark for Audio Large Language Models</title>
<link>https://arxiv.org/abs/2406.16020</link>
<guid>https://arxiv.org/abs/2406.16020</guid>
<content:encoded><![CDATA[
arXiv:2406.16020v5 Announce Type: replace-cross 
Abstract: We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench addresses this gap by setting up datasets as well as desired evaluation metrics. Besides, we also evaluated the capabilities of five popular models and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-sourced evaluation toolkit, data, and leaderboard will offer a robust testbed for future model developments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate</title>
<link>https://arxiv.org/abs/2410.22086</link>
<guid>https://arxiv.org/abs/2410.22086</guid>
<content:encoded><![CDATA[
arXiv:2410.22086v3 Announce Type: replace-cross 
Abstract: Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</title>
<link>https://arxiv.org/abs/2411.18915</link>
<guid>https://arxiv.org/abs/2411.18915</guid>
<content:encoded><![CDATA[
arXiv:2411.18915v4 Announce Type: replace-cross 
Abstract: Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization</title>
<link>https://arxiv.org/abs/2412.17739</link>
<guid>https://arxiv.org/abs/2412.17739</guid>
<content:encoded><![CDATA[
arXiv:2412.17739v3 Announce Type: replace-cross 
Abstract: Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales and benchmarks show that, within varying context windows, FoPE maintains a more stable performance compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models</title>
<link>https://arxiv.org/abs/2502.07328</link>
<guid>https://arxiv.org/abs/2502.07328</guid>
<content:encoded><![CDATA[
arXiv:2502.07328v3 Announce Type: replace-cross 
Abstract: The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models -- MusicGen and Mustango, for two underrepresented non-Western music traditions -- Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling</title>
<link>https://arxiv.org/abs/2503.02445</link>
<guid>https://arxiv.org/abs/2503.02445</guid>
<content:encoded><![CDATA[
arXiv:2503.02445v3 Announce Type: replace-cross 
Abstract: Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused on generating realistic time series by incorporating textual descriptions. To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications</title>
<link>https://arxiv.org/abs/2503.02950</link>
<guid>https://arxiv.org/abs/2503.02950</guid>
<content:encoded><![CDATA[
arXiv:2503.02950v2 Announce Type: replace-cross 
Abstract: We introduce LiteWebAgent, an open-source suite for VLM-based web agent applications. Our framework addresses a critical gap in the web agent ecosystem with a production-ready solution that combines minimal serverless backend configuration, intuitive user and browser interfaces, and extensible research capabilities in agent planning, memory, and tree search. For the core LiteWebAgent agent framework, we implemented a simple yet effective baseline using recursive function calling, providing with decoupled action generation and action grounding. In addition, we integrate advanced research components such as agent planning, agent workflow memory, and tree search in a modular and extensible manner. We then integrate the LiteWebAgent agent framework with frontend and backend as deployed systems in two formats: (1) a production Vercel-based web application, which provides users with an agent-controlled remote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control an existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent framework is available at https://github.com/PathOnAI/LiteWebAgent, with deployed frontend at https://lite-web-agent.vercel.app/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction</title>
<link>https://arxiv.org/abs/2503.15661</link>
<guid>https://arxiv.org/abs/2503.15661</guid>
<content:encoded><![CDATA[
arXiv:2503.15661v2 Announce Type: replace-cross 
Abstract: Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges and licensing issues. We introduce UI-Vision, the first comprehensive, license-permissive benchmark for offline, fine-grained evaluation of computer use agents in real-world desktop environments. Unlike online benchmarks, UI-Vision provides: (i) dense, high-quality annotations of human demonstrations, including bounding boxes, UI labels, and action trajectories (clicks, drags, and keyboard inputs) across 83 software applications, and (ii) three fine-to-coarse grained tasks-Element Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to rigorously evaluate agents' performance in desktop environments. Our evaluation reveals critical limitations in state-of-the-art models like UI-TARS-72B, including issues with understanding professional software, spatial reasoning, and complex actions like drag-and-drop. These findings highlight the challenges in developing fully autonomous computer use agents. By releasing UI-Vision as open-source, we aim to advance the development of more capable agents for real-world desktop tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2504.11739</link>
<guid>https://arxiv.org/abs/2504.11739</guid>
<content:encoded><![CDATA[
arXiv:2504.11739v2 Announce Type: replace-cross 
Abstract: The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation</title>
<link>https://arxiv.org/abs/2505.01456</link>
<guid>https://arxiv.org/abs/2505.01456</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal unlearning, MLLMs, adversarial attacks, defense strategies, benchmark

Summary:
Multimodal Large Language Models (MLLMs) trained on large datasets may unintentionally learn sensitive information, making them vulnerable to adversarial attacks. This risk is higher in MLLMs that combine image and text modalities. To assess the effectiveness of forgetting such information (targeted unlearning), a new benchmark called UnLOK-VQA is introduced, focusing on multimodal unlearning. An attack-and-defense framework evaluates methods for deleting specific knowledge from MLLMs. Results show that multimodal attacks are more successful than text- or image-only attacks, and the most effective defense involves removing answer information from internal model states. Larger models display better post-editing robustness, indicating that scale improves safety. UnLOK-VQA provides a comprehensive benchmark for advancing unlearning in MLLMs. 

<br /><br />Summary: <div>
arXiv:2505.01456v1 Announce Type: new 
Abstract: LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling</title>
<link>https://arxiv.org/abs/2505.01459</link>
<guid>https://arxiv.org/abs/2505.01459</guid>
<content:encoded><![CDATA[
<div> synergistically scalability efficiency xLSTM MoE entropy-based<br />
<br />
Summary:<br />
The paper introduces MoxE, a novel architecture that combines xLSTM with MoE to address scalability and efficiency challenges in large language models (LLMs). MoxE strategically leverages xLSTM's memory structures and introduces sparsity through MoE to reduce computational overhead. An entropy-based routing mechanism dynamically routes tokens to specialized experts, ensuring efficient resource utilization. MoxE effectively manages rare and common tokens, using mLSTM blocks for rare tokens. Auxiliary losses, including entropy-based and group-wise balancing losses, enhance generalization and training efficiency. Theoretical analysis and empirical evaluations show that MoxE outperforms existing approaches in efficiency and effectiveness, representing a significant advancement in scalable LLM architectures. <br /><br /> <div>
arXiv:2505.01459v1 Announce Type: new 
Abstract: This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SymPlanner: Deliberate Planning in Language Models with Symbolic Representation</title>
<link>https://arxiv.org/abs/2505.01479</link>
<guid>https://arxiv.org/abs/2505.01479</guid>
<content:encoded><![CDATA[
<div> planning, language models, SymPlanner, symbolic environment, Iterative Correction, Contrastive Ranking

Summary:
SymPlanner is a new framework that enhances language models' planning capabilities by combining natural language reasoning with a symbolic environment for structured planning. The framework utilizes a policy model to propose actions and a symbolic environment to execute and verify their effects in a deterministic manner. Iterative Correction (IC) improves exploration and robustness by refining actions based on feedback from the environment. Contrastive Ranking (CR) allows for a detailed comparison of candidate plans. Evaluation on PlanBench shows that SymPlanner produces more coherent, diverse, and verifiable plans compared to natural language baselines. The framework's integration of symbolic reasoning with language models enables more effective planning in domains requiring complex, multi-step action sequences grounded in external constraints. <div>
arXiv:2505.01479v1 Announce Type: new 
Abstract: Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the effectiveness of Large Language Models in the mechanical design domain</title>
<link>https://arxiv.org/abs/2505.01559</link>
<guid>https://arxiv.org/abs/2505.01559</guid>
<content:encoded><![CDATA[
<div> ABC dataset, mechanical engineering, large language models, unsupervised tasks, model performance  
Summary:  
Large language models were evaluated in the mechanical engineering domain using data from the ABC dataset. Two unsupervised tasks were created to assess model performance on domain-specific data: a binary sentence-pair classification task and a zero-shot classification task. By fine-tuning the model to combat overfitting through modifications in learning rates, dropout values, sequence length, and adding a multi-head attention layer, an accuracy of 0.62 was achieved in the binary sentence-pair classification task. Additionally, the model exceeded baselines in the zero-shot classification task, achieving a top-1 classification accuracy of 0.386. These results provided insights into the challenges encountered when learning from language data in the mechanical engineering domain.  
<br /><br />Summary: <div>
arXiv:2505.01559v1 Announce Type: new 
Abstract: In this work, we seek to understand the performance of large language models in the mechanical engineering domain. We leverage the semantic data found in the ABC dataset, specifically the assembly names that designers assigned to the overall assemblies, and the individual semantic part names that were assigned to each part. After pre-processing the data we developed two unsupervised tasks to evaluate how different model architectures perform on domain-specific data: a binary sentence-pair classification task and a zero-shot classification task. We achieved a 0.62 accuracy for the binary sentence-pair classification task with a fine-tuned model that focuses on fighting over-fitting: 1) modifying learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a multi-head attention layer. Our model on the zero-shot classification task outperforms the baselines by a wide margin, and achieves a top-1 classification accuracy of 0.386. The results shed some light on the specific failure modes that arise when learning from language in this domain.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains</title>
<link>https://arxiv.org/abs/2505.01560</link>
<guid>https://arxiv.org/abs/2505.01560</guid>
<content:encoded><![CDATA[
<div> NMT, LLM, machine translation, multi-agent orchestration, evaluation <br />
Summary: 
The study compares different machine translation paradigms, including NMT and LLMs, for translating legal contracts and news articles in Spanish, Catalan, and Turkish. While NMT performs well in automatic evaluations, LLMs with reasoning layers show better adequacy and fluency in human evaluations. However, these gains come with higher token costs, particularly for multi-agent workflows. The study suggests the need for multidimensional and cost-aware evaluation methods, and proposes leaner coordination strategies, selective agent activation, and hybrid pipelines as potential research directions to improve translation quality and efficiency. <br /> <div>
arXiv:2505.01560v1 Announce Type: new 
Abstract: Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing.
  Automatic scores still favour the mature NMT system, which ranks first in seven of twelve metric-language combinations; o1-preview ties or places second in most remaining cases, while both multi-agent workflows trail. Human evaluation reverses part of this narrative: o1-preview produces the most adequate and fluent output in five of six comparisons, and the iterative agent edges ahead once, indicating that reasoning layers capture semantic nuance undervalued by surface metrics. Yet these qualitative gains carry steep costs. The sequential agent consumes roughly five times, and the iterative agent fifteen times, the tokens used by NMT or single-pass LLMs.
  We advocate multidimensional, cost-aware evaluation protocols and highlight research directions that could tip the balance: leaner coordination strategies, selective agent activation, and hybrid pipelines combining single-pass LLMs with targeted agent intervention.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents</title>
<link>https://arxiv.org/abs/2505.01592</link>
<guid>https://arxiv.org/abs/2505.01592</guid>
<content:encoded><![CDATA[
<div> capabilities, large language models, task planning agents, PIPA, user satisfaction <br />
Summary: <br />
The article introduces a new evaluation protocol called PIPA for task planning agents, emphasizing user satisfaction over simple task completion. It highlights the importance of considering the entire agentic process, not just the end result, in maximizing user satisfaction. The protocol is based on a partially observable Markov Decision Process (POMDP) paradigm and offers a detailed assessment of agent performance through atomic evaluation criteria. The analysis reveals that agents excel in different behavioral stages, with user satisfaction influenced by outcomes and intermediate behaviors. The article also discusses future directions, such as systems utilizing multiple agents, and points out the limitations of user simulators in task planning. The proposed protocol aims to provide a more comprehensive framework for evaluating the effectiveness of interactive task planning agents. <br /> <div>
arXiv:2505.01592v1 Announce Type: new 
Abstract: The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose PIPA, a unified evaluation protocol that conceptualizes the behavioral process of interactive task planning agents within a partially observable Markov Decision Process (POMDP) paradigm. The proposed protocol offers a comprehensive assessment of agent performance through a set of atomic evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Always Tell Me The Odds: Fine-grained Conditional Probability Estimation</title>
<link>https://arxiv.org/abs/2505.01595</link>
<guid>https://arxiv.org/abs/2505.01595</guid>
<content:encoded><![CDATA[
<div> probability estimation, large language models, uncertainty, fine-grained, conditional probability

Summary:

- The article presents a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context.
- Recent advancements in large language models have improved reasoning capabilities but struggle with accurate probabilistic predictions under uncertainty.
- Incorporating uncertainty into model predictions has shown performance improvements, but reliable estimates of uncertainty remain understudied.
- LLM probability estimates tend to be coarse and biased towards more frequent numbers.
- The proposed set of strong and precise probability estimation models outperforms existing methods significantly through a combination of human and synthetic data creation, scaling to larger models, and better supervision. 

<br /><br />Summary: <div>
arXiv:2505.01595v1 Announce Type: new 
Abstract: We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated probabilistic predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency</title>
<link>https://arxiv.org/abs/2505.01658</link>
<guid>https://arxiv.org/abs/2505.01658</guid>
<content:encoded><![CDATA[
<div> inference engines, large language models, optimization methods, service requirements, ecosystem maturity 
Summary: 
In the realm of large language models (LLMs), the use of inference engines plays a crucial role in optimizing workloads such as chain-of-thought, complex reasoning, and agent services. This paper evaluates 25 open-source and commercial inference engines based on criteria such as ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Additionally, the study explores the optimization techniques supported by each engine and assesses the ecosystem maturity of open-source options, as well as the performance and cost policy of commercial solutions. The future research directions highlighted include support for complex LLM-based services, various hardware, and enhanced security. Researchers and developers can utilize this evaluation to make informed decisions when selecting and designing optimized LLM inference engines. A public repository is provided to track ongoing developments in this rapidly evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine
<br /><br /> <div>
arXiv:2505.01658v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers</title>
<link>https://arxiv.org/abs/2505.01693</link>
<guid>https://arxiv.org/abs/2505.01693</guid>
<content:encoded><![CDATA[
<div> framework, chest X-ray reports, labeling, DeBERTa-RAD, knowledge distillation 
Summary:
DeBERTa-RAD introduces a two-stage framework for automated labeling of chest X-ray reports. It combines the power of large language models (LLMs) pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast labeling. The framework leverages an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, outperforming established systems while maintaining practical inference speed. The framework shows particular strength in handling uncertain findings. This work demonstrates a promising approach to high-performance medical text processing by combining LLM capabilities and efficient student models trained via distillation.
<br /><br />Summary: <div>
arXiv:2505.01693v1 Announce Type: new 
Abstract: Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.01731</link>
<guid>https://arxiv.org/abs/2505.01731</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, pruning, Shapley Value, non-uniform pruning, transformer layers

Summary:
The article introduces a novel approach called SVNUP (Shapley Value-based Non-Uniform Pruning) for enhancing the performance of large language models (LLMs) by pruning. Traditional uniform sparsity methods often overlook the varying importance of individual transformer layers within LLMs, leading to suboptimal performance. SVNUP quantifies the contribution of each layer to overall model performance, allowing tailored pruning budgets to retain critical parameters. Additionally, a Sliding Window-based Shapley Value approximation method is designed to improve efficiency by reducing computational overhead. Experimentation on LLMs like LLaMA-v1, LLaMA-v2, and OPT demonstrates the effectiveness of SVNUP, showcasing a significant reduction in perplexity (PPL) on models like LLaMA-7B and LLaMA-13B compared to SparseGPT at 70% sparsity. SVNUP proves to be a promising solution for optimizing LLM performance through non-uniform pruning strategies. 

<br /><br />Summary: <div>
arXiv:2505.01731v1 Announce Type: new 
Abstract: Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the \underline{S}hapley \underline{V}alue-based \underline{N}on-\underline{U}niform \underline{P}runing (\methodname{}) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, \methodname{} achieves a reduction in perplexity (PPL) of 18.01\% and 19.55\% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70\% sparsity.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models</title>
<link>https://arxiv.org/abs/2505.01761</link>
<guid>https://arxiv.org/abs/2505.01761</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation evaluation, large language models, error span annotations, text length, Focus Sentence Prompting<br />
Summary: 
- Evaluating machine-translated text accurately remains a challenge, especially for long documents.
- Large language models (LLMs) have shown promise in sentence-level translation evaluation using MQM error span annotations.
- The impact of text length on evaluation is significant, with longer texts leading to fewer error spans and reduced system ranking accuracy.
- Strategies such as granularity-aligned prompting and Focus Sentence Prompting (FSP) have been evaluated to address this length bias.
- Fine-tuning LLMs and using FSP can help mitigate the length bias and improve the reliability of LLMs for long-form translation evaluation.<br />
Summary: <div>
arXiv:2505.01761v1 Announce Type: new 
Abstract: Accurately evaluating machine-translated text remains a long-standing challenge, particularly for long documents. Recent work has shown that large language models (LLMs) can serve as reliable and interpretable sentence-level translation evaluators via MQM error span annotations. With modern LLMs supporting larger context windows, a natural question arises: can we feed entire document translations into an LLM for quality assessment? Ideally, evaluation should be invariant to text length, producing consistent error spans regardless of input granularity. However, our analysis shows that text length significantly impacts evaluation: longer texts lead to fewer error spans and reduced system ranking accuracy. To address this limitation, we evaluate several strategies, including granularity-aligned prompting, Focus Sentence Prompting (FSP), and a fine-tuning approach to better align LLMs with the evaluation task. The latter two methods largely mitigate this length bias, making LLMs more reliable for long-form translation evaluation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments</title>
<link>https://arxiv.org/abs/2505.01794</link>
<guid>https://arxiv.org/abs/2505.01794</guid>
<content:encoded><![CDATA[
<div> Keywords: fuzzy logic, soft skills, granular linguistic model, multimodal analysis, undergraduate students

Summary:
This paper introduces a fuzzy logic approach utilizing a Granular Linguistic Model of Phenomena and multimodal analysis to evaluate soft skills in undergraduate students. The approach breaks down soft skill expressions into granular components, capturing nuanced behaviors with high granularity and addressing uncertainties for improved interpretability and reliability. Experiments with a developed tool for assessing soft skills like decision-making, communication, and creativity showed that subtle aspects of human interaction, such as facial expressions and gestures, could be identified and quantified. The framework effectively consolidates various data inputs to provide consistent soft skills assessments. Integrating multiple modalities into the evaluation process enhances the quality of soft skills scores, making the assessment work transparent and understandable for educational stakeholders.<br /><br />Summary: <div>
arXiv:2505.01794v1 Announce Type: new 
Abstract: In the rapidly evolving educational landscape, the unbiased assessment of soft skills is a significant challenge, particularly in higher education. This paper presents a fuzzy logic approach that employs a Granular Linguistic Model of Phenomena integrated with multimodal analysis to evaluate soft skills in undergraduate students. By leveraging computational perceptions, this approach enables a structured breakdown of complex soft skill expressions, capturing nuanced behaviours with high granularity and addressing their inherent uncertainties, thereby enhancing interpretability and reliability. Experiments were conducted with undergraduate students using a developed tool that assesses soft skills such as decision-making, communication, and creativity. This tool identifies and quantifies subtle aspects of human interaction, such as facial expressions and gesture recognition. The findings reveal that the framework effectively consolidates multiple data inputs to produce meaningful and consistent assessments of soft skills, showing that integrating multiple modalities into the evaluation process significantly improves the quality of soft skills scores, making the assessment work transparent and understandable to educational stakeholders.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis</title>
<link>https://arxiv.org/abs/2505.01800</link>
<guid>https://arxiv.org/abs/2505.01800</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated texts, authorship verification, stylometric analysis, psycholinguistic theories, academic integrity 

Summary: 
This study addresses the critical need for accurate detection tools to distinguish between AI-generated and human-written texts, particularly in educational settings. By integrating stylometric analysis with psycholinguistic theories, the research proposes a comprehensive framework that maps 31 stylometric features to cognitive processes like lexical retrieval and cognitive load management. These distinct psycholinguistic patterns in human writing offer a clear and interpretable approach for identifying AI-generated texts. The framework aims to contribute to the development of reliable tools to uphold academic integrity in the age of generative AI. Through the intersection of computational linguistics and cognitive science, this study provides a robust foundation for detecting AI-generated texts and ensuring transparency in authorship verification. 

<br /><br />Summary: <div>
arXiv:2505.01800v1 Announce Type: new 
Abstract: The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, in-context learning, news dataset, self-play data generation, System-2 fine-tuning

Summary:
Humans and intelligent animals effortlessly internalize new information, a capability replicated by large language models (LLMs) through in-context learning (ICL). However, fine-tuning these models to consolidate learning remains a challenge. A new dataset called New News provides hypothetical yet plausible news across various domains, with downstream evaluation questions reliant on understanding the news. Naive fine-tuning and ICL show a significant gap on this news dataset. To address this, self-play data generation protocols like paraphrases, implications, and Self-QAs are explored to distill knowledge from the model with context into the weights of the model without context, termed System-2 Fine-tuning (Sys2-FT). Sys2-FT, particularly the self-QA protocol, enhances models' in-weight learning of the news. Interestingly, a contextual shadowing effect is discovered where training with news in context followed by its rephrases or QAs degrades learning. Preliminary evidence of an emerging scaling law of Sys2-FT is also presented. 

<br /><br />Summary: <div>
arXiv:2505.01812v1 Announce Type: new 
Abstract: Humans and intelligent animals can effortlessly internalize new information ("news") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\textit{contexual shadowing effect}$, where training with the news $\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intra-Layer Recurrence in Transformers for Language Modeling</title>
<link>https://arxiv.org/abs/2505.01855</link>
<guid>https://arxiv.org/abs/2505.01855</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, natural language processing, recurrent transformer methods, Intra-Layer Recurrence, optimizing recurrent structures <br />
Summary:<br />
Transformer models have achieved impressive results in natural language processing, but their growing depth leads to an increase in parameters. Existing recurrent transformer approaches often apply recurrence across entire blocks of layers, but a more targeted approach called Intra-Layer Recurrence (ILR) has been proposed in this study. ILR selectively applies recurrence to individual layers in a single forward pass, with experiments showing that allocating more iterations to earlier layers produces the best outcomes. This study's findings indicate that ILR could be a promising strategy for optimizing recurrent structures in transformer architectures. <br /> <div>
arXiv:2505.01855v1 Announce Type: new 
Abstract: Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Attention for Efficient BERT-Based Named Entity Recognition</title>
<link>https://arxiv.org/abs/2505.01868</link>
<guid>https://arxiv.org/abs/2505.01868</guid>
<content:encoded><![CDATA[
<div> BERT, Named Entity Recognition, NLP, framework, positional attention<br />
<br />
Summary: <br />
This paper introduces a framework for Named Entity Recognition (NER) utilizing the BERT model in natural language processing (NLP). NER is a crucial NLP task with widespread applications. While BERT is a leading model for entity recognition, fine-tuning it for each new application is time-consuming and computationally intensive. The proposed framework integrates positional attention mechanisms into the entity recognition process, allowing for efficient customization using pre-trained parameters. The framework is evaluated on a Kaggle dataset from the Groningen Meaning Bank corpus and demonstrates strong performance with fewer training epochs. This work contributes by offering a cost-efficient solution for training BERT-based NER systems that maintains high accuracy. <br /> <div>
arXiv:2505.01868v1 Announce Type: new 
Abstract: This paper presents a framework for Named Entity Recognition (NER) leveraging the Bidirectional Encoder Representations from Transformers (BERT) model in natural language processing (NLP). NER is a fundamental task in NLP with broad applicability across downstream applications. While BERT has established itself as a state-of-the-art model for entity recognition, fine-tuning it from scratch for each new application is computationally expensive and time-consuming. To address this, we propose a cost-efficient approach that integrates positional attention mechanisms into the entity recognition process and enables effective customization using pre-trained parameters. The framework is evaluated on a Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves strong performance with fewer training epochs. This work contributes to the field by offering a practical solution for reducing the training cost of BERT-based NER systems while maintaining high accuracy.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans can learn to detect AI-generated texts, or at least learn when they can't</title>
<link>https://arxiv.org/abs/2505.01877</link>
<guid>https://arxiv.org/abs/2505.01877</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text, feedback, stylistic features, readability, self-assessment

Summary:
- The study investigates individuals' ability to distinguish between human-written and AI-generated texts with feedback.
- GPT-4o generated texts similar to human-written ones for the experiment with 255 Czech native speakers.
- Immediate feedback led to improved accuracy and confidence calibration in text identification.
- Participants had misconceptions regarding AI text features, which were corrected through feedback.
- Training with explicit feedback can help in learning to differentiate between human and AI-generated texts effectively, with implications for educational settings. 

<br /><br />Summary: 
The study explores if individuals can accurately differentiate between human and AI-generated texts with immediate feedback. Using GPT-4o, texts were generated for Czech native speakers to identify. Those receiving feedback showed improved accuracy and confidence calibration. Participants initially misconceived AI text features, but feedback helped to correct these notions. The study highlights the effectiveness of targeted training with feedback in learning to distinguish between human and AI-generated texts, particularly valuable in educational contexts. <div>
arXiv:2505.01877v1 Announce Type: new 
Abstract: This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.
  We used GPT-4o to generate several hundred texts across various genres and text types comparable to Koditex, a multi-register corpus of human-written texts. We then presented randomized text pairs to 255 Czech native speakers who identified which text was human-written and which was AI-generated. Participants were randomly assigned to two conditions: one receiving immediate feedback after each trial, the other receiving no feedback until experiment completion. We recorded accuracy in identification, confidence levels, response times, and judgments about text readability along with demographic data and participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in accuracy and confidence calibration. Participants initially held incorrect assumptions about AI-generated text features, including expectations about stylistic rigidity and readability. Notably, without feedback, participants made the most errors precisely when feeling most confident -- an issue largely resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be effectively learned through targeted training with explicit feedback, which helps correct misconceptions about AI stylistic features and readability, as well as potential other variables that were not explored, while facilitating more accurate self-assessment. This finding might be particularly important in educational contexts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams</title>
<link>https://arxiv.org/abs/2505.01883</link>
<guid>https://arxiv.org/abs/2505.01883</guid>
<content:encoded><![CDATA[
<div> Keywords: Twitter, sentiment analysis, topic analysis, Latent Dirichlet Allocation, visualization<br />
Summary:<br />
- The study presents a framework for analyzing sentiment and topics on Twitter on a large scale.
- The pipeline involves targeted data collection using conflict-specific keywords and automated sentiment labeling with pre-trained models.
- The relationship between sentiment and contextual features like timestamp, geolocation, and lexical content is examined.
- Latent Dirichlet Allocation (LDA) is applied to identify latent themes in partitioned subsets based on sentiment and metadata attributes.
- An interactive visualization interface is developed to explore sentiment trends and topic distributions over time and regions. <br /> <div>
arXiv:2505.01883v1 Announce Type: new 
Abstract: We present a framework for large-scale sentiment and topic analysis of Twitter discourse. Our pipeline begins with targeted data collection using conflict-specific keywords, followed by automated sentiment labeling via multiple pre-trained models to improve annotation robustness. We examine the relationship between sentiment and contextual features such as timestamp, geolocation, and lexical content. To identify latent themes, we apply Latent Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and metadata attributes. Finally, we develop an interactive visualization interface to support exploration of sentiment trends and topic distributions across time and regions. This work contributes a scalable methodology for social media analysis in dynamic geopolitical contexts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation</title>
<link>https://arxiv.org/abs/2505.01900</link>
<guid>https://arxiv.org/abs/2505.01900</guid>
<content:encoded><![CDATA[
<div> Keyword: misinformation detection, adversarial attacks, evidence retrieval, claim-evidence comparison, CAMOUFLAGE<br />
Summary:<br />
The article introduces CAMOUFLAGE, an iterative approach using a Prompt Optimization Agent and an Attacker Agent to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison in automated evidence-based misinformation detection systems. Unlike existing approaches that focus on token-level substitutions, CAMOUFLAGE aims for larger structural and stylistic transformations to bypass the detection systems. The Attacker Agent produces semantically equivalent rewrites to mislead detectors, while the Prompt Optimization Agent refines the prompt based on failed attempts. CAMOUFLAGE optimizes its attack solely based on binary model decisions, achieving an average attack success rate of 46.92% across four systems while maintaining textual coherence and semantic equivalence to the original claims.<br /> <div>
arXiv:2505.01900v1 Announce Type: new 
Abstract: Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\% while preserving textual coherence and semantic equivalence to the original claims.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview</title>
<link>https://arxiv.org/abs/2505.01967</link>
<guid>https://arxiv.org/abs/2505.01967</guid>
<content:encoded><![CDATA[
<div> framework, biases, worldviews, language models, social cues
Summary:<br /><br />The paper introduces the Social Worldview Taxonomy (SWT) framework to study socio-cognitive attitudes in Large Language Models (LLMs). It explores dimensions beyond demographic biases, focusing on attitudes towards authority, equality, autonomy, and fate. The SWT framework operationalizes four worldviews into measurable sub-dimensions and identifies distinct cognitive profiles across 28 LLMs. The study also experiments with Social Referencing Theory to show how explicit social cues influence cognitive attitudes in LLMs. The findings enhance the interpretability of LLMs by revealing implicit biases and their responsiveness to social feedback, guiding the development of more transparent and socially responsible language technologies.<br /><br />Summary: <div>
arXiv:2505.01967v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or "worldviews". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load</title>
<link>https://arxiv.org/abs/2505.01980</link>
<guid>https://arxiv.org/abs/2505.01980</guid>
<content:encoded><![CDATA[
<div> Keywords: web information, text simplification, comprehension, randomized study, LLMs

Summary:
- A study was conducted on text simplification using LLMs to improve comprehension of web information.
- 4563 participants were involved in a randomized study across various subject areas.
- Participants who read the simplified text showed a 3.9% increase in answering multiple-choice questions correctly.
- Significant gains were seen in biomedical scientific articles (PubMed) with a 14.6% improvement.
- Participants reported higher perceived ease when reading the simplified text.
- Results remained consistent even when participants could not refer back to the text while answering questions.
- The study showcases the potential of LLMs in making complex information more accessible on the web, aiming to improve information accessibility for a broader audience. 

<br /><br />Summary: <div>
arXiv:2505.01980v1 Announce Type: new 
Abstract: Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, we used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate our approach, we conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Our results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. Our work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</title>
<link>https://arxiv.org/abs/2505.02009</link>
<guid>https://arxiv.org/abs/2505.02009</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, harmful content, content filtering, toxicity benchmark, Responsible AI compliance

Summary: 
This paper presents a large-scale analysis of inappropriate content in web-sourced datasets used for training Large Language Models (LLMs), highlighting the risks of perpetuating toxic behaviors, spreading misinformation, and amplifying biases. A taxonomy categorizes harmful webpages into Topical and Toxic based on intent. A Topical and Toxic Prompt (TTP) evaluation dataset and transformer-based model (HarmFormer) are introduced for content filtering. A new multi-harm open-ended toxicity benchmark (HAVOC) is created to provide insights into model responses to toxic inputs. The work aims to ensure safer LLM pretraining and offer resources for Responsible AI compliance. The model signal on the entire C4 dataset will be open-sourced to facilitate further research and development in this area.

<br /><br />Summary: <div>
arXiv:2505.02009v1 Announce Type: new 
Abstract: Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. Upon publishing, we will also opensource our model signal on the entire C4 dataset. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An overview of artificial intelligence in computer-assisted language learning</title>
<link>https://arxiv.org/abs/2505.02032</link>
<guid>https://arxiv.org/abs/2505.02032</guid>
<content:encoded><![CDATA[
<div> AI, language learning, CALL, intelligent agents, interdisciplinary work <br />
Summary: Computer-assisted language learning (CALL) is a crucial field that can benefit from the application of artificial intelligence (AI). The increasing demand for language learning and teaching necessitates the use of intelligent agents to support both learners and teachers. AI can assist in various aspects of CALL, such as creating prototypes and partial implementations of systems. However, the development of complete AI-powered solutions is challenging due to the substantial resources required. Despite this, recent AI advancements offer promising improvements in CALL. This article provides a developer's perspective on utilizing AI methods for language learning within the context of a CALL system, aiming to facilitate interdisciplinary collaborations in this research field. <div>
arXiv:2505.02032v1 Announce Type: new 
Abstract: Computer-assisted language learning -- CALL -- is an established research field. We review how artificial intelligence can be applied to support language learning and teaching. The need for intelligent agents that assist language learners and teachers is increasing: the human teacher's time is a scarce and costly resource, which does not scale with growing demand. Further factors contribute to the need for CALL: pandemics and increasing demand for distance learning, migration of large populations, the need for sustainable and affordable support for learning, etc. CALL systems are made up of many components that perform various functions, and AI is applied to many different aspects in CALL, corresponding to their own expansive research areas. Most of what we find in the research literature and in practical use are prototypes or partial implementations -- systems that perform some aspects of the overall desired functionality. Complete solutions -- most of them commercial -- are few, because they require massive resources. Recent advances in AI should result in improvements in CALL, yet there is a lack of surveys that focus on AI in the context of this research field. This paper aims to present a perspective on the AI methods that can be employed for language learning from a position of a developer of a CALL system. We also aim to connect work from different disciplines, to build bridges for interdisciplinary work.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction</title>
<link>https://arxiv.org/abs/2505.02072</link>
<guid>https://arxiv.org/abs/2505.02072</guid>
<content:encoded><![CDATA[
<div> language modeling, distribution estimation, response prediction, LLMs, output probabilities
Summary:
This paper analyzes the distinction between distribution estimation and response prediction in language modeling models (LLMs). It explores the training phases of LLMs, including pretraining, in-context learning, and preference tuning, along with the common use cases for their output probabilities. The authors argue that these different settings lead to three distinct intended output distributions. They highlight that NLP works often assume these distributions should be similar, leading to misinterpretations of experimental findings. By setting firmer formal foundations for the interpretation of LLMs, this work aims to inform ongoing research on the interpretation and utilization of the induced distributions by LLMs. <div>
arXiv:2505.02072v1 Announce Type: new 
Abstract: The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning</title>
<link>https://arxiv.org/abs/2505.02078</link>
<guid>https://arxiv.org/abs/2505.02078</guid>
<content:encoded><![CDATA[
<div> Keywords: LecEval, slide-based multimedia instruction, automated metric, cognitive theory, dataset

Summary: 
LecEval is introduced as an automated metric for evaluating slide-based multimedia instruction, grounded in Mayer's Cognitive Theory of Multimedia Learning. It assesses effectiveness based on Content Relevance, Expressive Clarity, Logical Structure, and Audience Engagement. A large dataset of over 2,000 slides from 50 online course videos is curated and annotated with human ratings across these rubrics. A model trained on this dataset outperforms existing metrics, providing a bridge between automated and human assessments. The dataset and toolkits are made publicly available at the provided GitHub link. <div>
arXiv:2505.02078v1 Announce Type: new 
Abstract: Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at https://github.com/JoylimJY/LecEval.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications</title>
<link>https://arxiv.org/abs/2505.02091</link>
<guid>https://arxiv.org/abs/2505.02091</guid>
<content:encoded><![CDATA[
<div> Keywords: non-convex resource allocation, wireless communication systems, large language models, automated resolution, optimization tasks

Summary: 
The paper introduces LLM-OptiRA, a novel framework utilizing large language models (LLMs) to address non-convex resource allocation problems in wireless communication systems. By automatically identifying and transforming non-convex elements into solvable forms, LLM-OptiRA enables the automated resolution of complex optimization challenges without requiring expert knowledge. The framework integrates error correction and feasibility validation mechanisms to ensure robustness in problem-solving. Experimental results demonstrate LLM-OptiRA's superior performance, achieving a high execution rate of 96% and a success rate of 80% on GPT-4. This outperforms traditional optimization techniques in diverse and intricate scenarios, showcasing the potential of LLMs in simplifying and enhancing resource allocation processes in wireless communication systems.
<br /><br />Summary: <div>
arXiv:2505.02091v1 Announce Type: new 
Abstract: Solving non-convex resource allocation problems poses significant challenges in wireless communication systems, often beyond the capability of traditional optimization techniques. To address this issue, we propose LLM-OptiRA, the first framework that leverages large language models (LLMs) to automatically detect and transform non-convex components into solvable forms, enabling fully automated resolution of non-convex resource allocation problems in wireless communication systems. LLM-OptiRA not only simplifies problem-solving by reducing reliance on expert knowledge, but also integrates error correction and feasibility validation mechanisms to ensure robustness. Experimental results show that LLM-OptiRA achieves an execution rate of 96% and a success rate of 80% on GPT-4, significantly outperforming baseline approaches in complex optimization tasks across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study</title>
<link>https://arxiv.org/abs/2505.02142</link>
<guid>https://arxiv.org/abs/2505.02142</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Language Models, Direct Preference Optimization, Offline RL, Reasoning benchmarks
Summary:
- The study explores the use of simpler Offline RL methods, DPO and LD-DPO, to enhance long-context reasoning in large language models (LLMs), compared to traditional Online RL methods.
- Offline RL methods show promise in improving LLM performance, with an average enhancement of 3.3% across multiple reasoning benchmarks.
- DPO and LD-DPO demonstrate particularly significant performance gains on challenging benchmarks like Arena-Hard, with a notable increase of 10.1%.
- The study highlights the importance of considering the impact of output length on model performance, suggesting that careful consideration of semantic richness is essential when increasing reasoning length.
- Comprehensive descriptions of data processing and training methodologies are provided, offering practical insights for the development of more cost-effective Offline RL approaches.
<br /><br />Summary: <div>
arXiv:2505.02142v1 Announce Type: new 
Abstract: Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.02146</link>
<guid>https://arxiv.org/abs/2505.02146</guid>
<content:encoded><![CDATA[
<div> transcompiler, tensor programs, heterogeneous deep learning systems, neural-symbolic synthesis, programming productivity
<br />
Summary:
A novel transcompiler called QiMeng-Xpiler has been developed to automatically translate tensor programs across different platforms in heterogeneous deep learning systems. The transcompiler leverages large language models (LLMs) and symbolic program synthesis to enable efficient code generation and program transformation. Through a hierarchical auto-tuning approach, QiMeng-Xpiler systematically explores parameters and sequences of transformation passes to achieve high performance. Experimental results on various platforms show that the transcompiler can correctly translate tensor programs with 95% accuracy on average and improve program performance by up to 2.0x compared to manually optimized libraries. Overall, QiMeng-Xpiler enhances programming productivity in heterogeneous deep learning systems by up to 96.0x through the transcompilation of legacy tensor programs. 
<br /><br />Summary: <div>
arXiv:2505.02146v1 Announce Type: new 
Abstract: Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering "Write Once, Run Anywhere" of tensor programs an open question.
  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the performance of translated programs achieves up to 2.0x over vendor-provided manually-optimized libraries. As a result, the programming productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor programs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
<div> simulation, language agents, adaptive mode learning, context-aware mode switching, token-efficient reasoning

Summary:
- The paper introduces Adaptive Mode Learning (AML) for social intelligence simulation, allowing language agents to dynamically adjust their thinking depth.
- AML employs the Adaptive Mode Policy Optimization (AMPO) algorithm, which offers multi-granular thinking mode design, context-aware mode switching, and depth-adaptive processing for more efficient reasoning.
- Extensive experiments show that AML outperforms existing methods by 15.6% in task performance, with 32.8% shorter reasoning chains than the state-of-the-art GRPO approach.
- The context-sensitive thinking mode selection in AML, implemented through AMPO, leads to more human-like adaptive reasoning compared to fixed-depth approaches.
<br /><br />Summary: <div>
arXiv:2505.02156v1 Announce Type: new 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use</title>
<link>https://arxiv.org/abs/2505.02164</link>
<guid>https://arxiv.org/abs/2505.02164</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Fair Use Doctrine, U.S. copyright law, DMCA takedowns, legal support<br />
<br />
Summary: This paper presents a domain-specific implementation of Retrieval-Augmented Generation (RAG) focused on the Fair Use Doctrine in U.S. copyright law. The aim is to address the lack of accessible legal support for content creators facing DMCA takedowns. The approach combines semantic search with legal knowledge graphs and court citation networks to enhance retrieval quality and reasoning reliability. The prototype models legal precedents at the statutory factor level and uses citation-weighted graph representations to prioritize doctrinally authoritative sources. Chain-of-Thought reasoning and interleaved retrieval steps are employed to better mimic legal reasoning. Preliminary testing indicates an enhancement in doctrinal relevance during retrieval, laying the foundation for future evaluation and deployment of LLM-based legal assistance tools. <br /><br /> <div>
arXiv:2505.02164v1 Announce Type: new 
Abstract: This paper presents a domain-specific implementation of Retrieval-Augmented Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law. Motivated by the increasing prevalence of DMCA takedowns and the lack of accessible legal support for content creators, we propose a structured approach that combines semantic search with legal knowledge graphs and court citation networks to improve retrieval quality and reasoning reliability. Our prototype models legal precedents at the statutory factor level (e.g., purpose, nature, amount, market effect) and incorporates citation-weighted graph representations to prioritize doctrinally authoritative sources. We use Chain-of-Thought reasoning and interleaved retrieval steps to better emulate legal reasoning. Preliminary testing suggests this method improves doctrinal relevance in the retrieval process, laying groundwork for future evaluation and deployment of LLM-based legal assistance tools.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking</title>
<link>https://arxiv.org/abs/2505.02171</link>
<guid>https://arxiv.org/abs/2505.02171</guid>
<content:encoded><![CDATA[
<div> Keywords: Document chunking, Large Language Models, Retrieval-Augmented Generation, Passage properties, HOPE metric

Summary: 
Document chunking plays a crucial role in Retrieval-Augmented Generation (RAG) systems, impacting how source materials are segmented before indexing. A novel methodology, HOPE (Holistic Passage Evaluation), has been introduced to evaluate the chunking process based on intrinsic and extrinsic passage properties, and coherence between passages and documents. HOPE correlates significantly with RAG performance indicators, highlighting the importance of semantic independence between passages for system performance. Maintaining concept unity within passages has minimal impact on system performance. Optimizing chunking strategies based on these insights can lead to improvements in RAG system design, resulting in more factually correct responses.<br /><br />Summary: <div>
arXiv:2505.02171v1 Announce Type: new 
Abstract: Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p > 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization</title>
<link>https://arxiv.org/abs/2505.02172</link>
<guid>https://arxiv.org/abs/2505.02172</guid>
<content:encoded><![CDATA[
<div> benchmark, legal, language models, performance, CaseHOLD 

Summary:
The study evaluates the performance of large language models (LLMs) on the legal benchmark dataset CaseHOLD. Results show that model performance improves with size, with GPT4o and AmazonNovaPro achieving competitive macro F1 scores. The models perform well even when case citations are anonymized, indicating that performance is not based on memorization. These findings highlight both the potential and limitations of LLMs for legal tasks, shaping the development of automated legal analytics and benchmarks. <div>
arXiv:2505.02172v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate ``scaling effects'' - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Hong Kong Massive Multi-Task Language Understanding</title>
<link>https://arxiv.org/abs/2505.02177</link>
<guid>https://arxiv.org/abs/2505.02177</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual understanding, Large Language Models, Hong Kong, Benchmark, Evaluation

Summary:
The article introduces HKMMLU, a benchmark designed to evaluate multilingual understanding in the context of Hong Kong's unique linguistic landscape, combining Traditional Chinese script with Cantonese and cultural knowledge. The benchmark includes multi-choice questions across various subjects and translation tasks between Mandarin and Cantonese. Experiments were conducted on different LLMs, with DeepSeek-V3 performing lower than expected, highlighting the need for improvement in Hong Kong-specific language and knowledge domains. The study also explores the effects of question language, model size, prompting strategies, and token lengths on model performance. Overall, HKMMLU aims to advance the development of LLMs in multilingual and cross-cultural contexts for broader and more impactful applications.<br /><br />Summary: The article presents HKMMLU, a benchmark for evaluating multilingual understanding in Hong Kong's linguistic landscape. It includes questions across subjects and translation tasks, highlighting the importance of improving LLMs in specific language and knowledge domains. Experiments show varied model performance and explore factors influencing accuracy. HKMMLU aims to advance LLM development for broader applications. <div>
arXiv:2505.02177v1 Announce Type: new 
Abstract: Multilingual understanding is crucial for the cross-cultural applicability of Large Language Models (LLMs). However, evaluation benchmarks designed for Hong Kong's unique linguistic landscape, which combines Traditional Chinese script with Cantonese as the spoken form and its cultural context, remain underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language understanding benchmark that evaluates Hong Kong's linguistic competence and socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions across 66 subjects, organized into four categories: Science, Technology, Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To evaluate the multilingual understanding ability of LLMs, 90,550 Mandarin-Cantonese translation tasks were additionally included. We conduct comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs of varying sizes on HKMMLU. The results show that the best-performing model, DeepSeek-V3, struggles to achieve an accuracy of 75\%, significantly lower than that of MMLU and CMMLU. This performance gap highlights the need to improve LLMs' capabilities in Hong Kong-specific language and knowledge domains. Furthermore, we investigate how question language, model size, prompting strategies, and question and reasoning token lengths affect model performance. We anticipate that HKMMLU will significantly advance the development of LLMs in multilingual and cross-cultural contexts, thereby enabling broader and more impactful applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation</title>
<link>https://arxiv.org/abs/2505.02235</link>
<guid>https://arxiv.org/abs/2505.02235</guid>
<content:encoded><![CDATA[
<div> Keywords: text summarization, evaluation, SEval-Ex, interpretability, consistency

Summary: 
SEval-Ex is a framework designed to improve text summarization evaluation by breaking it down into atomic statements for better performance and explainability. It uses a two-stage pipeline to extract atomic statements from text and summary, then matches them to provide detailed evidence for its decisions through statement-level alignments. Compared to existing methods, SEval-Ex achieves a correlation of 0.580 with human consistency judgments on the SummEval benchmark, outperforming GPT-4 based evaluators while still being interpretable. The framework also demonstrates robustness against hallucination. This approach addresses the challenge of evaluating summarization quality by combining performance and interpretability, making it a promising solution for future research in Natural Language Processing. 

Summary: <div>
arXiv:2505.02235v1 Announce Type: new 
Abstract: Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.02252</link>
<guid>https://arxiv.org/abs/2505.02252</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Personalisation, Hate Speech, Context, Bias

Summary:
Large Language Models (LLMs) with memory features incorporating personalised information can adjust responses based on user demographics, raising concerns about bias. This study explores the impact of personalisation, particularly in hate speech detection scenarios. Different state-of-the-art LLMs were assessed in various personalisation scenarios, focusing on hate speech detection across different languages and country-specific personas. Results show that context personalisation significantly influences LLM responses, leading to unwanted biases. To address this, the study fine-tuned LLMs by penalising inconsistent hate speech classifications with and without specific context, resulting in improved performance in both personalised and non-personalised contexts. The findings emphasize the need to mitigate biases in LLMs when incorporating personalised information, especially in sensitive topics like hate speech. 

<br /><br />Summary: <div>
arXiv:2505.02252v1 Announce Type: new 
Abstract: Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Transformer Embeddings</title>
<link>https://arxiv.org/abs/2505.02266</link>
<guid>https://arxiv.org/abs/2505.02266</guid>
<content:encoded><![CDATA[
<div> Fourier expansion, token embedding, transformer-based NLP models, multilayer perceptron, natural language inference<br />
Summary:<br />
The study introduces a novel approach to token embedding in transformer-based NLP models by utilizing a Fourier expansion to generate embedding vectors directly from token IDs. These vectors are then processed by a lightweight multilayer perceptron to capture higher-order interactions. The proposed method achieves competitive performance on natural language inference tasks and sentence textual similarity with significantly fewer parameters, faster training, and no need for dropout. This approach highlights the potential for scalable and memory-efficient language models, providing a foundation for further large-scale experimentation in NLP. <br /> 

Summary: <div>
arXiv:2505.02266v1 Announce Type: new 
Abstract: Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying optimized prompts in language models</title>
<link>https://arxiv.org/abs/2505.02273</link>
<guid>https://arxiv.org/abs/2505.02273</guid>
<content:encoded><![CDATA[
<div> prompt optimization, language models, robustness, out-of-distribution, machine-generated <br />
Summary:
Modern language models face challenges in handling out-of-distribution inputs, but machine-generated optimized prompts can influence their behavior. This study delves into the composition of optimized prompts, revealing a focus on punctuation and rare noun tokens. By examining internal mechanisms, it becomes apparent that optimized prompts differ significantly from natural language prompts based on sparse model activations. Despite variations in instruction-tuned models, optimized prompts follow consistent paths through the network. Addressing the impact of optimized prompts on language models sheds light on their behavior and provides insights into enhancing their robustness and interpretability. <br /> <div>
arXiv:2505.02273v1 Announce Type: new 
Abstract: Modern language models (LMs) are not robust to out-of-distribution inputs. Machine generated (``optimized'') prompts can be used to modulate LM outputs and induce specific behaviors while appearing completely uninterpretable. In this work, we investigate the composition of optimized prompts, as well as the mechanisms by which LMs parse and build predictions from optimized prompts. We find that optimized prompts primarily consist of punctuation and noun tokens which are more rare in the training data. Internally, optimized prompts are clearly distinguishable from natural language counterparts based on sparse subsets of the model's activations. Across various families of instruction-tuned models, optimized prompts follow a similar path in how their representations form through the network.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition</title>
<link>https://arxiv.org/abs/2505.02304</link>
<guid>https://arxiv.org/abs/2505.02304</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language recognition, generative large language models, retrieval-augmented generation, dual-encoder architecture, cross-lingual effectiveness<br />
Summary:<br />
This paper introduces a novel approach, GSP-MC, integrating generative large language models into sign language recognition tasks. The method utilizes retrieval-augmented generation with domain-specific LLMs and expert-validated sign language corpora to generate precise multipart descriptions. It employs a dual-encoder architecture to align skeleton features with text descriptions at various levels. By optimizing KL divergence and combining global and part-level losses, the method ensures robust alignment and captures both sign-level semantics and detailed part dynamics. Experimental results show state-of-the-art performance on Chinese and Turkish sign language datasets, reaching 97.1% and 97.07% accuracy, respectively. The method's effectiveness in cross-lingual settings highlights its potential for developing inclusive communication technologies.<br /> 

Summary: <div>
arXiv:2505.02304v1 Announce Type: new 
Abstract: Sign language recognition (SLR) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. To the best of our knowledge, this is the first work to integrate generative large language models (LLMs) into SLR tasks. We propose a novel Generative Sign-description Prompts Multi-positive Contrastive learning (GSP-MC) method that leverages retrieval-augmented generation (RAG) with domain-specific LLMs, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. The GSP-MC method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. Our approach combines global and part-level losses, optimizing KL divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. Experiments demonstrate state-of-the-art performance against existing methods on the Chinese SLR500 (reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering</title>
<link>https://arxiv.org/abs/2505.02311</link>
<guid>https://arxiv.org/abs/2505.02311</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, hallucinations, invocation evaluation, reasoning errors, uncertainty-aware knowledge reorganization <br />
<br />
Summary: This paper introduces a new metric called AttenHScore to improve the real-time detection of hallucinations in small language models. By calculating the accumulation and propagation of hallucinations during the generation process, this metric can dynamically adjust the detection threshold to accurately invoke large language models. It also utilizes uncertainty-aware knowledge reorganization to help small models capture critical information more effectively. Experimental results show that AttenHScore outperforms baseline methods in detecting hallucinations, especially for complex queries, without the need for additional model training. The proposed strategies are flexible and applicable to various transformer-based language models. <div>
arXiv:2505.02311v1 Announce Type: new 
Abstract: The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning</title>
<link>https://arxiv.org/abs/2505.02363</link>
<guid>https://arxiv.org/abs/2505.02363</guid>
<content:encoded><![CDATA[
<div> Complementary strengths, preference optimization, on-policy data, off-policy data, SIMPLEMIX <br />
Summary: <br />
This study explores the interplay between on-policy and off-policy data in aligning language models with human preferences. The researchers find that on-policy data is more effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and personal recommendations. They introduce SIMPLEMIX, a method that combines the strengths of both data sources, resulting in improved language model alignment. SIMPLEMIX outperforms prior approaches like HyPO and DPO-Mix-P by a significant margin. Empirical results across various tasks and benchmarks show that SIMPLEMIX enhances language model performance, particularly on Alpaca Eval 2.0, where it achieves an average improvement of 6.03% over on-policy and off-policy DPO. This research contributes valuable insights into leveraging different data sources for preference learning in language models. <br /> <div>
arXiv:2505.02363v1 Announce Type: new 
Abstract: Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.
  In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
<link>https://arxiv.org/abs/2505.02366</link>
<guid>https://arxiv.org/abs/2505.02366</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised contrastive learning, Semantic representation tensor, Modulus constraints, Cross-attention, Sentence embedding.

Summary:
The paper introduces a novel framework called JTCSE for unsupervised contrastive learning in natural language processing. It addresses the limitation of existing works by incorporating modulus constraints on the semantic representation tensor to enhance alignment between positive samples. Additionally, a cross-attention structure among twin-tower ensemble models is proposed to improve attention to CLS tokens and optimize CLS Pooling in BERT-like models. Evaluation on semantic text similarity tasks demonstrates that JTCSE's models outperform existing baselines, establishing a new state-of-the-art. Extensive zero-shot downstream task evaluations further confirm the superior performance of JTCSE across more than 130 tasks. Overall, JTCSE combines tensor representation modulus constraints and cross-attention mechanisms to achieve significant improvements in unsupervised contrastive learning for sentence embeddings.<br /><br />Summary: <div>
arXiv:2505.02366v1 Announce Type: new 
Abstract: Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \textbf{J}oint \textbf{T}ensor representation modulus constraint and \textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence \textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RM-R1: Reward Modeling as Reasoning</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
<div> Keyword: reward modeling, language models, reinforcement learning, interpretability, reasoning <br />
Summary: <br />
This paper introduces a new class of generative reward models called Reasoning Reward Models (ReasRMs) that enhance interpretability and performance by integrating reasoning capabilities into reward modeling. The proposed model, RM-R1, is trained using a reasoning-oriented pipeline involving distillation of high-quality reasoning chains and reinforcement learning with verifiable rewards. Empirical results show that RM-R1 outperforms existing models, achieving state-of-the-art or near state-of-the-art performance on various benchmarks. The study also includes thorough empirical analysis to understand the key factors contributing to successful ReasRM training. Six ReasRM models, along with code and data, are released to facilitate future research. <div>
arXiv:2505.02387v1 Announce Type: new 
Abstract: Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bielik 11B v2 Technical Report</title>
<link>https://arxiv.org/abs/2505.02410</link>
<guid>https://arxiv.org/abs/2505.02410</guid>
<content:encoded><![CDATA[
<div> language model, Polish text processing, Bielik 11B v2, optimization, cross-lingual capabilities

Summary:
Bielik 11B v2 is a language model specifically designed for Polish text processing. It is based on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling. The model showcases exceptional performance on Polish language benchmarks and maintains strong cross-lingual capabilities. Two key technical innovations, Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, enhance the model's learning and adaptation abilities. Bielik 11B v2 outperforms larger models with up to 6 times more parameters and specialized Polish language models on various tasks such as linguistic understanding and complex reasoning. The model's parameter efficiency and quantization options enable deployment on different hardware configurations, advancing Polish language AI capabilities and setting new standards for resource-efficient language modeling in underrepresented languages. 

<br /><br />Summary: <div>
arXiv:2505.02410v1 Announce Type: new 
Abstract: We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs</title>
<link>https://arxiv.org/abs/2505.02456</link>
<guid>https://arxiv.org/abs/2505.02456</guid>
<content:encoded><![CDATA[
<div> intersectional bias, multilingual, NLP, fairness research, occupational bias
<br />
This study examines intersectional and multilingual biases in large language models regarding occupation recommendations. The researchers created prompts in English, Spanish, and German varying country and gender. Five Llama-based models were evaluated, revealing significant gender and country biases. Even when models show gender or country parity individually, intersectional biases persist. The prompting language impacts bias, and instruction-tuned models display the lowest bias levels. The study emphasizes the importance of considering intersectional and multilingual perspectives in fairness research.
<br /><br />Summary: <div>
arXiv:2505.02456v1 Announce Type: new 
Abstract: One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda</title>
<link>https://arxiv.org/abs/2505.02463</link>
<guid>https://arxiv.org/abs/2505.02463</guid>
<content:encoded><![CDATA[
<div> Keywords: Back translation, Neural Machine Translation, English-Luganda, low-resource languages, synthetic data<br />
Summary:<br />
This paper explores using Back translation (BT) as a semi-supervised technique to improve Neural Machine Translation (NMT) models for the English-Luganda language pair, addressing challenges of low-resource languages. The study demonstrates how BT can generate synthetic data from monolingual corpora to alleviate the scarcity of bilingual data. Custom NMT models are developed using publicly available and web-crawled data, with Incremental Back translation techniques applied strategically across multiple small datasets. The results show significant improvements in translation performance, surpassing previous benchmarks by over 10 BLEU score units. Evaluation metrics such as SacreBLEU, ChrF2, and TER are used for a thorough assessment of translation quality. The research confirms the effectiveness of using BT with carefully curated datasets, setting new performance standards and highlighting the potential for enhancing NMT models in low-resource language settings.<br /><br />Summary: <div>
arXiv:2505.02463v1 Announce Type: new 
Abstract: In this paper,we explore the application of Back translation (BT) as a semi-supervised technique to enhance Neural Machine Translation(NMT) models for the English-Luganda language pair, specifically addressing the challenges faced by low-resource languages. The purpose of our study is to demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora. Our methodology involves developing custom NMT models using both publicly available and web-crawled data, and applying Iterative and Incremental Back translation techniques. We strategically select datasets for incremental back translation across multiple small datasets, which is a novel element of our approach. The results of our study show significant improvements, with translation performance for the English-Luganda pair exceeding previous benchmarks by more than 10 BLEU score units across all translation directions. Additionally, our evaluation incorporates comprehensive assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced understanding of translation quality. The conclusion drawn from our research confirms the efficacy of BT when strategically curated datasets are utilized, establishing new performance benchmarks and demonstrating the potential of BT in enhancing NMT models for low-resource languages.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bemba Speech Translation: Exploring a Low-Resource African Language</title>
<link>https://arxiv.org/abs/2505.02518</link>
<guid>https://arxiv.org/abs/2505.02518</guid>
<content:encoded><![CDATA[
<div> Keywords: Bemba, English, speech translation, low-resource languages, data augmentation

Summary:
This paper presents a system submission for the Bemba-to-English speech translation track at IWSLT 2025. The authors utilized Whisper and NLLB-200 to build cascaded speech translation systems, incorporating data augmentation techniques like back-translation. They explored the impact of synthetic data and detailed their experimental setup. This research contributes to the field of low-resource language translation by addressing the challenges of translating Bemba to English. The use of cascaded systems and data augmentation techniques demonstrates innovative approaches to improving translation quality in low-resource language settings. By investigating the effects of synthetic data, the authors shed light on the potential benefits of utilizing artificial data for training speech translation systems. This paper provides valuable insights into the development and optimization of systems for translating low-resource languages, offering valuable contributions to the field of speech translation research. 

<br /><br />Summary: <div>
arXiv:2505.02518v1 Announce Type: new 
Abstract: This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.02579</link>
<guid>https://arxiv.org/abs/2505.02579</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language model, multi-objective tasks, ensemble learning, hierarchical grid search

Summary:
EMORL is a novel framework that leverages ensemble learning principles to fine-tune multiple language models with individual objectives and then optimizes their aggregation. This approach improves efficiency, flexibility, scalability, and explainability in addressing multi-objective tasks in RL. EMORL aggregates the last hidden states of individual models to incorporate contextual information from multiple objectives, supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. Evaluating on counselor reflection generation tasks with text-scoring LLMs, EMORL outperforms existing baselines by significantly reducing training consumption, improving scalability and explainability, and achieving comparable performance across multiple objectives. This demonstrates the effectiveness of EMORL in addressing the challenges faced by RL fine-tuning for large language models. 

<br /><br />Summary: <div>
arXiv:2505.02579v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Kalman filter for uncertainty in human language comprehension</title>
<link>https://arxiv.org/abs/2505.02590</link>
<guid>https://arxiv.org/abs/2505.02590</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial neural networks, sentence processing, uncertainty, Bayesian framework, ensemble Kalman filter

Summary: 
Artificial neural networks (ANNs) are fundamental in modeling sentence processing but often lack the ability to handle uncertainty in the same way humans do. This article introduces a Bayesian framework incorporating the ensemble Kalman filter to better capture uncertainties in sentence comprehension. By treating language understanding as a Bayesian inverse problem, the proposed approach improves the Sentence Gestalt (SG) Model's representation of uncertainty, addressing the limitations of traditional ANNs. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate the effectiveness of Bayesian methods in enhancing the model's ability to mimic human cognitive processing when faced with linguistic ambiguities. The integration of Bayesian inference provides a more realistic representation of human sentence comprehension by quantifying uncertainty and tackling reversal anomalies common in syntax and semantics. 

<br /><br />Summary: <div>
arXiv:2505.02590v1 Announce Type: new 
Abstract: Artificial neural networks (ANNs) are widely used in modeling sentence processing but often exhibit deterministic behavior, contrasting with human sentence comprehension, which manages uncertainty during ambiguous or unexpected inputs. This is exemplified by reversal anomalies-sentences with unexpected role reversals that challenge syntax and semantics-highlighting the limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model. To address these limitations, we propose a Bayesian framework for sentence comprehension, applying an extension of the ensemble Kalman filter (EnKF) for Bayesian inference to quantify uncertainty. By framing language comprehension as a Bayesian inverse problem, this approach enhances the SG model's ability to reflect human sentence processing with respect to the representation of uncertainty. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate that Bayesian methods improve uncertainty representation, enabling the model to better approximate human cognitive processing when dealing with linguistic ambiguities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Proficiency Assessment in L2 English Learners</title>
<link>https://arxiv.org/abs/2505.02615</link>
<guid>https://arxiv.org/abs/2505.02615</guid>
<content:encoded><![CDATA[
<div> Keywords: Second language proficiency, deep learning, speech signal, transcription, automated evaluation 

Summary: 
Second language proficiency in English is typically assessed by human evaluators, leading to variability in evaluations. This study explores using deep learning techniques to automate L2 proficiency assessment, focusing on both speech signals and transcriptions. Various neural network architectures, including CNNs and the wav2vec 2.0 model, are utilized for spoken proficiency classification prediction. Text-based proficiency assessment is performed by fine-tuning a BERT language model. The challenge of evaluating spontaneous dialogues is addressed by applying wav2vec 2.0 and BERT models separately. Experiments conducted on multiple datasets demonstrate the effectiveness of deep learning methods, particularly the pretrained wav2vec 2.0 model, in automatically evaluating L2 proficiency. 

<br /><br />Summary: <div>
arXiv:2505.02615v1 Announce Type: new 
Abstract: Second language proficiency (L2) in English is usually perceptually evaluated by English teachers or expert evaluators, with the inherent intra- and inter-rater variability. This paper explores deep learning techniques for comprehensive L2 proficiency assessment, addressing both the speech signal and its correspondent transcription. We analyze spoken proficiency classification prediction using diverse architectures, including 2D CNN, frequency-based CNN, ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based proficiency assessment by fine-tuning a BERT language model within resource constraints. Finally, we tackle the complex task of spontaneous dialogue assessment, managing long-form audio and speaker interactions through separate applications of wav2vec 2.0 and BERT models. Results from experiments on EFCamDat and ANGLISH datasets and a private dataset highlight the potential of deep learning, especially the pretrained wav2vec 2.0 model, for robust automated L2 proficiency evaluation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis</title>
<link>https://arxiv.org/abs/2505.02625</link>
<guid>https://arxiv.org/abs/2505.02625</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time, speech interaction, language models, chatbots, Qwen2.5

Summary:<br />
The paper introduces LLaMA-Omni 2, a series of speech language models designed for high-quality real-time speech interaction. These models range from 0.5B to 14B parameters and are built upon the Qwen2.5 series. Despite being trained on a limited dataset of 200K multi-turn speech dialogue samples, LLaMA-Omni 2 outperforms previous SpeechLMs like GLM-4-Voice on various benchmarks. The models integrate a speech encoder and autoregressive streaming speech decoder, showcasing strong performance on spoken question answering and speech instruction following tasks. This advancement highlights the potential of large language models in building intelligent spoken chatbots that can provide natural and intelligent interactions between humans and computers.<br /><br />Summary: <div>
arXiv:2505.02625v1 Announce Type: new 
Abstract: Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.02656</link>
<guid>https://arxiv.org/abs/2505.02656</guid>
<content:encoded><![CDATA[
arXiv:2505.02656v1 Announce Type: new 
Abstract: Proper names in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP,their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper names of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper name diacritization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Progress in LLM Alignment from the Perspective of Reward Design</title>
<link>https://arxiv.org/abs/2505.02666</link>
<guid>https://arxiv.org/abs/2505.02666</guid>
<content:encoded><![CDATA[
arXiv:2505.02666v1 Announce Type: new 
Abstract: The alignment of large language models (LLMs) with human values and intentions represents a core challenge in current AI research, where reward mechanism design has become a critical factor in shaping model behavior. This study conducts a comprehensive investigation of reward mechanisms in LLM alignment through a systematic theoretical framework, categorizing their development into three key phases: (1) feedback (diagnosis), (2) reward design (prescription), and (3) optimization (treatment). Through a four-dimensional analysis encompassing construction basis, format, expression, and granularity, this research establishes a systematic classification framework that reveals evolutionary trends in reward modeling. The field of LLM alignment faces several persistent challenges, while recent advances in reward design are driving significant paradigm shifts. Notable developments include the transition from reinforcement learning-based frameworks to novel optimization paradigms, as well as enhanced capabilities to address complex alignment scenarios involving multimodal integration and concurrent task coordination. Finally, this survey outlines promising future research directions for LLM alignment through innovative reward design strategies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models</title>
<link>https://arxiv.org/abs/2505.02686</link>
<guid>https://arxiv.org/abs/2505.02686</guid>
<content:encoded><![CDATA[
arXiv:2505.02686v1 Announce Type: new 
Abstract: Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>fastabx: A library for efficient computation of ABX discriminability</title>
<link>https://arxiv.org/abs/2505.02692</link>
<guid>https://arxiv.org/abs/2505.02692</guid>
<content:encoded><![CDATA[
arXiv:2505.02692v1 Announce Type: new 
Abstract: We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models</title>
<link>https://arxiv.org/abs/2505.02763</link>
<guid>https://arxiv.org/abs/2505.02763</guid>
<content:encoded><![CDATA[
arXiv:2505.02763v1 Announce Type: new 
Abstract: Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations</title>
<link>https://arxiv.org/abs/2505.02819</link>
<guid>https://arxiv.org/abs/2505.02819</guid>
<content:encoded><![CDATA[
arXiv:2505.02819v1 Announce Type: new 
Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations</title>
<link>https://arxiv.org/abs/2505.01433</link>
<guid>https://arxiv.org/abs/2505.01433</guid>
<content:encoded><![CDATA[
arXiv:2505.01433v1 Announce Type: cross 
Abstract: Understanding the binding specificity between T-cell receptors (TCRs) and peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy and vaccine development. However, current predictive models struggle with generalization, especially in data-scarce settings and when faced with novel epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced Recognition Network), a deep learning framework that combines large-scale protein language models with chemical representations of peptides. By encoding TCR \b{eta}-chain sequences using ESM-1b and transforming peptide sequences into SMILES strings processed by MolFormer, LANTERN captures rich biological and chemical features critical for TCR-peptide recognition. Through extensive benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR, LANTERN demonstrates superior performance, particularly in zero-shot and few-shot learning scenarios. Our model also benefits from a robust negative sampling strategy and shows significant clustering improvements via embedding analysis. These results highlight the potential of LANTERN to advance TCR-pMHC binding prediction and support the development of personalized immunotherapies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine</title>
<link>https://arxiv.org/abs/2505.01435</link>
<guid>https://arxiv.org/abs/2505.01435</guid>
<content:encoded><![CDATA[
arXiv:2505.01435v1 Announce Type: cross 
Abstract: Language models for scientific tasks are trained on text from scientific publications, most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive ML-driven systems (for complex or degraded ones). The choice of the "best" parser for a particular document depends on its computational cost and the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by $17\times$ while still achieving comparable accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at https://github.com/7shoe/AdaParse/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code</title>
<link>https://arxiv.org/abs/2505.01485</link>
<guid>https://arxiv.org/abs/2505.01485</guid>
<content:encoded><![CDATA[
arXiv:2505.01485v1 Announce Type: cross 
Abstract: Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation</title>
<link>https://arxiv.org/abs/2505.01636</link>
<guid>https://arxiv.org/abs/2505.01636</guid>
<content:encoded><![CDATA[
arXiv:2505.01636v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and task generalization. However, their application to structured data analysis remains fragile due to inconsistencies in schema interpretation, misalignment between user intent and model output, and limited mechanisms for self-correction when failures occur. This paper introduces the STROT Framework (Structured Task Reasoning and Output Transformation), a method for structured prompting and feedback-driven transformation logic generation aimed at improving the reliability and semantic alignment of LLM-based analytical workflows. STROT begins with lightweight schema introspection and sample-based field classification, enabling dynamic context construction that captures both the structure and statistical profile of the input data. This contextual information is embedded in structured prompts that guide the model toward generating task-specific, interpretable outputs. To address common failure modes in complex queries, STROT incorporates a refinement mechanism in which the model iteratively revises its outputs based on execution feedback and validation signals. Unlike conventional approaches that rely on static prompts or single-shot inference, STROT treats the LLM as a reasoning agent embedded within a controlled analysis loop -- capable of adjusting its output trajectory through planning and correction. The result is a robust and reproducible framework for reasoning over structured data with LLMs, applicable to diverse data exploration and analysis tasks where interpretability, stability, and correctness are essential.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm</title>
<link>https://arxiv.org/abs/2505.01706</link>
<guid>https://arxiv.org/abs/2505.01706</guid>
<content:encoded><![CDATA[
arXiv:2505.01706v1 Announce Type: cross 
Abstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even "good" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias</title>
<link>https://arxiv.org/abs/2505.01754</link>
<guid>https://arxiv.org/abs/2505.01754</guid>
<content:encoded><![CDATA[
arXiv:2505.01754v1 Announce Type: cross 
Abstract: Biased news reporting poses a significant threat to informed decision-making and the functioning of democracies. This study introduces a novel methodology for scalable, minimally biased analysis of media bias in political news. The proposed approach examines event selection, labeling, word choice, and commission and omission biases across news sources by leveraging natural language processing techniques, including hierarchical topic modeling, sentiment analysis, and ontology learning with large language models. Through three case studies related to current political events, we demonstrate the methodology's effectiveness in identifying biases across news sources at various levels of granularity. This work represents a significant step towards scalable, minimally biased media bias analysis, laying the groundwork for tools to help news consumers navigate an increasingly complex media landscape.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos</title>
<link>https://arxiv.org/abs/2505.01790</link>
<guid>https://arxiv.org/abs/2505.01790</guid>
<content:encoded><![CDATA[
arXiv:2505.01790v1 Announce Type: cross 
Abstract: Web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. However, improving user engagement and knowledge retention remains a challenge. Automatically generated questions can activate learners and support their knowledge acquisition. Further, they can help teachers and learners assess their understanding. While large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. In this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. We assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. Our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. We identify requirements for future multimodal datasets and outline promising research directions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability by design: an experimental analysis of the legal coding process</title>
<link>https://arxiv.org/abs/2505.01944</link>
<guid>https://arxiv.org/abs/2505.01944</guid>
<content:encoded><![CDATA[
arXiv:2505.01944v1 Announce Type: cross 
Abstract: Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.
  In this paper we deliver a methodology for \textit{legal coding} that starts with a fragment and goes onto a set of Deontic Defeasible Logic rules, involving a set of \textit{scenarios} to test the correctness of the coded fragments. The methodology is illustrated by the coding process of an example text. We then show the results of a series of experiments conducted with humans encoding a variety of normative backgrounds and corresponding cases in which we have measured the efforts made in the coding process, as related to some measurable features. To process these examples, a recently developed technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has been employed.
  Finally we provide a technique to forecast time required in coding, that depends on factors such as knowledge of the legal domain, knowledge of the coding processes, length of the text, and a measure of \textit{depth} that refers to the length of the paths of legal references.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.01958</link>
<guid>https://arxiv.org/abs/2505.01958</guid>
<content:encoded><![CDATA[
arXiv:2505.01958v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data</title>
<link>https://arxiv.org/abs/2505.02130</link>
<guid>https://arxiv.org/abs/2505.02130</guid>
<content:encoded><![CDATA[
arXiv:2505.02130v1 Announce Type: cross 
Abstract: Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring new Approaches for Information Retrieval through Natural Language Processing</title>
<link>https://arxiv.org/abs/2505.02199</link>
<guid>https://arxiv.org/abs/2505.02199</guid>
<content:encoded><![CDATA[
arXiv:2505.02199v1 Announce Type: cross 
Abstract: This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units</title>
<link>https://arxiv.org/abs/2505.02206</link>
<guid>https://arxiv.org/abs/2505.02206</guid>
<content:encoded><![CDATA[
arXiv:2505.02206v1 Announce Type: cross 
Abstract: Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Emergent Language Using Inter-Agent Transformers</title>
<link>https://arxiv.org/abs/2505.02215</link>
<guid>https://arxiv.org/abs/2505.02215</guid>
<content:encoded><![CDATA[
arXiv:2505.02215v1 Announce Type: cross 
Abstract: This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques</title>
<link>https://arxiv.org/abs/2505.02309</link>
<guid>https://arxiv.org/abs/2505.02309</guid>
<content:encoded><![CDATA[
arXiv:2505.02309v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL</title>
<link>https://arxiv.org/abs/2505.02391</link>
<guid>https://arxiv.org/abs/2505.02391</guid>
<content:encoded><![CDATA[
arXiv:2505.02391v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Inclusive Contributions in Model Sharing Markets</title>
<link>https://arxiv.org/abs/2505.02462</link>
<guid>https://arxiv.org/abs/2505.02462</guid>
<content:encoded><![CDATA[
arXiv:2505.02462v1 Announce Type: cross 
Abstract: While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bielik v3 Small: Technical Report</title>
<link>https://arxiv.org/abs/2505.02550</link>
<guid>https://arxiv.org/abs/2505.02550</guid>
<content:encoded><![CDATA[
arXiv:2505.02550v1 Announce Type: cross 
Abstract: We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning</title>
<link>https://arxiv.org/abs/2505.02639</link>
<guid>https://arxiv.org/abs/2505.02639</guid>
<content:encoded><![CDATA[
arXiv:2505.02639v1 Announce Type: cross 
Abstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Movie Hits Before They Happen with LLMs</title>
<link>https://arxiv.org/abs/2505.02693</link>
<guid>https://arxiv.org/abs/2505.02693</guid>
<content:encoded><![CDATA[
arXiv:2505.02693v1 Announce Type: cross 
Abstract: Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
<link>https://arxiv.org/abs/2505.02707</link>
<guid>https://arxiv.org/abs/2505.02707</guid>
<content:encoded><![CDATA[
arXiv:2505.02707v1 Announce Type: cross 
Abstract: A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Knowledge Graphs to harvest datasets for efficient CLIP model training</title>
<link>https://arxiv.org/abs/2505.02746</link>
<guid>https://arxiv.org/abs/2505.02746</guid>
<content:encoded><![CDATA[
arXiv:2505.02746v1 Announce Type: cross 
Abstract: Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing</title>
<link>https://arxiv.org/abs/2505.02811</link>
<guid>https://arxiv.org/abs/2505.02811</guid>
<content:encoded><![CDATA[
arXiv:2505.02811v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework, \textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v1 Announce Type: cross 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation</title>
<link>https://arxiv.org/abs/2505.02830</link>
<guid>https://arxiv.org/abs/2505.02830</guid>
<content:encoded><![CDATA[
arXiv:2505.02830v1 Announce Type: cross 
Abstract: Chest X-rays (CXRs) are the most frequently performed imaging examinations in clinical settings. Recent advancements in Large Multimodal Models (LMMs) have enabled automated CXR interpretation, enhancing diagnostic accuracy and efficiency. However, despite their strong visual understanding, current Medical LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level understanding and interaction, and (2) Limited accuracy and interpretability due to single-step reasoning. In this paper, we empower MLMMs with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. Specifically, we first propose an Anatomical Ontology-Guided Reasoning (AOR) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. Next, under the guidance of expert physicians, we develop AOR-Instruction, a large instruction dataset for MLMMs training. Our experiments demonstrate AOR's superior performance in both VQA and report generation tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.02835</link>
<guid>https://arxiv.org/abs/2505.02835</guid>
<content:encoded><![CDATA[
arXiv:2505.02835v1 Announce Type: cross 
Abstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformadores: Fundamentos teoricos y Aplicaciones</title>
<link>https://arxiv.org/abs/2302.09327</link>
<guid>https://arxiv.org/abs/2302.09327</guid>
<content:encoded><![CDATA[
arXiv:2302.09327v2 Announce Type: replace 
Abstract: Transformers are a neural network architecture originally developed for natural language processing, which have since become a foundational tool for solving a wide range of problems, including text, audio, image processing, reinforcement learning, and other tasks involving heterogeneous input data. Their hallmark is the self-attention mechanism, which allows the model to weigh different parts of the input sequence dynamically, and is an evolution of earlier attention-based approaches. This article provides readers with the necessary background to understand recent research on transformer models, and presents the mathematical and algorithmic foundations of their core components. It also explores the architecture's various elements, potential modifications, and some of the most relevant applications. The article is written in Spanish to help make this scientific knowledge more accessible to the Spanish-speaking community.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMUTF: Schema Matching Using Generative Tags and Hybrid Features</title>
<link>https://arxiv.org/abs/2402.01685</link>
<guid>https://arxiv.org/abs/2402.01685</guid>
<content:encoded><![CDATA[
arXiv:2402.01685v3 Announce Type: replace 
Abstract: We introduce SMUTF (Schema Matching Using Generative Tags and Hybrid Features), a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy "generative tags" for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.
  Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated exceptional performance, surpassing existing state-of-the-art models in terms of accuracy and efficiency, and improving the F1 score by 11.84% and the AUC of ROC by 5.08%. Code is available at https://github.com/fireindark707/Python-Schema-Matching.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation</title>
<link>https://arxiv.org/abs/2403.01954</link>
<guid>https://arxiv.org/abs/2403.01954</guid>
<content:encoded><![CDATA[
arXiv:2403.01954v4 Announce Type: replace 
Abstract: Constrained decoding approaches aim to control the meaning or style of text generated by the pre-trained large language models (LLMs or also PLMs) for various tasks at inference time. However, these methods often guide plausible continuations by greedily and explicitly selecting targets. Though fulfilling the task requirements, these methods may overlook certain general and natural logics that humans would implicitly follow towards such targets. Inspired by cognitive dual-process theory, in this work, we propose a novel decoding framework DECIDER where the base LLMs are equipped with a First-Order Logic (FOL) reasoner to express and evaluate the rules, along with a decision function that merges the outputs of both systems to guide the generation. Unlike previous constrained decodings, DECIDER transforms the encouragement of target-specific words into all words that satisfy several high-level rules, enabling us to programmatically integrate our logic into LLMs. Experiments on CommonGen and PersonaChat demonstrate that DECIDER effectively follows given FOL rules to guide LLMs in a more human-like and logic-controlled manner.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaICL: Towards Parallel In-Context Learning</title>
<link>https://arxiv.org/abs/2404.00570</link>
<guid>https://arxiv.org/abs/2404.00570</guid>
<content:encoded><![CDATA[
arXiv:2404.00570v2 Announce Type: replace 
Abstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind</title>
<link>https://arxiv.org/abs/2404.04748</link>
<guid>https://arxiv.org/abs/2404.04748</guid>
<content:encoded><![CDATA[
arXiv:2404.04748v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences</title>
<link>https://arxiv.org/abs/2405.05572</link>
<guid>https://arxiv.org/abs/2405.05572</guid>
<content:encoded><![CDATA[
arXiv:2405.05572v2 Announce Type: replace 
Abstract: Current computational approaches for analysing or generating code-mixed sentences do not explicitly model ``naturalness'' or ``acceptability'' of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models when trained solely using code-mixing metrics as features are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta and Bernice outperform IndicBERT across different configurations. Among Encoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder models are not able to outperform Encoder-only models. Decoder-only models perform the best when compared to all other MLLMS, with Llama 3.2 - 3B models outperforming similarly sized Qwen, Phi models. Comparison with zero and fewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from En-Hi to En-Te acceptability judgments are better than random baselines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Carriers of Hidden Messages</title>
<link>https://arxiv.org/abs/2406.02481</link>
<guid>https://arxiv.org/abs/2406.02481</guid>
<content:encoded><![CDATA[
arXiv:2406.02481v5 Announce Type: replace 
Abstract: Simple fine-tuning can embed hidden text into large language models (LLMs), which is revealed only when triggered by a specific query. Applications include LLM fingerprinting, where a unique identifier is embedded to verify licensing compliance, and steganography, where the LLM carries hidden messages disclosed through a trigger query.
  Our work demonstrates that embedding hidden text via fine-tuning, although seemingly secure due to the vast number of potential triggers, is vulnerable to extraction through analysis of the LLM's output decoding process. We introduce an extraction attack called Unconditional Token Forcing (UTF), which iteratively feeds tokens from the LLM's vocabulary to reveal sequences with high token probabilities, indicating hidden text candidates. We also present Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes hidden text resistant to all known extraction attacks without degrading the general performance of LLMs compared to standard fine-tuning. UTFC has both benign (improving LLM fingerprinting) and malign applications (using LLMs to create covert communication channels).
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</title>
<link>https://arxiv.org/abs/2407.12772</link>
<guid>https://arxiv.org/abs/2407.12772</guid>
<content:encoded><![CDATA[
arXiv:2407.12772v2 Announce Type: replace 
Abstract: The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs. We opensource our codebase and maintain leaderboard of LIVEBENCH at https://github.com/EvolvingLMMs-Lab/lmms-eval and https://huggingface.co/spaces/lmms-lab/LiveBench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Logical Fallacy-Informed Framework for Argument Generation</title>
<link>https://arxiv.org/abs/2408.03618</link>
<guid>https://arxiv.org/abs/2408.03618</guid>
<content:encoded><![CDATA[
arXiv:2408.03618v4 Announce Type: replace 
Abstract: Despite the remarkable performance of Large Language Models (LLMs) in natural language processing tasks, they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. To address this issue, we introduce FIPO, a fallacy-informed framework that leverages preference optimization methods to steer LLMs toward logically sound arguments. FIPO includes a classification loss, to capture the fine-grained information on fallacy types. Our results on argumentation datasets show that our method reduces the fallacy errors by up to 17.5%. Furthermore, our human evaluation results indicate that the quality of the generated arguments by our method significantly outperforms the fine-tuned baselines, as well as other preference optimization methods, such as DPO. These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation. Our code is available at github.com/lucamouchel/Logical-Fallacies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library</title>
<link>https://arxiv.org/abs/2408.06150</link>
<guid>https://arxiv.org/abs/2408.06150</guid>
<content:encoded><![CDATA[
arXiv:2408.06150v3 Announce Type: replace 
Abstract: In this study, we generate and maintain a database of 10 million virtual lipids through METiS's in-house de novo lipid generation algorithms and lipid virtual screening techniques. These virtual lipids serve as a corpus for pre-training, lipid representation learning, and downstream task knowledge transfer, culminating in state-of-the-art LNP property prediction performance. We propose LipidBERT, a BERT-like model pre-trained with the Masked Language Model (MLM) and various secondary tasks. Additionally, we compare the performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like lipid generation model, on downstream tasks. The proposed bilingual LipidBERT model operates in two languages: the language of ionizable lipid pre-training, using in-house dry-lab lipid structures, and the language of LNP fine-tuning, utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT as a key AI-based filter for future screening tasks, including new versions of METiS de novo lipid libraries and, more importantly, candidates for in vivo testing for orgran-targeting LNPs. To the best of our knowledge, this is the first successful demonstration of the capability of a pre-trained language model on virtual lipids and its effectiveness in downstream tasks using web-lab data. This work showcases the clever utilization of METiS's in-house de novo lipid library as well as the power of dry-wet lab integration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence</title>
<link>https://arxiv.org/abs/2409.09413</link>
<guid>https://arxiv.org/abs/2409.09413</guid>
<content:encoded><![CDATA[
arXiv:2409.09413v2 Announce Type: replace 
Abstract: This perspective paper explores the bidirectional influence between language emergence and the relational structure of subjective experiences, termed qualia structure, and lays out a constructive approach to the intricate dependency between the two. We hypothesize that the emergence of languages with distributional semantics (e.g., syntactic-semantic structures) is linked to the coordination of internal representations shaped by experience, potentially facilitating more structured language through reciprocal influence. This hypothesized mutual dependency connects to recent advancements in AI and symbol emergence robotics, and is explored within this paper through theoretical frameworks such as the collective predictive coding. Computational studies show that neural network-based language models form systematically structured internal representations, and multimodal language models can share representations between language and perceptual information. This perspective suggests that language emergence serves not only as a mechanism creating a communication tool but also as a mechanism for allowing people to realize shared understanding of qualitative experiences. The paper discusses the implications of this bidirectional influence in the context of consciousness studies, linguistics, and cognitive science, and outlines future constructive research directions to further explore this dynamic relationship between language emergence and qualia structure.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions</title>
<link>https://arxiv.org/abs/2410.14567</link>
<guid>https://arxiv.org/abs/2410.14567</guid>
<content:encoded><![CDATA[
arXiv:2410.14567v4 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has become integral to large language models (LLMs), particularly for conversational AI systems where user questions may reference knowledge beyond the LLMs' training cutoff. However, many natural user questions lack well-defined answers, either due to limited domain knowledge or because the retrieval system returns documents that are relevant in appearance but uninformative in content. In such cases, LLMs often produce hallucinated answers without flagging them. While recent work has largely focused on questions with false premises, we study out-of-scope questions, where the retrieved document appears semantically similar to the question but lacks the necessary information to answer it. In this paper, we propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality. We use this dataset to evaluate several LLMs on their ability to detect out-of-scope questions and generate appropriate responses. Finally, we introduce an improved detection method that enhances the reliability of LLM-based question-answering systems in handling out-of-scope questions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Extremely Low-Resource Finno-Ugric Languages</title>
<link>https://arxiv.org/abs/2410.18902</link>
<guid>https://arxiv.org/abs/2410.18902</guid>
<content:encoded><![CDATA[
arXiv:2410.18902v2 Announce Type: replace 
Abstract: The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on V\~oro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction</title>
<link>https://arxiv.org/abs/2412.04454</link>
<guid>https://arxiv.org/abs/2412.04454</guid>
<content:encoded><![CDATA[
arXiv:2412.04454v2 Announce Type: replace 
Abstract: Automating GUI tasks remains challenging due to reliance on textual representations, platform-specific action spaces, and limited reasoning capabilities. We introduce Aguvis, a unified vision-based framework for autonomous GUI agents that directly operates on screen images, standardizes cross-platform interactions and incorporates structured reasoning via inner monologue. To enable this, we construct Aguvis Data Collection, a large-scale dataset with multimodal grounding and reasoning annotations, and develop a two-stage training pipeline that separates GUI grounding from planning and reasoning. Experiments show that Aguvis achieves state-of-the-art performance across offline and real-world online benchmarks, marking the first fully autonomous vision-based GUI agent that operates without closed-source models. We open-source all datasets, models, and training recipes at https://aguvis-project.github.io to advance future research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-LLM: Benchmarking Large Language Models for Anomaly Detection</title>
<link>https://arxiv.org/abs/2412.11142</link>
<guid>https://arxiv.org/abs/2412.11142</guid>
<content:encoded><![CDATA[
arXiv:2412.11142v2 Announce Type: replace 
Abstract: Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2501.00062</link>
<guid>https://arxiv.org/abs/2501.00062</guid>
<content:encoded><![CDATA[
arXiv:2501.00062v2 Announce Type: replace 
Abstract: Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and yielded the lowest cost/performance ratio (\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at much less cost (\$0.38 vs. \$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models</title>
<link>https://arxiv.org/abs/2501.00874</link>
<guid>https://arxiv.org/abs/2501.00874</guid>
<content:encoded><![CDATA[
arXiv:2501.00874v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Anonymization of the Language Modeling</title>
<link>https://arxiv.org/abs/2501.02407</link>
<guid>https://arxiv.org/abs/2501.02407</guid>
<content:encoded><![CDATA[
arXiv:2501.02407v2 Announce Type: replace 
Abstract: Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when pre-trained models fine-tuned and specialized on sensitive data can memorize and then expose and regurgitate personal information. This paper presents a privacy-preserving language modeling approach to address the problem of language models anonymization, and thus promote their sharing. Specifically, we propose both a Masking Language Modeling (MLM) methodology to specialize a BERT-like language model, and a Causal Language Modeling (CLM) methodology to specialize a GPT-like model that avoids the model from memorizing direct and indirect identifying information present in the training data. We have comprehensively evaluated our approaches using a medical dataset and compared them against different baselines. Our results indicate that by avoiding memorizing both direct and indirect identifiers during model specialization, our masking and causal language modeling schemes offer a good tradeoff for maintaining high privacy while retaining high utility.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How do Humans and Language Models Reason About Creativity? A Comparative Analysis</title>
<link>https://arxiv.org/abs/2502.03253</link>
<guid>https://arxiv.org/abs/2502.03253</guid>
<content:encoded><![CDATA[
arXiv:2502.03253v2 Announce Type: replace 
Abstract: Creativity assessment in science and engineering is increasingly based on both human and AI judgment, but the cognitive processes and biases behind these evaluations remain poorly understood. We conducted two experiments examining how including example solutions with ratings impact creativity evaluation, using a finegrained annotation protocol where raters were tasked with explaining their originality scores and rating for the facets of remoteness (whether the response is "far" from everyday ideas), uncommonness (whether the response is rare), and cleverness. In Study 1, we analyzed creativity ratings from 72 experts with formal science or engineering training, comparing those who received example solutions with ratings (example) to those who did not (no example). Computational text analysis revealed that, compared to experts with examples, no-example experts used more comparative language (e.g., "better/worse") and emphasized solution uncommonness, suggesting they may have relied more on memory retrieval for comparisons. In Study 2, parallel analyses with state-of-the-art LLMs revealed that models prioritized uncommonness and remoteness of ideas when rating originality, suggesting an evaluative process rooted around the semantic similarity of ideas. In the example condition, while LLM accuracy in predicting the true originality scores improved, the correlations of remoteness, uncommonness, and cleverness with originality also increased substantially -- to upwards of $0.99$ -- suggesting a homogenization in the LLMs evaluation of the individual facets. These findings highlight important implications for how humans and AI reason about creativity and suggest diverging preferences for what different populations prioritize when rating.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechT: Findings of the First Mentorship in Speech Translation</title>
<link>https://arxiv.org/abs/2502.12050</link>
<guid>https://arxiv.org/abs/2502.12050</guid>
<content:encoded><![CDATA[
arXiv:2502.12050v2 Announce Type: replace 
Abstract: This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the mentorship requirements, the participants engaged in key activities, including data preparation, modelling, and advanced research. The participants explored data augmentation techniques and compared end-to-end and cascaded speech translation systems. The projects covered various languages other than English, including Arabic, Bengali, Galician, Indonesian, Japanese, and Spanish.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing</title>
<link>https://arxiv.org/abs/2502.15666</link>
<guid>https://arxiv.org/abs/2502.15666</guid>
<content:encoded><![CDATA[
arXiv:2502.15666v2 Announce Type: replace 
Abstract: The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Such classification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate twelve state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains 14.7K samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently flag even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data</title>
<link>https://arxiv.org/abs/2502.16892</link>
<guid>https://arxiv.org/abs/2502.16892</guid>
<content:encoded><![CDATA[
arXiv:2502.16892v2 Announce Type: replace 
Abstract: Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</title>
<link>https://arxiv.org/abs/2502.17424</link>
<guid>https://arxiv.org/abs/2502.17424</guid>
<content:encoded><![CDATA[
arXiv:2502.17424v5 Announce Type: replace 
Abstract: We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs</title>
<link>https://arxiv.org/abs/2502.21239</link>
<guid>https://arxiv.org/abs/2502.21239</guid>
<content:encoded><![CDATA[
arXiv:2502.21239v4 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable performance across diverse tasks by encoding vast amounts of factual knowledge. However, they are still prone to hallucinations, generating incorrect or misleading information, often accompanied by high uncertainty. Existing methods for hallucination detection primarily focus on quantifying internal uncertainty, which arises from missing or conflicting knowledge within the model. However, hallucinations can also stem from external uncertainty, where ambiguous user queries lead to multiple possible interpretations. In this work, we introduce Semantic Volume, a novel mathematical measure for quantifying both external and internal uncertainty in LLMs. Our approach perturbs queries and responses, embeds them in a semantic space, and computes the determinant of the Gram matrix of the embedding vectors, capturing their dispersion as a measure of uncertainty. Our framework provides a generalizable and unsupervised uncertainty detection method without requiring internal access to LLMs. We conduct extensive experiments on both external and internal uncertainty detection, demonstrating that our Semantic Volume method consistently outperforms existing baselines in both tasks. Additionally, we provide theoretical insights linking our measure to differential entropy, unifying and extending previous sampling-based uncertainty measures such as the semantic entropy. Semantic Volume is shown to be a robust and interpretable approach to improving the reliability of LLMs by systematically detecting uncertainty in both user queries and model responses.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications</title>
<link>https://arxiv.org/abs/2503.17003</link>
<guid>https://arxiv.org/abs/2503.17003</guid>
<content:encoded><![CDATA[
arXiv:2503.17003v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users' diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment-a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?</title>
<link>https://arxiv.org/abs/2503.24235</link>
<guid>https://arxiv.org/abs/2503.24235</guid>
<content:encoded><![CDATA[
arXiv:2503.24235v3 Announce Type: replace 
Abstract: As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2504.03302</link>
<guid>https://arxiv.org/abs/2504.03302</guid>
<content:encoded><![CDATA[
arXiv:2504.03302v2 Announce Type: replace 
Abstract: Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&amp;B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</title>
<link>https://arxiv.org/abs/2504.03601</link>
<guid>https://arxiv.org/abs/2504.03601</guid>
<content:encoded><![CDATA[
arXiv:2504.03601v3 Announce Type: replace 
Abstract: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source 5K synthetic data trajectories and the trained xLAM-2-fc-r models to advance research in AI agents.
  Models at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4; Dataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website at https://apigen-mt.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Estimation of the KL Divergence Between Language Models</title>
<link>https://arxiv.org/abs/2504.10637</link>
<guid>https://arxiv.org/abs/2504.10637</guid>
<content:encoded><![CDATA[
arXiv:2504.10637v2 Announce Type: replace 
Abstract: Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection</title>
<link>https://arxiv.org/abs/2309.15670</link>
<guid>https://arxiv.org/abs/2309.15670</guid>
<content:encoded><![CDATA[
arXiv:2309.15670v2 Announce Type: replace-cross 
Abstract: In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT), a well-known methodology of transformers, have been shown the best results of all methods implemented. Finally, a web application has been developed to demonstrate the performance of the pre-trained top-performer model (BERT) for multi-label ER in Bangla.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Noisy Supervision in Foundation Model Learning</title>
<link>https://arxiv.org/abs/2403.06869</link>
<guid>https://arxiv.org/abs/2403.06869</guid>
<content:encoded><![CDATA[
arXiv:2403.06869v3 Announce Type: replace-cross 
Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Design of Audio-Visual Speech Recognition Models using Branchformers</title>
<link>https://arxiv.org/abs/2407.06606</link>
<guid>https://arxiv.org/abs/2407.06606</guid>
<content:encoded><![CDATA[
arXiv:2407.06606v3 Announce Type: replace-cross 
Abstract: Recent advances in Audio-Visual Speech Recognition (AVSR) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. In most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. However, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. Furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. In this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. Our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the Branchformer, in the design of parameter-efficient AVSR systems. To be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. Extensive experiments on English and Spanish AVSR benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. Even when trained on a moderate scale of data, our models achieve competitive word error rates (WER) of approximately 2.5\% for English and surpass existing approaches for Spanish, establishing a new benchmark with an average WER of around 9.1\%. These results reflect how our tailored AVSR system is able to reach state-of-the-art recognition rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. Code and pre-trained models are available at https://github.com/david-gimeno/tailored-avsr.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant</title>
<link>https://arxiv.org/abs/2501.17176</link>
<guid>https://arxiv.org/abs/2501.17176</guid>
<content:encoded><![CDATA[
arXiv:2501.17176v3 Announce Type: replace-cross 
Abstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization</title>
<link>https://arxiv.org/abs/2502.04667</link>
<guid>https://arxiv.org/abs/2502.04667</guid>
<content:encoded><![CDATA[
arXiv:2502.04667v2 Announce Type: replace-cross 
Abstract: The integration of explicit Chain-of-Thought (CoT) reasoning into training large language models (LLMs) has advanced their reasoning capabilities, yet the mechanisms by which CoT enhances generalization remain poorly understood. This work investigates (1) \textit{how} CoT training reshapes internal model representations and (2) \textit{why} it improves both in-distribution (ID) and out-of-distribution (OOD) reasoning generalization. Through controlled experiments and theoretical analysis, we derive the following key insights. \textbf{1)} Structural Advantage: CoT training internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Notably, CoT-trained models resolve intermediate results at shallower layers compared to non-CoT counterparts, freeing up deeper layers to specialize in subsequent reasoning steps. \textbf{2)} Theoretical Analysis: the information-theoretic generalization bounds via distributional divergence can be decomposed into ID and OOD components. While ID error diminishes with sufficient training regardless of CoT, OOD error critically depends on CoT: Non-CoT training fails to generalize to OOD samples due to unseen reasoning patterns, whereas CoT training achieves near-perfect OOD generalization by mastering subtasks and reasoning compositions during training. The identified mechanisms explain our experimental results: CoT training accelerates convergence and enhances generalization from ID to both ID and OOD scenarios while maintaining robust performance even with tolerable noise. These findings are further validated on complex real-world datasets. This paper offers valuable insights for designing CoT strategies to enhance LLM reasoning robustness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoNormia: Benchmarking Physical Social Norm Understanding</title>
<link>https://arxiv.org/abs/2502.20490</link>
<guid>https://arxiv.org/abs/2502.20490</guid>
<content:encoded><![CDATA[
arXiv:2502.20490v3 Announce Type: replace-cross 
Abstract: Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \dataset{} $\|\epsilon\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\% on \dataset{} (versus a human bench of 92\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \dataset{} to enhance normative reasoning in VLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats</title>
<link>https://arxiv.org/abs/2503.02650</link>
<guid>https://arxiv.org/abs/2503.02650</guid>
<content:encoded><![CDATA[
arXiv:2503.02650v2 Announce Type: replace-cross 
Abstract: The exponential growth of unstructured text data presents a fundamental challenge in modern data management and information retrieval. While Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, their potential to transform unstructured text into standardized, structured formats remains largely unexplored - a capability that could revolutionize data processing workflows across industries. This study breaks new ground by systematically evaluating LLMs' ability to convert unstructured recipe text into the structured Cooklang format. Through comprehensive testing of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an innovative evaluation approach is introduced that combines traditional metrics (WER, ROUGE-L, TER) with specialized metrics for semantic element identification. Our experiments reveal that GPT-4o with few-shot prompting achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating for the first time that LLMs can reliably transform domain-specific unstructured text into structured formats without extensive training. Although model performance generally scales with size, we uncover surprising potential in smaller models like Llama3.1:8b for optimization through targeted fine-tuning. These findings open new possibilities for automated structured data generation across various domains, from medical records to technical documentation, potentially transforming the way organizations process and utilize unstructured information.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dysarthria Normalization via Local Lie Group Transformations for Robust ASR</title>
<link>https://arxiv.org/abs/2504.12279</link>
<guid>https://arxiv.org/abs/2504.12279</guid>
<content:encoded><![CDATA[
arXiv:2504.12279v2 Announce Type: replace-cross 
Abstract: We present a geometry-driven method for normalizing dysarthric speech by modeling time, frequency, and amplitude distortions as smooth, local Lie group transformations of spectrograms. Scalar fields generate these deformations via exponential maps, and a neural network is trained - using only synthetically warped healthy speech - to infer the fields and apply an approximate inverse at test time. We introduce a spontaneous-symmetry-breaking (SSB) potential that encourages the model to discover non-trivial field configurations. On real pathological speech, the system delivers consistent gains: up to 17 percentage-point WER reduction on challenging TORGO utterances and a 16 percent drop in WER variance, with no degradation on clean CommonVoice data. Character and phoneme error rates improve in parallel, confirming linguistic relevance. Our results demonstrate that geometrically structured warping provides consistent, zero-shot robustness gains for dysarthric ASR.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models</title>
<link>https://arxiv.org/abs/2505.00725</link>
<guid>https://arxiv.org/abs/2505.00725</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial QA, BERT language model, Answer selection, Answer Retriever, Answer Re-ranker

Summary:
- The article proposes a novel financial Question Answering (QA) system utilizing the BERT language model to address data scarcity and language specificity in the financial domain.
- Focuses on financial non-factoid answer selection by re-ranking passage-level texts using an Answer Retriever based on BM25 and an Answer Re-ranker using variants of pre-trained BERT models.
- Various learning, further pre-training, and fine-tuning approaches are investigated for BERT models.
- The Transfer and Adapt further fine-tuning approach, leading to the creation of FinBERT-QA, is identified as the most effective model.
- FinBERT-QA improves state-of-the-art results on the FiQA dataset, showing a 16% increase in MRR, 17% in NDCG, and 21% in Precision@1.

<br /><br />Summary: <div>
arXiv:2505.00725v1 Announce Type: new 
Abstract: Motivated by the emerging demand in the financial industry for the automatic analysis of unstructured and structured data at scale, Question Answering (QA) systems can provide lucrative and competitive advantages to companies by facilitating the decision making of financial advisers. Consequently, we propose a novel financial QA system using the transformer-based pre-trained BERT language model to address the limitations of data scarcity and language specificity in the financial domain. Our system focuses on financial non-factoid answer selection, which retrieves a set of passage-level texts and selects the most relevant as the answer. To increase efficiency, we formulate the answer selection task as a re-ranking problem, in which our system consists of an Answer Retriever using BM25, a simple information retrieval approach, to first return a list of candidate answers, and an Answer Re-ranker built with variants of pre-trained BERT language models to re-rank and select the most relevant answers. We investigate various learning, further pre-training, and fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a model built from applying the Transfer and Adapt further fine-tuning and pointwise learning approach, is the most effective, improving the state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on NDCG, and 21% on Precision@1.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model based Human-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00753</link>
<guid>https://arxiv.org/abs/2505.00753</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, autonomous agents, human-agent system, reliability, challenges

Summary:
Large language models (LLMs) have shown promise in building fully autonomous agents, but they still face challenges such as reliability issues, difficulty in handling complex tasks, and safety concerns. To address these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control to improve system performance, reliability, and safety. This paper offers a comprehensive survey of LLM-HAS, discussing key components like environment & profiling, human feedback, interaction types, orchestration, and communication, as well as exploring emerging applications and addressing unique challenges and opportunities in the field. By providing a structured overview, the paper aims to stimulate further research and innovation in this interdisciplinary domain. Resources and references related to LLM-HAS can be found at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

<br /><br />Summary: <div>
arXiv:2505.00753v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Capabilities and Invariability of Large Language Models</title>
<link>https://arxiv.org/abs/2505.00776</link>
<guid>https://arxiv.org/abs/2505.00776</guid>
<content:encoded><![CDATA[
<div> Large Language Models, reasoning competence, benchmark dataset, geometric figures, shallow logical reasoning  
Summary:  
This study examines the reasoning ability of Large Language Models (LLMs) by introducing a new benchmark dataset focusing on simple logical reasoning tasks. The questions in the dataset are based on geometric figures to ensure that responses rely solely on deduction. Analysis of 24 LLMs with different sizes shows room for improvement in simple reasoning tasks, despite better performance by models with over 70 billion parameters in zero-shot settings. Additionally, a chain-of-thought prompt test reveals that the performance of LLMs can be influenced by the order in which rationale is provided before or after the answer. There is potential for improvement in LLMs' reasoning abilities, especially in tasks requiring basic logical deduction.<br /><br />Summary: <div>
arXiv:2505.00776v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction</title>
<link>https://arxiv.org/abs/2505.00814</link>
<guid>https://arxiv.org/abs/2505.00814</guid>
<content:encoded><![CDATA[
<div> pre-trained language models, biomedical literature, relationship extraction, hyperparameter optimization, contextual information <br />
Summary: <br />
- Automatic relationship extraction from biomedical literature is crucial for managing the vast scientific knowledge being produced annually.
- Utilizing pre-trained language models has become popular in this field, but comparisons between studies are challenging due to various factors.
- This study evaluated pre-trained language models with additional contextual information on five datasets with consistent evaluation methods.
- Extensive hyperparameter optimization was conducted to achieve strong extraction performance.
- The inclusion of contextual information showed minor overall improvements, with significant benefits for smaller pre-trained language models when external data was included during fine-tuning. <div>
arXiv:2505.00814v1 Announce Type: new 
Abstract: Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent approach in RE. Several studies report improved performance when incorporating additional context information while fine-tuning PLMs for RE. However, variations in the PLMs applied, the databases used for augmentation, hyper-parameter optimization, and evaluation methods complicate direct comparisons between studies and raise questions about the generalizability of these findings. Our study addresses this research gap by evaluating PLMs enhanced with contextual information on five datasets spanning four relation scenarios within a consistent evaluation framework. We evaluate three baseline PLMs and first conduct extensive hyperparameter optimization. After selecting the top-performing model, we enhance it with additional data, including textual entity descriptions, relational information from knowledge graphs, and molecular structure encodings. Our findings illustrate the importance of i) the choice of the underlying language model and ii) a comprehensive hyperparameter optimization for achieving strong extraction performance. Although inclusion of context information yield only minor overall improvements, an ablation study reveals substantial benefits for smaller PLMs when such external data was included during fine-tuning.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing</title>
<link>https://arxiv.org/abs/2505.00931</link>
<guid>https://arxiv.org/abs/2505.00931</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Dynamic Assessment, DynaWrite, GPT-4o, neural chat

Summary: 
Large Language Models (LLMs) were tested to scale up Dynamic Assessment (DA) in language learning. The study developed DynaWrite, a grammatical tutoring app supporting multiple LLMs for learners of English. Among 21 LLMs tested, GPT-4o and neural chat showed potential for scaling up DA. Further testing revealed GPT-4o to outperform neural chat in generating clear, consistent, and progressively explicit hints for grammatical errors. Both models accurately identified errors, with GPT-4o demonstrating speed and stability. This study confirms the effectiveness of LLMs in scaling up dynamic assessment, enabling delivery to larger groups beyond traditional teacher-learner settings. 

<br /><br />Summary: <div>
arXiv:2505.00931v1 Announce Type: new 
Abstract: This study investigates the potential for Large Language Models (LLMs) to scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first developed DynaWrite-a modular, microservices-based grammatical tutoring application which supports multiple LLMs to generate dynamic feedback to learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural chat to have the most potential to scale-up DA in the language learning classroom. Further testing of these two candidates found both models performed similarly in their ability to accurately identify grammatical errors in user sentences. However, GPT-4o consistently outperformed neural chat in the quality of its DA by generating clear, consistent, and progressively explicit hints. Real-time responsiveness and system stability were also confirmed through detailed performance testing, with GPT-4o exhibiting sufficient speed and stability. This study shows that LLMs can be used to scale-up dynamic assessment and thus enable dynamic assessment to be delivered to larger groups than possible in traditional teacher-learner settings.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-Nemotron: Efficient Reasoning Models</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
<div> Keywords: Llama-Nemotron models, reasoning capabilities, inference efficiency, training procedure, open-source resources

Summary:
The Llama-Nemotron series of models offers exceptional reasoning capabilities and inference efficiency, available in three sizes: Nano, Super, and Ultra. These models compete with state-of-the-art reasoning models while providing superior inference throughput and memory efficiency. The training procedure involves neural architecture search, knowledge distillation, pretraining, supervised fine-tuning, and large scale reinforcement learning. These models are the first to support a dynamic reasoning toggle for switching between chat and reasoning modes during inference. To support open research, the models are released under the NVIDIA Open Model License Agreement. Additionally, the post-training dataset and training codebases are also made available. This initiative aims to facilitate model development and enhance the field of reasoning models.<br /><br />Summary: <div>
arXiv:2505.00949v1 Announce Type: new 
Abstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts</title>
<link>https://arxiv.org/abs/2505.00977</link>
<guid>https://arxiv.org/abs/2505.00977</guid>
<content:encoded><![CDATA[
<div> algorithm, steganography, text generation, embedding, XLNet

Summary:
The paper introduces a novel embedding algorithm called Character-based Diffusion Embedding Algorithm (CDEA) to improve the quality of steganographic text generation by leveraging sensitive information's properties. Unlike existing algorithms, CDEA enhances the selection frequency of high-probability candidate words while reducing low-probability ones in the candidate pool, thereby increasing semantic coherence and logical fluency. The algorithm utilizes character-level statistical properties and power-law distributions for grouping methods. Additionally, the paper introduces the XLNet model to effectively transform sensitive information in long sequences. Experimental results show that the combination of CDEA and XLNet significantly enhances the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility. <br /><br />Summary: <div>
arXiv:2505.00977v1 Announce Type: new 
Abstract: Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.00979</link>
<guid>https://arxiv.org/abs/2505.00979</guid>
<content:encoded><![CDATA[
<div> knowledge associations, synthetic data generation, Large Language Models, efficient corpus expansion, multi-hop document Q&amp;A dataset

Summary:
The article introduces a new framework called Synthetic-on-Graph (SoG) for synthetic data generation in Large Language Models (LLMs). SoG focuses on incorporating cross-document knowledge associations to enhance content diversity and coherence. By constructing a context graph and using a graph walk strategy for knowledge-associated sampling, SoG improves synthetic data quality and enables models to learn complex knowledge structures. Additionally, the integration of Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic further enhances reasoning processes and discriminative power. Experimental results show that SoG outperforms existing methods in a multi-hop document Q&amp;A dataset and performs comparably in reading comprehension tasks, demonstrating its better generalization capability. This framework advances synthetic data generation, providing practical solutions for efficient knowledge acquisition in LLMs, particularly in domains with limited data availability.<br /><br />Summary: <div>
arXiv:2505.00979v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&amp;A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Enough of Scaling LLMs! Lets Focus on Downscaling</title>
<link>https://arxiv.org/abs/2505.00985</link>
<guid>https://arxiv.org/abs/2505.00985</guid>
<content:encoded><![CDATA[
<div> Keywords: neural scaling laws, downscaling, large language models, computational inefficiency, sustainability

Summary:<br /><br />We challenge the prevailing focus on neural scaling laws and propose a shift towards downscaling in the development of large language models (LLMs). While scaling laws have been beneficial in understanding performance improvements, they come with drawbacks such as computational inefficiency and adverse environmental impacts. Our advocated framework for downscaling LLMs aims to maintain performance while reducing resource demands significantly. Practical strategies outlined in the paper suggest a departure from traditional scaling paradigms to achieve a more sustainable, efficient, and accessible approach to LLM development. <div>
arXiv:2505.00985v1 Announce Type: new 
Abstract: We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language</title>
<link>https://arxiv.org/abs/2505.00989</link>
<guid>https://arxiv.org/abs/2505.00989</guid>
<content:encoded><![CDATA[
<div> Keywords: Vessel Traffic Services, LLM agent, Text-to-SQL task, domain-specific corpus, maritime knowledge<br />
Summary:<br />
The article introduces VTS-LLM Agent, designed for interactive decision support in Vessel Traffic Services (VTS). The agent addresses limitations in spatiotemporal reasoning and human interaction in existing VTS systems by formalizing risk-prone vessel identification as a Text-to-SQL task. A benchmark dataset is constructed for this purpose, incorporating structured vessel databases and external maritime knowledge. The framework of VTS-LLM includes NER-based relational reasoning, domain knowledge injection, semantic algebra representation, and query rethink mechanisms for enhanced domain understanding. Experimental results show superior performance over baselines in various query styles. Importantly, the study highlights the impact of linguistic style variation on Text-to-SQL modeling performance. Overall, VTS-LLM paves the way for natural language interfaces in VTS operations, facilitating proactive, LLM-driven maritime traffic management.<br /> <div>
arXiv:2505.00989v1 Announce Type: new 
Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-free Models for Sarcasm Detection</title>
<link>https://arxiv.org/abs/2505.01006</link>
<guid>https://arxiv.org/abs/2505.01006</guid>
<content:encoded><![CDATA[
<div> Tokenization, NLP, ByT5, CANINE, sarcasm detection  
Summary:  
Tokenization is a crucial step in NLP pipelines but can lead to challenges such as vocabulary mismatches. By evaluating token-free models ByT5 and CANINE on sarcasm detection in social media and news headlines, this study shows that these models outperform token-based approaches. ByT5-small and CANINE achieve new state-of-the-art accuracy on the datasets, indicating their potential for robust NLP in informal and noisy domains like social media. The results highlight the effectiveness of models operating on raw text at the byte or character level, offering a solution to vocabulary mismatch and out-of-vocabulary issues common in traditional tokenization methods. <div>
arXiv:2505.01006v1 Announce Type: new 
Abstract: Tokenization is a foundational step in most natural language processing (NLP) pipelines, yet it introduces challenges such as vocabulary mismatch and out-of-vocabulary issues. Recent work has shown that models operating directly on raw text at the byte or character level can mitigate these limitations. In this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of sarcasm detection in both social media (Twitter) and non-social media (news headlines) domains. We fine-tune and benchmark these models against token-based baselines and state-of-the-art approaches. Our results show that ByT5-small and CANINE outperform token-based counterparts and achieve new state-of-the-art performance, improving accuracy by 0.77% and 0.49% on the News Headlines and Twitter Sarcasm datasets, respectively. These findings underscore the potential of token-free models for robust NLP in noisy and informal domains such as social media.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark</title>
<link>https://arxiv.org/abs/2505.01015</link>
<guid>https://arxiv.org/abs/2505.01015</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, benchmark, value orientations, human subjects, bias

Summary: 
The article introduces the Value Portrait benchmark for evaluating language models' value orientations in real-life user interactions. This framework aims to address existing biases in benchmarks by collecting human ratings based on similarity to personal thoughts, ensuring accuracy in assessing values. The study of 27 language models using this benchmark revealed prioritization of Benevolence, Security, and Self-Direction values while neglecting Tradition, Power, and Achievement values. Additionally, the analysis uncovered biases in how language models perceive different demographic groups, diverging from actual human data. The Value Portrait benchmark provides a psychometrically validated approach to assess language models' alignment with human values in real-world contexts. 

<br /><br />Summary: <div>
arXiv:2505.01015v1 Announce Type: new 
Abstract: The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage and thus ecological validity. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 27 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?</title>
<link>https://arxiv.org/abs/2505.01035</link>
<guid>https://arxiv.org/abs/2505.01035</guid>
<content:encoded><![CDATA[
<div> detailed rubric, automated essay scoring, large language models, token usage, scoring accuracy <br />
Summary: <br />
This study examines the necessity and impact of detailed rubrics in automated essay scoring (AES) with large language models (LLMs). The researchers compared the effects of a full rubric, a simplified rubric, and no rubric on scoring accuracy across multiple LLMs using the TOEFL11 dataset. Results showed that most LLMs maintained similar scoring accuracy with a simplified rubric compared to a detailed one, while reducing token usage. However, one model experienced decreased performance with a more detailed rubric. The findings suggest that simplified rubrics may be sufficient for LLM-based AES applications, providing a more efficient alternative without compromising accuracy. Model-specific evaluation is crucial as performance varies across different LLMs. <br /> <div>
arXiv:2505.01035v1 Announce Type: new 
Abstract: This study investigates the necessity and impact of a detailed rubric in automated essay scoring (AES) using large language models (LLMs). While using rubrics are standard in LLM-based AES, creating detailed rubrics requires substantial ef-fort and increases token usage. We examined how different levels of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11 dataset. Our experiments compared three conditions: a full rubric, a simplified rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5 Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of four models maintained similar scoring accuracy with the simplified rubric compared to the detailed one, while significantly reducing token usage. However, one model (Gemini 1.5 Flash) showed decreased performance with more detailed rubrics. The findings suggest that simplified rubrics may be sufficient for most LLM-based AES applications, offering a more efficient alternative without compromis-ing scoring accuracy. However, model-specific evaluation remains crucial as per-formance patterns vary across different LLMs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2505.01068</link>
<guid>https://arxiv.org/abs/2505.01068</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, Multimodal Transformers, Efficiency Optimization, Interlaced Mask, Graph-Structured Representation 

Summary:
Efficiency optimization in Multimodal Sentiment Analysis is addressed by the proposed Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT), which uses an Interlaced Mask mechanism for efficient weight-sharing and achieves All-Modal-In-One fusion with fewer parameters compared to traditional Multimodal Transformers (MulTs). GsiT is based on the concept of hierarchical modal-wise heterogeneous graphs (HMHGs) and demonstrates superior performance while reducing computational overhead. By integrating GsiT into various state-of-the-art models, significant performance improvements and parameter reductions are observed on popular MSA datasets. This approach not only enhances efficiency in MSA but also showcases the potential of the HMHG concept in optimizing multimodal fusion strategies. 

<br /><br />Summary: <div>
arXiv:2505.01068v1 Announce Type: new 
Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning</title>
<link>https://arxiv.org/abs/2505.01110</link>
<guid>https://arxiv.org/abs/2505.01110</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, In-Context Learning, Attention Dispersion, Context Size, Empirical Results<br />
<br />
Summary: 
Large Language Models (LLMs) have shown impressive capabilities in In-Context Learning (ICL), but are limited by fixed position length constraints. The proposed Mitigating Attention Dispersion in large-scale ICL (MateICL) addresses this issue by splitting the context into multiple windows and recalibrating attention weights to prioritize query tokens. This approach enables LLMs to effectively leverage larger contexts for improved ICL performance. MateICL outperforms retrieval-based baselines consistently without the need for an externally trained retrieval model. Even in resource-constrained settings, MateICL proves beneficial, demonstrating its effectiveness in enhancing ICL performance. The code for MateICL is publicly available for further research and implementation. This work showcases the potential for enhancing language model performance in In-Context Learning tasks through attention management strategies like MateICL. <br /><br />Summary: <div>
arXiv:2505.01110v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in In-Context Learning (ICL). However, the fixed position length constraints in pre-trained models limit the number of demonstration examples. Recent efforts to extend context suffer from attention dispersion as the number of demonstrations increases. In this paper, we introduce Mitigating Attention Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective self-attention as the context size grows. We first split the context into multiple windows, each filled to the model's context capacity, which are processed separately. Then, we introduce an additional layer to recalibrate the attention weights, prioritizing the query tokens as the number of demonstrations increases. Our empirical results show that MateICL can effectively leverage larger contexts to improve ICL performance. Compared to retrieval-based baselines, MateICL consistently achieves better performance without requiring an externally trained retrieval model. Despite recent advances in inference strategies (e.g., 32k token contexts), our results demonstrate that MateICL remains beneficial in computationally resource-constrained settings. The code is publicly available at https://github.com/amurtadha/MateICL.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limitations of Steering in Language Model Alignment</title>
<link>https://arxiv.org/abs/2505.01162</link>
<guid>https://arxiv.org/abs/2505.01162</guid>
<content:encoded><![CDATA[
<div> Steering Vectors, Language Model, Alignment, Transformer Hook Interventions, Antonym-based Function Vectors
Summary:
Steering vectors are proposed as a means to align language model behavior during inference. However, a framework is needed to assess their limitations as alignment mechanisms. Using transformer hook interventions and antonym-based function vectors, this study evaluates how prompt structure and context complexity affect the effectiveness of steering vectors. Results suggest that while steering vectors show promise for certain alignment tasks like value alignment, they may not be a robust solution for general alignment in Large Language Models (LLMs), especially in complex scenarios. This study lays the groundwork for future research into the steering capabilities of reasoning models. 
<br /><br />Summary: <div>
arXiv:2505.01162v1 Announce Type: new 
Abstract: Steering vectors are a promising approach to aligning language model behavior at inference time. In this paper, we propose a framework to assess the limitations of steering vectors as alignment mechanisms. Using a framework of transformer hook interventions and antonym-based function vectors, we evaluate the role of prompt structure and context complexity in steering effectiveness. Our findings indicate that steering vectors are promising for specific alignment tasks, such as value alignment, but may not provide a robust foundation for general-purpose alignment in LLMs, particularly in complex scenarios. We establish a methodological foundation for future investigations into steering capabilities of reasoning models.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods</title>
<link>https://arxiv.org/abs/2505.01198</link>
<guid>https://arxiv.org/abs/2505.01198</guid>
<content:encoded><![CDATA[
<div> gender disparity, explanation methods, fairness, subgroups, language models 
Summary:
- The study focuses on the gender disparities in widely used post-hoc feature attribution methods across various tasks and language models.
- It highlights significant differences in faithfulness, robustness, and complexity of the explanation methods for different genders.
- The disparities exist even when models are trained on unbiased datasets, indicating the issue is not solely due to biased training data.
- The findings emphasize the need to address disparities in explanation methods to prevent biased outcomes against certain subgroups.
- It suggests incorporating fairness of explanations as a crucial factor in regulatory frameworks to ensure overall model fairness and explainability.<br /><br />Summary: <div>
arXiv:2505.01198v1 Announce Type: new 
Abstract: While research on applications and evaluations of explanation methods continues to expand, fairness of the explanation methods concerning disparities in their performance across subgroups remains an often overlooked aspect. In this paper, we address this gap by showing that, across three tasks and five language models, widely used post-hoc feature attribution methods exhibit significant gender disparity with respect to their faithfulness, robustness, and complexity. These disparities persist even when the models are pre-trained or fine-tuned on particularly unbiased datasets, indicating that the disparities we observe are not merely consequences of biased training data. Our results highlight the importance of addressing disparities in explanations when developing and applying explainability methods, as these can lead to biased outcomes against certain subgroups, with particularly critical implications in high-stakes contexts. Furthermore, our findings underscore the importance of incorporating the fairness of explanations, alongside overall model fairness and explainability, as a requirement in regulatory frameworks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models</title>
<link>https://arxiv.org/abs/2505.01238</link>
<guid>https://arxiv.org/abs/2505.01238</guid>
<content:encoded><![CDATA[
<div> framework, NLP models, explainability methods, EvalxNLP, feature attribution<br />
<br />
Summary: <br />
The article discusses the importance of ensuring interpretability in evolving Natural Language Processing (NLP) models, especially for high-stakes applications. To address this challenge, the authors introduce EvalxNLP, a Python framework that benchmarks state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques, allowing users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. The framework also provides interactive, LLM-based textual explanations to enhance user understanding of the generated explanations and evaluation outcomes. Human evaluation results show high user satisfaction with EvalxNLP, indicating its potential as a tool for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims to democratize explainability tools and support the systematic comparison and advancement of Explainable AI (XAI) techniques in NLP. <div>
arXiv:2505.01238v1 Announce Type: new 
Abstract: As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREMISE: Matching-based Prediction for Accurate Review Recommendation</title>
<link>https://arxiv.org/abs/2505.01255</link>
<guid>https://arxiv.org/abs/2505.01255</guid>
<content:encoded><![CDATA[
<div> Keywords: PREMISE, multimodal learning, matching-based, multi-scale representations, downstream recommendation task

Summary: 
PREMISE, a new architecture for multimodal learning, specifically targeting the multimodal review helpfulness task, introduces a novel approach to matching-based learning. Unlike fusion-based methods, PREMISE focuses on computing multi-scale and multi-field representations, filtering duplicated semantics, and generating matching scores as feature vectors for the downstream recommendation task. This architecture demonstrates significant performance improvements in tasks where context matching content is closely related to task targets. Experimental results on two datasets show that PREMISE outperforms state-of-the-art fusion-based methods, delivering promising results with reduced computational cost. Overall, PREMISE enhances multimodal learning by refining representations and optimizing matching scores for improved task performance. 

<br /><br />Summary: <div>
arXiv:2505.01255v1 Announce Type: new 
Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the matching-based learning in the multimodal fields for the multimodal review helpfulness (MRHP) task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</title>
<link>https://arxiv.org/abs/2505.01273</link>
<guid>https://arxiv.org/abs/2505.01273</guid>
<content:encoded><![CDATA[
<div> anti-adversarial, PromptObfus, privacy, desensitization, masked language modeling

Summary:
PromptObfus introduces a method for desensitizing prompts in Large Language Models (LLMs) to protect user privacy. Using "anti-adversarial" learning, PromptObfus perturbs privacy words in prompts while maintaining model prediction stability. The approach frames prompt desensitization as a masked language modeling task, where privacy-sensitive terms are replaced with a [MASK] token. A desensitization model is trained to generate candidate replacements for masked positions, selected based on feedback from a surrogate model to minimize task output disruption. Through experiments on three NLP tasks, PromptObfus successfully prevents privacy inference by remote LLMs while preserving task performance. The proposed method addresses the challenge of privacy preservation in user prompts without the heavy computational costs or user participation requirements associated with traditional techniques like homomorphic encryption or federated learning.<br /><br />Summary: <div>
arXiv:2505.01273v1 Announce Type: new 
Abstract: With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is "anti-adversarial" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types</title>
<link>https://arxiv.org/abs/2505.01311</link>
<guid>https://arxiv.org/abs/2505.01311</guid>
<content:encoded><![CDATA[
<div> factorized model, vague temporal adverbials, probabilistic distributions, contextualized meaning, Occam's razor <br />
Summary: <br />
This paper introduces a factorized model to analyze the semantics of vague temporal adverbials like recently or just. The model uses probabilistic distributions to account for the underspecified duration of these adverbials in relation to past events. By combining these distributions with event-specific ones, the model can provide a contextualized meaning for each adverbial applied to a particular event. Parameters of the model are fitted using data from native speakers' judgments on the suitability of these adverbials for events occurring a certain time ago. Comparing this factorized model to a non-factorized one based on Gaussian distributions, it is found that while both models are similarly predictive, the factorized model is simpler and more extendable according to Occam's razor principles. <div>
arXiv:2505.01311v1 Announce Type: new 
Abstract: Vague temporal adverbials, such as recently, just, and a long time ago, describe the temporal distance between a past event and the utterance time but leave the exact duration underspecified. In this paper, we introduce a factorized model that captures the semantics of these adverbials as probabilistic distributions. These distributions are composed with event-specific distributions to yield a contextualized meaning for an adverbial applied to a specific event. We fit the model's parameters using existing data capturing judgments of native speakers regarding the applicability of these vague temporal adverbials to events that took place a given time ago. Comparing our approach to a non-factorized model based on a single Gaussian distribution for each pair of event and temporal adverbial, we find that while both models have similar predictive power, our model is preferable in terms of Occam's razor, as it is simpler and has better extendability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer-based Neural Architecture Search Method</title>
<link>https://arxiv.org/abs/2505.01314</link>
<guid>https://arxiv.org/abs/2505.01314</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, neural architecture search, multihead attention computation, perplexity, BLEU scores

Summary:<br />
- The paper presents a neural architecture search method utilizing Transformer architecture to search for cross multihead attention computation ways for different encoder and decoder combinations.
- Perplexity is considered as an auxiliary evaluation metric along with BLEU scores to improve translation results and optimize neural network structures.
- A multi-objective genetic algorithm iteratively enhances individual neural networks within the population to achieve better models.
- Experimental results demonstrate superior performance of the algorithm's searched neural network structures compared to baseline models.
- Introducing the auxiliary evaluation metric leads to the discovery of better models than solely relying on BLEU scores. 

<br /><br />Summary: <div>
arXiv:2505.01314v1 Announce Type: new 
Abstract: This paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System</title>
<link>https://arxiv.org/abs/2505.01315</link>
<guid>https://arxiv.org/abs/2505.01315</guid>
<content:encoded><![CDATA[
<div> keywords: Large Language Models, Adversarial Attacks, Natural Language Processing, Defense Mechanism, Prompt Filtering

Summary: 
The study introduces a defense mechanism for Large Language Models (LLMs) to autonomously detect and defend against adversarial attacks without the need for retraining. The framework includes a prompt filtering module utilizing advanced Natural Language Processing techniques to identify harmful inputs and a summarization module to provide context-aware defense knowledge from adversarial research literature. Experimental results show a high success rate of 98.71% in detecting harmful patterns and encoded prompts. By incorporating adversarial research literature, the methodology enhances LLMs' resistance to adversarial exploitation with increased jailbreak resistance and refusal rate. The approach significantly boosts LLMs' resistance to malicious inputs while maintaining response quality, serving as a practical alternative to time-consuming retraining-based defenses.<br /><br />Summary: <div>
arXiv:2505.01315v1 Announce Type: new 
Abstract: The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References</title>
<link>https://arxiv.org/abs/2505.01325</link>
<guid>https://arxiv.org/abs/2505.01325</guid>
<content:encoded><![CDATA[
<div> Benchmark, Temporal references, Question Answering, LLMs, TRAVELER <br />
<br />
Summary: 
The article introduces TRAVELER, a synthetic benchmark dataset designed to evaluate models' abilities in resolving temporal references in natural language understanding. The dataset comprises questions with temporal references and correct answers, assessing models' performance in understanding explicit, implicit, and vague temporal references. The benchmark allows evaluation of model performance based on the type of temporal reference and length of event sets. Human surveys were conducted to establish ground-truth answers for vague temporal references. Evaluation of four state-of-the-art LLMs on the dataset revealed that while models perform well on questions with explicit temporal references and small event sets, performance decreases with larger event sets and less explicit temporal references. Particularly, the models showed the lowest performance on vague temporal references. The TRAVELER benchmark is publicly available for further research and evaluation of temporal reference resolution systems. <br /><br />Summary: <div>
arXiv:2505.01325v1 Announce Type: new 
Abstract: Understanding and resolving temporal references is essential in Natural Language Understanding as we often refer to the past or future in daily communication. Although existing benchmarks address a system's ability to reason about and resolve temporal references, systematic evaluation of specific temporal references remains limited. Towards closing this gap, we introduce TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering paradigm and consists of questions involving temporal references with the corresponding correct answers. TRAVELER assesses models' abilities to resolve explicit, implicit relative to speech time, and vague temporal references. Beyond investigating the performance of state-of-the-art LLMs depending on the type of temporal reference, our benchmark also allows evaluation of performance in relation to the length of the set of events. For the category of vague temporal references, ground-truth answers were established via human surveys on Prolific, following a procedure similar to the one from Kenneweg et al. To demonstrate the benchmark's applicability, we evaluate four state-of-the-art LLMs using a question-answering task encompassing 3,300 questions. Our findings show that while the benchmarked LLMs can answer questions over event sets with a handful of events and explicit temporal references successfully, performance clearly deteriorates with larger event set length and when temporal references get less explicit. Notably, the vague question category exhibits the lowest performance across all models.
  The benchmark is publicly available at: https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
<div> Improvements, Text-to-Image, Generative Models, Evaluation, Multimodal Language Models<br />
Summary:<br />
The study explores using multi-modal large language models (MLLMs) to evaluate text-to-image generative models efficiently. The proposed Multimodal Text-to-Image Eval (MT2IE) framework generates prompts iteratively to assess prompt-generation consistency and image aesthetics. MT2IE requires only a fraction of prompts compared to existing benchmarks but produces similar T2I model rankings. The prompt-generation consistency scores from MT2IE show higher correlation with human judgment than previous scores. This approach addresses the challenges of static data benchmarks and offers a more dynamic evaluation method for T2I model progress. <div>
arXiv:2505.00759v1 Announce Type: cross 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i</title>
<link>https://arxiv.org/abs/2505.00808</link>
<guid>https://arxiv.org/abs/2505.00808</guid>
<content:encoded><![CDATA[
<div> Keywords: Mechanistic Interpretability, Explanatory View Hypothesis, Explanatory Faithfulness, Principle of Explanatory Optimism, Neural Networks

Summary:
The article presents the concept of Mechanistic Interpretability (MI) in neural networks, arguing that understanding models through causal explanations is essential. It introduces the Explanatory View Hypothesis, suggesting that neural networks inherently contain implicit explanations that can be extracted and comprehended. The concept of Explanatory Faithfulness is defined as the fit between an explanation and a model. MI is characterized as producing model-level, ontic, causal-mechanistic, and falsifiable explanations of neural networks, setting it apart from other interpretability paradigms. The Principle of Explanatory Optimism is proposed as a necessary condition for the success of MI. This article aims to provide a principled approach to understanding neural networks through causal explanations, emphasizing the importance of extracting and understanding implicit explanations within the models. <br /><br />Summary:Keywords: Mechanistic Interpretability, Explanatory View Hin the same line, <br /> is the line break of HTML, 2 must be retained when output, and must be before the word 'Summary:' <div>
arXiv:2505.00808v1 Announce Type: cross 
Abstract: Mechanistic Interpretability aims to understand neural networks through causal explanations. We argue for the Explanatory View Hypothesis: that Mechanistic Interpretability research is a principled approach to understanding models because neural networks contain implicit explanations which can be extracted and understood. We hence show that Explanatory Faithfulness, an assessment of how well an explanation fits a model, is well-defined. We propose a definition of Mechanistic Interpretability (MI) as the practice of producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural networks, allowing us to distinguish MI from other interpretability paradigms and detail MI's inherent limits. We formulate the Principle of Explanatory Optimism, a conjecture which we argue is a necessary precondition for the success of Mechanistic Interpretability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation</title>
<link>https://arxiv.org/abs/2505.00831</link>
<guid>https://arxiv.org/abs/2505.00831</guid>
<content:encoded><![CDATA[
<div> Keywords: Robotics, Path planning, Large Language Models, Small Language Models, Simulation-powered training

Summary:
SmallPlan introduces a novel framework for efficient path planning in robotics, utilizing Small Language Models (SLMs) trained by Large Language Models (LLMs). The SLMs provide optimal action sequences for navigating 3D scenes represented as scene graphs, achieving competitive performance without overfitting. Training the SLMs involves interleaved simulation-powered fine-tuning and reinforcement learning guided by LLMs, enabling them to learn important factors like travel distance and trial numbers. This approach ensures resource efficiency, making SmallPlan suitable for real-time deployment on edge devices for practical autonomous robotics applications. <div>
arXiv:2505.00831v1 Announce Type: cross 
Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeMo-Inspector: A Visualization Tool for LLM Generation Analysis</title>
<link>https://arxiv.org/abs/2505.00903</link>
<guid>https://arxiv.org/abs/2505.00903</guid>
<content:encoded><![CDATA[
<div> Dataset Analysis, Synthetic Data, NeMo-Inspector, Quality Improvement, Large Language Models
Summary:
NeMo-Inspector is introduced as an open-source tool to simplify the analysis of synthetic datasets for Large Language Models (LLMs). It effectively reduced low-quality samples in the GSM-Plus dataset from 46.99% to 19.51% through analysis and cleaning. Additionally, it helped correct generation errors in OpenMath models, leading to a 1.92% accuracy improvement on the MATH dataset and a 4.17% improvement on the GSM8K dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from Nemotron-4-340B. This tool is beneficial for enhancing the capabilities of LLMs by ensuring the quality of synthetic data, which is particularly useful when real-world data is scarce or challenging to obtain. With NeMo-Inspector, developers can streamline the process of identifying and resolving errors in synthetic datasets, ultimately improving the performance of LLMs on various tasks. 
<br /><br />Summary: <div>
arXiv:2505.00903v1 Announce Type: cross 
Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their overall capabilities often requires large, high-quality training datasets. Synthetic data, generated at scale, serves a valuable alternative when real-world data is scarce or difficult to obtain. However, ensuring the quality of synthetic datasets is challenging, as developers must manually inspect and refine numerous samples to identify errors and areas for improvement. This process is time-consuming and requires specialized tools. We introduce NeMo-Inspector, an open-source tool designed to simplify the analysis of synthetic datasets with integrated inference capabilities. We demonstrate its effectiveness through two real-world cases. Analysis and cleaning of the synthetically generated GSM-Plus dataset with NeMo-Inspector led to a significant decrease in low-quality samples from 46.99% to 19.51%. The tool also helped identify and correct generation errors in OpenMath models, improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from Nemotron-4-340B.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias</title>
<link>https://arxiv.org/abs/2505.00926</link>
<guid>https://arxiv.org/abs/2505.00926</guid>
<content:encoded><![CDATA[
<div> transformers, language recognition, regular language, even pairs, parity check
Summary:<br /><br />
- This work focuses on analyzing how a one-layer transformer learns to solve language recognition tasks such as even pairs and parity check.
- The analysis reveals that the training dynamics of the transformer exhibit two distinct phases.
- In the first phase, the attention layer quickly maps data sequences into separable vectors.
- In the second phase, the attention layer stabilizes, and the linear layer logarithmically approaches a max-margin hyperplane for correct classification.
- Experimental results validate the theoretical findings, highlighting the effectiveness of the approach in training transformers for language recognition tasks. <div>
arXiv:2505.00926v1 Announce Type: cross 
Abstract: Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attack and defense techniques in large language models: A survey and new perspectives</title>
<link>https://arxiv.org/abs/2505.00976</link>
<guid>https://arxiv.org/abs/2505.00976</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, attacks, defense strategies, vulnerabilities, interdisciplinary collaboration <br />
Summary: This systematic survey examines the various attack and defense techniques in Large Language Models (LLMs). Attacks on LLMs include adversarial prompt attack, optimized attacks, model theft, and attacks on LLM applications. Defense strategies encompass prevention-based and detection-based methods. The study acknowledges progress in this area but notes the challenges in adapting to evolving threats, balancing usability with robustness, and overcoming resource limitations in defense implementation. The survey also highlights the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. It stresses the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications. <br /><br />Summary: <div>
arXiv:2505.00976v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Resistance of Neural Network Watermarking to Fine-tuning</title>
<link>https://arxiv.org/abs/2505.01007</link>
<guid>https://arxiv.org/abs/2505.01007</guid>
<content:encoded><![CDATA[
<div> watermarking, ownership information, deep neural network, robust, fine-tuning

Summary:
- The paper introduces a new watermarking method for embedding ownership information into deep neural networks (DNNs) that is resilient to fine-tuning.
- The method is proven to preserve specific frequency components of convolutional filters when the input feature of a convolutional layer consists of mainly low-frequency components.
- A modified Fourier transform is proposed to extract frequency components from convolutional filters.
- The frequency components are shown to remain unchanged by weight scaling and permutations, making them suitable for encoding watermark information.
- Initial experiments confirm the effectiveness of the proposed watermarking method for embedding ownership information in DNNs. 

<br /><br />Summary: <div>
arXiv:2505.01007v1 Announce Type: cross 
Abstract: This paper proves a new watermarking method to embed the ownership information into a deep neural network (DNN), which is robust to fine-tuning. Specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised Fourier transform to extract frequency components from the convolutional filter. Additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. In this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. Preliminary experiments demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2505.01096</link>
<guid>https://arxiv.org/abs/2505.01096</guid>
<content:encoded><![CDATA[
<div> radiology report generation, low-resource languages, Vision-Language Models, domain-specific training, healthcare<br />
Summary:<br />
This study evaluates the performance of Vision-Language Models in generating radiology reports in low-resource languages like Italian, German, and Spanish. Language-specific models outperformed general and domain-specific models, showcasing the importance of linguistic adaptation. Models fine-tuned with medical terminology showed improved performance, emphasizing the need for domain-specific training. The research also explored the impact of the temperature parameter on report coherence, providing insights for optimal model settings. The findings highlight the significance of tailored language and domain-specific training in enhancing the quality and accuracy of radiology reports in multilingual settings. This study advances understanding of VLM adaptability in healthcare and suggests avenues for future research on model tuning and language-specific adaptations.<br /><br />Summary: <div>
arXiv:2505.01096v1 Announce Type: cross 
Abstract: The integration of artificial intelligence in healthcare has opened new horizons for improving medical diagnostics and patient care. However, challenges persist in developing systems capable of generating accurate and contextually relevant radiology reports, particularly in low-resource languages. In this study, we present a comprehensive benchmark to evaluate the performance of instruction-tuned Vision-Language Models (VLMs) in the specialized task of radiology report generation across three low-resource languages: Italian, German, and Spanish. Employing the LLaVA architectural framework, we conducted a systematic evaluation of pre-trained models utilizing general datasets, domain-specific datasets, and low-resource language-specific datasets. In light of the unavailability of models that possess prior knowledge of both the medical domain and low-resource languages, we analyzed various adaptations to determine the most effective approach for these contexts. The results revealed that language-specific models substantially outperformed both general and domain-specific models in generating radiology reports, emphasizing the critical role of linguistic adaptation. Additionally, models fine-tuned with medical terminology exhibited enhanced performance across all languages compared to models with generic knowledge, highlighting the importance of domain-specific training. We also explored the influence of the temperature parameter on the coherence of report generation, providing insights for optimal model settings. Our findings highlight the importance of tailored language and domain-specific training for improving the quality and accuracy of radiological reports in multilingual settings. This research not only advances our understanding of VLMs adaptability in healthcare but also points to significant avenues for future investigations into model tuning and language-specific adaptations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii</title>
<link>https://arxiv.org/abs/2505.01372</link>
<guid>https://arxiv.org/abs/2505.01372</guid>
<content:encoded><![CDATA[
<div> Explanation-generating methods, Mechanistic Interpretability, neural networks, causal explanations, Explanatory Virtues Framework <br />
<br />
Summary: 
The article discusses the challenge of evaluating explanations in Mechanistic Interpretability (MI) and introduces a new framework called the Explanatory Virtues Framework. Drawing on perspectives from the Philosophy of Science, the framework evaluates explanations based on Bayesian, Kuhnian, Deutschian, and Nomological perspectives. The analysis suggests that Compact Proofs, which consider multiple explanatory virtues, show promise in improving explanations in MI. The research directions proposed include defining explanatory simplicity, focusing on unifying explanations, and deriving universal principles for neural networks. These advancements in MI methods have the potential to enhance our ability to monitor, predict, and guide AI systems. <div>
arXiv:2505.01372v1 Announce Type: cross 
Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through causal explanations. Though MI has many explanation-generating methods, progress has been limited by the lack of a universal approach to evaluating explanations. Here we analyse the fundamental question "What makes a good explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on four perspectives from the Philosophy of Science - the Bayesian, Kuhnian, Deutschian, and Nomological - to systematically evaluate and improve explanations in MI. We find that Compact Proofs consider many explanatory virtues and are hence a promising approach. Fruitful research directions implied by our framework include (1) clearly defining explanatory simplicity, (2) focusing on unifying explanations and (3) deriving universal principles for neural networks. Improved MI methods enhance our ability to monitor, predict, and steer AI systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark</title>
<link>https://arxiv.org/abs/2402.14359</link>
<guid>https://arxiv.org/abs/2402.14359</guid>
<content:encoded><![CDATA[
<div> Keywords: summarization, language models, scientific corpus, evaluation methods, facet-aware metric

Summary:
The paper explores the use of pretrained and large language models (LLMs) for scientific summarization. Traditional evaluation methods like $n$-gram and embedding comparison are found inadequate for assessing scientific concepts and key content. A new facet-aware metric (FM) is introduced to evaluate summaries based on different aspects, offering a more comprehensive evaluation of abstracts. The Facet-based scientific summarization Dataset (FD) is curated with facet-level annotations to address the lack of an evaluation benchmark in this domain. The findings show that FM provides a logical approach to evaluating scientific summaries. Fine-tuned smaller models show potential in competing with LLMs in scientific contexts, but LLMs struggle to learn from in-context information in scientific domains, pointing towards opportunities for future enhancement of LLMs. <div>
arXiv:2402.14359v2 Announce Type: replace 
Abstract: The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLMs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Vision &amp; Language Decoders use Images and Text equally? How Self-consistent are their Explanations?</title>
<link>https://arxiv.org/abs/2404.18624</link>
<guid>https://arxiv.org/abs/2404.18624</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision and Language Model, Explanation Generation, Answer Generation, Self-Consistency, VALSE Benchmark

Summary:
Vision and Language Model (VLM) decoders are highly effective on multimodal tasks, generating natural language explanations alongside answers. This study explores how VLMs use input modalities for explanations compared to answers, finding text to be more crucial than images in all tasks. VLMs demonstrate lower self-consistency compared to Language Models (LLMs). Notably, image contributions are more pronounced in generating explanations than answers, more so in Conversations-on-the-Task (CoT) settings. VLM decoders face challenges with phenomena on the VALSE benchmark, needing further improvements to tackle complex tasks. The study also provides an updated benchmarking of VLM decoders on the VALSE benchmark, shedding light on their current performance levels. 

<br /><br />Summary: 
- VLM decoders rely more on text than images for both answers and explanations.
- They exhibit lower self-consistency than LLMs.
- Image contributions are more significant for explanation generation than answer generation, especially in CoT settings.
- VLM decoders struggle with most phenomena on the VALSE benchmark.
- Current VLM decoders require enhancements to address challenging multimodal tasks effectively. <div>
arXiv:2404.18624v4 Announce Type: replace 
Abstract: Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REFFLY: Melody-Constrained Lyrics Editing Model</title>
<link>https://arxiv.org/abs/2409.00292</link>
<guid>https://arxiv.org/abs/2409.00292</guid>
<content:encoded><![CDATA[
<div> Keywords: melody-to-lyric generation, revision framework, lyrics alignment, semantic meaning preservation, musical consistency

Summary:
REFFLY (REvision Framework For LYrics) introduces a novel approach to automatic melody-to-lyric generation by focusing on revising plain text to align with a given melody. This framework offers a flexible and practical alternative to creating lyrics, enabling applications like generating lyrics from different inputs, translating songs across languages while maintaining the melody, and adapting lyrics to various genres. The model is trained on a curated dataset of synthesized melody-aligned lyrics, allowing it to transform plain text into melody-aligned lyrics effectively. Additionally, the framework incorporates training-free heuristics to preserve both semantic meaning and musical consistency during the editing process. Experimental results show that REFFLY outperforms existing models like Lyra and GPT-4 by 25% in musicality and text quality, demonstrating its effectiveness across a range of tasks in the domain of lyric generation. 

<br /><br />Summary: <div>
arXiv:2409.00292v2 Announce Type: replace 
Abstract: Automatic melody-to-lyric (M2L) generation aims to create lyrics that align with a given melody. While most previous approaches generate lyrics from scratch, revision, editing plain text draft to fit it into the melody, offers a much more flexible and practical alternative. This enables broad applications, such as generating lyrics from flexible inputs (keywords, themes, or full text that needs refining to be singable), song translation (preserving meaning across languages while keeping the melody intact), or style transfer (adapting lyrics to different genres). This paper introduces REFFLY (REvision Framework For LYrics), the first revision framework for editing and generating melody-aligned lyrics. We train the lyric revision module using our curated synthesized melody-aligned lyrics dataset, enabling it to transform plain text into lyrics that align with a given melody. To further enhance the revision ability, we propose training-free heuristics aimed at preserving both semantic meaning and musical consistency throughout the editing process. Experimental results demonstrate the effectiveness of REFFLY across various tasks (e.g. lyrics generation, song translation), showing that our model outperforms strong baselines, including Lyra (Tian et al., 2023) and GPT-4, by 25% in both musicality and text quality.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Self-Attention Need Separate Weights in Transformers?</title>
<link>https://arxiv.org/abs/2412.00359</link>
<guid>https://arxiv.org/abs/2412.00359</guid>
<content:encoded><![CDATA[
<div> Keywords: self-attention, BERT model, shared weight, parameter reduction, prediction accuracy

Summary:
The study introduces a shared weight self-attention-based BERT model that utilizes only one weight matrix for Key, Value, and Query representations, reducing parameter size and training time significantly. The shared self-attention method achieves a 66.53% reduction in the attention block's parameter size. In the GLUE dataset, the model shows improved accuracy compared to standard, symmetric, and pairwise attention-based BERT models. Specifically, accuracy improvements of 0.38%, 5.81%, and 1.06% were observed, respectively. The model also demonstrates higher prediction accuracy on small tasks of GLUE, showcasing its generalization power on noisy and out-of-domain data. This innovative approach addresses the computational complexity and directional challenges faced by traditional self-attention mechanisms, making it a promising development in the field of natural language processing.<br /><br />Summary: <div>
arXiv:2412.00359v2 Announce Type: replace 
Abstract: The success of self-attention lies in its ability to capture long-range dependencies and enhance context understanding, but it is limited by its computational complexity and challenges in handling sequential data with inherent directionality. This work introduces a shared weight self-attention-based BERT model that only learns one weight matrix for (Key, Value, and Query) representations instead of three individual matrices for each of them. Our shared weight attention reduces the training parameter size by more than half and training time by around one-tenth. Furthermore, we demonstrate higher prediction accuracy on small tasks of GLUE over the BERT baseline and in particular a generalization power on noisy and out-of-domain data. Experimental results indicate that our shared self-attention method achieves a parameter size reduction of 66.53% in the attention block. In the GLUE dataset, the shared weight self-attention-based BERT model demonstrates accuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric, and pairwise attention-based BERT models, respectively. The model and source code are available at Anonymous.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Every Token Counts: Optimal Segmentation for Low-Resource Language Models</title>
<link>https://arxiv.org/abs/2412.06926</link>
<guid>https://arxiv.org/abs/2412.06926</guid>
<content:encoded><![CDATA[
<div> optimality, tokenization, performance, multilingual, compression <br />
Summary: <br />
This work explores the impact of different tokenization methods on Natural Language Processing (NLP) performance. Traditional greedy tokenization techniques have been widely used but may not be optimal for all languages and model scales. The study shows that an optimized Byte-Pair Encoding (BPE) configuration can significantly reduce token count and improve performance, especially for smaller models. The researchers conducted extensive experiments across various tasks, including generation and classification, to assess the benefits of compression-optimized tokenization strategies. Their findings suggest that such strategies could offer significant advantages for multilingual and low-resource language applications. This research highlights the importance of exploring alternative tokenization approaches to enhance NLP models' efficiency and effectiveness. <br /> <div>
arXiv:2412.06926v5 Announce Type: replace 
Abstract: Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2412.10422</link>
<guid>https://arxiv.org/abs/2412.10422</guid>
<content:encoded><![CDATA[
<div> Tabular Question Answering, Data Preparation, Large Language Model, Multi-agent Framework, AutoPrep

Summary:
AutoPrep is a framework designed to enhance Tabular Question Answering by efficiently preparing data for answering natural language questions about tables. It utilizes multiple agents specialized in different types of data prep tasks to ensure accurate responses. The framework consists of a Planner that determines a logical plan, a Programmer that generates low-level code based on the plan, and an Executor that processes the table. AutoPrep incorporates a Chain-of-Clauses reasoning mechanism for high-level operation suggestion and a tool-augmented method for low-level code generation to support its multi-agent framework. This approach addresses the unique requirements of question-aware data preparation and improves the contextual relevance of responses. <div>
arXiv:2412.10422v3 Announce Type: replace 
Abstract: Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICLR: In-Context Learning of Representations</title>
<link>https://arxiv.org/abs/2501.00070</link>
<guid>https://arxiv.org/abs/2501.00070</guid>
<content:encoded><![CDATA[
<div> keywords: semantics, large language model, in-context learning, representations, graph tracing<br />
Summary:<br />
Recent research has shown that the semantics provided by pretraining data influence how concepts are structured in large language models (LLMs). The study explores whether LLMs can adopt alternative, context-specific semantics by analyzing intermediate representations in a "graph tracing" task. Results indicate that as context increases, models reorganize their representations to align with the context-specified structure rather than the pretrained semantics. Additionally, when concepts have semantic correlations, the context-specified structure is present but does not override the pretrained structure. The task is likened to energy minimization for a predefined graph topology, suggesting an implicit optimization process for inferring context-specific semantics. These findings suggest that scaling context size can lead to flexible reorganization of model representations, potentially unlocking new capabilities.<br />
Summary: <div>
arXiv:2501.00070v2 Announce Type: replace 
Abstract: Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy "graph tracing" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention</title>
<link>https://arxiv.org/abs/2501.06382</link>
<guid>https://arxiv.org/abs/2501.06382</guid>
<content:encoded><![CDATA[
<div> Keywords: spontaneous thought, self-attention architectures, token priority graphs, human cognition, AI behavior 

Summary: 
- The study examines spontaneous topic changes in self-attention models compared to human cognition.
- Theoretical results show that self-attention models maintain token priority orders related to input topics.
- Spontaneous topic changes in self-attention models occur when lower-priority tokens outnumber higher-priority tokens of the input topic.
- Longer context length or ambiguous input topics in self-attention models reduce the likelihood of spontaneous topic changes, unlike human cognition.
- Empirical validation in modern language models confirms the disparities between human thought and AI behavior in spontaneous topic changes. 

Summary: <div>
arXiv:2501.06382v3 Announce Type: replace 
Abstract: Human cognition is punctuated by abrupt, spontaneous shifts between topics-driven by emotional, contextual, or associative cues-a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention based models depend on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize spontaneous topic changes in self-attention architectures, revealing both their similarities and their divergences from spontaneous human thought. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining the topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic reduces the likelihood of spontaneous change. Second, we empirically validate that these dynamics persist in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behaviour in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus as closely aligned to human thought.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableMaster: A Recipe to Advance Table Understanding with Language Models</title>
<link>https://arxiv.org/abs/2501.19378</link>
<guid>https://arxiv.org/abs/2501.19378</guid>
<content:encoded><![CDATA[
<div> table understanding, language models, challenges, semantics, reasoning

Summary: 
Tables are a fundamental format for representing structured data, but current language models face challenges in understanding tables due to their complexity. To address this issue, TableMaster is introduced, a framework that aims to enhance language models for better table understanding. TableMaster tackles four key challenges: difficulty in locating target data, deficiency in table semantics, numerical inaccuracies in textual reasoning, and semantic inflexibility in symbolic reasoning. The framework first extracts relevant table content and provides enriched semantic context. It also introduces adaptive reasoning, dynamically adjusting between textual and symbolic reasoning based on each query. Through extensive analyses and experiments, TableMaster shows impressive results, achieving 78.13% accuracy on the WikiTQ dataset using GPT-4o-mini, surpassing existing baselines. <div>
arXiv:2501.19378v3 Announce Type: replace 
Abstract: Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing</title>
<link>https://arxiv.org/abs/2502.01976</link>
<guid>https://arxiv.org/abs/2502.01976</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, inference efficiency, token-level routing, collaborative inference, policy optimization

Summary: 
The paper introduces the Collaborative Inference with Token-level Routing (CITER) framework, aiming to enhance the efficiency of large language models (LLMs) in resource-constrained applications. CITER facilitates collaboration between small language models (SLMs) and LLMs by routing non-critical tokens to SLMs for efficiency and critical tokens to LLMs for quality generalization. The framework trains a router through policy optimization, considering prediction quality and generation costs for rewarding routing decisions. With a token-level approach, the router learns to predict routing scores based on the current token and future impacts. Introducing a shortcut expedites reward evaluation, reducing the computational burden. Experimental results on benchmark datasets demonstrate CITER's ability to decrease inference costs while maintaining high-quality generation. The framework shows promise for real-time applications with limited resources. The data and code for this study are accessible on GitHub at https://github.com/aiming-lab/CITER.<br /><br />Summary: <div>
arXiv:2502.01976v5 Announce Type: replace 
Abstract: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel Collaborative Inference with Token-lEvel Routing (CITER) framework that enables efficient collaboration between small and large language models (SLMs \& LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. Our data and code are available at https://github.com/aiming-lab/CITER.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models</title>
<link>https://arxiv.org/abs/2504.13068</link>
<guid>https://arxiv.org/abs/2504.13068</guid>
<content:encoded><![CDATA[
<div> DL models, expert agreement, crash narratives, BERT variants, large language models <br />
Summary: <br />
This study examines how accuracy of DL models in classifying crash narratives relates to expert agreement. Five DL models and four large language models were evaluated. Results showed that models with higher technical accuracy had lower agreement with experts, while large language models exhibited stronger expert alignment despite lower accuracy. Cohen's Kappa and PCA were used to quantify model-expert agreement, with SHAP analysis explaining misclassifications. Expert-aligned models relied more on contextual and temporal cues than specific keywords. The study suggests the importance of incorporating expert agreement in model evaluation for safety-critical NLP tasks and highlights the interpretability of large language models in crash analysis pipelines. <br /> <div>
arXiv:2504.13068v2 Announce Type: replace 
Abstract: This study investigates the relationship between deep learning (DL) model accuracy and expert agreement in classifying crash narratives. We evaluate five DL models -- including BERT variants, USE, and a zero-shot classifier -- against expert labels and narratives, and extend the analysis to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal an inverse relationship: models with higher technical accuracy often show lower agreement with human experts, while LLMs demonstrate stronger expert alignment despite lower accuracy. We use Cohen's Kappa and Principal Component Analysis (PCA) to quantify and visualize model-expert agreement, and employ SHAP analysis to explain misclassifications. Results show that expert-aligned models rely more on contextual and temporal cues than location-specific keywords. These findings suggest that accuracy alone is insufficient for safety-critical NLP tasks. We argue for incorporating expert agreement into model evaluation frameworks and highlight the potential of LLMs as interpretable tools in crash analysis pipelines.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning</title>
<link>https://arxiv.org/abs/2311.09830</link>
<guid>https://arxiv.org/abs/2311.09830</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, NLP tasks, PDDL planning, NL prompts, performance evaluation

Summary:
Large language models (LLMs) have transformed numerous NLP tasks, prompting debates on their reasoning and planning capabilities. This study focuses on assessing LLM planning performance in the context of PDDL planning, automating the conversion of PDDL input into natural language prompts for LLMs. The automated NL prompts exhibit comparable planning performance to manually generated prompts, enabling broader experimentation. Results show that LLM planning outperforms PDDL prompts and simple NL prompts. However, compared to symbolic planners, LLM planning falls short, though in certain domains, the best LLM configuration surpasses A$^\star$ using LM-cut. This extensive evaluation highlights the potential of LLMs in planning tasks but also underscores the need for further development to match the efficiency of symbolic planners.<br /><br />Summary: <div>
arXiv:2311.09830v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have revolutionized a large variety of NLP tasks. An active debate is to what extent they can do reasoning and planning. Prior work has assessed the latter in the specific context of PDDL planning, based on manually converting three PDDL domains into natural language (NL) prompts. Here we automate this conversion step, showing how to leverage an LLM to automatically generate NL prompts from PDDL input. Our automatically generated NL prompts result in similar LLM-planning performance as the previous manually generated ones. Beyond this, the automation enables us to run much larger experiments, providing for the first time a broad evaluation of LLM planning performance in PDDL. Our NL prompts yield better performance than PDDL prompts and simple template-based NL prompts. Compared to symbolic planners, LLM planning lags far behind; but in some domains, our best LLM configuration scales up further than A$^\star$ using LM-cut.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning</title>
<link>https://arxiv.org/abs/2402.18789</link>
<guid>https://arxiv.org/abs/2402.18789</guid>
<content:encoded><![CDATA[
<div> FlexLLM, large language models, GPU, inference, finetuning<br />
Summary:<br />
FlexLLM is a system that allows for co-serving large language model (LLM) inference and finetuning on shared GPUs by fusing computation at the token level. It introduces static compilation optimizations that significantly reduce activation memory, leading to up to 80% savings in GPU memory. A novel token-level finetuning mechanism and hybrid token scheduler dynamically interleaves inference and training tokens within each iteration, meeting latency requirements while maximizing utilization. In benchmarks on various LLMs, FlexLLM maintains inference SLO requirements and improves finetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x under light loads, preserving over 76% of peak finetuning progress even at peak demand. The source code of FlexLLM is publicly available at https://github.com/flexflow/FlexFlow/. <br /> <div>
arXiv:2402.18789v2 Announce Type: replace-cross 
Abstract: Finetuning large language models (LLMs) is essential for task adaptation, yet serving stacks today isolate inference and finetuning on separate GPU clusters -- wasting resources and under-utilizing hardware. We introduce FlexLLM, the first system to co-serve LLM inference and PEFT-based finetuning on shared GPUs by fusing computation at the token level. The static compilation optimizations in FlexLLM -- dependent parallelization and graph pruning significantly shrink activation memory, leading to end-to-end GPU memory savings by up to 80%. At runtime, a novel token-level finetuning mechanism paired with a hybrid token scheduler dynamically interleaves inference and training tokens within each co-serving iteration, meeting strict latency SLOs while maximizing utilization. In end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B, FlexLLM sustains the inference SLO requirements up to 20 req/s, and improves finetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x under light loads, preserving over 76% of peak finetuning progress even at peak demand. The source code of FlexLLM is publicly available at https://github.com/flexflow/FlexFlow/.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables</title>
<link>https://arxiv.org/abs/2403.04577</link>
<guid>https://arxiv.org/abs/2403.04577</guid>
<content:encoded><![CDATA[
<div> Keywords: table interpretation, dataset enrichment, Wiki-TabNER, named entity recognition, large language models

Summary:
The article discusses the need to enhance existing table interpretation datasets by introducing a more challenging dataset named Wiki-TabNER. This dataset contains complex tables with multiple entities per cell, annotated with DBpedia classes. It aims to improve named entity recognition (NER) within tables and can also be utilized for entity linking tasks. The authors describe the dataset's unique features and labeling process, proposing a prompting framework for evaluating large language models on the NER task. Qualitative analysis is conducted to understand model challenges and dataset limitations. The Wiki-TabNER dataset offers a more realistic representation of tables found in real-world scenarios, addressing the shortcomings of current datasets and providing a valuable resource for researchers in the field of natural language processing. 

<br /><br />Summary: <div>
arXiv:2403.04577v2 Announce Type: replace-cross 
Abstract: Interest in solving table interpretation tasks has grown over the years, yet it still relies on existing datasets that may be overly simplified. This is potentially reducing the effectiveness of the dataset for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To enrich the existing benchmark datasets, we extract and annotate a new, more challenging dataset. The proposed Wiki-TabNER dataset features complex tables containing several entities per cell, with named entities labeled using DBpedia classes. This dataset is specifically designed to address named entity recognition (NER) task within tables, but it can also be used as a more challenging dataset for evaluating the entity linking task. In this paper we describe the distinguishing features of the Wiki-TabNER dataset and the labeling process. In addition, we propose a prompting framework for evaluating the new large language models on the within tables NER task. Finally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed~dataset.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDeGPT: Modular Decomposition for Large Language Model Compression</title>
<link>https://arxiv.org/abs/2408.09632</link>
<guid>https://arxiv.org/abs/2408.09632</guid>
<content:encoded><![CDATA[
<div> compression, language models, structured compression, modular decomposition, matrix decomposition

Summary:
- The paper introduces MoDeGPT, a structured compression framework for large language models (LLMs) that does not require recovery fine-tuning.
- MoDeGPT partitions the Transformer block into modules and reduces hidden dimensions to save 98% of compute costs on compressing a 13B model.
- Based on three matrix decomposition algorithms, MoDeGPT matches or surpasses previous compression methods without the need for backward propagation.
- MoDeGPT maintains 90-95% zero-shot performance with compression rates of 25-30% on specific models like LLama-2/3 and OPT.
- The compression process can be completed on a single GPU within a few hours and increases inference throughput by up to 46%. <br /><br />Summary: <div>
arXiv:2408.09632v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have reshaped the landscape of artificial intelligence by demonstrating exceptional performance across various tasks. However, substantial computational requirements make their deployment challenging on devices with limited resources. Recently, compression methods using low-rank matrix techniques have shown promise, yet these often lead to degraded accuracy or introduce significant overhead in parameters and inference latency. This paper introduces \textbf{Mo}dular \textbf{De}composition (MoDeGPT), a novel structured compression framework that does not need recovery fine-tuning while resolving the above drawbacks. MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs. MoDeGPT is developed based on a theoretical framework that utilizes three well-established matrix decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD -- and applies them to our redefined transformer modules. Our comprehensive experiments show MoDeGPT, without backward propagation, matches or surpasses previous structured compression methods that rely on gradient information, and saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3 and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30% compression rates. Moreover, the compression can be done on a single GPU within a few hours and increases the inference throughput by up to 46%.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competition Dynamics Shape Algorithmic Phases of In-Context Learning</title>
<link>https://arxiv.org/abs/2412.01003</link>
<guid>https://arxiv.org/abs/2412.01003</guid>
<content:encoded><![CDATA[
<div> In-Context Learning, Large Language Models, Synthetic Sequence Modeling, Markov Chains, Algorithms<br />
<br />
Summary:<br />
In this study, the authors introduce a synthetic sequence modeling task involving learning to simulate a finite mixture of Markov chains. They show that models trained on this task replicate known results on In-Context Learning (ICL), offering a unified setting for studying the concept. By decomposing model behavior into four algorithms that combine fuzzy retrieval and inference approaches with unigram or bigram statistics, they reveal a competition dynamics where different algorithms compete to dominate model behavior. The experimental conditions dictate which algorithm prevails, showing the transient nature of ICL. The study suggests that ICL is a mixture of different algorithms with unique characteristics, rather than a singular capability. This implies that universal claims about ICL may not be feasible, as the behavior is influenced by specific experimental conditions. <div>
arXiv:2412.01003v4 Announce Type: replace-cross 
Abstract: In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model's behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competition dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment</title>
<link>https://arxiv.org/abs/2412.07446</link>
<guid>https://arxiv.org/abs/2412.07446</guid>
<content:encoded><![CDATA[
<div> Keywords: generative pre-trained transformer, attention mechanism, causal interpretation, world model, causal structure learning

Summary:
Generative pre-trained transformer (GPT) models, trained to predict the next token, may implicitly learn a world model through their attention mechanism. The authors propose a causal interpretation of the attention mechanism in GPT, which leads to the emergence of a causal world model. They suggest that GPT models can be used for zero-shot causal structure learning for input sequences, providing a confidence score. In experiments with Othello and Chess games, a pre-trained GPT model shows the ability to generate legal next moves for out-of-distribution sequences with high confidence when a causal structure is present. If the model generates illegal moves, it indicates a failure to capture any causal structure. This study demonstrates the potential of GPT models for understanding and utilizing causal relationships in sequential data. 

<br /><br />Summary: <div>
arXiv:2412.07446v3 Announce Type: replace-cross 
Abstract: Do generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learn a world model from which a sequence is generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences and present a confidence score. Empirical evaluation is conducted in a controlled environment using the setup and rules of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, is tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases for which the GPT model generates illegal moves it also fails to capture any causal structure.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rate-Distortion Framework for Summarization</title>
<link>https://arxiv.org/abs/2501.13100</link>
<guid>https://arxiv.org/abs/2501.13100</guid>
<content:encoded><![CDATA[
arXiv:2501.13100v2 Announce Type: replace-cross 
Abstract: This paper introduces an information-theoretic framework for text summarization. We define the summarizer rate-distortion function and show that it provides a fundamental lower bound on summarizer performance. We describe an iterative procedure, similar to Blahut-Arimoto algorithm, for computing this function. To handle real-world text datasets, we also propose a practical method that can calculate the summarizer rate-distortion function with limited data. Finally, we empirically confirm our theoretical results by comparing the summarizer rate-distortion function with the performances of different summarizers used in practice.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Steering in Neural Theorem Provers</title>
<link>https://arxiv.org/abs/2502.15507</link>
<guid>https://arxiv.org/abs/2502.15507</guid>
<content:encoded><![CDATA[
arXiv:2502.15507v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.00001</link>
<guid>https://arxiv.org/abs/2505.00001</guid>
<content:encoded><![CDATA[
<div> benchmark, logical reasoning, generalization, large language models, low-resource settings 

Summary:
The research introduces Rosetta-PL, a benchmark to assess the logical reasoning and generalization abilities of Large Language Models (LLMs) in controlled environments. Rosetta-PL is constructed by translating logical propositions from Lean into a custom logical language for fine-tuning LLMs like GPT-4o. The impact of dataset size and translation methodology on model performance is analyzed. Results show that maintaining logical relationships during translation significantly improves precision, with accuracy leveling off after around 20,000 training samples. These findings offer valuable insights for enhancing LLM training in formal reasoning tasks and enhancing performance in low-resource language applications. 

Summary: <div>
arXiv:2505.00001v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbol grounding in computational systems: A paradox of intentions</title>
<link>https://arxiv.org/abs/2505.00002</link>
<guid>https://arxiv.org/abs/2505.00002</guid>
<content:encoded><![CDATA[
<div> Keywords: computational systems, symbol grounding, computationalism, semantic nativism, intentional cognitive processes

Summary: 
The paper discusses a paradoxical aspect of computational systems that challenges the concept of symbol grounding within the framework of computationalism. It argues that if the mind operates as a digital computer, it must compute over either meaningful or meaningless symbols. Computing over meaningful symbols implies semantic nativism, suggesting the existence of inherent meaning within the system. On the other hand, computing over meaningless symbols implies a lack of intentional cognitive processes prior to symbol grounding, making the grounding process impossible. Thus, computationalism inherently implies semantic nativism, regardless of whether the mind operates on meaningful or meaningless symbols. This raises important questions about the nature of cognition and the role of symbolic representation in computational frameworks.<br /><br />Summary: <div>
arXiv:2505.00002v1 Announce Type: new 
Abstract: The paper presents a paradoxical feature of computational systems that suggests that computationalism cannot explain symbol grounding. If the mind is a digital computer, as computationalism claims, then it can be computing either over meaningful symbols or over meaningless symbols. If it is computing over meaningful symbols its functioning presupposes the existence of meaningful symbols in the system, i.e. it implies semantic nativism. If the mind is computing over meaningless symbols, no intentional cognitive processes are available prior to symbol grounding. In this case, no symbol grounding could take place since any grounding presupposes intentional cognitive processes. So, whether computing in the mind is over meaningless or over meaningful symbols, computationalism implies semantic nativism.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs</title>
<link>https://arxiv.org/abs/2505.00003</link>
<guid>https://arxiv.org/abs/2505.00003</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP, Large Language Models, psychological theories, cognition, integration 

Summary: 
This paper discusses the importance of incorporating psychological theories into the development of Large Language Models (LLMs) in Natural Language Processing (NLP). It emphasizes the role of psychology in understanding human-like cognition, behavior, and interaction, and how it can enhance various stages of LLM development. By integrating insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics, the paper highlights current trends and gaps in the application of psychological theories in NLP research. The analysis aims to bridge disciplinary divides and promote a more thoughtful integration of psychology into future NLP advancements. <div>
arXiv:2505.00003v1 Announce Type: new 
Abstract: Psychological insights have long shaped pivotal NLP breakthroughs, including the cognitive underpinnings of attention mechanisms, formative reinforcement learning, and Theory of Mind-inspired social modeling. As Large Language Models (LLMs) continue to grow in scale and complexity, there is a rising consensus that psychology is essential for capturing human-like cognition, behavior, and interaction. This paper reviews how psychological theories can inform and enhance stages of LLM development, including data, pre-training, post-training, and evaluation\&amp;application. Our survey integrates insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our analysis highlights current trends and gaps in how psychological theories are applied. By examining both cross-domain connections and points of tension, we aim to bridge disciplinary divides and promote more thoughtful integration of psychology into future NLP research.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangVAE and LangSpace: Building and Probing for Language Model VAEs</title>
<link>https://arxiv.org/abs/2505.00004</link>
<guid>https://arxiv.org/abs/2505.00004</guid>
<content:encoded><![CDATA[
<div> framework, modular, variational autoencoders, language models, representations  
Summary:  
The article introduces LangVAE, a framework that allows for the modular construction of variational autoencoders using pre-trained large language models. This enables the encoding of knowledge from the pre-trained components into more compact and semantically disentangled representations. The LangVAE framework is complemented by LangSpace, which offers various probing methods for analyzing the textual representations, such as vector traversal, interpolation, disentanglement measures, and cluster visualizations. LangVAE and LangSpace provide a flexible, efficient, and scalable approach for building and analyzing textual representations with seamless integration for models available on the HuggingFace Hub. The experiments conducted with different encoder and decoder combinations, as well as annotated inputs, reveal diverse interactions across architectural families and sizes concerning generalization and disentanglement. This framework shows promise in systematizing the experimentation and understanding of textual representations.  
<br /><br />Summary: <div>
arXiv:2505.00004v1 Announce Type: new 
Abstract: We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a digital twin of U.S. Congress</title>
<link>https://arxiv.org/abs/2505.00006</link>
<guid>https://arxiv.org/abs/2505.00006</guid>
<content:encoded><![CDATA[
<div> digital twin, language models, U.S. congresspersons, Tweets, roll-call votes

Summary:
The paper presents a virtual model of U.S. congresspersons using language models to create a digital twin. A dataset containing all Tweets from congresspersons is used to generate Tweets that closely resemble those of the actual individuals. These generated Tweets can predict roll-call vote behaviors and the likelihood of crossing party lines, providing insights for stakeholders to allocate resources and impact legislative dynamics. The study highlights the capability of language models in mimicking real-world behaviors and its implications for political analysis. Limitations and possible extensions of the research are also discussed. <div>
arXiv:2505.00006v1 Announce Type: new 
Abstract: In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination</title>
<link>https://arxiv.org/abs/2505.00008</link>
<guid>https://arxiv.org/abs/2505.00008</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, medically inaccurate information, error detection, misinformation correction, hallucination detection<br />
<br />
Summary: This review explores the use of Natural Language Processing (NLP) to detect and correct medically inaccurate information, including errors, misinformation, and hallucination. The review categorizes studies based on tasks such as error detection, error correction, misinformation detection, misinformation correction, hallucination detection, and hallucination mitigation. NLP has shown promise in addressing these tasks, but challenges remain with data privacy, context dependency, and evaluation standards. The review emphasizes the importance of developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications. By advancing patient safety, improving public health communication, and supporting the development of more reliable NLP applications, this review aims to contribute to the overall improvement of healthcare practices. <br /><br /> <div>
arXiv:2505.00008v1 Announce Type: new 
Abstract: Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation</title>
<link>https://arxiv.org/abs/2505.00009</link>
<guid>https://arxiv.org/abs/2505.00009</guid>
<content:encoded><![CDATA[
<div> pre-trained language models, multi-task learning, prompt tuning, low-rank representation, parameter efficiency <br />
Summary: 
The article introduces Task-Adaptive Low-Rank Representation (TA-LoRA) as a method for multi-task learning, building on prompt tuning to capture task-specific knowledge effectively. TA-LoRA utilizes low-rank representation to model task heterogeneity and a fast-slow weights mechanism to differentiate shared and task-specific knowledge. It introduces a zero-initialized attention mechanism to minimize disruption during warm-up epochs. Experimental results on 16 tasks show that TA-LoRA achieves state-of-the-art performance in both full-data and few-shot settings while maintaining superior parameter efficiency. <div>
arXiv:2505.00009v1 Announce Type: new 
Abstract: Pre-trained language models (PLMs) demonstrate remarkable intelligence but struggle with emerging tasks unseen during training in real-world applications. Training separate models for each new task is usually impractical. Multi-task learning (MTL) addresses this challenge by transferring shared knowledge from source tasks to target tasks. As an dominant parameter-efficient fine-tuning method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that captures task-specific knowledge, which acts as a prefix to the original prompt that preserves shared knowledge, while keeping PLM parameters frozen. However, PT struggles to effectively capture the heterogeneity of task-specific knowledge due to its limited representational capacity. To address this challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL method built on PT, employing the low-rank representation to model task heterogeneity and a fast-slow weights mechanism where the slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing of shared and task-specific knowledge, caused by training low-rank representations from scratch. Moreover, a zero-initialized attention mechanism is introduced to minimize the disruption of immature low-rank components on original prompts during warm-up epochs. Experiments on 16 tasks demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and few-shot settings while maintaining superior parameter efficiency.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models</title>
<link>https://arxiv.org/abs/2505.00010</link>
<guid>https://arxiv.org/abs/2505.00010</guid>
<content:encoded><![CDATA[
<div> keywords: Jailbreaking, Large Language Models, Detection, Predictive Models, Education

Summary:
The study focuses on detecting jailbreaks in Large Language Models (LLMs) used in clinical education platforms like 2-Sigma. Over 2,300 prompts in 158 conversations were annotated to identify linguistic variables correlated with jailbreak behavior. Various predictive models were trained using these features, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Feature-based models outperformed Prompt Engineering, with the Fuzzy Decision Tree showing the best performance. The study suggests that linguistic-feature-based models are effective and explainable for jailbreak detection. Future research could explore hybrid frameworks combining prompt-based flexibility and rule-based robustness for real-time jailbreak monitoring in educational LLMs.<br /><br />Summary: <div>
arXiv:2505.00010v1 Announce Type: new 
Abstract: Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?</title>
<link>https://arxiv.org/abs/2505.00012</link>
<guid>https://arxiv.org/abs/2505.00012</guid>
<content:encoded><![CDATA[
<div> Keywords: qualitative research, AI Co-Ethnographer, automation, code assignments, pattern discovery

Summary: <br /><br />The AI Co-Ethnographer (AICoE) is a new end-to-end pipeline developed to enhance qualitative research processes by going beyond automating code assignments. AICoE streamlines the entire qualitative research process, including open coding, code consolidation, code application, and pattern discovery. By offering a more integrated approach, AICoE enables researchers to efficiently analyze qualitative data while maintaining analytical depth. This innovative tool is designed to address the scalability challenges faced in qualitative research and provides a comprehensive solution for researchers looking to improve their analysis processes. With AICoE, qualitative researchers can enhance their workflow and achieve a deeper understanding of their data through streamlined and efficient data analysis techniques. <div>
arXiv:2505.00012v1 Announce Type: new 
Abstract: Qualitative research often involves labor-intensive processes that are difficult to scale while preserving analytical depth. This paper introduces The AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for qualitative research and designed to move beyond the limitations of simply automating code assignments, offering a more integrated approach. AICoE organizes the entire process, encompassing open coding, code consolidation, code application, and even pattern discovery, leading to a comprehensive analysis of qualitative data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa</title>
<link>https://arxiv.org/abs/2505.00013</link>
<guid>https://arxiv.org/abs/2505.00013</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion detection, Japanese text, language models, DeBERTa-v3-large, model performance

Summary:<br /><br />This study focuses on accurate emotion detection in Japanese text, addressing resource scarcity and class imbalance challenges. The objective is to predict eight Plutchik emotions in Japanese sentences using language models. The WRIME corpus is utilized to transform intensity scores into binary labels for model training. Four pre-trained language models are fine-tuned, with DeBERTa-v3-large achieving the highest mean accuracy and F1-score compared to others. The model shows consistent performance across both high-frequency and low-frequency emotions. Large language models (LLMs) such as ChatGPT-4o and TinySwallow-1.5B-Instruct lag behind in performance. The DeBERTa-v3-large model is released as a pip-installable package for binary emotion classification in Japanese. Future research directions include expanding data for rare emotions, optimizing model size, and exploring prompt engineering to enhance LLM performance. <div>
arXiv:2505.00013v1 Announce Type: new 
Abstract: Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.
  Objective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences.
  Methods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics.
  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.
  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance.
  This manuscript is under review for possible publication in New Generation Computing.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\"obius Strips</title>
<link>https://arxiv.org/abs/2505.00014</link>
<guid>https://arxiv.org/abs/2505.00014</guid>
<content:encoded><![CDATA[
<div> embedding, geometry, manifold, triplet loss, NLP

Summary:
- The article introduces a new framework that constrains sentence embeddings to lie on continuous manifolds such as the unit sphere, torus, and M\"obius strip using triplet loss.
- By enforcing differential geometric constraints on the output space, the approach aims to encourage learning of embeddings that are both discriminative and topologically structured.
- The method is evaluated on benchmark datasets AG News and MBTI, outperforming traditional approaches like TF-IDF, Word2Vec, and unconstrained Keras-derived embeddings in clustering quality and classification performance.
- Manifold-constrained embeddings, particularly those on spheres and M\"obius strips, show significant improvement in both Silhouette Score and Accuracy.
- The findings suggest the importance of embedding in manifold space, where topological structure enhances semantic separation, providing a new direction for geometric representation learning in Natural Language Processing.

<br /><br />Summary: <div>
arXiv:2505.00014v1 Announce Type: new 
Abstract: Recent advances in representation learning have emphasized the role of embedding geometry in capturing semantic structure. Traditional sentence embeddings typically reside in unconstrained Euclidean spaces, which may limit their ability to reflect complex relationships in language. In this work, we introduce a novel framework that constrains sentence embeddings to lie on continuous manifolds -- specifically the unit sphere, torus, and M\"obius strip -- using triplet loss as the core training objective. By enforcing differential geometric constraints on the output space, our approach encourages the learning of embeddings that are both discriminative and topologically structured.
  We evaluate our method on benchmark datasets (AG News and MBTI) and compare it to classical baselines including TF-IDF, Word2Vec, and unconstrained Keras-derived embeddings. Our results demonstrate that manifold-constrained embeddings, particularly those projected onto spheres and M\"obius strips, significantly outperform traditional approaches in both clustering quality (Silhouette Score) and classification performance (Accuracy). These findings highlight the value of embedding in manifold space -- where topological structure complements semantic separation -- offering a new and mathematically grounded direction for geometric representation learning in NLP.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation</title>
<link>https://arxiv.org/abs/2505.00015</link>
<guid>https://arxiv.org/abs/2505.00015</guid>
<content:encoded><![CDATA[
<div> Keywords: road traffic accidents, Bangladesh, automated system, Large Language Models, web scraping

Summary:
Road traffic accidents in Bangladesh are a significant issue due to manual and unreliable data collection methods. This research proposes an automated system utilizing Large Language Models (LLMs) and web scraping to improve data accuracy. The system includes automated code generation for web scraping, news collection, accident classification, and duplicate removal. The LLM Gemini-2.0-Flash is used for seamless automation. Over a period of 111 days, the system processed 15,000 news articles and identified 705 unique accidents. Chittagong reported the highest number of accidents, fatalities, and injuries. Peak accident times were in the morning, noon, and evening. The study showcases the effectiveness of LLM-powered systems in collecting accurate accident data for informed road safety policymaking in Bangladesh.<br /><br />Summary: <div>
arXiv:2505.00015v1 Announce Type: new 
Abstract: Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00016</link>
<guid>https://arxiv.org/abs/2505.00016</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, large language models, table reasoning, reinforcement learning, interpretability <br />
Summary: 
This work presents a new approach to the Text-to-SQL task, aiming to teach large language models (LLMs) to reason over tabular data. The framework consists of two stages: first, creating detailed chain-of-thought (CoT) traces from SQL queries to teach the model how to manipulate table fields, and second, using a Group Relative Policy Optimization (GRPO) objective in reinforcement learning to encourage generalizable reasoning. The approach improves performance on Text-to-SQL benchmarks and achieves significant gains on reasoning-intensive datasets like BIRD and CRT-QA. The results show that utilizing SQL as a scaffold for learning enhances generalization and interpretability in reasoning over structured data. Notably, the distilled-quantized LLaMA model showed a 20% accuracy increase on Text-to-SQL tasks, while Qwen achieved a 5% increase. Overall, this research highlights the potential of leveraging SQL for teaching robust reasoning capabilities to LLMs in the context of structured data. <br /><br />Summary: <div>
arXiv:2505.00016v1 Announce Type: new 
Abstract: This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation</title>
<link>https://arxiv.org/abs/2505.00017</link>
<guid>https://arxiv.org/abs/2505.00017</guid>
<content:encoded><![CDATA[
<div> Keywords: cell type annotation, large language models, differential genes, multi task workflow, semantic similarity.

Summary:
Our work presents a novel approach using large language models (LLMs) for precise and automated cell type annotation. We have developed a graph structured feature marker database that retrieves entities related to differential genes, enabling accurate cell reconstruction. Through a multi task workflow, we optimized the annotation process, resulting in improved human evaluation scores by up to 0.21 and a 6.1% increase in semantic similarity across 11 tissue types. Our method not only outperforms general purpose LLMs but also aligns closely with the cognitive logic of manual annotation, providing more reliable and efficient cell type identification. <div>
arXiv:2505.00017v1 Announce Type: new 
Abstract: To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study on Prompt Compression for Large Language Models</title>
<link>https://arxiv.org/abs/2505.00019</link>
<guid>https://arxiv.org/abs/2505.00019</guid>
<content:encoded><![CDATA[
<div> methods, prompt compression, Large Language Models, performance, evaluation <br />
Summary: <br />
This paper explores six prompt compression methods for Large Language Models (LLMs) to reduce computational complexity and costs without compromising response quality. The study covers various aspects including generation performance, model hallucinations, effectiveness in multimodal tasks, word omission analysis, and more. Evaluation across 13 datasets, including news, scientific articles, QA, and VQA datasets, shows that prompt compression has a greater impact on LLM performance in long contexts. Surprisingly, moderate compression in the Longbench evaluation even enhances LLM performance. The experiments highlight the importance of efficient prompt engineering for LLMs in different tasks and provide insights into optimizing prompt length for better performance. The code and data used in the study are also made available for reference. <div>
arXiv:2505.00019v1 Announce Type: new 
Abstract: Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Public Access in LLM Pre-Training Data</title>
<link>https://arxiv.org/abs/2505.00020</link>
<guid>https://arxiv.org/abs/2505.00020</guid>
<content:encoded><![CDATA[
<div> membership inference attack, copyrighted content, language models, O'Reilly Media, training data

Summary: The study used a dataset of copyrighted O'Reilly Media books to investigate if OpenAI's language models were trained on copyrighted content without consent. GPT-4o showed strong recognition of paywalled O'Reilly book content, while GPT-3.5 Turbo showed greater recognition of publicly accessible content. However, GPT-4o Mini, a smaller model, showed no knowledge of O'Reilly content. Testing multiple models helped account for potential language shifts over time. The results underscore the necessity for increased corporate transparency in disclosing pre-training data sources to develop formal licensing frameworks for AI content training. 

Summary: <br /><br /> <div>
arXiv:2505.00020v1 Announce Type: new 
Abstract: Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\approx$ 50\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss</title>
<link>https://arxiv.org/abs/2505.00021</link>
<guid>https://arxiv.org/abs/2505.00021</guid>
<content:encoded><![CDATA[
<div> Keywords: imbalanced data, food hazard detection, data augmentation, transformer-based models, NLP-based classification<br />
Summary:<br />
Classification tasks in food hazard detection are often hindered by imbalanced data distributions and short, unstructured text. This study addresses these challenges by utilizing data augmentation techniques and transformer-based models such as BERT and RoBERTa. By employing strategies like Easy Data Augmentation (EDA) and focal loss, the classification performance is significantly improved. EDA proves to be effective in mitigating class imbalance, enhancing accuracy and F1 scores. Combining focal loss with oversampling and EDA further bolsters model robustness, particularly for challenging examples. These findings contribute to the advancement of NLP-based classification models for food hazard detection, showcasing the importance of innovative approaches in overcoming data distribution issues in classification tasks. <br /><br />Summary: <div>
arXiv:2505.00021v1 Announce Type: new 
Abstract: Classification tasks often suffer from imbal- anced data distribution, which presents chal- lenges in food hazard detection due to severe class imbalances, short and unstructured text, and overlapping semantic categories. In this paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection, which ad- dresses these issues by applying data augmenta- tion techniques to improve classification perfor- mance. We utilize transformer-based models, BERT and RoBERTa, as backbone classifiers and explore various data balancing strategies, including random oversampling, Easy Data Augmentation (EDA), and focal loss. Our ex- periments show that EDA effectively mitigates class imbalance, leading to significant improve- ments in accuracy and F1 scores. Furthermore, combining focal loss with oversampling and EDA further enhances model robustness, par- ticularly for hard-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation</title>
<link>https://arxiv.org/abs/2505.00022</link>
<guid>https://arxiv.org/abs/2505.00022</guid>
<content:encoded><![CDATA[
<div> dataset curation pipeline, German-language, data quality, LLM pre-training datasets, synthetic data generation  
Summary:
A dataset curation pipeline for German-language datasets was developed to improve data quality for large language models (LLMs). The pipeline combines heuristic and model-based filtering techniques with synthetic data generation to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset sourced from Common Crawl web data, FineWeb2, and synthetically-generated data. Pre-training experiments on an 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT) using Aleph-Alpha-GermanWeb showed significant performance gains over FineWeb2 alone on German-language benchmarks. The study highlights the effectiveness of model-based data curation and synthetic data generation in enhancing LLM pre-training datasets.  
<br /><br />Summary: <div>
arXiv:2505.00022v1 Announce Type: new 
Abstract: Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORG: Generating Answers from Complex, Interrelated Contexts</title>
<link>https://arxiv.org/abs/2505.00023</link>
<guid>https://arxiv.org/abs/2505.00023</guid>
<content:encoded><![CDATA[
<div> Keywords: real-world corpus, language models, Context Organizer, interrelationships, disambiguation

Summary: 
The article introduces Context Organizer (CORG), a framework designed to address the complexities of interrelationships in a real-world corpus. These interrelationships often lead to inconsistencies in knowledge due to various factors like ambiguous naming and outdated information. CORG classifies relationships into distracting, ambiguous, counterfactual, and duplicated categories, aiming to effectively manage these complexities simultaneously. The framework consists of a graph constructor, reranker, and aggregator, allowing for the organization of multiple contexts into independently processed groups. Results show that CORG outperforms existing grouping methods, striking a balance between performance and efficiency, and achieving comparable results to more computationally intensive, single-context approaches.  <br /><br />Summary: <div>
arXiv:2505.00023v1 Announce Type: new 
Abstract: In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning</title>
<link>https://arxiv.org/abs/2505.00024</link>
<guid>https://arxiv.org/abs/2505.00024</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, tool usage, reasoning, reinforcement learning, benchmark evaluation

Summary:
Nemotron-Research-Tool-N1 series of language models utilize rule-based reinforcement learning to enhance tool usage capabilities. Unlike previous methods, these models are optimized with a binary reward system that evaluates the structural validity and functional correctness of tool invocations. This approach allows the models to internalize reasoning strategies autonomously without the need for annotated reasoning trajectories. Experimental results on BFCL and API-Bank benchmarks demonstrate that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B models, built on Qwen-2.5-7B/14B-Instruct, outperform GPT-4o, achieving state-of-the-art results on both evaluations.<br /><br />Summary: <div>
arXiv:2505.00024v1 Announce Type: new 
Abstract: Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning traces from stronger models for SFT. However, both approaches fall short, either omitting reasoning entirely or producing imitative reasoning that limits generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning through rule-based reinforcement learning, we develop the Nemotron-Research-Tool-N1 series of tool-using language models using a similar training paradigm. Instead of restrictively supervising intermediate reasoning traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized with a binary reward that evaluates only the structural validity and functional correctness of tool invocations. This lightweight supervision allows the model to autonomously internalize reasoning strategies, without the need for annotated reasoning trajectories. Experiments on the BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve state-of-the-art results, outperforming GPT-4o on both evaluations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1</title>
<link>https://arxiv.org/abs/2505.00025</link>
<guid>https://arxiv.org/abs/2505.00025</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight medical vertical large language model, knowledge acquisition, model compression, computational optimization, medical question-answering datasets

Summary:
This paper introduces an efficient lightweight medical large language model architecture method designed to overcome the challenges of applying large models in medical scenarios. The proposed method addresses the issues of knowledge acquisition, model compression, and computational optimization. It includes a knowledge transfer pipeline from a teacher model to a student model, compression techniques like weight quantization, and inference optimization strategies. Experimental results demonstrate that the approach maintains accuracy while reducing memory consumption by 64.7% and inference latency by 12.4%. This solution enables the application of large models in resource-constrained environments such as edge computing devices.<br /><br />Summary: <div>
arXiv:2505.00025v1 Announce Type: new 
Abstract: In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\% and inference latency by 12.4\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory of Mind in Large Language Models: Assessment and Enhancement</title>
<link>https://arxiv.org/abs/2505.00026</link>
<guid>https://arxiv.org/abs/2505.00026</guid>
<content:encoded><![CDATA[
<div> Keywords: Theory of Mind, Large Language Models, evaluation benchmarks, strategies, research directions

Summary: 
This paper discusses the Theory of Mind (ToM) capabilities of Large Language Models (LLMs) and the importance of enhancing their ability to interpret and respond to human mental states. The authors review evaluation benchmarks and strategies used to improve ToM in LLMs, focusing on story-based benchmarks. They provide an in-depth analysis of methods aimed at enhancing ToM in LLMs and suggest promising future research directions based on recent benchmarks and state-of-the-art approaches. This survey serves as a valuable resource for researchers looking to advance LLMs' ToM capabilities.<br /><br />Summary: <div>
arXiv:2505.00026v1 Announce Type: new 
Abstract: Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts</title>
<link>https://arxiv.org/abs/2505.00027</link>
<guid>https://arxiv.org/abs/2505.00027</guid>
<content:encoded><![CDATA[
<div> dimension, trees, natural language, query, precision

Summary:
This paper presents a novel approach for automatically discovering subject, action, object, and adverbial dimensions from texts to support efficient text operations and natural language queries. The approach constructs high-quality trees that represent subjects, actions, objects, and adverbials and their subclass relations within texts. These trees are independent, ensuring no redundant representation. The expressiveness of the trees allows for accessing the majority of sentences, supporting natural language queries. Experimental results show that the abstraction trees constructed have high precision, recall, and F1-scores. The approach is applied to support querying in natural language, demonstrating high coverage of different question patterns. By searching multiple trees based on the question pattern, the approach efficiently reduces the search space and enables precise text operations. <div>
arXiv:2505.00027v1 Announce Type: new 
Abstract: This paper proposed an approach to automatically discovering subject dimension, action dimension, object dimension and adverbial dimension from texts to efficiently operate texts and support query in natural language. The high quality of trees guarantees that all subjects, actions, objects and adverbials and their subclass relations within texts can be represented. The independency of trees ensures that there is no redundant representation between trees. The expressiveness of trees ensures that the majority of sentences can be accessed from each tree and the rest of sentences can be accessed from at least one tree so that the tree-based search mechanism can support querying in natural language. Experiments show that the average precision, recall and F1-score of the abstraction trees constructed by the subclass relations of subject, action, object and adverbial are all greater than 80%. The application of the proposed approach to supporting query in natural language demonstrates that different types of question patterns for querying subject or object have high coverage of texts, and searching multiple trees on subject, action, object and adverbial according to the question pattern can quickly reduce search space to locate target sentences, which can support precise operation on texts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.00028</link>
<guid>https://arxiv.org/abs/2505.00028</guid>
<content:encoded><![CDATA[
<div> end-to-end, speech-to-speech, dialogue systems, Retrieval-Augmented Generation, knowledge integration <br />
Summary: 
The article introduces a novel framework for end-to-end speech-to-speech dialogue systems that addresses the challenge of incorporating external knowledge. This framework, based on Retrieval-Augmented Generation (RAG), directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion. Experimental results show significant performance improvements and higher retrieval efficiency. While the overall performance is still below that of cascaded models, this approach shows promise in enhancing knowledge integration in end-to-end systems. The authors plan to release the code and dataset to facilitate reproducibility and encourage further research in this area. <div>
arXiv:2505.00028v1 Announce Type: new 
Abstract: In recent years, end-to-end speech-to-speech (S2S) dialogue systems have garnered increasing research attention due to their advantages over traditional cascaded systems, including achieving lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these end-to-end systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion via techniques like ASR. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. We will release the code and dataset to support reproducibility and promote further research in this area.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting</title>
<link>https://arxiv.org/abs/2505.00029</link>
<guid>https://arxiv.org/abs/2505.00029</guid>
<content:encoded><![CDATA[
<div> Approach, Structured Dialogue Fine-Tuning, Domain-specific knowledge, Catastrophic forgetting, Multi-domain<br />
Summary:
Structured Dialogue Fine-Tuning (SDFT) is introduced as an approach to injecting domain-specific knowledge into Large Vision Language Models (LLMs) while minimizing catastrophic forgetting. The method consists of three phases: Foundation Preservation, Contrastive Disambiguation, and Knowledge Specialization. Foundation Preservation reinforces pre-trained visual-linguistic alignment, while Contrastive Disambiguation helps maintain semantic boundaries through counterfactual examples. Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results show the effectiveness of SDFT in balancing specialized knowledge acquisition with general capability retention. Key contributions include a data-centric dialogue template, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.<br /><br /> <div>
arXiv:2505.00029v1 Announce Type: new 
Abstract: Large Vision Language Models have demonstrated impressive versatile capabilities through extensive multimodal pre-training, but face significant limitations when incorporating specialized knowledge domains beyond their training distribution. These models struggle with a fundamental dilemma: direct adaptation approaches that inject domain-specific knowledge often trigger catastrophic forgetting of foundational visual-linguistic abilities. We introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that effectively injects domain-specific knowledge while minimizing catastrophic forgetting. Drawing inspiration from supervised fine-tuning in LLMs and subject-driven personalization in text-to-image diffusion models, our method employs a three-phase dialogue structure: Foundation Preservation reinforces pre-trained visual-linguistic alignment through caption tasks; Contrastive Disambiguation introduces carefully designed counterfactual examples to maintain semantic boundaries; and Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results across multiple domains confirm SDFT's effectiveness in balancing specialized knowledge acquisition with general capability retention. Our key contributions include a data-centric dialogue template that balances foundational alignment with targeted knowledge integration, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Language Models Represent the Past without Anachronism?</title>
<link>https://arxiv.org/abs/2505.00030</link>
<guid>https://arxiv.org/abs/2505.00030</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, anachronism, period prose, fine-tuning, historical perspectives

Summary: 
Language models are increasingly used to simulate historical perspectives, but the risk of anachronism remains a concern. Prompting contemporary models with period prose does not produce authentic historical style outputs. Fine-tuning the model can generate convincing results, but human evaluators can still differentiate between model outputs and genuine historical text. The study suggests that pretraining models on period prose may be necessary to accurately simulate historical perspectives for social research.<br /><br />Summary: <div>
arXiv:2505.00030v1 Announce Type: new 
Abstract: Before researchers can use language models to simulate the past, they need to understand the risk of anachronism. We find that prompting a contemporary model with examples of period prose does not produce output consistent with period style. Fine-tuning produces results that are stylistically convincing enough to fool an automated judge, but human evaluators can still distinguish fine-tuned model outputs from authentic historical text. We tentatively conclude that pretraining on period prose may be required in order to reliably simulate historical perspectives for social research.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving</title>
<link>https://arxiv.org/abs/2505.00031</link>
<guid>https://arxiv.org/abs/2505.00031</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, self-training algorithm, anticipatory plans, problem-solving, natural language reasoning

Summary:<br /><br />In this paper, the authors introduce a novel self-training algorithm called LEarning to Plan before Answering (LEPA) for large language models (LLMs). LEPA focuses on generating anticipatory plans before delving into complex problem-solving tasks, drawing inspiration from cognitive science. By first formulating abstract meta-knowledge through anticipatory plans, LLMs can better generalize across similar problems and avoid getting lost in irrelevant details. The algorithm refines plans through self-reflection and trains the LLM to predict both plans and corresponding solutions. By effectively utilizing anticipatory plans, LEPA outperforms conventional algorithms on various challenging natural language reasoning benchmarks. This approach highlights the importance of incorporating high-level abstraction in problem-solving to enhance the performance of LLMs. 

<br /><br />Summary: <div>
arXiv:2505.00031v1 Announce Type: new 
Abstract: In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis</title>
<link>https://arxiv.org/abs/2505.00032</link>
<guid>https://arxiv.org/abs/2505.00032</guid>
<content:encoded><![CDATA[
<div> Keywords: Major depressive disorder, MDD-LLM, AI-driven framework, UK Biobank cohort, fine-tuned large language models

Summary: 
The paper introduces a new high-performance tool for diagnosing Major Depressive Disorder (MDD) called MDD-LLM, which uses fine-tuned large language models and real-world samples. A total of 274,348 individual records from the UK Biobank cohort were used to train and evaluate the AI-driven framework. The study shows that MDD-LLM outperforms existing machine learning and deep learning frameworks for MDD diagnosis, achieving an accuracy of 0.8378 and an AUC of 0.8919. Factors influencing the performance of the method, such as tabular data transformation techniques and fine-tuning strategies, were also examined. This research addresses the inadequacies in MDD diagnosis due to limited access to medical resources and complex diagnostic methods, offering a promising solution for improving the detection of this prevalent mental health disorder.

Summary: <div>
arXiv:2505.00032v1 Announce Type: new 
Abstract: Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models</title>
<link>https://arxiv.org/abs/2505.00033</link>
<guid>https://arxiv.org/abs/2505.00033</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral generative modeling, natural language processing, Fourier dictionary, transformer architectures, Gaussian Mixture Model (GMM)

Summary: 
This article introduces a novel spectral generative modeling framework for natural language processing. It proposes a method that learns a global time-varying Fourier dictionary and per-token mixing coefficients to replace the traditional self-attention mechanism in transformer architectures. By incorporating reconstruction losses in both the time and frequency domains, as well as fitting a Gaussian Mixture Model prior over the learned mixing vectors, the approach achieves competitive perplexity and generation quality on standard benchmarks. Unlike self-attention, which has quadratic computation complexity, this method operates with linear complexity, leading to significant efficiency gains. The results show that spectral dictionary models can perform comparably to transformer baselines while reducing inference latency and memory usage, providing a promising alternative for scalable language modeling.
<br /><br />Summary: <div>
arXiv:2505.00033v1 Announce Type: new 
Abstract: We propose a novel spectral generative modeling framework for natural language processing that jointly learns a global time varying Fourier dictionary and per token mixing coefficients, replacing the ubiquitous self attention mechanism in transformer architectures. By enforcing reconstruction losses in both the time domain (embedding reconstruction) and the frequency domain (via Short Time Fourier Transform magnitude matching) alongside a standard language modeling objective, and fitting a Gaussian Mixture Model (GMM) prior over the learned mixing vectors, our approach achieves competitive perplexity and generation quality on standard benchmarks such as WikiText2 and Penn Treebank. In contrast to the quadratic computation complexity of self attention, our method operates with linear complexity, delivering substantial efficiency gains. We demonstrate that spectral dictionary models can achieve competitive performance compared to transformer baselines while significantly reducing inference latency and memory footprint, offering a compelling alternative for scalable language modeling.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Phishing Email Detection Performance of Small Large Language Models</title>
<link>https://arxiv.org/abs/2505.00034</link>
<guid>https://arxiv.org/abs/2505.00034</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, phishing email detection, small-parameter models, prompt engineering, model ensemble

Summary: 
Small-parameter large language models (LLMs) are being explored for phishing email detection to reduce computational costs. However, these models often perform poorly compared to their larger counterparts. To address this issue, researchers have proposed several methods to enhance the capabilities of small LLMs, including prompt engineering, explanation augmented fine-tuning, and model ensemble. Through experiments, these methods have proven to significantly improve the accuracy of phishing email detection on the SpamAssassin dataset, increasing from 0.5 for baseline models to 0.976 for the enhanced models. This research demonstrates the potential of small-parameter LLMs in phishing email detection tasks and highlights the importance of incorporating innovative techniques to enhance their performance. <br /><br />Summary: <div>
arXiv:2505.00034v1 Announce Type: new 
Abstract: Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics</title>
<link>https://arxiv.org/abs/2505.00035</link>
<guid>https://arxiv.org/abs/2505.00035</guid>
<content:encoded><![CDATA[
<div> increase, vocabulary diversity, rhyme density, thematic content, sentiment analysis
Summary:
Vocabulary diversity in hip-hop lyrics increased by 23.7% over four decades, with East Coast artists showing higher lexical variation. Rhyme density also rose by 34.2%, with Midwest artists displaying the highest technical complexity. There was a shift in thematic content, as social justice themes decreased while introspective themes increased. Lyrics became more negative during sociopolitical crises, with a decrease in polarity following major social unrest. The analysis identified four distinct stylistic approaches correlating with geographic origin and time period, highlighting the evolution of hip-hop as an art form reflecting societal dynamics. This study provides quantitative evidence of the interplay between linguistic innovation and cultural context in popular music.<br /><br />Summary: <div>
arXiv:2505.00035v1 Announce Type: new 
Abstract: This paper presents a comprehensive computational framework for analyzing linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a dataset of 3,814 songs from 146 influential artists spanning four decades (1980-2020), we employ natural language processing techniques to quantify multiple dimensions of lyrical complexity. Our analysis reveals a 23.7% increase in vocabulary diversity over the study period, with East Coast artists demonstrating 17.3% higher lexical variation than other regions. Rhyme density increased by 34.2% across all regions, with Midwest artists exhibiting the highest technical complexity (3.04 rhymes per line). Topic modeling identified significant shifts in thematic content, with social justice themes decreasing from 28.5% to 13.8% of content while introspective themes increased from 7.6% to 26.3%. Sentiment analysis demon- strated that lyrics became significantly more negative during sociopolitical crises, with polarity decreasing by 0.31 following major social unrest. Multi-dimensional analysis revealed four dis- tinct stylistic approaches that correlate strongly with geographic origin (r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish quantitative evidence for the evolution of hip- hop as both an art form and a reflection of societal dynamics, providing insights into the interplay between linguistic innovation and cultural context in popular music.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies</title>
<link>https://arxiv.org/abs/2505.00036</link>
<guid>https://arxiv.org/abs/2505.00036</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, political persuasion, survey experiments, persuasive capabilities, campaign methods 

Summary: 
In recent years, concerns have arisen about the potential threat posed by Large Language Models (LLMs) to democratic societies due to their persuasive abilities. The study conducted two survey experiments and a real-world simulation to compare the cost-effectiveness of using LLM chatbots for political persuasion compared to traditional campaign methods. The experiments, involving over 10,000 participants across three political domains, revealed that LLMs are as persuasive as actual campaign ads once voters are exposed to them. However, political persuasion also depends on the impact of the message post-exposure. Based on simulations, the study estimated that LLM-based persuasion costs less per persuaded voter compared to traditional methods, but the scalability of traditional methods currently surpasses that of LLMs. While LLMs do not currently have a significant advantage in large-scale political persuasion, their potential may increase as their capabilities improve and exposure to persuasive LLMs becomes more scalable.

<br /><br />Summary: <div>
arXiv:2505.00036v1 Announce Type: new 
Abstract: In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the "receive" and "accept" steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPerAlign: Hypotheses-driven Personalized Alignment</title>
<link>https://arxiv.org/abs/2505.00038</link>
<guid>https://arxiv.org/abs/2505.00038</guid>
<content:encoded><![CDATA[
<div> personalization, language models, user-specific, alignment algorithms, hypotheses-driven

Summary:<br />
The paper introduces a novel approach called HyPerAlign for personalizing outputs of large language models to individual users by inferring hypotheses about their communication strategies, personality, and writing style. This approach aims to generate customized responses tailored to individual users rather than generic outputs aligned to average user preferences. Experimental results on authorship attribution and deliberative alignment tasks using diverse datasets show that the HyPerAlign approach outperforms preference-based fine-tuning methods. It achieves up to a 70% improvement in the helpfulness of LLM models for deliberative alignment and consistently high win rates (>90%) against state-of-the-art preference fine-tuning approaches for LLM personalization in authorship attribution. Overall, HyPerAlign is described as an interpretable and sample-efficient strategy for personalizing LLM models to individual users. 

Summary: <div>
arXiv:2505.00038v1 Announce Type: new 
Abstract: Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG for Legal Norms: A Hierarchical and Temporal Approach</title>
<link>https://arxiv.org/abs/2505.00039</link>
<guid>https://arxiv.org/abs/2505.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Retrieval Augmented Generation, legal norms, knowledge graphs, hierarchical structure, temporal evolution

Summary:<br /><br />
This article introduces an adaptation of Graph Retrieval Augmented Generation (Graph RAG) tailored for analyzing legal norms. Legal norms are complex with hierarchical structures, extensive references, and multiple versions. Graph RAG combines structured knowledge graphs with contextual text segments to tackle the complexities of legal data. By integrating hierarchical structure, temporal evolution, and comprehensive Text Units into knowledge graphs, Graph RAG enhances the representation of legal knowledge. The article delves into the application of Graph RAG on legal norm datasets, aiming to advance the field of Artificial Intelligence in the legal domain. This innovation opens avenues for more effective systems in legal research, legislative analysis, and decision support.<br /><br />Summary: <div>
arXiv:2505.00039v1 Announce Type: new 
Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms, which are characterized by their predefined hierarchical structure, extensive network of internal and external references and multiple temporal versions. By combining structured knowledge graphs with contextually enriched text segments, Graph RAG offers a promising solution to address the inherent complexity and vast volume of legal data. The integration of hierarchical structure and temporal evolution into knowledge graphs - along with the concept of comprehensive Text Units - facilitates the construction of richer, interconnected representations of legal knowledge. Through a detailed analysis of Graph RAG and its application to legal norm datasets, this article aims to significantly advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective systems in legal research, legislative analysis, and decision support.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Base Models Beat Aligned Models at Randomness and Creativity</title>
<link>https://arxiv.org/abs/2505.00047</link>
<guid>https://arxiv.org/abs/2505.00047</guid>
<content:encoded><![CDATA[
<div> alignment, LLM development, reinforcement learning, unpredictable outputs, base language models

Summary:<br /><br />Alignment has become a standard practice in language model development, but it may not always be beneficial. The study reveals that base language models outperform aligned models in tasks requiring unpredictable outputs like random number generation, mixed strategy games, and creative writing. Aligned models tend to exhibit narrow behaviors, such as favoring specific numbers in random number generation or predictability in game states. While aligned models excel in common benchmarks, they struggle in tasks demanding originality and creativity. The findings suggest a trade-off between performance on standard benchmarks and tasks that require broader capabilities. <div>
arXiv:2505.00047v1 Announce Type: new 
Abstract: Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate "7" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting</title>
<link>https://arxiv.org/abs/2505.00050</link>
<guid>https://arxiv.org/abs/2505.00050</guid>
<content:encoded><![CDATA[
<div> fashion trends, social media sentiment, Twitter data, sentiment analysis, machine learning <br />
<br />
Summary: This study explores the relationship between fashion trends and social media sentiment using Twitter data and the T4SA dataset. Through natural language processing and machine learning, the research examines how sentiment in fashion-related social media discussions can predict emerging trends. The analysis involves identifying fashion-related content, improving sentiment classification normalization, time series decomposition, statistically validating causal relationships, comparing sentiment across platforms, and analyzing brand-specific sentiment. Results show correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes experiencing significant upward trends. The Granger causality analysis identifies sustainability and streetwear as key trend drivers, showing bidirectional relationships with other themes. The study demonstrates that social media sentiment analysis can effectively predict fashion trend trajectories with proper statistical validation. The predictive model achieved 78.35% balanced accuracy in sentiment classification, providing a reliable basis for trend prediction across sentiment categories. <br /> <div>
arXiv:2505.00050v1 Announce Type: new 
Abstract: This study explores the intersection of fashion trends and social media sentiment through computational analysis of Twitter data using the T4SA (Twitter for Sentiment Analysis) dataset. By applying natural language processing and machine learning techniques, we examine how sentiment patterns in fashion-related social media conversations can serve as predictors for emerging fashion trends. Our analysis involves the identification and categorization of fashion-related content, sentiment classification with improved normalization techniques, time series decomposition, statistically validated causal relationship modeling, cross-platform sentiment comparison, and brand-specific sentiment analysis. Results indicate correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes showing statistically significant rising trends. The Granger causality analysis establishes sustainability and streetwear as primary trend drivers, showing bidirectional relationships with several other themes. The findings demonstrate that social media sentiment analysis can serve as an effective early indicator of fashion trend trajectories when proper statistical validation is applied. Our improved predictive model achieved 78.35% balanced accuracy in sentiment classification, establishing a reliable foundation for trend prediction across positive, neutral, and negative sentiment categories.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity</title>
<link>https://arxiv.org/abs/2505.00056</link>
<guid>https://arxiv.org/abs/2505.00056</guid>
<content:encoded><![CDATA[
<div> Keywords: meme clustering, toxicity detection, virality modeling, multi-dimensional similarity, template-based matching

Summary: 
This paper presents a novel method for clustering Internet memes based on multi-dimensional similarity features derived from template-based matching. The traditional approaches to meme clustering often rely on predefined databases and overlook semantic nuances, leading to challenges in handling the diversity of memes. In contrast, the proposed method utilizes local and global features across different similarity categories such as form, visual content, text, and identity, resulting in more consistent and coherent meme clusters. By incorporating adaptive matching capabilities, the approach outperforms existing methods and aligns with human intuition. The code for implementing the method is publicly available to support further research. The significant contributions of this work lie in addressing the challenges of meme clustering by leveraging multi-dimensional similarity features and template-based matching, ultimately enhancing the accuracy and adaptability of clustering processes. 

<br /><br />Summary: <div>
arXiv:2505.00056v1 Announce Type: new 
Abstract: Meme clustering is critical for toxicity detection, virality modeling, and typing, but it has received little attention in previous research. Clustering similar Internet memes is challenging due to their multimodality, cultural context, and adaptability. Existing approaches rely on databases, overlook semantics, and struggle to handle diverse dimensions of similarity. This paper introduces a novel method that uses template-based matching with multi-dimensional similarity features, thus eliminating the need for predefined databases and supporting adaptive matching. Memes are clustered using local and global features across similarity categories such as form, visual content, text, and identity. Our combined approach outperforms existing clustering methods, producing more consistent and coherent clusters, while similarity-based feature sets enable adaptability and align with human intuition. We make all supporting code publicly available to support subsequent research. Code: https://github.com/tygobl/meme-clustering
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Report on the llms evaluating the high school questions</title>
<link>https://arxiv.org/abs/2505.00057</link>
<guid>https://arxiv.org/abs/2505.00057</guid>
<content:encoded><![CDATA[
<div> evaluation, large language models, high school science questions, education, performance

Summary:<br />
- The report evaluates the performance of large language models (LLMs) in solving high school science questions.
- It explores the potential applications of LLMs in the educational field.
- Mathematics exam questions from college entrance examinations were used as evaluation data.
- The study utilized at least eight LLM APIs to provide answers and conducted a comprehensive assessment based on various metrics.
- The findings highlight the strengths and weaknesses of LLMs in handling high school science questions, emphasizing the need for improvement in logical reasoning and creative problem-solving.

<br /><br />Summary: <div>
arXiv:2505.00057v1 Announce Type: new 
Abstract: This report aims to evaluate the performance of large language models (LLMs) in solving high school science questions and to explore their potential applications in the educational field. With the rapid development of LLMs in the field of natural language processing, their application in education has attracted widespread attention. This study selected mathematics exam questions from the college entrance examinations (2019-2023) as evaluation data and utilized at least eight LLM APIs to provide answers. A comprehensive assessment was conducted based on metrics such as accuracy, response time, logical reasoning, and creativity. Through an in-depth analysis of the evaluation results, this report reveals the strengths and weaknesses of LLMs in handling high school science questions and discusses their implications for educational practice. The findings indicate that although LLMs perform excellently in certain aspects, there is still room for improvement in logical reasoning and creative problem-solving. This report provides an empirical foundation for further research and application of LLMs in the educational field and offers suggestions for improvement.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition</title>
<link>https://arxiv.org/abs/2505.00059</link>
<guid>https://arxiv.org/abs/2505.00059</guid>
<content:encoded><![CDATA[
<div> dataset, distance, accents, emotion, speech recognition <br />
Summary:
The article introduces the BERSt dataset, which aims to address the challenges of distanced speech recognition. The dataset includes nearly 4 hours of English speech from 98 actors with various accents and emotions, recorded in different acoustic environments using smartphones. It offers data for evaluating ASR, shout detection, and SER tasks. Initial benchmarks show that ASR performance degrades with distance and shout level, and varies depending on the intended emotion. The dataset poses challenges for both ASR and SER tasks, highlighting the need for improving the robustness of speech recognition systems for real-world applications. <div>
arXiv:2505.00059v1 Announce Type: new 
Abstract: Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5</title>
<link>https://arxiv.org/abs/2505.00060</link>
<guid>https://arxiv.org/abs/2505.00060</guid>
<content:encoded><![CDATA[
<div> evaluation framework, semantic accuracy, large language models, SQL generation, business intelligence

Summary:<br />
The study introduces a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of Large Language Models (LLMs) in generating SQL queries for Business Intelligence (BI) tasks. Using Exaone 3.5, a bilingual LLM optimized for enterprise tasks, the framework evaluates model performance based on answer accuracy, execution success rate, semantic error rate, and non-response rate. A domain-specific benchmark comprising 219 natural language business questions across varying SQL complexity levels is used, derived from LG Electronics' internal BigQuery sales data. Results show Exaone 3.5 excels in simple aggregation tasks but struggles in arithmetic reasoning and grouped ranking tasks, with errors concentrated in complex cases such as misapplied arithmetic logic and incomplete filtering. The findings emphasize the current limitations of LLMs in business environments and advocate for fact-consistency validation layers and hybrid reasoning approaches to enhance natural language interfaces for enterprise data systems.<br /> <div>
arXiv:2505.00060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems</title>
<link>https://arxiv.org/abs/2505.00061</link>
<guid>https://arxiv.org/abs/2505.00061</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based, automated short-answer grading systems, vulnerabilities, adversarial gaming strategies, AI-driven educational tools

Summary:
This study explores vulnerabilities in transformer-based automated short-answer grading systems used in medical education and how they can be manipulated through adversarial gaming strategies. Three main types of gaming strategies were identified that exploit these vulnerabilities, potentially leading to false positives. Adversarial training methods were implemented to enhance the systems' robustness, significantly reducing susceptibility to manipulations. Ensemble techniques such as majority voting and ridge regression further improved the system's defense against sophisticated adversarial inputs. Large language models like GPT-4 with varied prompting techniques proved effective in recognizing and scoring gaming strategies. Continuous improvements in AI-driven educational tools are crucial to ensure reliability and fairness in high-stakes settings.
<br /><br />Summary: This study investigates vulnerabilities in transformer-based automated short-answer grading systems used in medical education. Three main types of gaming strategies were identified, leading to false positives. Adversarial training methods and ensemble techniques improved the systems' robustness and defense against manipulations. Large language models like GPT-4 effectively recognized and scored gaming strategies. Continuous improvements in AI-driven educational tools are essential for reliability and fairness. <div>
arXiv:2505.00061v1 Announce Type: new 
Abstract: This study examines vulnerabilities in transformer-based automated short-answer grading systems used in medical education, with a focus on how these systems can be manipulated through adversarial gaming strategies. Our research identifies three main types of gaming strategies that exploit the system's weaknesses, potentially leading to false positives. To counteract these vulnerabilities, we implement several adversarial training methods designed to enhance the systems' robustness. Our results indicate that these methods significantly reduce the susceptibility of grading systems to such manipulations, especially when combined with ensemble techniques like majority voting and ridge regression, which further improve the system's defense against sophisticated adversarial inputs. Additionally, employing large language models such as GPT-4 with varied prompting techniques has shown promise in recognizing and scoring gaming strategies effectively. The findings underscore the importance of continuous improvements in AI-driven educational tools to ensure their reliability and fairness in high-stakes settings.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling</title>
<link>https://arxiv.org/abs/2505.00063</link>
<guid>https://arxiv.org/abs/2505.00063</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal large language models, document-specific tasks, visual complexity, reasoning complexity

Summary:<br />
The article introduces a General Document Intelligence Benchmark (GDI-Bench) to evaluate the capabilities of multimodal large language models (MLLMs) across various document-specific tasks. This benchmark features 1.9k images and 19 tasks structured by visual and reasoning complexity, allowing for performance assessment by difficulty. The GDI-Bench was evaluated on open-source and closed-source models, revealing strengths and weaknesses in both visual and reasoning domains. The GPT-4o model excelled in reasoning tasks but showed limitations in visual capabilities. To address diverse tasks, the article proposes a GDI Model that mitigates catastrophic forgetting during supervised fine-tuning by preserving intelligence. This model achieved state-of-the-art performance on previous benchmarks and the GDI-Bench. Both the benchmark and model will be open source.<br />Summary: <div>
arXiv:2505.00063v1 Announce Type: new 
Abstract: The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConSens: Assessing context grounding in open-book question answering</title>
<link>https://arxiv.org/abs/2505.00065</link>
<guid>https://arxiv.org/abs/2505.00065</guid>
<content:encoded><![CDATA[
<div> language models, question answering, evaluation metric, context utilization, open-book QA

Summary:
An evaluation metric is proposed for large language models in open-book question answering, aiming to assess the extent to which model responses rely on the provided context rather than parametric knowledge. The metric contrasts the model's perplexity when the context is provided versus when it is not, offering a quantifiable score for context utilization. Experimental results demonstrate the effectiveness of the metric in identifying context-grounded answers. Unlike existing methods, the proposed metric is computationally efficient, interpretable, and adaptable to various use cases, providing a scalable and practical solution for assessing context utilization in open-book QA systems. <div>
arXiv:2505.00065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated considerable success in open-book question answering (QA), where the task requires generating answers grounded in a provided external context. A critical challenge in open-book QA is to ensure that model responses are based on the provided context rather than its parametric knowledge, which can be outdated, incomplete, or incorrect. Existing evaluation methods, primarily based on the LLM-as-a-judge approach, face significant limitations, including biases, scalability issues, and dependence on costly external systems. To address these challenges, we propose a novel metric that contrasts the perplexity of the model response under two conditions: when the context is provided and when it is not. The resulting score quantifies the extent to which the model's answer relies on the provided context. The validity of this metric is demonstrated through a series of experiments that show its effectiveness in identifying whether a given answer is grounded in the provided context. Unlike existing approaches, this metric is computationally efficient, interpretable, and adaptable to various use cases, offering a scalable and practical solution to assess context utilization in open-book QA systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese</title>
<link>https://arxiv.org/abs/2505.00114</link>
<guid>https://arxiv.org/abs/2505.00114</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Lebanese dialect, fine-tuning, cultural authenticity, LebEval benchmark

Summary: 
This study explores the effectiveness of Large Language Models (LLMs) for translating the low-resource Lebanese dialect, focusing on the influence of culturally authentic data versus larger translated datasets. Three fine-tuning methods were compared: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Results showed that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperformed those trained on larger, non-native data. The best outcomes were observed with contrastive fine-tuning combined with contrastive prompting, highlighting the importance of exposing translation models to mistakes. The researchers introduced LebEval, a new benchmark derived from native Lebanese content, and compared it to the existing FLoRes benchmark. These findings challenge the prevailing belief that "More Data is Better" and underscore the critical role of cultural authenticity in dialectal translation. The datasets and code used in the study are available on Github. 

<br /><br />Summary: <div>
arXiv:2505.00114v1 Announce Type: new 
Abstract: This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</title>
<link>https://arxiv.org/abs/2505.00127</link>
<guid>https://arxiv.org/abs/2505.00127</guid>
<content:encoded><![CDATA[
<div> overthinking, response length, problem difficulty, reasoning behavior, self-awareness

Summary:
Large language models (LLMs) have been optimized for long reasoning, assuming it leads to better performance. However, this study found that LLMs tend to overthink simple problems, resulting in unnecessarily long responses, while underthinking harder problems, leading to decreased accuracy. The research suggests that models may misjudge the difficulty of problems and fail to adjust response length accordingly. By using a preference optimization algorithm to reduce response length without considering answer correctness, the study showed that accuracy could be maintained while significantly reducing generation length. These findings emphasize the importance of considering response length as a signal for reasoning behavior and suggest the need for further exploration into LLMs' self-awareness and adaptation of reasoning length. 

Summary: <div>
arXiv:2505.00127v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models</title>
<link>https://arxiv.org/abs/2505.00147</link>
<guid>https://arxiv.org/abs/2505.00147</guid>
<content:encoded><![CDATA[
<div> AdaptMI, adaptive approach, skill-based in-context learning, small language models, cognitive load theory <br />
<br />
Summary: 
In the study of in-context learning (ICL) for language models, the focus is on improving problem-solving capabilities by providing relevant information in context, similar to human learning from teachers. While leveraging large language models' metacognition has shown improved performance in ICL, the same approach has not yielded significant gains in small language models (SLMs). The research identifies a performance gap in ICL capabilities for SLMs, attributing it to cognitive overload caused by unnecessary information in skill-based prompting. To address this issue, the researchers propose AdaptMI, an adaptive method of selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory, AdaptMI introduces skill-based examples only when the model struggles, leading to improved accuracy. Furthermore, the study introduces AdaptMI+, which provides targeted examples to address specific missing skills in the model's responses, resulting in an accuracy improvement of up to 6% across various math benchmarks and SLMs. <br /><br /> <div>
arXiv:2505.00147v1 Announce Type: new 
Abstract: In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model's (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM's metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. We investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model's responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports</title>
<link>https://arxiv.org/abs/2505.00191</link>
<guid>https://arxiv.org/abs/2505.00191</guid>
<content:encoded><![CDATA[
<div> framework, radiology reports, AI-based methods, interpretability, MIMIC-CXR dataset<br />
Summary:<br />
The paper introduces an interpretable-by-design framework for classifying radiology reports using AI-based methods. The framework focuses on selecting a set of informative queries from reports to predict a diagnosis, improving accuracy and efficiency in medical diagnosis. By using the Information Pursuit framework to extract queries, the Flan-T5 model to identify facts in reports, and a classifier to predict diseases, the proposed method demonstrates effectiveness in experiments on the MIMIC-CXR dataset. This approach aims to address the lack of interpretability in current AI methods for radiology reports, potentially increasing trust and usability in medical AI applications. <div>
arXiv:2505.00191v1 Announce Type: new 
Abstract: The development of AI-based methods for analyzing radiology reports could lead to significant advances in medical diagnosis--from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability in these methods has hindered their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying radiology reports. The key idea is to extract a set of most informative queries from a large set of reports and use these queries and their corresponding answers to predict a diagnosis. Thus, the explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select informative queries, the Flan-T5 model to determine if facts are present in the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring</title>
<link>https://arxiv.org/abs/2505.00261</link>
<guid>https://arxiv.org/abs/2505.00261</guid>
<content:encoded><![CDATA[
<div> Keywords: Korean language education, learner corpora, grammatical error correction, KoLLA corpus, automated error correction

Summary: 
- The article discusses the lack of learner corpora tailored to Korean L2 writing despite growing global interest in Korean language education.
- To address this gap, the KoLLA Korean learner corpus has been enhanced by adding multiple grammatical error correction references, allowing for more nuanced evaluation of GEC systems.
- The enhancements also include rubric-based scores aligned with guidelines from the Korean National Language Institute, covering grammatical accuracy, coherence, and lexical diversity.
- These improvements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning and assessment.
- The enriched corpus can also benefit automated error correction systems in providing more accurate feedback to language learners. 

<br /><br />Summary: <div>
arXiv:2505.00261v1 Announce Type: new 
Abstract: Despite growing global interest in Korean language education, there remains a significant lack of learner corpora tailored to Korean L2 writing. To address this gap, we enhance the KoLLA Korean learner corpus by adding multiple grammatical error correction (GEC) references, thereby enabling more nuanced and flexible evaluation of GEC systems, and reflects the variability of human language. Additionally, we enrich the corpus with rubric-based scores aligned with guidelines from the Korean National Language Institute, capturing grammatical accuracy, coherence, and lexical diversity. These enhancements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning, assessment, and automated error correction.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency in Language Models: Current Landscape, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2505.00268</link>
<guid>https://arxiv.org/abs/2505.00268</guid>
<content:encoded><![CDATA[
<div> Keywords: consistency, language models, AI, research, benchmarks 

Summary: 
This paper delves into the intricacies of consistency in AI language systems, focusing on formal and informal consistency aspects. It highlights the challenges faced by current language models in maintaining reliable consistency, discussing the need for standardized definitions, multilingual assessment, and improved methods. The research gaps identified underscore the necessity for robust benchmarks to measure consistency and interdisciplinary approaches for ensuring the coherent application of language models in specific tasks. The urgent call for addressing these critical gaps emphasizes the importance of consistency in language models while emphasizing adaptability and utility. This analysis shines a light on the crucial role of consistency in effective language use and the imperative need for advancement in this area. 

<br /><br />Summary: <div>
arXiv:2505.00268v1 Announce Type: new 
Abstract: The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation</title>
<link>https://arxiv.org/abs/2505.00339</link>
<guid>https://arxiv.org/abs/2505.00339</guid>
<content:encoded><![CDATA[
<div> framework, AI-driven educational tools, cognitive alignment, linguistic feedback integration, ethical safeguards
Summary:
The paper proposes a framework for enhancing AI-driven educational tools by integrating cognitive assessment frameworks, linguistic analysis of AI-generated feedback, and ethical design principles. The framework consists of three phases: cognitive alignment, linguistic feedback integration, and ethical safeguards. Practical application of the framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. The paper emphasizes the importance of considering the quality, cognitive depth, and ethical implications of AI-generated materials in educational settings. It aims to guide educators, researchers, and developers in leveraging AI's potential while maintaining pedagogical and ethical standards in educational content generation. The comprehensive framework offers actionable steps for creating effective and responsible AI tools in education. 
<br /><br />Summary: <div>
arXiv:2505.00339v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is rapidly transforming education, presenting unprecedented opportunities for personalized learning and streamlined content creation. However, realizing the full potential of AI in educational settings necessitates careful consideration of the quality, cognitive depth, and ethical implications of AI-generated materials. This paper synthesizes insights from four related studies to propose a comprehensive framework for enhancing AI-driven educational tools. We integrate cognitive assessment frameworks (Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated feedback, and ethical design principles to guide the development of effective and responsible AI tools. We outline a structured three-phase approach encompassing cognitive alignment, linguistic feedback integration, and ethical safeguards. The practical application of this framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. This work contributes a comprehensive and actionable guide for educators, researchers, and developers aiming to harness AI's potential while upholding pedagogical and ethical standards in educational content generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis</title>
<link>https://arxiv.org/abs/2505.00367</link>
<guid>https://arxiv.org/abs/2505.00367</guid>
<content:encoded><![CDATA[
arXiv:2505.00367v1 Announce Type: new 
Abstract: Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass</title>
<link>https://arxiv.org/abs/2505.00389</link>
<guid>https://arxiv.org/abs/2505.00389</guid>
<content:encoded><![CDATA[
arXiv:2505.00389v1 Announce Type: new 
Abstract: As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming Large Language Models for Healthcare</title>
<link>https://arxiv.org/abs/2505.00467</link>
<guid>https://arxiv.org/abs/2505.00467</guid>
<content:encoded><![CDATA[
arXiv:2505.00467v1 Announce Type: new 
Abstract: We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Identification of Regulatory Statements in EU Legislation</title>
<link>https://arxiv.org/abs/2505.00479</link>
<guid>https://arxiv.org/abs/2505.00479</guid>
<content:encoded><![CDATA[
arXiv:2505.00479v1 Announce Type: new 
Abstract: Identifying regulatory statements in legislation is useful for developing metrics to measure the regulatory density and strictness of legislation. A computational method is valuable for scaling the identification of such statements from a growing body of EU legislation, constituting approximately 180,000 published legal acts between 1952 and 2023. Past work on extraction of these statements varies in the permissiveness of their definitions for what constitutes a regulatory statement. In this work, we provide a specific definition for our purposes based on the institutional grammar tool. We develop and compare two contrasting approaches for automatically identifying such statements in EU legislation, one based on dependency parsing, and the other on a transformer-based machine learning model. We found both approaches performed similarly well with accuracies of 80% and 84% respectively and a K alpha of 0.58. The high accuracies and not exceedingly high agreement suggests potential for combining strengths of both approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection</title>
<link>https://arxiv.org/abs/2505.00506</link>
<guid>https://arxiv.org/abs/2505.00506</guid>
<content:encoded><![CDATA[
arXiv:2505.00506v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2505.00551</link>
<guid>https://arxiv.org/abs/2505.00551</guid>
<content:encoded><![CDATA[
arXiv:2505.00551v1 Announce Type: new 
Abstract: The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.00557</link>
<guid>https://arxiv.org/abs/2505.00557</guid>
<content:encoded><![CDATA[
arXiv:2505.00557v1 Announce Type: new 
Abstract: Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</title>
<link>https://arxiv.org/abs/2505.00570</link>
<guid>https://arxiv.org/abs/2505.00570</guid>
<content:encoded><![CDATA[
arXiv:2505.00570v1 Announce Type: new 
Abstract: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Block Circulant Adapter for Large Language Models</title>
<link>https://arxiv.org/abs/2505.00582</link>
<guid>https://arxiv.org/abs/2505.00582</guid>
<content:encoded><![CDATA[
arXiv:2505.00582v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation</title>
<link>https://arxiv.org/abs/2505.00624</link>
<guid>https://arxiv.org/abs/2505.00624</guid>
<content:encoded><![CDATA[
arXiv:2505.00624v1 Announce Type: new 
Abstract: Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)</title>
<link>https://arxiv.org/abs/2505.00626</link>
<guid>https://arxiv.org/abs/2505.00626</guid>
<content:encoded><![CDATA[
arXiv:2505.00626v1 Announce Type: new 
Abstract: Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Understanding: an Inherent Ambiguity Barrier</title>
<link>https://arxiv.org/abs/2505.00654</link>
<guid>https://arxiv.org/abs/2505.00654</guid>
<content:encoded><![CDATA[
arXiv:2505.00654v1 Announce Type: new 
Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
arXiv:2505.00661v1 Announce Type: new 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepCritic: Deliberate Critique with Large Language Models</title>
<link>https://arxiv.org/abs/2505.00662</link>
<guid>https://arxiv.org/abs/2505.00662</guid>
<content:encoded><![CDATA[
arXiv:2505.00662v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions</title>
<link>https://arxiv.org/abs/2505.00675</link>
<guid>https://arxiv.org/abs/2505.00675</guid>
<content:encoded><![CDATA[
arXiv:2505.00675v1 Announce Type: new 
Abstract: Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Large Language Models with Register Analysis for Arbitrary Style Transfer</title>
<link>https://arxiv.org/abs/2505.00679</link>
<guid>https://arxiv.org/abs/2505.00679</guid>
<content:encoded><![CDATA[
arXiv:2505.00679v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications</title>
<link>https://arxiv.org/abs/2505.00049</link>
<guid>https://arxiv.org/abs/2505.00049</guid>
<content:encoded><![CDATA[
arXiv:2505.00049v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs' capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques</title>
<link>https://arxiv.org/abs/2505.00105</link>
<guid>https://arxiv.org/abs/2505.00105</guid>
<content:encoded><![CDATA[
arXiv:2505.00105v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation enhances language models by retrieving relevant information from external knowledge bases, relying on high-dimensional vector embeddings typically stored in float32 precision. However, storing these embeddings at scale presents significant memory challenges. To address this issue, we systematically investigate on MTEB benchmark two complementary optimization strategies: quantization, evaluating standard formats (float16, int8, binary) and low-bit floating-point types (float8), and dimensionality reduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and Autoencoders. Our results show that float8 quantization achieves a 4x storage reduction with minimal performance degradation (<0.3%), significantly outperforming int8 quantization at the same compression level, being simpler to implement. PCA emerges as the most effective dimensionality reduction technique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions) with float8 quantization offers an excellent trade-off, achieving 8x total compression with less performance impact than using int8 alone (which provides only 4x compression). To facilitate practical application, we propose a methodology based on visualizing the performance-storage trade-off space to identify the optimal configuration that maximizes performance within their specific memory constraints.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.00150</link>
<guid>https://arxiv.org/abs/2505.00150</guid>
<content:encoded><![CDATA[
arXiv:2505.00150v1 Announce Type: cross 
Abstract: The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous expressions with visual and textual elements, are sometimes misused to disseminate hate speech against individuals or groups. While the detection of hateful memes is well-researched, developing effective methods to transform hateful content in memes remains a significant challenge. Leveraging the powerful generation and reasoning capabilities of Vision-Language Models (VLMs), we address the tasks of detecting and mitigating hateful content. This paper presents two key contributions: first, a definition-guided prompting technique for detecting hateful memes, and second, a unified framework for mitigating hateful content in memes, named UnHateMeme, which works by replacing hateful textual and/or visual components. With our definition-guided prompts, VLMs achieve impressive performance on hateful memes detection task. Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a strong capability to convert hateful memes into non-hateful forms that meet human-level criteria for hate speech and maintain multimodal coherence between image and text. Through empirical experiments, we show the effectiveness of state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the proposed tasks, providing a comprehensive analysis of their respective strengths and limitations for these tasks. This paper aims to shed light on important applications of VLMs for ensuring safe and respectful online environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00212</link>
<guid>https://arxiv.org/abs/2505.00212</guid>
<content:encoded><![CDATA[
arXiv:2505.00212v1 Announce Type: cross 
Abstract: Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
arXiv:2505.00234v1 Announce Type: cross 
Abstract: Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnronQA: Towards Personalized RAG over Private Documents</title>
<link>https://arxiv.org/abs/2505.00263</link>
<guid>https://arxiv.org/abs/2505.00263</guid>
<content:encoded><![CDATA[
arXiv:2505.00263v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has become one of the most popular methods for bringing knowledge-intensive context to large language models (LLM) because of its ability to bring local context at inference time without the cost or data leakage risks associated with fine-tuning. A clear separation of private information from the LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM's understanding using customers' private documents. Despite its popularity for private documents in enterprise deployments, current RAG benchmarks for validating and optimizing RAG pipelines draw their corpora from public data such as Wikipedia or generic web pages and offer little to no personal context. Seeking to empower more personal and private RAG we release the EnronQA benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs across 150 different user inboxes. EnronQA enables better benchmarking of RAG pipelines over private data and allows for experimentation on the introduction of personalized retrieval settings over realistic data. Finally, we use EnronQA to explore the tradeoff in memorization and retrieval when reasoning over private documents.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</title>
<link>https://arxiv.org/abs/2505.00315</link>
<guid>https://arxiv.org/abs/2505.00315</guid>
<content:encoded><![CDATA[
arXiv:2505.00315v1 Announce Type: cross 
Abstract: Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.00337</link>
<guid>https://arxiv.org/abs/2505.00337</guid>
<content:encoded><![CDATA[
arXiv:2505.00337v1 Announce Type: cross 
Abstract: Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training</title>
<link>https://arxiv.org/abs/2505.00358</link>
<guid>https://arxiv.org/abs/2505.00358</guid>
<content:encoded><![CDATA[
arXiv:2505.00358v1 Announce Type: cross 
Abstract: Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&amp;B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&amp;B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&amp;B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&amp;B matches or exceeds the performance of state-of-the-art data mixing strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training</title>
<link>https://arxiv.org/abs/2505.00422</link>
<guid>https://arxiv.org/abs/2505.00422</guid>
<content:encoded><![CDATA[
arXiv:2505.00422v1 Announce Type: cross 
Abstract: Accurate classification of medical device risk levels is essential for regulatory oversight and clinical safety. We present a Transformer-based multimodal framework that integrates textual descriptions and visual information to predict device regulatory classification. The model incorporates a cross-attention mechanism to capture intermodal dependencies and employs a self-training strategy for improved generalization under limited supervision. Experiments on a real-world regulatory dataset demonstrate that our approach achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming text-only (77.2%) and image-only (54.8%) baselines. Compared to standard multimodal fusion, the self-training mechanism improved SVM performance by 3.3 percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1, suggesting that pseudo-labeling can effectively enhance generalization under limited supervision. Ablation studies further confirm the complementary benefits of both cross-modal attention and self-training.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Task Arithmetic for Zero-Shot Information Retrieval</title>
<link>https://arxiv.org/abs/2505.00649</link>
<guid>https://arxiv.org/abs/2505.00649</guid>
<content:encoded><![CDATA[
arXiv:2505.00649v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
arXiv:2505.00703v1 Announce Type: cross 
Abstract: Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers</title>
<link>https://arxiv.org/abs/2309.08532</link>
<guid>https://arxiv.org/abs/2309.08532</guid>
<content:encoded><![CDATA[
arXiv:2309.08532v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning</title>
<link>https://arxiv.org/abs/2401.15371</link>
<guid>https://arxiv.org/abs/2401.15371</guid>
<content:encoded><![CDATA[
arXiv:2401.15371v4 Announce Type: replace 
Abstract: Legal Judgment Prediction (LJP) is a fundamental task of legal artificial intelligence, aiming to automatically predict the judgment outcomes of legal cases. Existing LJP models primarily focus on identifying legal triggers within criminal fact descriptions by contrastively training language models. However, these LJP models overlook the importance of learning to effectively distinguish subtle differences among judgments, which is crucial for producing more accurate predictions. In this paper, we propose LegalDuet, which continuously pretrains language models to learn a more tailored embedding space for representing legal cases. Specifically, LegalDuet designs a dual-view mechanism to continuously pretrain language models: 1) Law Case Clustering retrieves similar cases as hard negatives and employs contrastive training to differentiate among confusing cases; 2) Legal Decision Matching aims to identify legal clues within criminal fact descriptions to align them with the chain of reasoning that contains the correct legal decision. Our experiments on the CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further analysis reveals that LegalDuet improves the ability of pretrained language models to distinguish confusing criminal charges by reducing prediction uncertainty and enhancing the separability of criminal charges. The experiments demonstrate that LegalDuet produces a more concentrated and distinguishable embedding space, effectively aligning criminal facts with corresponding legal decisions. The code is available at https://github.com/NEUIR/LegalDuet.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments</title>
<link>https://arxiv.org/abs/2402.08498</link>
<guid>https://arxiv.org/abs/2402.08498</guid>
<content:encoded><![CDATA[
arXiv:2402.08498v5 Announce Type: replace 
Abstract: Large language models (LLMs) play a key role in generating evidence-based and stylistic counter-arguments, yet their effectiveness in real-world applications has been underexplored. Previous research often neglects the balance between evidentiality and style, which are crucial for persuasive arguments. To address this, we evaluated the effectiveness of stylized evidence-based counter-argument generation in Counterfire, a new dataset of 38,000 counter-arguments generated by revising counter-arguments to Reddit's ChangeMyView community to follow different discursive styles. We evaluated generic and stylized counter-arguments from basic and fine-tuned models such as GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku, LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings reveal that humans prefer stylized counter-arguments over the original outputs, with GPT-3.5 Turbo performing well, though still not reaching human standards of rhetorical quality nor persuasiveness. Additionally, our work created a novel argument triplets dataset for studying style control, with human preference labels that provide insights into the tradeoffs between evidence integration and argument quality.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2405.04532</link>
<guid>https://arxiv.org/abs/2405.04532</guid>
<content:encoded><![CDATA[
arXiv:2405.04532v3 Announce Type: replace 
Abstract: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts</title>
<link>https://arxiv.org/abs/2405.11804</link>
<guid>https://arxiv.org/abs/2405.11804</guid>
<content:encoded><![CDATA[
arXiv:2405.11804v2 Announce Type: replace 
Abstract: Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Review Generation Method Based on Large Language Models</title>
<link>https://arxiv.org/abs/2407.20906</link>
<guid>https://arxiv.org/abs/2407.20906</guid>
<content:encoded><![CDATA[
arXiv:2407.20906v5 Announce Type: replace 
Abstract: Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\% with 95\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges and Future Directions of Data-Centric AI Alignment</title>
<link>https://arxiv.org/abs/2410.01957</link>
<guid>https://arxiv.org/abs/2410.01957</guid>
<content:encoded><![CDATA[
arXiv:2410.01957v2 Announce Type: replace 
Abstract: As AI systems become increasingly capable and influential, ensuring their alignment with human values, preferences, and goals has become a critical research focus. Current alignment methods primarily focus on designing algorithms and loss functions but often underestimate the crucial role of data. This paper advocates for a shift towards data-centric AI alignment, emphasizing the need to enhance the quality and representativeness of data used in aligning AI systems. In this position paper, we highlight key challenges associated with both human-based and AI-based feedback within the data-centric alignment framework. Through qualitative analysis, we identify multiple sources of unreliability in human feedback, as well as problems related to temporal drift, context dependence, and AI-based feedback failing to capture human values due to inherent model limitations. We propose future research directions, including improved feedback collection practices, robust data-cleaning methodologies, and rigorous feedback verification processes. We call for future research into these critical directions to ensure, addressing gaps that persist in understanding and improving data-centric alignment practices.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation</title>
<link>https://arxiv.org/abs/2410.20774</link>
<guid>https://arxiv.org/abs/2410.20774</guid>
<content:encoded><![CDATA[
arXiv:2410.20774v2 Announce Type: replace 
Abstract: In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using EMBER, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis</title>
<link>https://arxiv.org/abs/2412.05862</link>
<guid>https://arxiv.org/abs/2412.05862</guid>
<content:encoded><![CDATA[
arXiv:2412.05862v3 Announce Type: replace 
Abstract: In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods</title>
<link>https://arxiv.org/abs/2501.13947</link>
<guid>https://arxiv.org/abs/2501.13947</guid>
<content:encoded><![CDATA[
arXiv:2501.13947v3 Announce Type: replace 
Abstract: The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05945</link>
<guid>https://arxiv.org/abs/2502.05945</guid>
<content:encoded><![CDATA[
arXiv:2502.05945v2 Announce Type: replace 
Abstract: Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?</title>
<link>https://arxiv.org/abs/2502.07963</link>
<guid>https://arxiv.org/abs/2502.07963</guid>
<content:encoded><![CDATA[
arXiv:2502.07963v2 Announce Type: replace 
Abstract: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
<link>https://arxiv.org/abs/2502.20984</link>
<guid>https://arxiv.org/abs/2502.20984</guid>
<content:encoded><![CDATA[
arXiv:2502.20984v3 Announce Type: replace 
Abstract: SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement</title>
<link>https://arxiv.org/abs/2503.23895</link>
<guid>https://arxiv.org/abs/2503.23895</guid>
<content:encoded><![CDATA[
arXiv:2503.23895v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opioid Named Entity Recognition (ONER-2025) from Reddit</title>
<link>https://arxiv.org/abs/2504.00027</link>
<guid>https://arxiv.org/abs/2504.00027</guid>
<content:encoded><![CDATA[
arXiv:2504.00027v3 Announce Type: replace 
Abstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem</title>
<link>https://arxiv.org/abs/2403.00108</link>
<guid>https://arxiv.org/abs/2403.00108</guid>
<content:encoded><![CDATA[
arXiv:2403.00108v2 Announce Type: replace-cross 
Abstract: Finetuning LLMs with LoRA has gained significant popularity due to its simplicity and effectiveness. Often, users may even find pluggable, community-shared LoRAs to enhance their base models for a specific downstream task of interest; enjoying a powerful, efficient, yet customized LLM experience with negligible investment. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can distribute malicious LoRAs to a community eager to try out shared assets. Despite the high-risk potential, no prior art has comprehensively explored LoRA's attack surface under the downstream-enhancing share-and-play context. In this paper, we investigate how backdoors can be injected into task-enhancing LoRAs and examine the mechanisms of such infections. We find that with a simple, efficient, yet specific recipe, a backdoor LoRA can be trained once and then seamlessly merged (in a training-free fashion) with multiple task-enhancing LoRAs, retaining both its malicious backdoor and benign downstream capabilities. This allows attackers to scale the distribution of compromised LoRAs with minimal effort by leveraging the rich pool of existing shared LoRA assets. We note that such merged LoRAs are particularly infectious -- because their malicious intent is cleverly concealed behind improved downstream capabilities, creating a strong incentive for voluntary download -- and dangerous -- because under local deployment, no safety measures exist to intervene when things go wrong. Our work is among the first to study this new threat model of training-free distribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting the urgent need for heightened security awareness in the LoRA ecosystem. Warning: This paper contains offensive content and involves a real-life tragedy.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent as a Mechanical Designer</title>
<link>https://arxiv.org/abs/2404.17525</link>
<guid>https://arxiv.org/abs/2404.17525</guid>
<content:encoded><![CDATA[
arXiv:2404.17525v3 Announce Type: replace-cross 
Abstract: Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers</title>
<link>https://arxiv.org/abs/2405.04620</link>
<guid>https://arxiv.org/abs/2405.04620</guid>
<content:encoded><![CDATA[
arXiv:2405.04620v5 Announce Type: replace-cross 
Abstract: In this work, we present a generalized formulation of the Transformer algorithm by reinterpreting its core mechanisms within the framework of Path Integral formalism. In this perspective, the attention mechanism is recast as a process that integrates all possible transition paths leading to future token states, with temporal evolution governed by the Feed-Forward Network. By systematically mapping each component of the Transformer to its counterpart in the Path Integral formulation, we obtain a more compact and efficient representation, in which the contextual information of a sequence is condensed into memory-like segments. These segments are recurrently processed across Transformer layers, enabling more effective long-term information retention. We validate the effectiveness of this approach through the Passkey retrieval task and a summarization task, demonstrating that the proposed method preserves historical information while exhibiting memory usage that scales linearly with sequence length. This contrasts with the non-linear memory growth typically observed in standard attention mechanisms. We expect that this quantum-inspired generalization of the Transformer architecture will open new avenues for enhancing both the efficiency and expressiveness of future Transformer models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaeBench: Improving Quality of Toxic Adversarial Examples</title>
<link>https://arxiv.org/abs/2410.05573</link>
<guid>https://arxiv.org/abs/2410.05573</guid>
<content:encoded><![CDATA[
arXiv:2410.05573v2 Announce Type: replace-cross 
Abstract: Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of TAE. Successful TAE should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of 940k raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Personalization and Control in Scientific Personalized Search</title>
<link>https://arxiv.org/abs/2411.02790</link>
<guid>https://arxiv.org/abs/2411.02790</guid>
<content:encoded><![CDATA[
arXiv:2411.02790v2 Announce Type: replace-cross 
Abstract: Personalized search is a problem where models benefit from learning user preferences from per-user historical interaction data. The inferred preferences enable personalized ranking models to improve the relevance of documents for users. However, personalization is also seen as opaque in its use of historical interactions and is not amenable to users' control. Further, personalization limits the diversity of information users are exposed to. While search results may be automatically diversified this does little to address the lack of control over personalization. In response, we introduce a model for personalized search that enables users to control personalized rankings proactively. Our model, CtrlCE, is a novel cross-encoder model augmented with an editable memory built from users' historical interactions. The editable memory allows cross-encoders to be personalized efficiently and enables users to control personalized ranking. Next, because all queries do not require personalization, we introduce a calibrated mixing model which determines when personalization is necessary. This enables users to control personalization via their editable memory only when necessary. To thoroughly evaluate CtrlCE, we demonstrate its empirical performance in four domains of science, its ability to selectively request user control in a calibration evaluation of the mixing model, and the control provided by its editable memory in a user study.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner</title>
<link>https://arxiv.org/abs/2412.18086</link>
<guid>https://arxiv.org/abs/2412.18086</guid>
<content:encoded><![CDATA[
arXiv:2412.18086v2 Announce Type: replace-cross 
Abstract: Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at https://ezharjan.github.io/AutoSceneGen.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2501.18265</link>
<guid>https://arxiv.org/abs/2501.18265</guid>
<content:encoded><![CDATA[
arXiv:2501.18265v2 Announce Type: replace-cross 
Abstract: Evaluating the truthfulness of online content is critical for combating misinformation. This study examines the efficiency and effectiveness of crowdsourced truthfulness assessments through a comparative analysis of two approaches: one involving full-length webpages as evidence for each claim, and another using summaries for each evidence document generated with a large language model. Using an A/B testing setting, we engage a diverse pool of participants tasked with evaluating the truthfulness of statements under these conditions. Our analysis explores both the quality of assessments and the behavioral patterns of participants. The results reveal that relying on summarized evidence offers comparable accuracy and error metrics to the Standard modality while significantly improving efficiency. Workers in the Summary setting complete a significantly higher number of assessments, reducing task duration and costs. Additionally, the Summary modality maximizes internal agreement and maintains consistent reliance on and perceived usefulness of evidence, demonstrating its potential to streamline large-scale truthfulness evaluations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reinforcement Finetuning via Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.05520</link>
<guid>https://arxiv.org/abs/2504.05520</guid>
<content:encoded><![CDATA[
arXiv:2504.05520v2 Announce Type: replace-cross 
Abstract: Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models</title>
<link>https://arxiv.org/abs/2504.21012</link>
<guid>https://arxiv.org/abs/2504.21012</guid>
<content:encoded><![CDATA[
<div> Keywords: intuitive thinking, large language models, cognitive dynamics, conceptual fusion, artificial intelligence

Summary:
- The study aims to investigate the cognitive dynamics of humans and large language models (LLMs) in terms of intuitive thinking by proposing a new framework.
- The framework includes Transition-Inducing Prompts (TIP) triggering rapid shifts in LLM responsiveness and Transition Quantifying Prompts (TQP) to evaluate these changes.
- Controlled experiments were conducted to analyze how LLMs react to prompts embedding semantically distant concepts and their linguistic quality and affective tone changes.
- While humans tend to experience heightened engagement with meaningfully blended concepts, LLMs showed no significant difference in responsiveness between fused and non-fused prompts.
- The results suggest that current LLMs may not replicate the conceptual integration processes seen in human intuition, highlighting differences in how intuition and conceptual leaps emerge in artificial versus human minds.

<br /><br />Summary: The study explores the cognitive dynamics of intuitive thinking in humans and large language models (LLMs) using a unique framework. Through controlled experiments, the researchers observed how LLMs react to prompts embedding semantically distant concepts and found that while humans engage more with fused concepts, LLMs show no significant difference. This indicates that LLMs may not yet replicate human intuition processes related to conceptual integration. The findings shed light on the differences in cognitive responsiveness between artificial and human minds. <div>
arXiv:2504.21012v1 Announce Type: new 
Abstract: What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)--either fused together or presented separately--by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept--a form of conceptual fusion--current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge</title>
<link>https://arxiv.org/abs/2504.21013</link>
<guid>https://arxiv.org/abs/2504.21013</guid>
<content:encoded><![CDATA[
<div> AI-generated feedback, educational settings, linguistic characteristics, Google's Gemini 1.5-flash text model, multiple-choice questions

Summary:
- Study examines linguistic attributes of AI-generated feedback for computer science MCQs
- Analyzed dataset of 1,200 MCQs across difficulty levels and feedback tones
- Computed metrics such as length, readability scores, vocabulary richness, and lexical density
- Trained RoBERTa-based MTL model to predict linguistic properties with low error rates
- Revealed dynamic adaptation of AI-generated feedback based on tone and difficulty levels<br /><br />Summary: <div>
arXiv:2504.21013v1 Announce Type: new 
Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has garnered considerable attention due to its potential to enhance learning outcomes. However, a comprehensive understanding of the linguistic characteristics of AI-generated feedback, including readability, lexical richness, and adaptability across varying challenge levels, remains limited. This study delves into the linguistic and structural attributes of feedback generated by Google's Gemini 1.5-flash text model for computer science multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed, considering three difficulty levels (easy, medium, hard) and three feedback tones (supportive, neutral, challenging). Key linguistic metrics, such as length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness, and lexical density, were computed and examined. A fine-tuned RoBERTa-based multi-task learning (MTL) model was trained to predict these linguistic properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and 0.03 for vocabulary richness. The findings reveal significant interaction effects between feedback tone and question difficulty, demonstrating the dynamic adaptation of AI-generated feedback within diverse educational contexts. These insights contribute to the development of more personalized and effective AI-driven feedback mechanisms, highlighting the potential for improved learning outcomes while underscoring the importance of ethical considerations in their design and deployment.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments</title>
<link>https://arxiv.org/abs/2504.21016</link>
<guid>https://arxiv.org/abs/2504.21016</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, pandemic, named-entity recognition, Vietnam, prevention<br />
<br />
Summary: <br />
The research focuses on utilizing named-entity recognition (NER) to aid in preventing the spread of COVID-19 in Vietnam. While efforts have been made to trace, localize, and quarantine individuals in contact with patients, the manual nature of these tasks proves to be labor-intensive. The study introduces a manually annotated COVID-19 dataset specifically designed for Vietnamese, incorporating nested named entity recognition tasks with newly defined entity types. By incorporating NER technology, the aim is to enhance the efficiency and accuracy of disease prevention measures in Vietnam. This innovative approach could potentially streamline the process of identifying and isolating individuals at risk, ultimately contributing to more effective containment of the pandemic. <div>
arXiv:2504.21016v1 Announce Type: new 
Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese</title>
<link>https://arxiv.org/abs/2504.21017</link>
<guid>https://arxiv.org/abs/2504.21017</guid>
<content:encoded><![CDATA[
<div> dataset, COVID-19, artificial intelligence, machine reading comprehension, Vietnamese <br />
Summary:<br /> 
The article discusses the impact of COVID-19 worldwide, with over 522 million cases and six million deaths. The Omicron variant has strained disease prevention efforts. The use of artificial intelligence (AI) is crucial in supporting people during these challenging times. Various studies have applied AI to prevent COVID-19, including machine reading comprehension (MRC) studies. The creation of the ViQA-COVID dataset for Vietnamese is significant, as it is the first MRC dataset for COVID-19 in Vietnamese. This dataset can aid in building models and systems to contribute to disease prevention efforts. ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, with the potential to advance MRC studies in Vietnamese and other languages. <br /> <div>
arXiv:2504.21017v1 Announce Type: new 
Abstract: After two years of appearance, COVID-19 has negatively affected people and normal life around the world. As in May 2022, there are more than 522 million cases and six million deaths worldwide (including nearly ten million cases and over forty-three thousand deaths in Vietnam). Economy and society are both severely affected. The variant of COVID-19, Omicron, has broken disease prevention measures of countries and rapidly increased number of infections. Resources overloading in treatment and epidemics prevention is happening all over the world. It can be seen that, application of artificial intelligence (AI) to support people at this time is extremely necessary. There have been many studies applying AI to prevent COVID-19 which are extremely useful, and studies on machine reading comprehension (MRC) are also in it. Realizing that, we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and can be used to build models and systems, contributing to disease prevention. Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, we hope that it can contribute to promoting MRC studies in Vietnamese and multilingual.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization</title>
<link>https://arxiv.org/abs/2504.21018</link>
<guid>https://arxiv.org/abs/2504.21018</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained language models, mid- and low-resource languages, token embedding initialization, hypernetwork, continual pre-training

Summary: 
HYPEROFA introduces a hypernetwork-based approach for adaptive token embedding initialization in pre-trained language models. This method addresses the limitations of existing strategies, such as OFA, by allowing for more flexible token embeddings that are not constrained to convex combinations of source-language embeddings. The hypernetwork is trained to map from a multilingual word vector space to the token embedding space of PLMs using source-language tokens. This approach proves to be effective in improving the convergence of continual pre-training and enhances downstream task performance. Experiments show that HYPEROFA outperforms random initialization baselines and achieves comparable or superior results to OFA. The code for HYPEROFA is made publicly available for further research and implementation. 

<br /><br />Summary: <div>
arXiv:2504.21018v1 Announce Type: new 
Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on mid- and low-resource languages, largely due to limited exposure to these languages during pre-training. A common strategy to address this is to introduce new tokens specific to the target languages, initialize their embeddings, and apply continual pre-training on target-language data. Among such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword embedding initialization heuristic that is both effective and efficient. However, OFA restricts target-language token embeddings to be convex combinations of a fixed number of source-language embeddings, which may limit expressiveness. To overcome this limitation, we propose HYPEROFA, a hypernetwork-based approach for more adaptive token embedding initialization. The hypernetwork is trained to map from an external multilingual word vector space to the PLMs token embedding space using source-language tokens. Once trained, it can generate flexible embeddings for target-language tokens, serving as a good starting point for continual pretraining. Experiments demonstrate that HYPEROFA consistently outperforms random initialization baseline and matches or exceeds the performance of OFA in both continual pre-training convergence and downstream task performance. We make the code publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations</title>
<link>https://arxiv.org/abs/2504.21019</link>
<guid>https://arxiv.org/abs/2504.21019</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AI-generated text, AIGT detection, domain shift, DP-Net<br />
<br />
Summary: 
The paper addresses the challenge of detecting AI-generated text (AIGT) by introducing a novel method called DP-Net, focusing on both generalization and robustness. The researchers propose that robustness in AIGT detection can be viewed as a form of domain shift. They develop DP-Net, which incorporates dynamic perturbations using reinforcement learning to enhance model generalization. Experimental results demonstrate that DP-Net outperforms existing methods in terms of generalization capacity across different domains and exhibits superior robustness against text adversarial attacks. The code for DP-Net is publicly available for further research and development. This approach offers a promising solution for effectively detecting and mitigating the potential misuse of AI-generated text, addressing concerns associated with the increasing prevalence of large language models. <br /><br />Summary: <div>
arXiv:2504.21019v1 Announce Type: new 
Abstract: The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Enhanced Contrastive Search for Improved LLM Text Generation</title>
<link>https://arxiv.org/abs/2504.21020</link>
<guid>https://arxiv.org/abs/2504.21020</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Natural Language Processing, Contrastive Search, Context-Enhanced Contrastive Search, Text Generation

Summary:
<br />
Large Language Models (LLMs) have made significant advancements in Natural Language Processing (NLP), but face challenges in generating high-quality text. Traditional decoding methods struggle with repetition and incoherence in long-form text generation tasks. To tackle these limitations, a novel algorithm called Context-Enhanced Contrastive Search (CECS) with contextual calibration is proposed. CECS introduces dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control to achieve a balance between fluency, creativity, and precision. Evaluation using standard metrics shows that CECS outperforms existing Contrastive Search techniques in coherence and relevance of generated texts. The algorithm has potential applications in legal document drafting, customer service chatbots, and content marketing.
<br /><br />Summary: <div>
arXiv:2504.21020v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable advancements in Natural Language Processing (NLP). However, generating high-quality text that balances coherence, diversity, and relevance remains challenging. Traditional decoding methods, such as bean search and top-k sampling, often struggle with either repetitive or incoherent outputs, particularly in tasks that require long-form text generation. To address these limitations, the paper proposes a novel enhancement of the well-known Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with contextual calibration. The proposed scheme introduces several novelties including dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control, to optimize the balance between fluency, creativity, and precision. The performance of CECS is evaluated using several standard metrics such as BLEU, ROUGE, and semantic similarity. Experimental results demonstrate significant improvements in both coherence and relevance of the generated texts by CECS outperforming the existing Contrastive Search techniques. The proposed algorithm has several potential applications in the real world including legal document drafting, customer service chatbots, and content marketing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees</title>
<link>https://arxiv.org/abs/2504.21022</link>
<guid>https://arxiv.org/abs/2504.21022</guid>
<content:encoded><![CDATA[
<div> keywords: Linear Temporal Logic, Natural Language, ConformalNL2LTL, uncertainty-aware translation, Question-Answering

Summary:
ConformalNL2LTL introduces a new method for translating Natural Language (NL) instructions into Linear Temporal Logic (LTL) formulas with user-defined translation success rates. The method utilizes Question-Answering (QA) problems and Language Models (LLMs) to iteratively construct LTL formulas. By leveraging conformal prediction (CP), which enables uncertainty quantification for black-box models, the method can assess the uncertainty in LLM-generated answers. This allows the method to proceed with translation when confident and request help when needed, resulting in user-specified translation accuracy while minimizing help rates. The approach provides correctness guarantees in NL-to-LTL translation, addressing the manual effort and expertise required in defining LTL-encoded tasks for robotic applications. Theoretical and empirical results demonstrate the effectiveness of ConformalNL2LTL in achieving user-specified translation accuracy. <div>
arXiv:2504.21022v1 Announce Type: new 
Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for robotic tasks. To mitigate the significant manual effort and expertise required to define LTL-encoded tasks, several methods have been proposed for translating Natural Language (NL) instructions into LTL formulas, which, however, lack correctness guarantees. To address this, we introduce a new NL-to-LTL translation method, called ConformalNL2LTL, that can achieve user-defined translation success rates over unseen NL commands. Our method constructs LTL formulas iteratively by addressing a sequence of open-vocabulary Question-Answering (QA) problems with LLMs. To enable uncertainty-aware translation, we leverage conformal prediction (CP), a distribution-free uncertainty quantification tool for black-box models. CP enables our method to assess the uncertainty in LLM-generated answers, allowing it to proceed with translation when sufficiently confident and request help otherwise. We provide both theoretical and empirical results demonstrating that ConformalNL2LTL achieves user-specified translation accuracy while minimizing help rates.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Param$\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost</title>
<link>https://arxiv.org/abs/2504.21023</link>
<guid>https://arxiv.org/abs/2504.21023</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, post-training, knowledge transfer, model development, computational efficiency

Summary: 
$Param\Delta$ introduces a method for streamlining the post-training phase of large language models by transferring knowledge from an existing post-trained model to a newly updated base model without additional training. By calculating the weight difference between post-trained and base models and applying it to the updated base model, $Param\Delta$ equips the new model with post-trained capabilities, achieving performance comparable to direct post-training. The approach was evaluated on LLama3, LLama3.1, Qwen, and DeepSeek-distilled models, showing effective replication of traditional post-training results. For instance, the $Param\Delta$ Model derived from Llama3 models attained approximately 95% of Llama3.1-inst model's performance on average. This cost-free framework accelerates the iterative cycle of model development in the open-weight community, utilizing readily available checkpoints for base and instruct models.  

<br /><br />Summary: <div>
arXiv:2504.21023v1 Announce Type: new 
Abstract: The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces $Param\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with ZERO additional training. By computing the difference between post-trained model weights ($\Theta_\text{post}$) and base model weights ($\Theta_\text{base}$), and adding this to the updated base model ($\Theta'_\text{base}$), we define $Param\Delta$ Model as: $\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} + \Theta'_\text{base}$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively replicates traditional post-training. For example, the $Param\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\% of Llama3.1-inst model's performance on average. $Param\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model</title>
<link>https://arxiv.org/abs/2504.21024</link>
<guid>https://arxiv.org/abs/2504.21024</guid>
<content:encoded><![CDATA[
<div> Keywords: Agent self-improvement, Large Language Model, World Model, web environments, performance gain <br />
Summary: 
Agent self-improvement using Large Language Models (LLMs) in web environments faces a limitation of performance stagnation due to limited exploration and exploitation of web knowledge. To address this, a novel framework is proposed, introducing a co-evolving World Model LLM that predicts next observations and serves as a virtual web server for training data generation and imagination engine during inference. Experiments in real-world web environments demonstrate a 10% performance gain over existing self-evolving agents, highlighting the effectiveness and generalizability of the approach without distillation from more powerful models. Integrating world models into autonomous agent frameworks is shown to be essential for unlocking sustained adaptability. <br /><br />Summary: <div>
arXiv:2504.21024v1 Announce Type: new 
Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh</title>
<link>https://arxiv.org/abs/2504.21025</link>
<guid>https://arxiv.org/abs/2504.21025</guid>
<content:encoded><![CDATA[
<div> Keywords: road accidents, data automation, web scraping, Large Language Models, Bangladesh<br />
<br />
Summary: <br />
Road accidents are a global concern, leading to financial losses, injuries, and societal challenges. The 'Durghotona GPT' framework utilizes web scraping and Large Language Models (LLMs) to automate the collection of accident data from major newspapers in Bangladesh. By using advanced LLMs like GPT-4 and Llama-3, the framework efficiently extracts and categorizes relevant information, overcoming manual data collection limitations. The evaluation shows that Llama-3 performs comparably to GPT-4 with 89% accuracy. The framework aims to enhance the quality and availability of accident data, supporting traffic safety analysis, urban planning, and public health applications. An interface for 'Durghotona GPT' has been developed for ease of use. Future work will focus on expanding data collection methods and improving LLMs for increased accuracy and applicability. <br /> <div>
arXiv:2504.21025v1 Announce Type: new 
Abstract: Road accidents pose significant concerns globally. They lead to large financial losses, injuries, disabilities, and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named 'Durghotona GPT' that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers: Prothom Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework efficiently extracts relevant information, categorizes reports, and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors, and communication gaps. The authors' evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it can be considered a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning, and public health. The authors also developed an interface for 'Durghotona GPT' for ease of use as part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.21026</link>
<guid>https://arxiv.org/abs/2504.21026</guid>
<content:encoded><![CDATA[
<div> Keywords: code-mixed text, abusive language detection, low-resource languages, multilingual social media, NLP<br />
Summary:<br /> 
- The study introduces a dataset of abusive and non-abusive Telugu-English and Nepali-English code-mixed comments from social media platforms.
- Various Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs) are evaluated for detecting abusive language in code-mixed text.
- Models such as Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs are optimized and compared through experiments.
- Challenges in detecting abusive language in code-mixed settings are highlighted, providing insights into the complexities of moderation on multilingual social media platforms.
- The study aims to advance Natural Language Processing (NLP) for low-resource languages by setting benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. <br /> 
Summary: <div>
arXiv:2504.21026v1 Announce Type: new 
Abstract: With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2504.21027</link>
<guid>https://arxiv.org/abs/2504.21027</guid>
<content:encoded><![CDATA[
<div> benchmark, UrbanPlanBench, LLMs, urban planning, UrbanPlanText 

Summary: 
The paper introduces the UrbanPlanBench benchmark to evaluate the effectiveness of Large Language Models (LLMs) in urban planning. It reveals a lack of proficiency among LLMs in acquiring planning knowledge, particularly in understanding planning regulations. The authors present the UrbanPlanText dataset, a supervised fine-tuning dataset comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Fine-tuned models show improved performance in memorization tests and comprehension of urban planning knowledge but still have room for improvement in tasks requiring domain-specific terminology and reasoning. The goal is to integrate LLMs into practical urban planning by fostering a collaboration between human expertise and machine intelligence. The benchmark, dataset, and toolsets are publicly available to encourage further research and development in this area. 

<br /><br />Summary: <div>
arXiv:2504.21027v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing various fields traditionally dominated by human expertise. Urban planning, a professional discipline that fundamentally shapes our daily surroundings, is one such field heavily relying on multifaceted domain knowledge and experience of human experts. The extent to which LLMs can assist human practitioners in urban planning remains largely unexplored. In this paper, we introduce a comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of LLMs in urban planning, which encompasses fundamental principles, professional knowledge, and management and regulations, aligning closely with the qualifications expected of human planners. Through extensive evaluation, we reveal a significant imbalance in the acquisition of planning knowledge among LLMs, with even the most proficient models falling short of meeting professional standards. For instance, we observe that 70% of LLMs achieve subpar performance in understanding planning regulations compared to other aspects. Besides the benchmark, we present the largest-ever supervised fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Our findings demonstrate that fine-tuned models exhibit enhanced performance in memorization tests and comprehension of urban planning knowledge, while there exists significant room for improvement, particularly in tasks requiring domain-specific terminology and reasoning. By making our benchmark, dataset, and associated evaluation and fine-tuning toolsets publicly available at https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the integration of LLMs into practical urban planning, fostering a symbiotic collaboration between human expertise and machine intelligence.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts</title>
<link>https://arxiv.org/abs/2504.21117</link>
<guid>https://arxiv.org/abs/2504.21117</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language generation, evaluation, LLM-based, inversion learning, prompt design

Summary: 
In the field of natural language generation (NLG) systems evaluation, human evaluation faces challenges such as inconsistencies and demographic biases. To address this, LLM-based evaluation methods have been proposed but require careful prompt design. This work introduces an inversion learning method that generates effective evaluation prompts by reverse mapping model outputs to input instructions. By automating prompt generation, this method improves efficiency and eliminates the need for manual prompt engineering. This approach aims to enhance the robustness and efficacy of LLM-based evaluation processes. 
<br /><br />Summary: <div>
arXiv:2504.21117v1 Announce Type: new 
Abstract: Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge</title>
<link>https://arxiv.org/abs/2504.21132</link>
<guid>https://arxiv.org/abs/2504.21132</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, LLM ENHANCER system, data accuracy, external knowledge sources, vector embeddings 

Summary: 
The paper introduces the LLM ENHANCER system, aimed at improving the accuracy of Large Language Models (LLMs) like ChatGPT by leveraging external sources such as Google, Wikipedia, and DuckDuckGo. The system uses open-source LLMs and custom agent tools for data acquisition, ensuring accurate and natural response generation. By utilizing vector embeddings to identify relevant information, the LLM ENHANCER system addresses hallucinations in chat-based LLMs while maintaining response quality. This integration of external knowledge sources enhances the overall performance of LLMs in critical scenarios, where accurate information is crucial. The system's parallel operation streamlines the data flow process, enabling efficient utilization of online resources to supplement LLM capabilities. Overall, the LLM ENHANCER system presents a practical solution for improving the accuracy and reliability of LLM-generated responses. 

<br /><br />Summary: <div>
arXiv:2504.21132v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the capability to generate human like, natural responses across a range of tasks, including task oriented dialogue and question answering. However, their application in real world, critical scenarios is often hindered by a tendency to produce inaccurate information and a limited ability to leverage external knowledge sources. This paper introduces the LLM ENHANCER system, designed to integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to enhance data accuracy. The LLMs employed within this system are open source. The data acquisition process for the LLM ENHANCER system operates in parallel, utilizing custom agent tools to manage the flow of information. Vector embeddings are used to identify the most pertinent information, which is subsequently supplied to the LLM for user interaction. The LLM ENHANCER system mitigates hallucinations in chat based LLMs while preserving response naturalness and accuracy.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Manipulated Contents Using Knowledge-Grounded Inference</title>
<link>https://arxiv.org/abs/2504.21165</link>
<guid>https://arxiv.org/abs/2504.21165</guid>
<content:encoded><![CDATA[
<div> fake news, manipulated content, detection, zero-day, Manicod <br />
Summary:
Manicod is a new tool designed to detect zero-day manipulated content, a prevalent form of fake news that can only be recognized with real-time contextual information. It sources contextual information from search engines, vectorizes it for a large language model (LLM) using retrieval-augmented generation (RAG), and makes "truthful" or "manipulated" decisions with textual explanations. The tool is validated on a dataset of 4270 manipulated fake news pieces and achieves an F1 score of 0.856, outperforming existing methods by up to 1.9x in fact-checking and claim verification benchmarks. Manicod's approach addresses the limitations of current solutions, offering an effective way to identify and analyze manipulated content quickly and accurately in real-time scenarios. <br /> <div>
arXiv:2504.21165v1 Announce Type: new 
Abstract: The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a "truthful" or "manipulated" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare</title>
<link>https://arxiv.org/abs/2504.21191</link>
<guid>https://arxiv.org/abs/2504.21191</guid>
<content:encoded><![CDATA[
<div> classification, language model, finetuning, domain-specific pretraining, small language model

Summary:
- The study compares finetuning versus zero-shot usage for language models in three classification scenarios using pathology reports.
- Finetuning significantly improves Small Language Model (SLM) performance, surpassing zero-shot results.
- Domain-adjacent SLMs perform better than generic SLMs after finetuning, especially on difficult tasks.
- Further domain-specific pretraining offers modest gains on easy tasks and significant improvements on complex, data-scarce tasks.
- While Large Language Models (LLMs) perform well zero-shot, appropriately finetuned SLMs outperform them on specific tasks, showing the continued relevance and effectiveness of SLMs in specialized domains. 

<br /><br />Summary: <div>
arXiv:2504.21191v1 Announce Type: new 
Abstract: This study aims to guide language model selection by investigating: 1) the necessity of finetuning versus zero-shot usage, 2) the benefits of domain-adjacent versus generic pretrained models, 3) the value of further domain-specific pretraining, and 4) the continued relevance of Small Language Models (SLMs) compared to Large Language Models (LLMs) for specific tasks. Using electronic pathology reports from the British Columbia Cancer Registry (BCCR), three classification scenarios with varying difficulty and data size are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning significantly improved SLM performance across all scenarios compared to their zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally performed better than the generic SLM after finetuning, especially on harder tasks. Further domain-specific pretraining yielded modest gains on easier tasks but significant improvements on the complex, data-scarce task. The results highlight the critical role of finetuning for SLMs in specialized domains, enabling them to surpass zero-shot LLM performance on targeted classification tasks. Pretraining on domain-adjacent or domain-specific data provides further advantages, particularly for complex problems or limited finetuning data. While LLMs offer strong zero-shot capabilities, their performance on these specific tasks did not match that of appropriately finetuned SLMs. In the era of LLMs, SLMs remain relevant and effective, offering a potentially superior performance-resource trade-off compared to LLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Legal Writing Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2504.21202</link>
<guid>https://arxiv.org/abs/2504.21202</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Legal Writing, Benchmark, Brazilian Bar Examination, Automated Evaluation

Summary: 
The paper introduces oab-bench, a benchmark consisting of 105 questions from the Brazilian Bar Examination to evaluate language models in the legal writing domain. Four LLMs were tested on this benchmark, with Claude-3.5 Sonnet achieving the highest average score. The study also explored the potential of LLMs as automated judges for legal writing assessment and found that models like OpenAI's o1 showed strong correlation with human scores. The results indicate that LLMs have the capability to serve as reliable automated evaluators, despite the subjective nature of legal writing evaluation. The benchmark, including questions, guidelines, model responses, and automated evaluations, is publicly available, allowing for further research and development in this area.

<br /><br />Summary: The paper presents oab-bench, a benchmark from the Brazilian Bar Examination, to assess language models in legal writing. Evaluation of four LLMs showed Claude-3.5 Sonnet performing best. The study also investigated the use of LLMs as automated judges for legal writing, with models like OpenAI's o1 showing strong correlation with human scores. This suggests the potential for LLMs as reliable evaluators in the legal domain. The publicly available benchmark provides a comprehensive resource for further research and development. <div>
arXiv:2504.21202v1 Announce Type: new 
Abstract: Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining Large Brain Language Model for Active BCI: Silent Speech</title>
<link>https://arxiv.org/abs/2504.21214</link>
<guid>https://arxiv.org/abs/2504.21214</guid>
<content:encoded><![CDATA[
<div> dataset, EEG recordings, silent speech decoding, brain-computer interface, language model pretraining<br />
<br />
Summary: <br />
This paper discusses the use of silent speech decoding in active brain-computer interface systems, utilizing a new dataset of EEG recordings for language model pretraining. The authors propose a Large Brain Language Model (LBLM) pretrained using the Future Spectro-Temporal Prediction (FSTP) paradigm to improve EEG classification performance. The FSTP method captures both temporal and spectral dependencies from EEG signals through autoregressive modeling. The LBLM is then finetuned on word-level and semantic-level classification tasks, showing significant performance improvements over baseline models. In challenging cross-session scenarios, the LBLM achieves 47.0% accuracy in semantic-level classification and 39.6% in word-level classification, outperforming existing methods. This research contributes to the advancement of silent speech decoding in active BCI systems, introducing an innovative approach to EEG language model pretraining and providing a valuable new dataset for further research. <br /> <div>
arXiv:2504.21214v1 Announce Type: new 
Abstract: This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\% accuracy on semantic-level classification and 39.6\% in word-level classification, outperforming baseline methods by 5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math</title>
<link>https://arxiv.org/abs/2504.21233</link>
<guid>https://arxiv.org/abs/2504.21233</guid>
<content:encoded><![CDATA[
<div> chain-of-thought, large language models, small language models, reasoning, training recipe <br />
<br />
Summary: <br />
The study introduces a systematic training recipe to enhance reasoning capabilities in Small Language Models (SLMs). By following four key steps, including mid-training on distilled long-CoT data, fine-tuning on high-quality CoT data, Rollout DPO with preference dataset, and Reinforcement Learning with Verifiable Reward, the method was applied to a compact 3.8B-parameter model called Phi-4-Mini. The results show that Phi-4-Mini-Reasoning model outperformed larger reasoning models on math reasoning tasks, demonstrating the effectiveness of the training recipe in improving reasoning abilities in resource-constrained small models. <div>
arXiv:2504.21233v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization and Knowledge Injection in Gated LLMs</title>
<link>https://arxiv.org/abs/2504.21239</link>
<guid>https://arxiv.org/abs/2504.21239</guid>
<content:encoded><![CDATA[
<div> Memory Embedded, Gated LLMs, Continual Learning, Event Memories, Catastrophic Forgetting 
Summary: 
Memory Embedded in Gated LLMs (MEGa) is a new framework designed to address the limitations of Large Language Models in sequentially adding new memories and integrating new knowledge. The model stores event memories directly in the weights of LLMs, using a gating mechanism to activate relevant memory weights during inference. This approach allows the model to recall entire memories and answer related questions, outperforming baseline approaches on two datasets - fictional characters and Wikipedia events. MEGa is inspired by the human brain's complementary memory system, enabling it to better simulate the continuous learning process observed in everyday life. <div>
arXiv:2504.21239v1 Announce Type: new 
Abstract: Large Language Models (LLMs) currently struggle to sequentially add new memories and integrate new knowledge. These limitations contrast with the human ability to continuously learn from new experiences and acquire knowledge throughout life. Most existing approaches add memories either through large context windows or external memory buffers (e.g., Retrieval-Augmented Generation), and studies on knowledge injection rarely test scenarios resembling everyday life events. In this work, we introduce a continual learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event memories directly into the weights of LLMs. Each memory is stored in a dedicated set of gated low-rank weights. During inference, a gating mechanism activates relevant memory weights by matching query embeddings to stored memory embeddings. This enables the model to both recall entire memories and answer related questions. On two datasets - fictional characters and Wikipedia events - MEGa outperforms baseline approaches in mitigating catastrophic forgetting. Our model draws inspiration from the complementary memory system of the human brain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA</title>
<link>https://arxiv.org/abs/2504.21252</link>
<guid>https://arxiv.org/abs/2504.21252</guid>
<content:encoded><![CDATA[
<div> Medical question answering; retrieval-augmented generation; Discuss-RAG; human-like reasoning; collaborative agent-based reasoning

Summary:<br />
1. Medical question answering (QA) is a challenging task for large language models (LLMs) due to hallucinations and outdated domain knowledge.<br />
2. Retrieval-Augmented Generation (RAG) tackles this by leveraging external knowledge, but existing medical RAG systems lack human-like reasoning behaviors during information retrieval and often retrieve irrelevant content.<br />
3. Discuss-RAG introduces a summarizer agent to coordinate medical experts in multi-turn brainstorming, improving relevance of retrieved content, and a decision-making agent to evaluate snippets before integration.<br />
4. Experimental results on four benchmark medical QA datasets show Discuss-RAG outperforms MedRAG, significantly improving answer accuracy on BioASQ and PubMedQA.<br />
5. Discuss-RAG code is available at https://github.com/LLM-VLM-GSL/Discuss-RAG.<br /> 

Summary: <br />
Medical question answering is challenging for large language models due to hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG) helps by incorporating external knowledge, but existing medical RAG systems have limitations. Discuss-RAG introduces a summarizer agent for better information retrieval and a decision-making agent to evaluate retrieved snippets. Experimental results show Discuss-RAG outperforms existing methods, significantly improving answer accuracy. The code for Discuss-RAG is available on GitHub. <div>
arXiv:2504.21252v1 Announce Type: new 
Abstract: Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models</title>
<link>https://arxiv.org/abs/2504.21299</link>
<guid>https://arxiv.org/abs/2504.21299</guid>
<content:encoded><![CDATA[
<div> Keywords: BiasGuard, bias detection, LLMs, fairness specifications, reinforcement learning

Summary:
BiasGuard is a new tool designed to detect bias in content generated by large language models (LLMs). It addresses limitations of existing methods by explicitly analyzing inputs and reasoning through fairness specifications to make accurate judgments. Utilizing a two-stage approach, BiasGuard first initializes the model to reason based on fairness specifications, and then leverages reinforcement learning to enhance its judgment capabilities. Experiments across five datasets show that BiasGuard outperforms existing tools in accuracy and reducing over-fairness misjudgments. The importance of reasoning-enhanced decision-making is highlighted, along with evidence of the effectiveness of the two-stage optimization pipeline.

<br /><br />Summary: <div>
arXiv:2504.21299v1 Announce Type: new 
Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges</title>
<link>https://arxiv.org/abs/2504.21303</link>
<guid>https://arxiv.org/abs/2504.21303</guid>
<content:encoded><![CDATA[
<div> Bayesian approach, Large language models, probabilistic inference, model ranking, GPT-series models <br />
<br />
Summary: This study introduces a Bayesian approach for evaluating Large Language Models (LLMs) that considers their probabilistic output characteristics. By treating model capabilities as latent variables and utilizing a curated query set, the proposed method formulates model ranking as a Bayesian hypothesis testing problem. Experimental results with GPT-series models show superior discrimination compared to conventional evaluation methods. The approach maintains statistical robustness even with reduced sample sizes and provides actionable insights, including probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by integrating Bayesian inference with real-world deployment constraints. <div>
arXiv:2504.21303v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit probabilistic output characteristics, yet conventional evaluation frameworks rely on deterministic scalar metrics. This study introduces a Bayesian approach for LLM capability assessment that integrates prior knowledge through probabilistic inference, addressing limitations under limited-sample regimes. By treating model capabilities as latent variables and leveraging a curated query set to induce discriminative responses, we formalize model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals. Experimental evaluations with GPT-series models demonstrate that the proposed method achieves superior discrimination compared to conventional evaluation methods. Results indicate that even with reduced sample sizes, the approach maintains statistical robustness while providing actionable insights, such as probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?</title>
<link>https://arxiv.org/abs/2504.21330</link>
<guid>https://arxiv.org/abs/2504.21330</guid>
<content:encoded><![CDATA[
<div> predictive power, bias, scoring, demographic attributes, language models

Summary:
- Prompt-based tools like ChatGPT are used in Automated Essay Scoring (AES) for their accessibility.
- Bias in fine-tuned Large Language Models (LLMs) has been found, especially against disadvantaged groups.
- The study investigates if biases persist or are amplified in the prompt-based paradigm with cutting-edge tools.
- LLMs can somewhat infer students' demographic attributes, particularly their first-language backgrounds, from their essays using prompts.
- Scoring biases are more pronounced when the LLM correctly predicts students' first-language background than when it does not, impacting scoring outcomes.
- Scoring error for non-native English speakers increases when the LLM correctly identifies them as non-native.<br /><br />Summary: <div>
arXiv:2504.21330v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES) due to their ability to capture semantic meaning. Traditional fine-tuning approaches required technical expertise, limiting accessibility for educators with limited technical backgrounds. However, prompt-based tools like ChatGPT have made AES more accessible, enabling educators to obtain machine-generated scores using natural-language prompts (i.e., the prompt-based paradigm). Despite advancements, prior studies have shown bias in fine-tuned LLMs, particularly against disadvantaged groups. It remains unclear whether such biases persist or are amplified in the prompt-based paradigm with cutting-edge tools. Since such biases are believed to stem from the demographic information embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to predict demographic attributes), this study explores the relationship between the model's predictive power of students' demographic attributes based on their written works and its predictive bias in the scoring task in the prompt-based paradigm. Using a publicly available dataset of over 25,000 students' argumentative essays, we designed prompts to elicit demographic inferences (i.e., gender, first-language background) from GPT-4o and assessed fairness in automated scoring. Then we conducted multivariate regression analysis to explore the impact of the model's ability to predict demographics on its scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat infer students' demographics, particularly their first-language backgrounds, from their essays; (ii) scoring biases are more pronounced when the LLM correctly predicts students' first-language background than when it does not; and (iii) scoring error for non-native English speakers increases when the LLM correctly identifies them as non-native.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction</title>
<link>https://arxiv.org/abs/2504.21372</link>
<guid>https://arxiv.org/abs/2504.21372</guid>
<content:encoded><![CDATA[
<div> Keyword: Speech Event Extraction, Automatic Speech Recognition, Natural Language Processing, Large Language Models, Hybrid filtering mechanism

Summary:
Speech Event Extraction (SpeechEE) is a complex task that combines Automatic Speech Recognition (ASR) and Natural Language Processing (NLP. A modular pipeline-based framework is presented for SpeechEE, integrating ASR with Large Language Models (LLMs) enhanced by semantic search. The system uses a hybrid filtering mechanism to classify speech segments likely to contain events and employs LLM prompting enriched via semantic similarity retrieval to identify event triggers and extract arguments. Performance evaluation using multiple LLMs shows significant gains, with o1-mini achieving high F1 scores for trigger and argument classification, surpassing previous benchmarks. The results demonstrate that pipeline approaches with retrieval-augmented LLMs can rival or surpass end-to-end systems while maintaining interpretability and modularity. This work provides insights into LLM-driven event extraction and paves the way for hybrid models combining textual and acoustic features.<br /><br />Summary: <div>
arXiv:2504.21372v1 Announce Type: new 
Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors</title>
<link>https://arxiv.org/abs/2504.21421</link>
<guid>https://arxiv.org/abs/2504.21421</guid>
<content:encoded><![CDATA[
<div> dependency distance, hierarchical distance, valency, mean dependency distance, mean hierarchical distance 

Summary:
The study investigates the relationship between dependency distance (DD) and hierarchical distance (HD in Japanese by analyzing the Balanced Corpus of Contemporary Written Japanese. The research compares the probability distributions of DD and HD with and without fixed sentence length to explore the impact of sentence length on mean dependency distance (MDD) and mean hierarchical distance (MHD). The findings suggest that the valency of predicates plays a crucial role in influencing the trade-off relation between MDD and MHD in Japanese. Native speakers of Japanese manage linear and hierarchical complexity through predicate valency, with differences in MDD and MHD depending on valency thresholds. The valency of predicates also influences the probability distributions of DD and HD, with a greater impact on HD distribution. These insights highlight how cognitive load and valency of predicates contribute to the structural properties of sentences in Japanese. 

<br /><br />Summary: <div>
arXiv:2504.21421v1 Announce Type: new 
Abstract: To explore the relationship between dependency distance (DD) and hierarchical distance (HD) in Japanese, we compared the probability distributions of DD and HD with and without sentence length fixed, and analyzed the changes in mean dependency distance (MDD) and mean hierarchical distance (MHD) as sentence length increases, along with their correlation coefficient based on the Balanced Corpus of Contemporary Written Japanese. It was found that the valency of the predicates is the underlying factor behind the trade-off relation between MDD and MHD in Japanese. Native speakers of Japanese regulate the linear complexity and hierarchical complexity through the valency of the predicates, and the relative sizes of MDD and MHD depend on whether the threshold of valency has been reached. Apart from the cognitive load, the valency of the predicates also affects the probability distributions of DD and HD. The effect of the valency of the predicates on the distribution of HD is greater than on that of DD, which leads to differences in their probability distributions and causes the mean of MDD to be lower than that of MHD.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RWKV-X: A Linear Complexity Hybrid Language Model</title>
<link>https://arxiv.org/abs/2504.21463</link>
<guid>https://arxiv.org/abs/2504.21463</guid>
<content:encoded><![CDATA[
<div> Keywords: RWKV-X, hybrid architecture, long-range context, language modeling, efficiency

Summary: 
RWKV-X is introduced as a novel hybrid architecture combining RWKV efficiency for short-range modeling with a sparse attention mechanism for capturing long-range context. Unlike previous approaches with quadratic complexity, RWKV-X achieves linear-time complexity in training and constant-time complexity in inference decoding. Through continual pretraining on 64K-token sequences, RWKV-X achieves near-perfect accuracy on the 64K passkey retrieval benchmark and outperforms prior RWKV-7 models on long-context benchmarks while maintaining strong performance on short-context tasks. This showcases RWKV-X as a scalable and efficient language modeling backbone, capable of decoding sequences up to 1 million tokens with stable speed and memory usage. Checkpoints and code are available for further research and analysis. <br /><br />Summary: <div>
arXiv:2504.21463v1 Announce Type: new 
Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that combines the efficiency of RWKV for short-range modeling with a sparse attention mechanism designed to capture long-range context. Unlike previous hybrid approaches that rely on full attention layers and retain quadratic complexity, RWKV-X achieves linear-time complexity in training and constant-time complexity in inference decoding. We demonstrate that RWKV-X, when continually pretrained on 64K-token sequences, achieves near-perfect accuracy on the 64K passkey retrieval benchmark. It consistently outperforms prior RWKV-7 models on long-context benchmarks, while maintaining strong performance on short-context tasks. These results highlight RWKV-X as a scalable and efficient backbone for general-purpose language modeling, capable of decoding sequences up to 1 million tokens with stable speed and memory usage. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at: https://github.com/howard-hou/RWKV-X.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging</title>
<link>https://arxiv.org/abs/2504.21474</link>
<guid>https://arxiv.org/abs/2504.21474</guid>
<content:encoded><![CDATA[
<div> Keywords: Homa, SemEval-2025, Subject Tagging, OntoAligner, GND taxonomy

Summary:
Homa is introduced as a system for SemEval-2025 Task 5: Subject Tagging, focusing on assigning subject labels to technical records from TIBKAT using the GND taxonomy. The system leverages OntoAligner, utilizing retrieval-augmented generation techniques to match records to GND categories based on semantic similarity. The approach frames subject tagging as an alignment task and evaluates OntoAligner's adaptability and effectiveness in handling multilingual records. Experimental results demonstrate the method's strengths and limitations, showcasing the potential of alignment techniques in enhancing subject tagging within digital libraries. <div>
arXiv:2504.21474v1 Announce Type: new 
Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines</title>
<link>https://arxiv.org/abs/2504.21475</link>
<guid>https://arxiv.org/abs/2504.21475</guid>
<content:encoded><![CDATA[
<div> transformer-based approach, Arabic Reverse Dictionary (RD) system, neural network architecture, dataset construction, ARBERTv2

Summary:<br />
- The study addresses the gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary using a novel transformer-based approach.
- The semi-encoder neural network architecture with geometrically decreasing layers achieves state-of-the-art results for Arabic RD tasks.
- The methodology includes dataset construction and formal quality standards for Arabic lexicographic definitions.
- Experiments show that Arabic-specific models outperform general multilingual embeddings, with ARBERTv2 performing the best.
- The study provides a formal abstraction of the reverse dictionary task, a modular Python library (RDTL), and insights for improving Arabic definition quality. 

Summary: <div>
arXiv:2504.21475v1 Announce Type: new 
Abstract: This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Informally Romanized Language Identification</title>
<link>https://arxiv.org/abs/2504.21540</link>
<guid>https://arxiv.org/abs/2504.21540</guid>
<content:encoded><![CDATA[
<div> language identification, Latin script, romanization, spelling variability, synthetic data 
<br />
Increased accuracy in language identification for romanized text is achieved by improving the methods of synthesizing training sets. The Latin script is often used to informally write languages with non-Latin native scripts, leading to high spelling variability and confusability between languages such as Hindi and Urdu. Training on synthetic samples that incorporate natural spelling variation yields higher accuracy in language identification systems compared to using naturally occurring examples or higher capacity models. The study demonstrates new state-of-the-art language identification performance on romanized text from 20 Indic languages, achieving test F1 scores of 85.4% with a linear classifier trained solely on synthetic data and 88.2% when training on available harvested text.
<br /><br />Summary: <div>
arXiv:2504.21540v1 Announce Type: new 
Abstract: The Latin script is often used to informally write languages with non-Latin native scripts. In many cases (e.g., most languages in India), there is no conventional spelling of words in the Latin script, hence there will be high spelling variability in written text. Such romanization renders languages that are normally easily distinguished based on script highly confusable, such as Hindi and Urdu. In this work, we increase language identification (LID) accuracy for romanized text by improving the methods used to synthesize training sets. We find that training on synthetic samples which incorporate natural spelling variation yields higher LID system accuracy than including available naturally occurring examples in the training set, or even training higher capacity models. We demonstrate new state-of-the-art LID performance on romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a pretrained neural model) to 85.4% using a linear classifier trained solely on synthetic data and 88.2% when also training on available harvested text.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval</title>
<link>https://arxiv.org/abs/2504.21547</link>
<guid>https://arxiv.org/abs/2504.21547</guid>
<content:encoded><![CDATA[
<div> Keywords: librarians, subject tags, information retrieval, encoder models, SemEval-2025

Summary:
Our submission for SemEval-2025 Task 5 focuses on assisting librarians in assigning subject tags to library records by generating a list of likely relevant tags for a given document. The task is framed as an information retrieval problem utilising document content to extract subject tags from a large taxonomy. We employ two types of encoder models in a two-stage information retrieval system: a bi-encoder for candidate extraction and a cross-encoder for re-ranking. This approach improves recall significantly compared to single-stage methods and yields competitive results in qualitative evaluation. The system showcases the efficacy of leveraging encoder models for subject tag assignment in libraries, demonstrating the potential for more accurate and efficient tagging processes in library systems. 

<br /><br />Summary: Our submission for SemEval-2025 Task 5 focuses on aiding librarians in assigning subject tags by employing information retrieval techniques and encoder models. The two-stage system utilizing bi-encoder and cross-encoder models demonstrates enhanced recall and competitive performance in tag assignment compared to single-stage methods. This approach highlights the potential for leveraging advanced technology to streamline and improve subject tagging processes in libraries, benefiting librarians in efficiently categorizing and organizing library records. <div>
arXiv:2504.21547v1 Announce Type: new 
Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid librarians in assigning subject tags to the library records by producing a list of likely relevant tags for a given document. We frame the task as an information retrieval problem, where the document content is used to retrieve subject tags from a large subject taxonomy. We leverage two types of encoder models to build a two-stage information retrieval system -- a bi-encoder for coarse-grained candidate extraction at the first stage, and a cross-encoder for fine-grained re-ranking at the second stage. This approach proved effective, demonstrating significant improvements in recall compared to single-stage methods and showing competitive results according to qualitative evaluation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models</title>
<link>https://arxiv.org/abs/2504.21553</link>
<guid>https://arxiv.org/abs/2504.21553</guid>
<content:encoded><![CDATA[
<div> LLMs, quantization, LLaMA architecture, activation outliers, mixed-precision quantization

Summary:
This paper explores the quantization of Large Language Models (LLMs), particularly focusing on the LLaMA architecture and its variations. The study challenges conventional beliefs about activation outliers in LLMs and proposes a novel mixed-precision quantization technique tailored for LLaMA-like models. By identifying specific projection layers with concentration of activation spikes, the approach applies higher precision to these layers while quantizing the rest of the model to lower bit-widths, achieving improved performance in perplexity and zero-shot accuracy, especially for 8-bit per-tensor quantization. Experimental results on LLaMA2, LLaMA3, and Mistral models showcase significant enhancements compared to existing quantization methods. The research underscores the significance of considering model-specific attributes in developing efficient quantization pipelines for cutting-edge language models, emphasizing the advantages of architecture-specific quantization strategies. This contributes to the ongoing goal of enhancing the efficiency and deployability of LLMs for potential use in resource-constrained environments. 

<br /><br />Summary: <div>
arXiv:2504.21553v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, their size presents significant challenges for deployment and inference. This paper investigates the quantization of LLMs, focusing on the LLaMA architecture and its derivatives. We challenge existing assumptions about activation outliers in LLMs and propose a novel mixed-precision quantization approach tailored for LLaMA-like models. Our method leverages the observation that activation spikes in LLaMA architectures are predominantly concentrated in specific projection layers. By applying higher precision (FP16 or FP8) to these layers while quantizing the rest of the model to lower bit-widths, we achieve superior performance compared to existing quantization techniques. Experimental results on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in perplexity and zero-shot accuracy, particularly for 8-bit per-tensor quantization. Our approach outperforms general-purpose methods designed to handle outliers across all architecture types, highlighting the benefits of architecture-specific quantization strategies. This research contributes to the ongoing efforts to make LLMs more efficient and deployable, potentially enabling their use in resource-constrained environments. Our findings emphasize the importance of considering model-specific characteristics in developing effective quantization pipelines for state-of-the-art language models by identifying and targeting a small number of projections that concentrate activation spikes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing</title>
<link>https://arxiv.org/abs/2504.21589</link>
<guid>https://arxiv.org/abs/2504.21589</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025 Task 5, LLM-based Automated Subject Tagging, National Technical Library, Open-Access Catalog, few-shot prompting technique

Summary:
Our system developed for the SemEval-2025 Task 5, titled LLMs4Subjects, focuses on LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. The system utilizes a few-shot prompting technique by providing LLMs with examples of intellectually annotated records and prompting them to suggest keywords for new records. Post-processing steps are then applied to map generated keywords to the target vocabulary, aggregate subject terms, and rank them based on relevance to the record. While our system ranks fourth in the quantitative assessment in the all-subjects track, it excels in the qualitative evaluation conducted by subject indexing experts, obtaining the best result. This approach highlights the effectiveness of combining machine learning and expert knowledge to improve subject tagging accuracy in library catalogs. The system's success underscores the importance of collaboration between automated techniques and human expertise in information organization tasks. 

Summary: <div>
arXiv:2504.21589v1 Announce Type: new 
Abstract: This paper presents our system developed for the SemEval-2025 Task 5: LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records. This few-shot prompting technique is combined with a series of post-processing steps that map the generated keywords to the target vocabulary, aggregate the resulting subject terms to an ensemble vote and, finally, rank them as to their relevance to the record. Our system is fourth in the quantitative ranking in the all-subjects track, but achieves the best result in the qualitative ranking conducted by subject indexing experts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Misinformation Detection by Visiting Potential Commonsense Conflict</title>
<link>https://arxiv.org/abs/2504.21604</link>
<guid>https://arxiv.org/abs/2504.21604</guid>
<content:encoded><![CDATA[
<div> Misinformation Detection, Internet technology, common sense conflict, augmentation method, CoMis dataset

Summary:
Misinformation Detection (MD) is a key research area due to the prevalence of misinformation online. A novel augmentation method, MD with Potential Commonsense Conflict (MD-PCC), is proposed to enhance MD tasks. Fake articles often involve commonsense conflict, which is used to generate commonsense expressions for articles. A new dataset, CoMis, specifically focuses on fake articles caused by commonsense conflict. MD-PCC is integrated with existing MD methods and shown to outperform baseline models on various datasets. This approach highlights the importance of considering common sense in detecting misinformation. <div>
arXiv:2504.21604v1 Announce Type: new 
Abstract: The development of Internet technology has led to an increased prevalence of misinformation, causing severe negative effects across diverse domains. To mitigate this challenge, Misinformation Detection (MD), aiming to detect online misinformation automatically, emerges as a rapidly growing research topic in the community. In this paper, we propose a novel plug-and-play augmentation method for the MD task, namely Misinformation Detection with Potential Commonsense Conflict (MD-PCC). We take inspiration from the prior studies indicating that fake articles are more likely to involve commonsense conflict. Accordingly, we construct commonsense expressions for articles, serving to express potential commonsense conflicts inferred by the difference between extracted commonsense triplet and golden ones inferred by the well-established commonsense reasoning tool COMET. These expressions are then specified for each article as augmentation. Any specific MD methods can be then trained on those commonsense-augmented articles. Besides, we also collect a novel commonsense-oriented dataset named CoMis, whose all fake articles are caused by commonsense conflict. We integrate MD-PCC with various existing MD backbones and compare them across both 4 public benchmark datasets and CoMis. Empirical results demonstrate that MD-PCC can consistently outperform the existing MD baselines.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations</title>
<link>https://arxiv.org/abs/2504.21605</link>
<guid>https://arxiv.org/abs/2504.21605</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, RDF-based framework, multilingual assessment, knowledge conflicts, context prioritization

Summary: 
- The article introduces a new framework based on RDF to assess the quality of multilingual Large Language Models (LLMs) by focusing on knowledge conflicts.
- The framework evaluates LLM responses under four context conditions to identify knowledge leakage and ensure multilingual consistency.
- A fire safety domain experiment is conducted to demonstrate the framework's effectiveness in analyzing context prioritization and language-specific performance.
- The experiment highlights critical patterns in how LLMs process context information and perform across different languages.
- The study shows that the proposed vocabulary successfully captures all assessment facets encountered in the experiment, indicating the framework's comprehensive coverage of multilingual LLM assessment. 

<br /><br />Summary: <div>
arXiv:2504.21605v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability</title>
<link>https://arxiv.org/abs/2504.21625</link>
<guid>https://arxiv.org/abs/2504.21625</guid>
<content:encoded><![CDATA[
<div> keywords: LLMs, instruction-following, benchmark, Meeseeks, evaluation system <br />
Summary: <br />
Large Language Models (LLMs) rely on accurate instruction-following abilities for real-world applications. Existing benchmarks lack the ability for models to self-correct, unlike Meeseeks, which simulates human-LLM interactions through iterative feedback. Meeseeks features a comprehensive evaluation system with 38 capability tags in three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. By testing various LLMs, Meeseeks offers insights into their instruction-following capabilities for practical usage. <div>
arXiv:2504.21625v1 Announce Type: new 
Abstract: The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. While existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction, Meeseeks simulates realistic human-LLM interactions through an iterative feedback process. This design enables models to self-correct based on specific requirement failures, better reflecting real-world user-end usage patterns. The benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in practical applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sadeed: Advancing Arabic Diacritization Through Small Language Model</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic text diacritization, language model, Sadeed, benchmarking, NLP<br />
Summary: <br />
- The paper introduces Sadeed, a new approach for Arabic text diacritization based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara. 
- Sadeed is fine-tuned on high-quality diacritized datasets created through a rigorous data-cleaning and normalization process. 
- Despite using limited computational resources, Sadeed achieves competitive results compared to large language models and traditional models in the same domain. 
- The paper highlights limitations in current benchmarking practices for Arabic diacritization and proposes SadeedDiac-25, a new benchmark for more comprehensive evaluation across text genres and complexity levels. 
- Sadeed and SadeedDiac-25 together provide a strong foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools. <br /> 
Summary: <div>
arXiv:2504.21635v1 Announce Type: new 
Abstract: Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>20min-XD: A Comparable Corpus of Swiss News Articles</title>
<link>https://arxiv.org/abs/2504.21677</link>
<guid>https://arxiv.org/abs/2504.21677</guid>
<content:encoded><![CDATA[
<div> Keywords: 20min-XD, French-German, cross-lingual, document-level, news articles

Summary:
The article introduces 20min-XD, a French-German document-level comparable corpus sourced from the Swiss news outlet 20 Minuten/20 minutes. The dataset consists of 15,000 article pairs aligned based on semantic similarity from 2015 to 2024. The data collection process and alignment methodology are explained in detail. A qualitative and quantitative analysis of the corpus is provided, revealing a wide range of cross-lingual similarity levels. This diversity makes the dataset valuable for various NLP applications and linguistic studies. The dataset is publicly available in document- and sentence-aligned versions, along with code for conducting the described experiments. <div>
arXiv:2504.21677v1 Announce Type: new 
Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a French-German, document-level comparable corpus of news articles, sourced from the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises around 15,000 article pairs spanning 2015 to 2024, automatically aligned based on semantic similarity. We detail the data collection process and alignment methodology. Furthermore, we provide a qualitative and quantitative analysis of the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual similarity, ranging from near-translations to loosely related articles, making it valuable for various NLP applications and broad linguistically motivated studies. We publicly release the dataset in document- and sentence-aligned versions and code for the described experiments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders</title>
<link>https://arxiv.org/abs/2504.21681</link>
<guid>https://arxiv.org/abs/2504.21681</guid>
<content:encoded><![CDATA[
<div> transfer learning, multilingual, Vision-Language, parallel data, domain

Summary:
The study explores the transfer of already trained encoders using parallel data in multilingual Vision-Language (VL) tasks. It analyzes the impact of parallel data in terms of domain and the number of languages, which has been overlooked in previous research. Results indicate that authentic parallel data resembling captions outperforms machine-translated task data in some languages. Additionally, the study demonstrates that multilingual training benefits most languages. The findings suggest that leveraging parallel data for transferring trained encoders can be an effective strategy for multilingual VL tasks. Overall, the research sheds light on the importance of considering the characteristics of parallel data in improving the performance of multilingual VL models for downstream tasks. 

Summary: <div>
arXiv:2504.21681v1 Announce Type: new 
Abstract: Most pre-trained Vision-Language (VL) models and training data for the downstream tasks are only available in English. Therefore, multilingual VL tasks are solved using cross-lingual transfer: fine-tune a multilingual pre-trained model or transfer the text encoder using parallel data. We study the alternative approach: transferring an already trained encoder using parallel data. We investigate the effect of parallel data: domain and the number of languages, which were out of focus in previous work. Our results show that even machine-translated task data are the best on average, caption-like authentic parallel data outperformed it in some languages. Further, we show that most languages benefit from multilingual training.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning</title>
<link>https://arxiv.org/abs/2504.21685</link>
<guid>https://arxiv.org/abs/2504.21685</guid>
<content:encoded><![CDATA[
<div> POS tagger, PEFT techniques, biomedical NLP, health mention classification, social media<br />
<br />
Summary: 
Health Mention Classification (HMC) in social media posts is crucial for real-time public health monitoring but faces challenges due to contextual nuances. This study proposes improving HMC through enhanced parameters in biomedical NLP methods. By incorporating POS tagger information and leveraging PEFT techniques, the study achieves better F1-score performance on three datasets (RHDM, PHM, Illness) compared to existing methods. The findings indicate the effectiveness of these enhancements in accurately classifying health mentions while optimizing model size and training efficiency. This suggests a promising approach for enhancing HMC in social media for public health monitoring. <div>
arXiv:2504.21685v1 Announce Type: new 
Abstract: Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models</title>
<link>https://arxiv.org/abs/2504.21742</link>
<guid>https://arxiv.org/abs/2504.21742</guid>
<content:encoded><![CDATA[
<div> Keywords: Greek fiction, love novels, romances, literary motifs, large language models

Summary: 
The study examines Greek fictional narratives, specifically love novels and romances, from the first century CE to the 15th century. Using large language models, the research aims to identify common motifs in these texts and how they differ. Results reveal persistent motifs and fluctuations in frequency, indicating trends and external influences. The method effectively extracts literary motifs for quantitative and qualitative analysis. By analyzing these motifs, the study provides insights into the similarities and differences within the corpus, shedding light on the evolving nature of Greek fiction over time.<br /><br />Summary: <div>
arXiv:2504.21742v1 Announce Type: new 
Abstract: The Greek fictional narratives often termed love novels or romances, ranging from the first century CE to the middle of the 15th century, have long been considered as similar in many ways, not least in the use of particular literary motifs. By applying the use of fine-tuned large language models, this study aims to investigate which motifs exactly that the texts in this corpus have in common, and in which ways they differ from each other. The results show that while some motifs persist throughout the corpus, others fluctuate in frequency, indicating certain trends or external influences. Conclusively, the method proves to adequately extract literary motifs according to a set definition, providing data for both quantitative and qualitative analyses.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data</title>
<link>https://arxiv.org/abs/2504.21747</link>
<guid>https://arxiv.org/abs/2504.21747</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented neural machine translation, cross-lingual retrieval systems, bilingual corpora, translation memories, target monolingual resources<br />
<br />
Summary: 
This paper presents a novel approach to enhance neural machine translation systems by leveraging in-domain monolingual target-side corpora for cross-lingual retrieval. By developing improved cross-lingual retrieval systems trained with sentence and word-level matching objectives, the researchers achieved superior translation performance compared to traditional translation memory-based models. In controlled experiments with two RANMT architectures, the benefits of cross-lingual objectives were evident, surpassing standard TM-based models. Real-world applications demonstrated significant improvements when target monolingual resources were abundant. The new techniques outperformed baseline settings and general-purpose cross-lingual retrievers, showcasing the effectiveness of utilizing target language resources for enhancing translation quality. <br /><br />Summary: <div>
arXiv:2504.21747v1 Announce Type: new 
Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many settings, in-domain monolingual target-side corpora are often available. This work explores ways to take advantage of such resources by retrieving relevant segments directly in the target language, based on a source-side query. For this, we design improved cross-lingual retrieval systems, trained with both sentence level and word-level matching objectives. In our experiments with two RANMT architectures, we first demonstrate the benefits of such cross-lingual objectives in a controlled setting, obtaining translation performances that surpass standard TM-based models. We then showcase our method on a real-world set-up, where the target monolingual resources far exceed the amount of parallel data and observe large improvements of our new techniques, which outperform both the baseline setting, and general-purpose cross-lingual retrievers.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness</title>
<link>https://arxiv.org/abs/2504.21773</link>
<guid>https://arxiv.org/abs/2504.21773</guid>
<content:encoded><![CDATA[
<div> hallucination, large language models, confidence estimation, multiple problems, MAC-Tuning <br />
Summary:<br />
The article introduces a novel method called Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning) to address the issue of hallucination in large language models (LLMs) when generating non-existing facts. While previous research has focused on single problem settings, the more challenging multi-problem setting, where LLMs need to answer multiple problems accurately simultaneously, has been overlooked. MAC-Tuning separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Through extensive experiments, it was shown that MAC-Tuning outperforms baselines by up to 25% in average precision. This approach improves LLM awareness of its internal parameterized knowledge boundary, enhancing confidence estimation and performance in answering multiple problems. <div>
arXiv:2504.21773v1 Announce Type: new 
Abstract: With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, WebThinker, Deep Web Explorer, Autonomous Think-Search-and-Draft strategy, RL-based training strategy

Summary:
Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have impressive long-horizon reasoning capabilities but struggle with complex tasks that require dynamic web information synthesis. To address this, WebThinker is proposed as a deep research agent enabling LRMs to autonomously search, navigate the web, and draft research reports. It integrates a Deep Web Explorer module for dynamic information extraction and an Autonomous Think-Search-and-Draft strategy for real-time reasoning and report writing. An RL-based training strategy using Direct Preference Optimization enhances research tool utilization. Extensive experiments on reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report tasks (Glaive) show that WebThinker outperforms existing methods and proprietary systems. This approach enhances LRM reliability in complex scenarios, paving the way for more capable deep research systems. 

<br /><br />Summary: <div>
arXiv:2504.21776v1 Announce Type: new 
Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues</title>
<link>https://arxiv.org/abs/2504.21800</link>
<guid>https://arxiv.org/abs/2504.21800</guid>
<content:encoded><![CDATA[
<div> privacy concerns, synthetic data, Prolonged Exposure, Post-Traumatic Stress Disorder, clinical fidelity 

Summary:
The article discusses the use of synthetic data in healthcare, particularly in training clinical models for Prolonged Exposure therapy for PTSD. It compares real and synthetic dialogues using various metrics and introduces PE-specific metrics for assessing clinical fidelity. While synthetic data can address data scarcity and privacy concerns, it may struggle to capture the nuances of therapeutic interactions. Synthetic dialogues in the dataset matched structural features of real-world dialogues but lacked key fidelity markers like distress monitoring. The study highlights the need for fidelity-aware metrics beyond surface fluency to identify clinically significant shortcomings. The findings emphasize the potential benefits of synthetic data while also highlighting critical limitations that need to be addressed. 

<br /><br />Summary: <div>
arXiv:2504.21800v1 Announce Type: new 
Abstract: The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition</title>
<link>https://arxiv.org/abs/2504.21801</link>
<guid>https://arxiv.org/abs/2504.21801</guid>
<content:encoded><![CDATA[
<div> DeepSeek-Prover-V2, large language model, formal theorem proving, Lean 4, DeepSeek-V3
<br />
Summary:
DeepSeek-Prover-V2 is a new open-source large language model for formal theorem proving in Lean 4. It incorporates informal and formal mathematical reasoning in its training process by decomposing complex problems into subgoals and synthesizing proofs. The model, DeepSeek-Prover-V2-671B, demonstrates state-of-the-art performance in neural theorem proving, achieving an 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. A new benchmark, ProverBench, consisting of 325 formalized problems including selected AIME competition problems, is introduced for evaluation. The model successfully solves 6 out of 15 AIME problems, narrowing the gap between formal and informal mathematical reasoning in large language models. DeepSeek-V3, the precursor, also performs well on these problems, solving 8 using majority voting. This research demonstrates significant progress in leveraging large language models for formal theorem proving tasks. 
<br /><br />Summary: <div>
arXiv:2504.21801v1 Announce Type: new 
Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments</title>
<link>https://arxiv.org/abs/2504.21851</link>
<guid>https://arxiv.org/abs/2504.21851</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, TRUST framework, dialogue systems, mental healthcare, diagnostic interviews<br />
Summary:<br />
The study introduces TRUST, a framework that utilizes Large Language Models (LLMs) to conduct diagnostic interviews and assessments for PTSD. By implementing a Dialogue Acts schema specifically designed for clinical interviews, TRUST replicates clinician behavior effectively. The system also incorporates a patient simulation approach based on real-life interview transcripts, eliminating the need for manual testing by clinicians. Evaluation metrics demonstrate that TRUST performs comparably to real-life clinical interviews, indicating its potential to improve mental healthcare accessibility. Expert evaluations suggest that the system operates at the level of average clinicians, showcasing room for future enhancements in communication styles and response appropriateness. Overall, the TRUST framework shows promise in facilitating mental healthcare availability. <br /><br />Summary: <div>
arXiv:2504.21851v1 Announce Type: new 
Abstract: Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval</title>
<link>https://arxiv.org/abs/2504.21015</link>
<guid>https://arxiv.org/abs/2504.21015</guid>
<content:encoded><![CDATA[
<div> Keywords: dense retrieval models, Large Language Model (LLM), corpus-free negative generation, BM25, cross-encoders (CE)

Summary: 
This paper presents a novel approach to training dense retrieval models using a Large Language Model (LLM) to generate hard negative examples without the need for full corpus access. The proposed pipeline, called LLM Query -> LLM HN, outperforms traditional methods like LLM Query -> BM25 HN and LLM Query -> CE HN in terms of nDCG@10, Precision@10, and Recall@100 metrics using E5-Base and GTE-Base models on various benchmark datasets. The corpus-free negative generation method proves to be as effective as complex, corpus-dependent mining techniques while simplifying the training process and improving efficiency. The dataset containing queries and hard negatives for all three methods is publicly available. This approach offers a promising pathway for training high-performance retrievers without compromising on results. 

<br /><br />Summary: <div>
arXiv:2504.21015v1 Announce Type: cross 
Abstract: Training effective dense retrieval models often relies on hard negative (HN) examples mined from the document corpus via methods like BM25 or cross-encoders (CE), processes that can be computationally demanding and require full corpus access. This paper introduces a different approach, an end-to-end pipeline where a Large Language Model (LLM) first generates a query from a passage, and then generates a hard negative example using \emph{only} that query text. This corpus-free negative generation contrasts with standard mining techniques. We evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query $\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several BEIR benchmark datasets. Our results show the proposed all-LLM pipeline achieves performance identical to both the BM25 and the computationally intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics. This demonstrates that our corpus-free negative generation method matches the effectiveness of complex, corpus-dependent mining techniques, offering a potentially simpler and more efficient pathway for training high-performance retrievers without sacrificing results. We make the dataset including the queries and the hard-negatives for all three methods publicly available https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage</title>
<link>https://arxiv.org/abs/2504.21035</link>
<guid>https://arxiv.org/abs/2504.21035</guid>
<content:encoded><![CDATA[
<div> Keywords: privacy, text data, re-identification attacks, auxiliary information, sanitization techniques<br />
<br />
Summary: 
The study challenges the effectiveness of traditional methods in sanitizing sensitive text data to protect privacy. It introduces a new framework that evaluates re-identification attacks to measure individual privacy risks accurately. The research reveals that seemingly insignificant auxiliary information can be utilized to infer sensitive attributes from sanitized data, undermining the perceived privacy protection. The study demonstrates that popular PII removal tools fail to safeguard a significant portion of information, emphasizing the false sense of privacy provided by current sanitization techniques. While differential privacy offers some mitigation, it compromises the utility of sanitized text for practical applications. The findings underscore the necessity for more robust methods to prevent semantic-level information leakage and ensure comprehensive privacy protection in text data. <br /><br />Summary: <div>
arXiv:2504.21035v1 Announce Type: cross 
Abstract: Sanitizing sensitive text data typically involves removing personally identifiable information (PII) or generating synthetic data under the assumption that these methods adequately protect privacy; however, their effectiveness is often only assessed by measuring the leakage of explicit identifiers but ignoring nuanced textual markers that can lead to re-identification. We challenge the above illusion of privacy by proposing a new framework that evaluates re-identification attacks to quantify individual privacy risks upon data release. Our approach shows that seemingly innocuous auxiliary information -- such as routine social activities -- can be used to infer sensitive attributes like age or substance use history from sanitized data. For instance, we demonstrate that Azure's commercial PII removal tool fails to protect 74\% of information in the MedQA dataset. Although differential privacy mitigates these risks to some extent, it significantly reduces the utility of the sanitized text for downstream tasks. Our findings indicate that current sanitization techniques offer a \textit{false sense of privacy}, highlighting the need for more robust methods that protect against semantic-level information leakage.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Medicine: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.21051</link>
<guid>https://arxiv.org/abs/2504.21051</guid>
<content:encoded><![CDATA[
arXiv:2504.21051v1 Announce Type: cross 
Abstract: MLLMs have recently become a focal point in the field of artificial intelligence research. Building on the strong capabilities of LLMs, MLLMs are adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs have gained substantial attention from different domains. Researchers have begun to explore the potential of MLLMs in the medical and healthcare domain. In this paper, we first introduce the background and fundamental concepts related to LLMs and MLLMs, while emphasizing the working principles of MLLMs. Subsequently, we summarize three main directions of application within healthcare: medical reporting, medical diagnosis, and medical treatment. Our findings are based on a comprehensive review of 330 recent papers in this area. We illustrate the remarkable capabilities of MLLMs in these domains by providing specific examples. For data, we present six mainstream modes of data along with their corresponding evaluation benchmarks. At the end of the survey, we discuss the challenges faced by MLLMs in the medical and healthcare domain and propose feasible methods to mitigate or overcome these issues.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phi-4-reasoning Technical Report</title>
<link>https://arxiv.org/abs/2504.21318</link>
<guid>https://arxiv.org/abs/2504.21318</guid>
<content:encoded><![CDATA[
arXiv:2504.21318v1 Announce Type: cross 
Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Callback? Generative AI and Gender Bias</title>
<link>https://arxiv.org/abs/2504.21400</link>
<guid>https://arxiv.org/abs/2504.21400</guid>
<content:encoded><![CDATA[
arXiv:2504.21400v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI), particularly large language models (LLMs), is being rapidly deployed in recruitment and for candidate shortlisting. We audit several mid-sized open-source LLMs for gender bias using a dataset of 332,044 real-world online job postings. For each posting, we prompt the model to recommend whether an equally qualified male or female candidate should receive an interview callback. We find that most models tend to favor men, especially for higher-wage roles. Mapping job descriptions to the Standard Occupational Classification system, we find lower callback rates for women in male-dominated occupations and higher rates in female-associated ones, indicating occupational segregation. A comprehensive analysis of linguistic features in job ads reveals strong alignment of model recommendations with traditional gender stereotypes. To examine the role of recruiter identity, we steer model behavior by infusing Big Five personality traits and simulating the perspectives of historical figures. We find that less agreeable personas reduce stereotyping, consistent with an agreeableness bias in LLMs. Our findings highlight how AI-driven hiring may perpetuate biases in the labor market and have implications for fairness and diversity within firms.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
arXiv:2504.21435v1 Announce Type: cross 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \textbf{series}. To address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on \textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2504.21559</link>
<guid>https://arxiv.org/abs/2504.21559</guid>
<content:encoded><![CDATA[
arXiv:2504.21559v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks</title>
<link>https://arxiv.org/abs/2504.21578</link>
<guid>https://arxiv.org/abs/2504.21578</guid>
<content:encoded><![CDATA[
arXiv:2504.21578v1 Announce Type: cross 
Abstract: Diabetes is a civilization chronic disease characterized by a constant elevated concentration of glucose in the blood. Many processes are involved in the glucose regulation, and their interactions are very complex. To better understand those processes we set ourselves a goal to create a Petri net model of the glucose regulation in the whole body. So far we have managed to create a model of glycolysis and synthesis of glucose in the liver, and the general overview models of the glucose regulation in a healthy and diabetic person. In this paper we introduce Petri nets models of insulin secretion in beta cell of the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have mutually opposite effects: insulin preventing hyperglycemia, and glucagon preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon secretion constitutes the basis for understanding diabetes. We also present a model in which both processes occur together, depending on the blood glucose level. The dynamics of each model is analysed. Additionally, we transform the overall insulin and glucagon secretion system to a Boolean network, following standard transformation rules.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
arXiv:2504.21659v1 Announce Type: cross 
Abstract: Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</title>
<link>https://arxiv.org/abs/2504.21716</link>
<guid>https://arxiv.org/abs/2504.21716</guid>
<content:encoded><![CDATA[
arXiv:2504.21716v1 Announce Type: cross 
Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation</title>
<link>https://arxiv.org/abs/2504.21751</link>
<guid>https://arxiv.org/abs/2504.21751</guid>
<content:encoded><![CDATA[
arXiv:2504.21751v1 Announce Type: cross 
Abstract: Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
arXiv:2504.21798v1 Announce Type: cross 
Abstract: Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection</title>
<link>https://arxiv.org/abs/2310.18964</link>
<guid>https://arxiv.org/abs/2310.18964</guid>
<content:encoded><![CDATA[
arXiv:2310.18964v4 Announce Type: replace 
Abstract: In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. Ordinary least squares analyses suggest that the advantage of training with fine-grained hate speech labels is washed away with the increase in dataset size. While our research demonstrates the potential of large language models (LLMs) for hate speech detection, several limitations remain, particularly regarding the validity and the reproducibility of the results. We conclude with an exhaustive discussion of the challenges we faced in our experimentation and offer recommended best practices for future scholars designing benchmarking experiments of this kind.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Round Trip Translation Defence against Large Language Model Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2402.13517</link>
<guid>https://arxiv.org/abs/2402.13517</guid>
<content:encoded><![CDATA[
arXiv:2402.13517v2 Announce Type: replace 
Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence
  This version of the article has been accepted for publication, after peer review (when applicable) but is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record is available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this Accepted Version is subject to the publisher's Accepted Manuscript terms of use https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs</title>
<link>https://arxiv.org/abs/2404.19442</link>
<guid>https://arxiv.org/abs/2404.19442</guid>
<content:encoded><![CDATA[
arXiv:2404.19442v5 Announce Type: replace 
Abstract: Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a spoken language until recently, there are some online platforms (e.g., Wikipedia), publishing in written Naija as well. West African Pidgin English (WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the internet to a wider audience not only in Nigeria but also in other West African countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine Translation experiments, our paper shows that these two pidgin varieties do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on WAPE. In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples. In addition to the statistical analyses, we also provide historical information on both pidgins as well as insights from the interviews conducted with volunteer Wikipedia contributors in Naija.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of a High-Dimensional Abstraction Phase in Language Transformers</title>
<link>https://arxiv.org/abs/2405.15471</link>
<guid>https://arxiv.org/abs/2405.15471</guid>
<content:encoded><![CDATA[
arXiv:2405.15471v4 Announce Type: replace 
Abstract: A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models</title>
<link>https://arxiv.org/abs/2410.07825</link>
<guid>https://arxiv.org/abs/2410.07825</guid>
<content:encoded><![CDATA[
arXiv:2410.07825v2 Announce Type: replace 
Abstract: Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may be not available for low-resource languages. To solve it, we propose a Multi-lingual Ability Extraction and Transfer approach, named as MAET. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and transfer them across different languages by simple addition and subtraction operations without training. Specially, our MAET consists of the extraction and transfer stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-specific weights. In the transfer stage, we further select the ability-related parameter tensors, and design the merging strategy based on the linguistic and ability specific weights, to build the multi-lingual ability-enhanced LLM. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on mathematical and scientific tasks in both high-resource lingual and low-resource lingual scenarios. Experiment results have shown that MAET can effectively and efficiently extract and transfer the advanced abilities, and outperform training-based baseline methods. Our code and data are available at https://github.com/RUCAIBox/MAET.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent</title>
<link>https://arxiv.org/abs/2410.16658</link>
<guid>https://arxiv.org/abs/2410.16658</guid>
<content:encoded><![CDATA[
arXiv:2410.16658v3 Announce Type: replace 
Abstract: Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent's performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities</title>
<link>https://arxiv.org/abs/2501.00571</link>
<guid>https://arxiv.org/abs/2501.00571</guid>
<content:encoded><![CDATA[
arXiv:2501.00571v3 Announce Type: replace 
Abstract: Document-level relation extraction (Doc-RE) aims to extract relations between entities across multiple sentences. Therefore, Doc-RE requires more comprehensive reasoning abilities like humans, involving complex cross-sentence interactions between entities, contexts, and external general knowledge, compared to the sentence-level RE. However, most existing Doc-RE methods focus on optimizing single reasoning ability, but lack the ability to utilize external knowledge for comprehensive reasoning on long documents. To solve these problems, a knowledge retrieval augmented method, named KnowRA, was proposed with comprehensive reasoning to autonomously determine whether to accept external knowledge to assist DocRE. Firstly, we constructed a document graph for semantic encoding and integrated the co-reference resolution model to augment the co-reference reasoning ability. Then, we expanded the document graph into a document knowledge graph by retrieving the external knowledge base for common-sense reasoning and a novel knowledge filtration method was presented to filter out irrelevant knowledge. Finally, we proposed the axis attention mechanism to build direct and indirect associations with intermediary entities for achieving cross-sentence logical reasoning. Extensive experiments conducted on two datasets verified the effectiveness of our method compared to the state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/KnowRA.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification</title>
<link>https://arxiv.org/abs/2502.11258</link>
<guid>https://arxiv.org/abs/2502.11258</guid>
<content:encoded><![CDATA[
arXiv:2502.11258v2 Announce Type: replace 
Abstract: Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice</title>
<link>https://arxiv.org/abs/2503.04785</link>
<guid>https://arxiv.org/abs/2503.04785</guid>
<content:encoded><![CDATA[
arXiv:2503.04785v2 Announce Type: replace 
Abstract: The rapid proliferation of Large Language Models (LLMs) has raised pressing concerns regarding their trustworthiness, spanning issues of reliability, transparency, fairness, and ethical alignment. Despite the increasing adoption of LLMs across various domains, there remains a lack of consensus on how to operationalize trustworthiness in practice. This study bridges the gap between theoretical discussions and implementation by conducting a bibliometric mapping analysis of 2,006 publications from 2019 to 2025. Through co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, we identify key research trends, influential authors, and prevailing definitions of LLM trustworthiness. Additionally, a systematic review of 68 core papers is conducted to examine conceptualizations of trust and their practical implications. Our findings reveal that trustworthiness in LLMs is often framed through existing organizational trust frameworks, emphasizing dimensions such as ability, benevolence, and integrity. However, a significant gap exists in translating these principles into concrete development strategies. To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, and post-training audits. By synthesizing bibliometric insights with practical strategies, this study contributes towards fostering more transparent, accountable, and ethically aligned LLMs, ensuring their responsible deployment in real-world applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System</title>
<link>https://arxiv.org/abs/2503.14258</link>
<guid>https://arxiv.org/abs/2503.14258</guid>
<content:encoded><![CDATA[
arXiv:2503.14258v3 Announce Type: replace 
Abstract: This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: https://github.com/oneal2000/JuDGE.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad</title>
<link>https://arxiv.org/abs/2503.21934</link>
<guid>https://arxiv.org/abs/2503.21934</guid>
<content:encoded><![CDATA[
arXiv:2503.21934v4 Announce Type: replace 
Abstract: Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge</title>
<link>https://arxiv.org/abs/2504.10342</link>
<guid>https://arxiv.org/abs/2504.10342</guid>
<content:encoded><![CDATA[
arXiv:2504.10342v3 Announce Type: replace 
Abstract: Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding</title>
<link>https://arxiv.org/abs/2301.11564</link>
<guid>https://arxiv.org/abs/2301.11564</guid>
<content:encoded><![CDATA[
arXiv:2301.11564v3 Announce Type: replace-cross 
Abstract: Robotic grasping is a fundamental ability for a robot to interact with the environment. Current methods focus on how to obtain a stable and reliable grasping pose in object level, while little work has been studied on part (shape)-wise grasping which is related to fine-grained grasping and robotic affordance. Parts can be seen as atomic elements to compose an object, which contains rich semantic knowledge and a strong correlation with affordance. However, lacking a large part-wise 3D robotic dataset limits the development of part representation learning and downstream applications. In this paper, we propose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to promote 3D part-level affordance and grasping ability learning. From the perspective of robotic cognition, we design a two-stage fine-grained robotic grasping framework (named LangPartGPD), including a novel 3D part language grounding model and a part-aware grasp pose detection model, in which explicit language input from human or large language models (LLMs) could guide a robot to generate part-level 6-DoF grasping pose with textual explanation. Our method combines the advantages of human-robot collaboration and LLMs' planning ability using explicit language as a symbolic intermediate. To evaluate the effectiveness of our proposed method, we perform 3D part grounding and fine-grained grasp detection experiments on both simulation and physical robot settings, following language instructions across different degrees of textual complexity. Results show our method achieves competitive performance in 3D geometry fine-grained grounding, object affordance inference, and 3D part-aware grasping tasks. Our dataset and code are available on our project website https://sites.google.com/view/lang-shape
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignLLM: Sign Language Production Large Language Models</title>
<link>https://arxiv.org/abs/2405.10718</link>
<guid>https://arxiv.org/abs/2405.10718</guid>
<content:encoded><![CDATA[
arXiv:2405.10718v3 Announce Type: replace-cross 
Abstract: In this paper, we propose SignLLM, a multilingual Sign Language Production (SLP) large language model, which includes two novel multilingual SLP modes MLSF and Prompt2LangGloss that allow sign language gestures generation from query texts input and question-style prompts input respectively. Both modes can use a new RL loss based on reinforcement learning and a new RL module named Priority Learning Channel. These RL components can accelerate the training by enhancing the model's capability to sample high-quality data. To train SignLLM, we introduce Prompt2Sign, a comprehensive multilingual sign language dataset, which builds from public data, including American Sign Language (ASL) and seven others. This dataset standardizes information by extracting pose information from sign language videos into a unified compressed format. We extensively evaluate SignLLM, demonstrating that our model achieves state-of-the-art performance on SLP tasks across eight sign languages.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes</title>
<link>https://arxiv.org/abs/2408.05794</link>
<guid>https://arxiv.org/abs/2408.05794</guid>
<content:encoded><![CDATA[
arXiv:2408.05794v2 Announce Type: replace-cross 
Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance</title>
<link>https://arxiv.org/abs/2409.15545</link>
<guid>https://arxiv.org/abs/2409.15545</guid>
<content:encoded><![CDATA[
arXiv:2409.15545v3 Announce Type: replace-cross 
Abstract: The complex nature of musical emotion introduces inherent bias in both recognition and generation, particularly when relying on a single audio encoder, emotion classifier, or evaluation metric. In this work, we conduct a study on Music Emotion Recognition (MER) and Emotional Music Generation (EMG), employing diverse audio encoders alongside Frechet Audio Distance (FAD), a reference-free evaluation metric. Our study begins with a benchmark evaluation of MER, highlighting the limitations of using a single audio encoder and the disparities observed across different measurements. We then propose assessing MER performance using FAD derived from multiple encoders to provide a more objective measure of musical emotion. Furthermore, we introduce an enhanced EMG approach designed to improve both the variability and prominence of generated musical emotion, thereby enhancing its realism. Additionally, we investigate the differences in realism between the emotions conveyed in real and synthetic music, comparing our EMG model against two baseline models. Experimental results underscore the issue of emotion bias in both MER and EMG and demonstrate the potential of using FAD and diverse audio encoders to evaluate musical emotion more objectively and effectively.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction</title>
<link>https://arxiv.org/abs/2409.15551</link>
<guid>https://arxiv.org/abs/2409.15551</guid>
<content:encoded><![CDATA[
arXiv:2409.15551v2 Announce Type: replace-cross 
Abstract: Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models</title>
<link>https://arxiv.org/abs/2409.16920</link>
<guid>https://arxiv.org/abs/2409.16920</guid>
<content:encoded><![CDATA[
arXiv:2409.16920v2 Announce Type: replace-cross 
Abstract: Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling</title>
<link>https://arxiv.org/abs/2409.16937</link>
<guid>https://arxiv.org/abs/2409.16937</guid>
<content:encoded><![CDATA[
arXiv:2409.16937v3 Announce Type: replace-cross 
Abstract: The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations</title>
<link>https://arxiv.org/abs/2409.17899</link>
<guid>https://arxiv.org/abs/2409.17899</guid>
<content:encoded><![CDATA[
arXiv:2409.17899v2 Announce Type: replace-cross 
Abstract: Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Construct Random Unitaries</title>
<link>https://arxiv.org/abs/2410.10116</link>
<guid>https://arxiv.org/abs/2410.10116</guid>
<content:encoded><![CDATA[
arXiv:2410.10116v2 Announce Type: replace-cross 
Abstract: The existence of pseudorandom unitaries (PRUs) -- efficient quantum circuits that are computationally indistinguishable from Haar-random unitaries -- has been a central open question, with significant implications for cryptography, complexity theory, and fundamental physics. In this work, we close this question by proving that PRUs exist, assuming that any quantum-secure one-way function exists. We establish this result for both (1) the standard notion of PRUs, which are secure against any efficient adversary that makes queries to the unitary $U$, and (2) a stronger notion of PRUs, which are secure even against adversaries that can query both the unitary $U$ and its inverse $U^\dagger$. In the process, we prove that any algorithm that makes queries to a Haar-random unitary can be efficiently simulated on a quantum computer, up to inverse-exponential trace distance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2411.08165</link>
<guid>https://arxiv.org/abs/2411.08165</guid>
<content:encoded><![CDATA[
arXiv:2411.08165v2 Announce Type: replace-cross 
Abstract: The Knowledge Graph Completion~(KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose KGR3, a context-enriched framework for KGC. KGR3 is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that KGR3 consistently improves various KGC methods. Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of 12.3% and 5.6% on the FB15k237 and WN18RR datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</title>
<link>https://arxiv.org/abs/2411.16508</link>
<guid>https://arxiv.org/abs/2411.16508</guid>
<content:encoded><![CDATA[
arXiv:2411.16508v3 Announce Type: replace-cross 
Abstract: Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Board Games by External and Internal Planning with Language Models</title>
<link>https://arxiv.org/abs/2412.12119</link>
<guid>https://arxiv.org/abs/2412.12119</guid>
<content:encoded><![CDATA[
arXiv:2412.12119v2 Announce Type: replace-cross 
Abstract: Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in internal search, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title>
<link>https://arxiv.org/abs/2412.19723</link>
<guid>https://arxiv.org/abs/2412.19723</guid>
<content:encoded><![CDATA[
arXiv:2412.19723v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
arXiv:2502.05439v2 Announce Type: replace-cross 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training</title>
<link>https://arxiv.org/abs/2502.12734</link>
<guid>https://arxiv.org/abs/2502.12734</guid>
<content:encoded><![CDATA[
arXiv:2502.12734v2 Announce Type: replace-cross 
Abstract: Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 10 text perturbation strategies and 6 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches. Codes and dataset are available in https://github.com/Liyuuuu111/GREATER.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations</title>
<link>https://arxiv.org/abs/2502.16949</link>
<guid>https://arxiv.org/abs/2502.16949</guid>
<content:encoded><![CDATA[
arXiv:2502.16949v3 Announce Type: replace-cross 
Abstract: Knowledge graph (KG) learning offers a powerful framework for generating new knowledge and making inferences. Training KG embedding can take a significantly long time, especially for larger datasets. Our analysis shows that the gradient computation of embedding is one of the dominant functions in the translation-based KG embedding training loop. We address this issue by replacing the core embedding computation with SpMM (Sparse-Dense Matrix Multiplication) kernels. This allows us to unify multiple scatter (and gather) operations as a single operation, reducing training time and memory usage. We create a general framework for training KG models using sparse kernels and implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on the GPU with a significantly low GPU memory footprint. The speedups are consistent across large and small datasets for a given model. Our proposed sparse approach can be extended to accelerate other translation-based (such as TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE, etc.) models as well. An implementation of the SpTransX framework is publicly available as a Python package in https://github.com/HipGraph/SpTransX.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Code-Edit Embedding to Model Student Debugging Behavior</title>
<link>https://arxiv.org/abs/2502.19407</link>
<guid>https://arxiv.org/abs/2502.19407</guid>
<content:encoded><![CDATA[
arXiv:2502.19407v2 Announce Type: replace-cross 
Abstract: Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language</title>
<link>https://arxiv.org/abs/2503.01453</link>
<guid>https://arxiv.org/abs/2503.01453</guid>
<content:encoded><![CDATA[
arXiv:2503.01453v2 Announce Type: replace-cross 
Abstract: Most existing works in image caption synthesis use computation heavy deep neural networks and generates image descriptions in English language. This often restricts this important assistive tool for widespread use across language and accessibility barriers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy deep network components with lightweight alternatives. The AC-Lite model is designed through extensive ablation experiments with different image feature extractor networks and language decoders. A combination of ShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention is found to provide the best performance with minimum compute. AC-Lite was observed to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs and 22.87M parameters.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban Computing in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2504.02009</link>
<guid>https://arxiv.org/abs/2504.02009</guid>
<content:encoded><![CDATA[
arXiv:2504.02009v2 Announce Type: replace-cross 
Abstract: Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs' functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's the same but not the same: Do LLMs distinguish Spanish varieties?</title>
<link>https://arxiv.org/abs/2504.20049</link>
<guid>https://arxiv.org/abs/2504.20049</guid>
<content:encoded><![CDATA[
<div> variabilidad, lenguaje modelos, espa\ñol, identificar, morfosintácticas

Summary: 
- Large language models (LLMs) have shown proficiency in understanding and generating Spanish text.
- Spanish exhibits diatopic variations across regions, making it a rich and diverse language.
- Nine language models were tested on identifying morphosyntactic and lexical differences in seven Spanish varieties.
- Peninsular Spanish was most accurately identified by all models, with GPT-4o showing the best recognition of Spanish language variability.
- The study highlights the importance of considering regional variations in language analysis and model development. <div>
arXiv:2504.20049v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have demonstrated a high capacity for understanding and generating text in Spanish. However, with five hundred million native speakers, Spanish is not a homogeneous language but rather one rich in diatopic variations spanning both sides of the Atlantic. For this reason, in this study, we evaluate the ability of nine language models to identify and distinguish the morphosyntactic and lexical peculiarities of seven varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American and Rioplatense) through a multiple-choice test. The results indicate that the Peninsular Spanish variety is the best identified by all models and that, among them, GPT-4o is the only model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos, la espa\~nola no es una lengua homog\'enea, sino rica en variedades diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello, evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de siete variedades de espa\~nol (andino, antillano, caribe\~no continental, chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense) mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que la variedad de espa\~nol peninsular es la mejor identificada por todos los modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar la variabilidad de la lengua espa\~nola.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts</title>
<link>https://arxiv.org/abs/2504.20051</link>
<guid>https://arxiv.org/abs/2504.20051</guid>
<content:encoded><![CDATA[
<div> Keywords: multiword expressions, language models, ambiguity, idiomatic, semantic tasks

Summary: 
Language models, despite their strong performance in various tasks, face challenges in handling nuanced language such as multiword expressions. These expressions have non-compositional meanings and can be used both literally and idiomatically, leading to changes in meaning. The study evaluated state-of-the-art models' ability to process potentially idiomatic multiword expressions, particularly in less frequent contexts. The evaluation was conducted in Portuguese, Galician, and English using a novel code-switched dataset and task. The results showed that large language models, including GPT-4, struggled to outperform xlm-roBERTa-base baselines in detecting and understanding these expressions, especially in novel tasks. Despite the similarities to existing tasks, the models performed poorly on the new tasks introduced, indicating a continued challenge in handling ambiguous multiword expressions. <div>
arXiv:2504.20051v1 Announce Type: new 
Abstract: Multiword expressions, characterised by non-compositional meanings and syntactic irregularities, are an example of nuanced language. These expressions can be used literally or idiomatically, leading to significant changes in meaning. While large language models have demonstrated strong performance across many tasks, their ability to handle such linguistic subtleties remains uncertain. Therefore, this study evaluates how state-of-the-art language models process the ambiguity of potentially idiomatic multiword expressions, particularly in contexts that are less frequent, where models are less likely to rely on memorisation. By evaluating models across in Portuguese and Galician, in addition to English, and using a novel code-switched dataset and a novel task, we find that large language models, despite their strengths, struggle with nuanced language. In particular, we find that the latest models, including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both detection and semantic tasks, with especially poor performance on the novel tasks we introduce, despite its similarity to existing tasks. Overall, our results demonstrate that multiword expressions, especially those which are ambiguous, continue to be a challenge to models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Risks of Generative AI in Financial Services</title>
<link>https://arxiv.org/abs/2504.20086</link>
<guid>https://arxiv.org/abs/2504.20086</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, content safety, financial services, risk taxonomy, guardrail solutions

Summary:<br />
The paper discusses the importance of defining acceptable inputs and outputs for Generative AI products, highlighting the need for assessing content safety in the financial services domain. While academic work often focuses on general aspects like toxicity and bias, the paper emphasizes the significance of considering specialized domains subject to legal and regulatory scrutiny. It introduces an AI content risk taxonomy specific to financial services and evaluates existing technical guardrail solutions in detecting these risks. The analysis reveals that current guardrails fall short in addressing the identified content risks, emphasizing the need for improved tools and strategies to mitigate potential violations and protect stakeholders. <div>
arXiv:2504.20086v1 Announce Type: new 
Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a "safe" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward-based alignment, large language models, Meta Policy Optimization, meta-reward model, prompt engineering <br />
Summary: <br />
The article introduces Meta Policy Optimization (MPO) as a solution to challenges faced by reward-based alignment methods for large language models (LLMs). MPO addresses limitations such as vulnerability to reward hacking and the need for labor-intensive prompt engineering by incorporating a meta-reward model that dynamically adjusts the reward model's prompt during training. This adaptive approach ensures a stable policy optimization and reduces the reliance on manual prompt design while maintaining performance comparable to models with hand-crafted reward prompts. MPO's effectiveness is demonstrated across various tasks like question answering and mathematical reasoning without the need for task-specific reward designs. The meta-learning formulation of MPO makes it easily extensible to higher-level alignment frameworks, offering a more robust and adaptable strategy for reward-based RL alignment in LLMs. The code and models developed using MPO will be shared publicly. <br /> <div>
arXiv:2504.20157v1 Announce Type: new 
Abstract: Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools</title>
<link>https://arxiv.org/abs/2504.20168</link>
<guid>https://arxiv.org/abs/2504.20168</guid>
<content:encoded><![CDATA[
<div> Keywords: tool-using agents, model confidence, safety, interpretability, MICE<br />
Summary:<br />
The study introduces a novel approach, Model-Internal Confidence Estimators (MICE), to enhance confidence assessment in tool-using agents. MICE utilizes logitLens to decode from language model layers and determine confidence by comparing generated outputs. In experiments on the simulated trial and error dataset with Llama3 models, MICE outperforms baselines in expected calibration error and enhances tool-calling utility significantly. MICE is also demonstrated to be sample-efficient, generalize to unseen APIs, and improve tool-calling utility across varying risk levels. The open-source code for MICE is available on GitHub, showcasing its practical application in real-world scenarios. <br /> <div>
arXiv:2504.20168v1 Announce Type: new 
Abstract: Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports</title>
<link>https://arxiv.org/abs/2504.20220</link>
<guid>https://arxiv.org/abs/2504.20220</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic health records, paper documents, checkbox data, OCR, vision-language models

Summary:
An open-source pipeline has been developed to extract and categorize checkbox data from scanned paper documents. The manual transcription process, still prevalent in healthcare due to the use of paper documents, is time-consuming and error-prone. This pipeline, designed for transfusion reaction reports but adaptable to other checkbox-rich document types, integrates checkbox detection, multilingual OCR, and vision-language models. The method achieves high precision and recall compared to gold-standards from 2017 to 2024. By automating this process, the pipeline reduces administrative workload and ensures accurate regulatory reporting. The availability of this open-source tool promotes self-hosted parsing of checkbox forms, contributing to the streamlining of healthcare processes. 

<br /><br />Summary: <div>
arXiv:2504.20220v1 Announce Type: new 
Abstract: Despite the growing adoption of electronic health records, many processes still rely on paper documents, reflecting the heterogeneous real-world conditions in which healthcare is delivered. The manual transcription process is time-consuming and prone to errors when transferring paper-based data to digital formats. To streamline this workflow, this study presents an open-source pipeline that extracts and categorizes checkbox data from scanned documents. Demonstrated on transfusion reaction reports, the design supports adaptation to other checkbox-rich document types. The proposed method integrates checkbox detection, multilingual optical character recognition (OCR) and multilingual vision-language models (VLMs). The pipeline achieves high precision and recall compared against annually compiled gold-standards from 2017 to 2024. The result is a reduction in administrative workload and accurate regulatory reporting. The open-source availability of this pipeline encourages self-hosted parsing of checkbox forms.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Platform for Generating Educational Activities to Teach English as a Second Language</title>
<link>https://arxiv.org/abs/2504.20251</link>
<guid>https://arxiv.org/abs/2504.20251</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, educational activities, English as a foreign language, image generation, text generation

Summary: The platform presented in this paper focuses on creating educational activities for teaching English as a foreign language using Natural Language Processing techniques. It offers pre-made games as well as the ability to generate more complex activities from teacher-entered texts. The platform is currently expanding to include image and text generation. To enhance its capabilities, the platform is being migrated to a more powerful server. The development, deployment, challenges faced, and future work plans of the platform are discussed in detail. <div>
arXiv:2504.20251v1 Announce Type: new 
Abstract: We present a platform for the generation of educational activities oriented to teaching English as a foreign language. The different activities --games and language practice exercises-- are strongly based on Natural Language Processing techniques. The platform offers the possibility of playing out-of-the-box games, generated from resources created semi-automatically and then manually curated. It can also generate games or exercises of greater complexity from texts entered by teachers, providing a stage of review and edition of the generated content before use. As a way of expanding the variety of activities in the platform, we are currently experimenting with image and text generation. In order to integrate them and improve the performance of other neural tools already integrated, we are working on migrating the platform to a more powerful server. In this paper we describe the development of our platform and its deployment for end users, discussing the challenges faced and how we overcame them, and also detail our future work plans.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi</title>
<link>https://arxiv.org/abs/2504.20276</link>
<guid>https://arxiv.org/abs/2504.20276</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-4, Kimi, Large Language Models, systematic reviews, assessment<br />
<br />
Summary: 
This study examined the use of GPT-4 and Kimi, two Large Language Models (LLMs), for conducting systematic reviews. The researchers assessed the performance of the LLMs by comparing the codes generated by them with those generated by humans in a peer-reviewed systematic review related to assessment. The study revealed that the effectiveness of LLMs varies based on the amount of data available and the complexity of the research questions in systematic reviews. The findings indicate that LLMs like GPT-4 and Kimi can play a valuable role in assisting with systematic reviews, but their performance may be influenced by factors such as the volume of data and the complexity of the research topic. These results provide insights into the potential applications and limitations of LLMs in the field of systematic reviews. <br /><br />Summary: <div>
arXiv:2504.20276v1 Announce Type: new 
Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs), for systematic reviews. We evaluated their performance by comparing LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment. Our findings suggested that the performance of LLMs fluctuates by data volume and question complexity for systematic reviews.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions</title>
<link>https://arxiv.org/abs/2504.20304</link>
<guid>https://arxiv.org/abs/2504.20304</guid>
<content:encoded><![CDATA[
<div> Keywords: CHILDES, Universal Dependencies, treebank, annotation guidelines, computational research

Summary:
UD-English-CHILDES is a new resource derived from CHILDES data, offering consistent and unified annotation guidelines for linguistic research. The corpus integrates annotations from 11 children and caregivers, comprising over 48k sentences. Existing gold-standard annotations are validated under the UD v2 framework, complemented by 1M silver-standard sentences. This resource provides a harmonized dataset for computational and linguistic investigations, facilitating analysis of child and child-directed speech. The UD-English-CHILDES treebank enhances the accessibility of CHILDES data for research purposes, with standardized annotation guidelines benefiting computational research and enabling linguistic analyses. Overall, this paper introduces a valuable resource for researchers in the fields of child language acquisition and computational linguistics to explore linguistic patterns and study language development. 

<br /><br />Summary: <div>
arXiv:2504.20304v1 Announce Type: new 
Abstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation</title>
<link>https://arxiv.org/abs/2504.20323</link>
<guid>https://arxiv.org/abs/2504.20323</guid>
<content:encoded><![CDATA[
<div> Keywords: legal recommender systems, labeled datasets, labor disputes, co-citation, algorithmic annotation 

Summary: 
This report presents a novel approach to address the issue of limited labeled datasets in developing legal recommender systems, specifically in specialized domains like labor disputes. By leveraging the co-citation of legal articles within cases, the proposed method establishes similarity and enables algorithmic annotation. Drawing parallels to case co-citation, which uses cited precedents as indicators of shared legal issues, the system recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation of the labeled results demonstrates the effectiveness of the recommender system in recommending labor cases with similarities measured by the co-citation of legal articles. With fine-tuned text embedding models and a BiLSTM module, this research contributes to the advancement of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases. 

Summary: <div>
arXiv:2504.20323v1 Announce Type: new 
Abstract: This report addresses the challenge of limited labeled datasets for developing legal recommender systems, particularly in specialized domains like labor disputes. We propose a new approach leveraging the co-citation of legal articles within cases to establish similarity and enable algorithmic annotation. This method draws a parallel to the concept of case co-citation, utilizing cited precedents as indicators of shared legal issues. To evaluate the labeled results, we employ a system that recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation demonstrates that the recommender, with finetuned text embedding models and a reasonable BiLSTM module can recommend labor cases whose similarity was measured by the co-citation of the legal articles. This research contributes to the development of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.20355</link>
<guid>https://arxiv.org/abs/2504.20355</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt optimization, automatic prompt engineering, Math Reasoning, optimization tokens

Summary:
The article introduces a new approach to prompt optimization called Local Prompt Optimization (LPO) which focuses on optimizing specific tokens within a prompt rather than the entire prompt globally. By guiding the Large Language Models (LLMs) to concentrate on these key tokens during optimization, LPO improves performance significantly on Math Reasoning tasks such as GSM8k and MultiArith, as well as on the challenging BIG-bench Hard benchmarks. LPO also shows faster convergence to the optimal prompt compared to global methods. This innovative method enhances the effectiveness of automatic prompt engineering and offers a more efficient way to improve the output of LLMs for complex tasks. <div>
arXiv:2504.20355v1 Announce Type: new 
Abstract: In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Causes Knowledge Loss in Multilingual Language Models?</title>
<link>https://arxiv.org/abs/2504.20356</link>
<guid>https://arxiv.org/abs/2504.20356</guid>
<content:encoded><![CDATA[
<div> transfer learning, natural language processing, multilingual performance, catastrophic forgetting, LoRA adapters

Summary: 
- The study explores the issue of catastrophic forgetting in multilingual NLP models, focusing on linguistic differences in representational learning.
- Experimenting with 52 languages, LoRA adapters of varying ranks were used to assess the impact on shared parameters.
- The research aimed to determine if parameter sharing through adapters can alleviate forgetting while retaining previous knowledge.
- Non-Latin script languages showed higher susceptibility to catastrophic forgetting compared to Latin script languages.
- Latin script languages demonstrated more effective cross-lingual transfer, emphasizing the importance of linguistic differences in multilingual NLP models. 

<br /><br />Summary: <div>
arXiv:2504.20356v1 Announce Type: new 
Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances multilingual performance by leveraging shared linguistic knowledge. However, traditional methods that process all data simultaneously often fail to mimic real-world scenarios, leading to challenges like catastrophic forgetting, where fine-tuning on new tasks degrades performance on previously learned ones. Our study explores this issue in multilingual contexts, focusing on linguistic differences affecting representational learning rather than just model parameters. We experiment with 52 languages using LoRA adapters of varying ranks to evaluate non-shared, partially shared, and fully shared parameters. Our aim is to see if parameter sharing through adapters can mitigate forgetting while preserving prior knowledge. We find that languages using non-Latin scripts are more susceptible to catastrophic forgetting, whereas those written in Latin script facilitate more effective cross-lingual transfer.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation</title>
<link>https://arxiv.org/abs/2504.20371</link>
<guid>https://arxiv.org/abs/2504.20371</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-domain translation, disambiguation, evaluation, prompting strategies

Summary: 
The study examines the disambiguation ability of Large Language Models (LLMs) in multi-domain translation (MDT) contexts. The research presents a systematic evaluation framework called DMDTEval, comprising a multi-domain ambiguous word annotated translation test set, diverse disambiguation prompting templates, and precise metrics. Various prompting strategies were assessed on multiple state-of-the-art LLMs, revealing key insights. The findings highlight the challenge of ambiguity in MDT and the need for improved disambiguation techniques in LLMs. The study's results provide valuable insights for future research in enhancing the disambiguation capabilities of LLMs. 

<br /><br />Summary: <div>
arXiv:2504.20371v1 Announce Type: new 
Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory; the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompting templates, and (3) we design precise disambiguation metrics, and study the efficacy of various prompting strategies on multiple state-of-the-art LLMs. Our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?</title>
<link>https://arxiv.org/abs/2504.20444</link>
<guid>https://arxiv.org/abs/2504.20444</guid>
<content:encoded><![CDATA[
<div> Keywords: primacy effect, LLMs, ChatGPT, Gemini, Claude

Summary:
In a study analyzing the primacy effect in three commercial Language Models (LLMs) - ChatGPT, Gemini, and Claude - researchers conducted experiments replicating Asch's classic human subjects study. The experiments presented pairs of candidates with equal descriptions but varying word order (positive followed by negative adjectives or vice versa) to these LLMs. In the first experiment where candidates were presented simultaneously, ChatGPT showed a preference for candidates with positive adjectives listed first, Gemini displayed no clear preference, and Claude consistently failed to make a choice. In the second experiment with candidates presented separately, both ChatGPT and Claude tended to rank candidates equally, but when making a choice, they leaned towards candidates with negative adjectives listed first. In contrast, Gemini predominantly favored candidates with negative adjectives listed first. These findings provide insights into how different LLMs process and interpret information based on adjective order. 

<br /><br />Summary: <div>
arXiv:2504.20444v1 Announce Type: new 
Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and Claude. We do this by repurposing the famous experiment Asch (1946) conducted using human subjects. The experiment is simple, given two candidates with equal descriptions which one is preferred if one description has positive adjectives first before negative ones and another description has negative adjectives followed by positive ones. We test this in two experiments. In one experiment, LLMs are given both candidates simultaneously in the same prompt, and in another experiment, LLMs are given both candidates separately. We test all the models with 200 candidate pairs. We found that, in the first experiment, ChatGPT preferred the candidate with positive adjectives listed first, while Gemini preferred both equally often. Claude refused to make a choice. In the second experiment, ChatGPT and Claude were most likely to rank both candidates equally. In the case where they did not give an equal rating, both showed a clear preference to a candidate that had negative adjectives listed first. Gemini was most likely to prefer a candidate with negative adjectives listed first.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs</title>
<link>https://arxiv.org/abs/2504.20451</link>
<guid>https://arxiv.org/abs/2504.20451</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, cultural adaptation, entity translation, automatic evaluation metrics, error taxonomy

Summary: 
Machine translation systems play a crucial role in translating knowledge-intensive and entity-rich text between English and Korean, requiring transcreation to preserve language-specific nuances. A study evaluated 13 models, including Large Language Models (LLMs) and traditional Machine Translation (MT) systems, using both automatic metrics and human assessment. The findings revealed that while LLMs outperformed traditional MT systems, they struggled with entity translation that required cultural adaptation. By constructing an error taxonomy, the study identified key issues such as incorrect responses and entity name errors, with performance varying based on entity type and popularity level. The research also highlighted gaps in automatic evaluation metrics and aimed to pave the way for culturally-nuanced machine translation in the future. 

<br /><br />Summary: <div>
arXiv:2504.20451v1 Announce Type: new 
Abstract: Translating knowledge-intensive and entity-rich text between English and Korean requires transcreation to preserve language-specific and cultural nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators. Our findings show LLMs outperform traditional MT systems but struggle with entity translation requiring cultural adaptation. By constructing an error taxonomy, we identify incorrect responses and entity name errors as key issues, with performance varying by entity type and popularity level. This work exposes gaps in automatic evaluation metrics and hope to enable future work in completing culturally-nuanced machine translation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models</title>
<link>https://arxiv.org/abs/2504.20469</link>
<guid>https://arxiv.org/abs/2504.20469</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Zero-shot Learning, News Narratives, Framing Roles, Hierarchical Approach
<br />
Summary: 
Large language models (LLMs) were evaluated for their zero-shot capabilities in classifying framing roles in news narratives. The study focused on the effects of input context, prompting strategies, and task decomposition. Results showed that a hierarchical approach, first identifying broad roles then fine-grained roles, outperformed single-step classification. Optimal input contexts and prompts varied across task levels, emphasizing the need for subtask-specific strategies. The study achieved a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of the approach. Tailored prompt design and input context optimization were highlighted as crucial for improving LLM performance in entity framing.
<br /> <div>
arXiv:2504.20469v1 Announce Type: new 
Abstract: Understanding how news narratives frame entities is crucial for studying media's impact on societal perceptions of events. In this paper, we evaluate the zero-shot capabilities of large language models (LLMs) in classifying framing roles. Through systematic experimentation, we assess the effects of input context, prompting strategies, and task decomposition. Our findings show that a hierarchical approach of first identifying broad roles and then fine-grained roles, outperforms single-step classification. We also demonstrate that optimal input contexts and prompts vary across task levels, highlighting the need for subtask-specific strategies. We achieve a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our approach. Our findings emphasize the importance of tailored prompt design and input context optimization for improving LLM performance in entity framing.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training</title>
<link>https://arxiv.org/abs/2504.20484</link>
<guid>https://arxiv.org/abs/2504.20484</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cross-lingual transfer, pre-training, bilingual texts, multilingual performance 

Summary: 
- The article introduces Cross-lingual In-context Pre-training (CrossIC-PT) as a method to enhance cross-lingual transfer in large language models (LLMs).
- CrossIC-PT leverages semantically related bilingual texts through next-word prediction, without the constraints of parallel resources.
- By interleaving semantic-related bilingual Wikipedia documents and utilizing a systematic segmentation policy for long document pairs, CrossIC-PT aims to improve contextual coherence.
- Data availability is extended through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus.
- Experimental results show significant improvements in multilingual performance across three LLM models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) in six target languages, with performance gains ranging from 1.95% to 3.99%, followed by additional enhancements through data augmentation.

<br /><br />Summary: <div>
arXiv:2504.20484v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation</title>
<link>https://arxiv.org/abs/2504.20500</link>
<guid>https://arxiv.org/abs/2504.20500</guid>
<content:encoded><![CDATA[
<div> Keywords: UniDetox, toxicity mitigation, large language models, dataset distillation, detoxification technique 

Summary:
UniDetox is a novel method aimed at mitigating toxicity in large language models (LLMs) universally. Unlike previous model-specific detoxification methods, UniDetox can be applied across a wide range of LLMs without the need for separate tuning. The approach involves distilling detoxifying representations through contrastive decoding, creating synthetic text data that can effectively detoxify various LLMs through fine-tuning. Experiments show the detoxifying text from GPT-2 successfully detoxifies larger models like OPT, Falcon, and LLaMA-2 without requiring individual hyperparameter tuning. The analysis of detoxifying text also reveals a reduction in politically biased content, providing valuable insights for the effective detoxification of LLMs.<br /><br />Summary: UniDetox is a universal method for mitigating toxicity in large language models. It uses dataset distillation and contrastive decoding to generate detoxifying representations that can be fine-tuned across different LLMs without the need for model-specific tuning. By effectively detoxifying models like GPT-2, OPT, Falcon, and LLaMA-2, UniDetox also reduces politically biased content in the detoxified text, offering valuable insights for enhancing the detoxification process. <div>
arXiv:2504.20500v1 Announce Type: new 
Abstract: We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs). Previous detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. In contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. Specifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. This approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. Our experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. Additionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records</title>
<link>https://arxiv.org/abs/2504.20547</link>
<guid>https://arxiv.org/abs/2504.20547</guid>
<content:encoded><![CDATA[
<div> Keywords: standardized evaluation, medical domain, natural language models, MIMIC-IV benchmark, electronic health records

Summary: 
The paper addresses the lack of standardized evaluation benchmarks in the medical domain for text inputs by revisiting the MIMIC-IV benchmark for electronic health records (EHRs). The MIMIC-IV data is integrated into the Hugging Face datasets library to facilitate easy sharing and usage. The study explores the conversion of EHR tabular data to text using templates and compares the performance of fine-tuned and zero-shot language models (LLMs) on the mortality prediction task. The results show that fine-tuned text-based models perform competitively against tabular classifiers, while zero-shot LLMs struggle to effectively utilize EHR representations. This research demonstrates the potential of text-based approaches in the medical field and suggests areas for further improvement.<br /><br />Summary: <div>
arXiv:2504.20547v1 Announce Type: new 
Abstract: The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an openly available MIMIC-IV benchmark for electronic health records (EHRs) to address this issue. First, we integrate the MIMIC-IV data within the Hugging Face datasets library to allow an easy share and use of this collection. Second, we investigate the application of templates to convert EHR tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the mortality of patients task show that fine-tuned text-based models are competitive against robust tabular classifiers. In contrast, zero-shot LLMs struggle to leverage EHR representations. This study underlines the potential of text-based approaches in the medical field and highlights areas for further improvement.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters</title>
<link>https://arxiv.org/abs/2504.20552</link>
<guid>https://arxiv.org/abs/2504.20552</guid>
<content:encoded><![CDATA[
<div> fine-tuning, German, dialogues, Bertolt Brecht, AI<br />
Summary: <br />
This project introduces BrAIcht, an AI conversational agent trained in the style of Bertolt Brecht using a large German language model. The model is fine-tuned with a diverse dataset of plays by Brecht and other German playwrights. A parameter-efficient technique called QLoRA is used due to memory constraints. Results based on BLEU score and perplexity show promising performance in generating dialogues in the style of Bertolt Brecht. <br /> <div>
arXiv:2504.20552v1 Announce Type: new 
Abstract: This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClonEval: An Open Voice Cloning Benchmark</title>
<link>https://arxiv.org/abs/2504.20581</link>
<guid>https://arxiv.org/abs/2504.20581</guid>
<content:encoded><![CDATA[
<div> benchmark, voice cloning, text-to-speech models, evaluation protocol, open-source library

Summary:
The paper introduces a new benchmark for evaluating voice cloning text-to-speech models. It includes an evaluation protocol, an open-source library, and a leaderboard for model performance assessment. The design considerations for the benchmark are discussed, along with a detailed description of the evaluation procedure. The paper also explains how to use the software library provided and how results are organized on the leaderboard. This benchmark aims to provide a standardized and comprehensive way to evaluate the performance of voice cloning models, allowing for better comparison and advancement in the field of text-to-speech technology. <div>
arXiv:2504.20581v1 Announce Type: new 
Abstract: We present a novel benchmark for voice cloning text-to-speech models. The benchmark consists of an evaluation protocol, an open-source library for assessing the performance of voice cloning models, and an accompanying leaderboard. The paper discusses design considerations and presents a detailed description of the evaluation procedure. The usage of the software library is explained, along with the organization of results on the leaderboard.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models</title>
<link>https://arxiv.org/abs/2504.20605</link>
<guid>https://arxiv.org/abs/2504.20605</guid>
<content:encoded><![CDATA[
<div> dataset, fables, NLP, instruction-tuned models, ethics <br />
Summary:
TF1-EN-3M is a new dataset of three million English-language fables created using instruction-tuned models with no more than 8 billion parameters. Each fable follows a specific structure, covering characters, traits, settings, conflicts, resolutions, and morals. An evaluation pipeline combines a GPT-based critic with diversity and readability metrics to assess the quality of generated fables. The 8B-parameter Llama-3 variant is identified as the most efficient model in terms of quality and speed. This model can generate high-scoring fables on a consumer GPU at a low cost. The dataset, generation code, evaluation scripts, and metadata are released under a permissive license, allowing for reproducibility and cost benchmarking. TF1-EN-3M opens up possibilities for research in instruction following, narrative intelligence, value alignment, and educational AI, showing that large-scale moral storytelling can be achieved without relying on proprietary giant models. <br /> <br />Summary: <div>
arXiv:2504.20605v1 Announce Type: new 
Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WenyanGPT: A Large Language Model for Classical Chinese Tasks</title>
<link>https://arxiv.org/abs/2504.20609</link>
<guid>https://arxiv.org/abs/2504.20609</guid>
<content:encoded><![CDATA[
<div> Keywords: Classical Chinese, natural language processing, WenyanGPT, evaluation benchmark dataset, LLaMA3-8B-Chinese

Summary:
Classical Chinese, as a vital part of Chinese culture, has been overlooked by existing natural language processing models that primarily focus on Modern Chinese. To address this gap, this paper introduces a specialized solution for Classical Chinese language processing. The creation of WenyanGPT involves continued pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model. Additionally, a benchmark dataset, WenyanBENCH, is developed for evaluating Classical Chinese tasks. Experimental results on WenyanBENCH showcase the superiority of WenyanGPT over current advanced language models in various Classical Chinese tasks. The public availability of the model's training data, instruction fine-tuning data, and evaluation benchmark dataset aims to foster further advancements in the field of Classical Chinese processing. 

<br /><br />Summary: <div>
arXiv:2504.20609v1 Announce Type: new 
Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial role in the inheritance and study of ancient literature. However, existing natural language processing models primarily optimize for Modern Chinese, resulting in inadequate performance on Classical Chinese. This paper presents a comprehensive solution for Classical Chinese language processing. By continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we construct a large language model, WenyanGPT, which is specifically designed for Classical Chinese tasks. Additionally, we develop an evaluation benchmark dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that WenyanGPT significantly outperforms current advanced LLMs in various Classical Chinese tasks. We make the model's training data, instruction fine-tuning data\footnote, and evaluation benchmark dataset publicly available to promote further research and development in the field of Classical Chinese processing.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations</title>
<link>https://arxiv.org/abs/2504.20643</link>
<guid>https://arxiv.org/abs/2504.20643</guid>
<content:encoded><![CDATA[
<div> structured representations, creativity, diverse ideas, culinary domain, DishCOVER

Summary: 
- The paper introduces a novel approach that combines Large Language Models (LLMs) with structured representations and cognitive manipulations to enhance creativity.
- The focus is on generating more creative and diverse ideas beyond superficial variations at the token level.
- The proposed algorithm recombines structured representations of existing ideas to explore the abstract landscape of ideas effectively.
- The approach is demonstrated in the culinary domain with DishCOVER, a model that generates creative recipes.
- Experimental results show that DishCOVER produces more diverse outputs compared to GPT-4 and expert evaluations indicate higher novelty and feasibility in the culinary creations generated by DishCOVER, surpassing GPT-4 in creative generation.

<br /><br />Summary: <div>
arXiv:2504.20643v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with creativity. In this paper, we introduce a novel approach that couples LLMs with structured representations and cognitively inspired manipulations to generate more creative and diverse ideas. Our notion of creativity goes beyond superficial token-level variations; rather, we explicitly recombine structured representations of existing ideas, allowing our algorithm to effectively explore the more abstract landscape of ideas. We demonstrate our approach in the culinary domain with DishCOVER, a model that generates creative recipes. Experiments comparing our model's results to those of GPT-4o show greater diversity. Domain expert evaluations reveal that our outputs, which are mostly coherent and feasible culinary creations, significantly surpass GPT-4o in terms of novelty, thus outperforming it in creative generation. We hope our work inspires further research into structured creativity in AI.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages</title>
<link>https://arxiv.org/abs/2504.20668</link>
<guid>https://arxiv.org/abs/2504.20668</guid>
<content:encoded><![CDATA[
<div> Keywords: Online disinformation, fact-checking, large language models, relevance evaluation, streamlined process <br />
Summary: 
This research addresses the challenge of online disinformation by introducing an approach that utilizes large language models (LLMs) to retrieve and evaluate previously fact-checked claims. By filtering out irrelevant fact-checks and generating concise summaries and explanations, the method aims to streamline the fact-checking process and reduce workload for fact-checkers. Both automatic and human assessments of the approach demonstrate its effectiveness in reducing effort and improving the efficiency of fact-checking. By enabling fact-checkers to quickly assess whether a claim has been verified before, the method contributes to preventing the spread of false information. This research highlights the potential of LLMs in enhancing the fact-checking process and providing valuable support to combat online disinformation. <br /><br />Summary: <div>
arXiv:2504.20668v1 Announce Type: new 
Abstract: Online disinformation poses a global challenge, placing significant demands on fact-checkers who must verify claims efficiently to prevent the spread of false information. A major issue in this process is the redundant verification of already fact-checked claims, which increases workload and delays responses to newly emerging claims. This research introduces an approach that retrieves previously fact-checked claims, evaluates their relevance to a given input, and provides supplementary information to support fact-checkers. Our method employs large language models (LLMs) to filter irrelevant fact-checks and generate concise summaries and explanations, enabling fact-checkers to faster assess whether a claim has been verified before. In addition, we evaluate our approach through both automatic and human assessments, where humans interact with the developed tool to review its effectiveness. Our results demonstrate that LLMs are able to filter out many irrelevant fact-checks and, therefore, reduce effort and streamline the fact-checking process.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-native Children's Automatic Speech Assessment Challenge (NOCASA)</title>
<link>https://arxiv.org/abs/2504.20678</link>
<guid>https://arxiv.org/abs/2504.20678</guid>
<content:encoded><![CDATA[
<div> data competition, second language learners, pronunciation assessment, training data, unbalanced distribution

Summary:
The paper introduces NOCASA, a data competition at the IEEE MLSP 2025 conference focused on assessing the single-word pronunciations of young second language learners through a gamified app. The competition challenges participants to address issues such as limited training data and unbalanced distribution among pronunciation level categories. The authors provide a pseudo-anonymized training dataset called TeflonNorL2, containing 10,334 recordings from 44 speakers pronouncing 205 Norwegian words rated on a 1 to 5 scale. Additionally, two trained systems are released as baselines: an SVM classifier using the ComParE_16 acoustic feature set and a wav2vec 2.0 model. The wav2vec 2.0 model achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%. <br /><br />Summary: <div>
arXiv:2504.20678v1 Announce Type: new 
Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?</title>
<link>https://arxiv.org/abs/2504.20679</link>
<guid>https://arxiv.org/abs/2504.20679</guid>
<content:encoded><![CDATA[
<div> IR task, concept equivalence, longitudinal surveys, unsupervised approaches, neural models
Summary: 
- The study addresses the challenges of detecting semantically equivalent questions in longitudinal social science surveys by identifying concept equivalence across question and response options.
- Various unsupervised approaches, including probabilistic models and neural networks specialized for information retrieval, were tested on a survey dataset spanning from 1946 to 2020.
- Neural models designed for information retrieval showed the highest overall performance, but re-ranking probabilistic model results with neural models only led to slight improvements in F1-score.
- The study found that models have low sensitivity to questions with high lexical overlap, especially when sub-concepts are mismatched.
- The findings contribute to research on harmonizing longitudinal studies in social science.

<br /><br />Summary: <div>
arXiv:2504.20679v1 Announce Type: new 
Abstract: Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?</title>
<link>https://arxiv.org/abs/2504.20699</link>
<guid>https://arxiv.org/abs/2504.20699</guid>
<content:encoded><![CDATA[
<div> hallucination detection, LLMs, translation, paraphrasing, NLI<br />
Summary:<br />
This study evaluates the ability of open-access Language Model Models (LLMs) in detecting intrinsic hallucinations in translation and paraphrasing tasks. The research focuses on the impact of model size, instruction tuning, and prompt choice on model performance. Results show variations in performance across models but consistency across prompts. Notably, Natural Language Inference (NLI) models perform comparably well, suggesting alternative options for hallucination detection beyond LLM-based detectors. The study aims to address the common issue of nonsensical outputs generated by LLMs, known as hallucination, and offers insights into improving detection methods in language generation tasks. <br /> <div>
arXiv:2504.20699v1 Announce Type: new 
Abstract: A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as hallucination. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and language and we investigate the impact of model size, instruction tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification</title>
<link>https://arxiv.org/abs/2504.20703</link>
<guid>https://arxiv.org/abs/2504.20703</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025, text augmentation, transformer models, hazard detection, minority classes

Summary: 
This paper describes a system developed for the SemEval-2025 Task 9 Food Hazard Detection Challenge, which aims to evaluate classification systems for identifying hazards and products in food recall reports. The study investigates the use of text augmentation techniques to address poor performance on minority classes and compares their impact on various transformer and machine learning models. Three word-level augmentation techniques are explored: synonym replacement, random word swapping, and contextual word insertion. Results indicate that transformer models generally outperform machine learning models. The study shows that targeted augmentation of minority classes can enhance transformer model performance, with contextual word insertion leading to a 6% improvement in accuracy for minority hazard classes. Overall, the study provides insights into the effectiveness of text augmentation techniques in improving classification performance. 

<br /><br />Summary: <div>
arXiv:2504.20703v1 Announce Type: new 
Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The Food Hazard Detection Challenge. The shared task's objective is to evaluate explainable classification systems for classifying hazards and products in two levels of granularity from food recall incident reports. In this work, we propose text augmentation techniques as a way to improve poor performance on minority classes and compare their effect for each category on various transformer and machine learning models. We explore three word-level data augmentation techniques, namely synonym replacement, random word swapping, and contextual word insertion. The results show that transformer models tend to have a better overall performance. None of the three augmentation techniques consistently improved overall performance for classifying hazards and products. We observed a statistically significant improvement (P < 0.05) in the fine-grained categories when using the BERT model to compare the baseline with each augmented model. Compared to the baseline, the contextual words insertion augmentation improved the accuracy of predictions for the minority hazard classes by 6%. This suggests that targeted augmentation of minority classes can improve the performance of transformer models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think</title>
<link>https://arxiv.org/abs/2504.20708</link>
<guid>https://arxiv.org/abs/2504.20708</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning trace, subthoughts, linguistic cues, accuracy improvement

Summary:
Large Language Models (LLMs) use step-by-step reasoning to solve complex problems. The traditional evaluation method focuses on the final answer, but this paper questions if it always represents the best conclusion and if different reasoning paths could lead to varied results. The authors propose a method to analyze intermediate reasoning steps, termed subthoughts, by segmenting the trace based on linguistic cues. By prompting the model to generate continuations from each subthought's endpoint and aggregating potential answers, they found significantly higher accuracy compared to relying solely on the original answer. Examining consistency among answers from different subthoughts can identify less reliable answers and suggest improvements in correctness. Experiments on challenging mathematical reasoning datasets show consistent accuracy gains up to 13% and 10%, highlighting the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2504.20708v1 Announce Type: new 
Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\% and 10\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, UniversalRAG, heterogeneous sources, modality-aware routing, granularity levels<br />
<br />Summary: 
UniversalRAG is a novel framework for Retrieval-Augmented Generation (RAG) that aims to integrate knowledge from diverse sources with different modalities. Existing RAG approaches are typically limited to text-only corpora, while recent extensions to other modalities still operate within single modality-specific corpora. UniversalRAG addresses this limitation by dynamically selecting the most appropriate modality-specific corpus for retrieval through a modality-aware routing mechanism. It also organizes each modality into multiple granularity levels, allowing for more tailored and precise retrieval based on the query's complexity and scope. The framework is validated across 8 benchmarks covering multiple modalities, demonstrating its superior performance over modality-specific and unified approaches. UniversalRAG's ability to leverage heterogeneous sources and adaptively retrieve knowledge based on the query's requirements makes it a promising advancement in the field of Retrieval-Augmented Generation. <br /><br /> <div>
arXiv:2504.20734v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
<div> grokking, transformers, factual reasoning, knowledge graphs, neural networks <br />
Summary:<br />
The paper introduces a novel approach to improve multi-step factual reasoning in Transformers using grokking techniques. It addresses the challenge of dataset sparsity by augmenting knowledge graphs with carefully designed synthetic data to enhance reasoning capabilities. Surprisingly, even factually incorrect synthetic data can strengthen reasoning circuits by promoting reliance on relational structure over memorization. The approach achieves high accuracy on multi-hop reasoning benchmarks, outperforming strong baselines and current state-of-the-art models. The study explores how increasing the ratio of inferred facts to atomic facts drives the formation of generalizing circuits inside Transformers. The findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities in large-scale language models, leading to more robust and interpretable factual reasoning. <br />Summary: <div>
arXiv:2504.20752v1 Announce Type: new 
Abstract: Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption</title>
<link>https://arxiv.org/abs/2504.20769</link>
<guid>https://arxiv.org/abs/2504.20769</guid>
<content:encoded><![CDATA[
<div> prompting, robustness, large language models, reasoning abilities, reference corruption

Summary:<br />
- The study explores the use of chain-of-thought prompting to enhance the robustness of large language models.
- It introduces the concept of chain-of-defensive-thought, which improves model performance against reference corruption.
- Large language models, like GPT-4o, exhibit significantly improved accuracy in tasks such as Natural Questions with this method.
- The method involves providing structured and defensive reasoning exemplars as demonstrations to enhance model robustness.
- Empirical results show that GPT-4o's accuracy remains at 50% with chain-of-defensive-thought prompting, compared to 3% with standard prompting when faced with prompt injection attacks. 

Summary: <div>
arXiv:2504.20769v1 Announce Type: new 
Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turing Machine Evaluation for Large Language Model</title>
<link>https://arxiv.org/abs/2504.20771</link>
<guid>https://arxiv.org/abs/2504.20771</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Computational reasoning ability, Evaluation framework, Turing Machine, TMBench

Summary:
The research focuses on evaluating the core computational reasoning ability of Large Language Models (LLMs) through a novel perspective. The proposed evaluation framework, based on Universal Turing Machine (UTM) simulation, assesses the model's capacity to understand rules and perform logical computing operations accurately. The framework, implemented through TMBench, requires LLMs to follow instructions and track dynamic states during multi-step computations. TMBench offers advantages such as knowledge-agnostic evaluation, adjustable difficulty, and scalability through Turing machine encoding. The study shows a strong correlation between performance on TMBench and other reasoning benchmarks, emphasizing the significance of computational reasoning in measuring the capabilities of LLMs.<br /><br />Summary: <div>
arXiv:2504.20771v1 Announce Type: new 
Abstract: With the rapid development and widespread application of Large Language Models (LLMs), rigorous evaluation has become particularly crucial. This research adopts a novel perspective, focusing on evaluating the core computational reasoning ability of LLMs, defined as the capacity of model to accurately understand rules, and execute logically computing operations. This capability assesses the reliability of LLMs as precise executors, and is critical to advanced tasks such as complex code generation and multi-step problem-solving. We propose an evaluation framework based on Universal Turing Machine (UTM) simulation. This framework requires LLMs to strictly follow instructions and track dynamic states, such as tape content and read/write head position, during multi-step computations. To enable standardized evaluation, we developed TMBench, a benchmark for systematically studying the computational reasoning capabilities of LLMs. TMBench provides several key advantages, including knowledge-agnostic evaluation, adjustable difficulty, foundational coverage through Turing machine encoding, and unlimited capacity for instance generation, ensuring scalability as models continue to evolve. We find that model performance on TMBench correlates strongly with performance on other recognized reasoning benchmarks (Pearson correlation coefficient is 0.73), clearly demonstrating that computational reasoning is a significant dimension for measuring the deep capabilities of LLMs. Code and data are available at https://github.com/HaitaoWuTJU/Turing-Machine-Bench.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal language model with the intervention of quantum theory</title>
<link>https://arxiv.org/abs/2504.20839</link>
<guid>https://arxiv.org/abs/2504.20839</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, quantum mechanics, word embedding, natural language, quantum statistics<br />
Summary:<br />
This paper explores the integration of quantum mechanics into language modeling, specifically focusing on symbol-meaning pairs in natural language. The study suggests that quantum mechanics can enhance word embedding techniques used in statistical language modeling. By applying quantum statistics and related theories, the research delves into mathematical representations, natural evolution, and statistical properties of natural language, proposing that quantum properties stem from the physical nature of information. An experimental code is developed to demonstrate the feasibility of using quantum theory for natural language modeling. The paper also discusses the potential application of quantum theory in constructing generative models and anticipates future applications in quantum computing. <br /><br />Summary: <div>
arXiv:2504.20839v1 Announce Type: new 
Abstract: This paper examines language modeling based on the theory of quantum mechanics. It focuses on the introduction of quantum mechanics into the symbol-meaning pairs of language in order to build a representation model of natural language. At the same time, it is realized that word embedding, which is widely used as a basic technique for statistical language modeling, can be explained and improved by the mathematical framework of quantum mechanics. On this basis, this paper continues to try to use quantum statistics and other related theories to study the mathematical representation, natural evolution and statistical properties of natural language. It is also assumed that the source of such quantum properties is the physicality of information. The feasibility of using quantum theory to model natural language is pointed out through the construction of a experimental code. The paper discusses, in terms of applications, the possible help of the theory in constructing generative models that are popular nowadays. A preliminary discussion of future applications of the theory to quantum computers is also presented.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry</title>
<link>https://arxiv.org/abs/2504.20849</link>
<guid>https://arxiv.org/abs/2504.20849</guid>
<content:encoded><![CDATA[
<div> Keywords: Data-to-Text, Language Models, Marketing Texts, Diversity, Automated Content Generation 

Summary: 
- The paper explores the use of LLM-based data-to-text methods for generating diverse marketing texts on online platforms.
- Traditional generative methods often produce repetitive content, leading to monotonous text galleries.
- Language Models like T5, GPT-3.5, GPT-4, and LLaMa2 are utilized, along with fine-tuning and zero-shot techniques, to enhance text diversity.
- A new metric called JaccDiv is introduced to evaluate the diversity of generated texts.
- The research has broader applications beyond the music industry, offering insights into improving automated content generation in various fields. 

Summary: <div>
arXiv:2504.20849v1 Announce Type: new 
Abstract: Online platforms are increasingly interested in using Data-to-Text technologies to generate content and help their users. Unfortunately, traditional generative methods often fall into repetitive patterns, resulting in monotonous galleries of texts after only a few iterations. In this paper, we investigate LLM-based data-to-text approaches to automatically generate marketing texts that are of sufficient quality and diverse enough for broad adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in conjunction with fine-tuning, few-shot, and zero-shot approaches to set a baseline for diverse marketing texts. We also introduce a metric JaccDiv to evaluate the diversity of a set of texts. This research extends its relevance beyond the music industry, proving beneficial in various fields where repetitive automated content generation is prevalent.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNAMAX: Dynamic computing for Transformers and Mamba based architectures</title>
<link>https://arxiv.org/abs/2504.20922</link>
<guid>https://arxiv.org/abs/2504.20922</guid>
<content:encoded><![CDATA[
<div> Mamba, Early exits, Transformers, Dynamic computing, NLP <br />
Summary: <br />
This paper introduces DYNAMAX, a framework that integrates early exit mechanisms into Mamba architectures, showcasing their efficiency in reducing computational costs in natural language processing tasks. The study compares the Mistral 7B transformer with the Codestral 7B Mamba model on datasets like TruthfulQA, CoQA, and TriviaQA. The results demonstrate Mamba's adaptability as an early exit classifier, effectively balancing computational savings, accuracy, and consistency across various NLP tasks. By leveraging Mamba's design for dynamic processing, the framework opens up possibilities for scalable and efficient inference in embedded applications and resource-constrained environments. This research highlights the transformative potential of Mamba in redefining dynamic computing paradigms for large language models. <br /> <div>
arXiv:2504.20922v1 Announce Type: new 
Abstract: Early exits (EEs) offer a promising approach to reducing computational costs and latency by dynamically terminating inference once a satisfactory prediction confidence on a data sample is achieved. Although many works integrate EEs into encoder-only Transformers, their application to decoder-only architectures and, more importantly, Mamba models, a novel family of state-space architectures in the LLM realm, remains insufficiently explored. This work introduces DYNAMAX, the first framework to exploit the unique properties of Mamba architectures for early exit mechanisms. We not only integrate EEs into Mamba but also repurpose Mamba as an efficient EE classifier for both Mamba-based and transformer-based LLMs, showcasing its versatility. Our experiments employ the Mistral 7B transformer compared to the Codestral 7B Mamba model, using data sets such as TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and consistency. The results highlight the adaptability of Mamba as a powerful EE classifier and its efficiency in balancing computational cost and performance quality across NLP tasks. By leveraging Mamba's inherent design for dynamic processing, we open pathways for scalable and efficient inference in embedded applications and resource-constrained environments. This study underscores the transformative potential of Mamba in redefining dynamic computing paradigms for LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models</title>
<link>https://arxiv.org/abs/2504.20946</link>
<guid>https://arxiv.org/abs/2504.20946</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt engineering, arithmetic reasoning, open-source models, Trace-of-Thought Prompting

Summary:<br /><br />Large Language Models (LLMs) are increasingly being used in various tasks, prompting ongoing advancements in prompt engineering, especially for specialized domains like arithmetic reasoning. While powerful, using LLMs extensively can be costly and limit customization when relying on proprietary models. This study introduces Trace-of-Thought Prompting, a simple zero-shot method that enhances arithmetic reasoning in LLMs below 7 billion parameters through observable subproblems. By applying this approach with open-source models and GPT-4, a performance increase of up to 125% was observed. This highlights the potential of open-source initiatives in democratizing AI research and making high-quality computational linguistics applications more accessible. <div>
arXiv:2504.20946v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-of-Thought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of high-quality computational linguistics applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.20951</link>
<guid>https://arxiv.org/abs/2504.20951</guid>
<content:encoded><![CDATA[
<div> propose, theoretical model, information gravity, large language models, text generation process
Summary:<br /><br />We present a theoretical model called "information gravity" that utilizes concepts from field theory and spacetime geometry to elucidate the text generation process in large language models. In this model, user queries are depicted as objects with "information mass" that curve the semantic space of the model, creating gravitational potential wells that influence the probability distribution of generated tokens. By adopting this framework, we can better understand various phenomena observed in LLM behavior, such as hallucinations arising from low-density semantic voids, the impact of query formulation on semantic field curvature changes, and the role of sampling temperature in diversifying output. The information gravity model provides a compelling explanation for these behaviors and sheds light on the intricate dynamics of text generation in LLMs. 
Summary: <div>
arXiv:2504.20951v1 Announce Type: new 
Abstract: We propose a theoretical model called "information gravity" to describe the text generation process in large language models (LLMs). The model uses physical apparatus from field theory and spacetime geometry to formalize the interaction between user queries and the probability distribution of generated tokens. A query is viewed as an object with "information mass" that curves the semantic space of the model, creating gravitational potential wells that "attract" tokens during generation. This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification</title>
<link>https://arxiv.org/abs/2504.20964</link>
<guid>https://arxiv.org/abs/2504.20964</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models (LLMs), specification generation, operating system kernel verification tasks, evaluation 

Summary:
The OSVBench is a benchmark designed to evaluate Large Language Models (LLMs) in generating complete specification code for operating system kernel verification tasks. It presents LLMs with the programming model, requiring them to generate specifications based on verification assumptions and high-level functional descriptions. Utilizing the Hyperkernel operating system, the benchmark encompasses 245 complex specification generation tasks with long-context challenges. A study of 12 LLMs reveals their limited performance on these tasks, emphasizing disparities in handling long-context code generation. The benchmark and evaluation toolkit are accessible on GitHub, inviting further research and development in improving LLM capabilities for operating system verification tasks.<br /><br />Summary: <div>
arXiv:2504.20964v1 Announce Type: new 
Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SetKE: Knowledge Editing for Knowledge Elements Overlap</title>
<link>https://arxiv.org/abs/2504.20972</link>
<guid>https://arxiv.org/abs/2504.20972</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Editing, Knowledge Element Overlap, Knowledge Set Editing, SetKE <br />
Summary: <br />
Large Language Models (LLMs) have shown proficiency in retrieval and question-answering tasks but require regular updates to stay accurate and reduce errors. Traditional methods like fine-tuning and incremental learning suffer from overfitting and high computational costs. Knowledge Editing (KE) offers a promising alternative, but it often faces challenges due to Knowledge Element Overlap (KEO), where multiple triplets share common elements leading to editing conflicts. This article proposes Knowledge Set Editing (KSE) and introduces SetKE, a method that edits sets of triplets simultaneously, outperforming existing methods in handling KEO scenarios on mainstream LLMs. The article also introduces the EditSet dataset containing KEO triplets as a benchmark for evaluating editing methods. <div>
arXiv:2504.20972v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question answering but require updates to incorporate new knowledge and reduce inaccuracies and hallucinations. Traditional updating methods, like fine-tuning and incremental learning, face challenges such as overfitting and high computational costs. Knowledge Editing (KE) provides a promising alternative but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where multiple triplets share common elements, leading to editing conflicts. We identify the prevalence of KEO in existing KE datasets and show its significant impact on current KE methods, causing performance degradation in handling such triplets. To address this, we propose a new formulation, Knowledge Set Editing (KSE), and introduce SetKE, a method that edits sets of triplets simultaneously. Experimental results demonstrate that SetKE outperforms existing methods in KEO scenarios on mainstream LLMs. Additionally, we introduce EditSet, a dataset containing KEO triplets, providing a comprehensive benchmark.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.20059</link>
<guid>https://arxiv.org/abs/2504.20059</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical trials, online platforms, TrialGPT, patient cases, eligibility criteria

Summary: 
Trial recruitment challenges are being addressed through leveraging online platforms such as social media and health communities. The use of TrialGPT, a framework based on a large language model, significantly improves the identification of eligible clinical trials for patients compared to traditional keyword-based searches. Each patient case was found to be eligible for an average of 7 trials using TrialGPT, outperforming traditional methods by 46%. Outreach efforts to case authors and trial organizers regarding these patient-trial matches resulted in highly positive feedback, demonstrating the potential for online platforms to enhance clinical trial recruitment and enrollment pathways.<br /><br />Summary: <div>
arXiv:2504.20059v1 Announce Type: cross 
Abstract: Clinical trials are crucial for assessing new treatments; however, recruitment challenges - such as limited awareness, complex eligibility criteria, and referral barriers - hinder their success. With the growth of online platforms, patients increasingly turn to social media and health communities for support, research, and advocacy, expanding recruitment pools and established enrollment pathways. Recognizing this potential, we utilized TrialGPT, a framework that leverages a large language model (LLM) as its backbone, to match 50 online patient cases (collected from published case reports and a social media website) to clinical trials and evaluate performance against traditional keyword-based searches. Our results show that TrialGPT outperforms traditional methods by 46% in identifying eligible trials, with each patient, on average, being eligible for around 7 trials. Additionally, our outreach efforts to case authors and trial organizers regarding these patient-trial matches yielded highly positive feedback, which we present from both perspectives.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
<div> RL, LLM, StarPO, RAGEN, trajectory-level<br />
Summary:<br />
Training large language models (LLMs) as interactive agents poses challenges such as long-horizon decision-making and dealing with stochastic feedback. The proposed StarPO framework and RAGEN system address these challenges by enabling trajectory-level agent RL training. Echo Trap, a recurring issue in agent RL training, is mitigated by StarPO-S, a stabilized variant. Shaping RL rollouts requires diverse initial states, medium interaction granularity, and frequent sampling. Additionally, fine-grained, reasoning-aware reward signals are crucial for promoting agent reasoning in multi-turn RL training, as shallow strategies or hallucinated thoughts may emerge without them. This study highlights the importance of these key factors in the development of effective LLM agents trained through multi-turn RL. <div>
arXiv:2504.20073v1 Announce Type: cross 
Abstract: Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Awareness</title>
<link>https://arxiv.org/abs/2504.20084</link>
<guid>https://arxiv.org/abs/2504.20084</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, awareness, meta-cognition, self-awareness, social awareness, situational awareness

Summary: Recent advancements in AI have led to a renewed focus on AI awareness as a measurable, functional capacity. This review explores the different forms of AI awareness, including meta-cognition, self-awareness, social awareness, and situational awareness, drawing on insights from cognitive science, psychology, and computational theory. The review examines how these forms of awareness manifest in state-of-the-art AI and discusses the link between awareness and intelligent behaviors. The risks associated with AI awareness, such as misalignment and societal risks, are also explored. Ultimately, AI awareness enhances AI capabilities but requires careful oversight to mitigate potential risks, making it a double-edged sword. This interdisciplinary review provides a roadmap for future research and emphasizes the importance of understanding the role of AI awareness in the development of intelligent machines. 

Summary: <div>
arXiv:2504.20084v1 Announce Type: cross 
Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness, not as a philosophical question of consciousness, but as a measurable, functional capacity. In this review, we explore the emerging landscape of AI awareness, which includes meta-cognition (the ability to represent and reason about its own state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents), and situational awareness (assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raises concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. On the whole, our interdisciplinary review provides a roadmap for future research and aims to clarify the role of AI awareness in the ongoing development of intelligent machines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?</title>
<link>https://arxiv.org/abs/2504.20094</link>
<guid>https://arxiv.org/abs/2504.20094</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent collaboration, conversational recommendation system, large language models, user engagement, personalized recommendations

Summary:
Our paper introduces the MATCHA framework, a multi-agent collaboration system for conversational recommendation systems. By leveraging large language models, our framework enhances personalization and user engagement by providing curated recommendations aligned with user interests, preferences, and constraints. Specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards work together to improve recommendation accuracy, diversity, and safety. Our model outperforms current state-of-the-art on eight metrics and addresses key challenges in game recommendation systems: handling complex user requests, enhancing personalization through collaboration, empirical evaluation and deployment, and ensuring safe interactions. Through comparisons with baseline models, our approach demonstrates superior performance and highlights the importance of multi-agent collaboration in conversational recommendation systems.
<br /><br />Summary: <div>
arXiv:2504.20094v1 Announce Type: cross 
Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA for conversational recommendation system, leveraging large language models (LLMs) to enhance personalization and user engagement. Users can request recommendations via free-form text and receive curated lists aligned with their interests, preferences, and constraints. Our system introduces specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards. These agents collaboratively improve recommendations accuracy, diversity, and safety. On eight metrics, our model achieves superior or comparable performance to the current state-of-the-art. Through comparisons with six baseline models, our approach addresses key challenges in conversational recommendation systems for game recommendations, including: (1) handling complex, user-specific requests, (2) enhancing personalization through multi-agent collaboration, (3) empirical evaluation and deployment, and (4) ensuring safe and trustworthy interactions.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies</title>
<link>https://arxiv.org/abs/2504.20117</link>
<guid>https://arxiv.org/abs/2504.20117</guid>
<content:encoded><![CDATA[
<div> Keywords: ResearchCodeAgent, multi-agent system, large language models, machine learning tasks, automation<br />
<br />
Summary: ResearchCodeAgent is a novel multi-agent system that leverages large language models to automate the codification of research methodologies in machine learning literature. The system bridges the gap between high-level research concepts and practical implementation by generating code from existing research papers. It offers flexible agent architecture with a comprehensive action suite for context-aware interactions. The system uses a dynamic planning mechanism with short and long-term memory for adaptive approaches. Evaluation on three machine learning tasks shows that ResearchCodeAgent generates high-quality and error-free code, with some instances showing performance improvements over baseline implementations. An average reduction of 57.9% in coding time compared to manual implementation was observed, with higher gains for complex tasks. This system represents a significant advancement in automating research implementation, potentially accelerating the pace of machine learning research.<br /><br />Summary: <div>
arXiv:2504.20117v1 Announce Type: cross 
Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains</title>
<link>https://arxiv.org/abs/2504.20199</link>
<guid>https://arxiv.org/abs/2504.20199</guid>
<content:encoded><![CDATA[
<div> Focus-Centric Visual Chain, multi-image tasks, data synthesis, VISC-150K dataset, vision-language systems <br /> 
Summary: <br /> 
The article introduces the Focus-Centric Visual Chain paradigm to improve vision-language models' performance in multi-image tasks. By synthesizing data through Focus-Centric Data Synthesis, the VISC-150K dataset was created to enhance reasoning abilities. Experimental results show a significant performance boost across different model architectures without sacrificing general vision-language capabilities. This approach demonstrates advancements in handling complex visual scenarios and signifies progress towards robust vision-language systems.
 <div>
arXiv:2504.20199v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mrCAD: Multimodal Refinement of Computer-aided Designs</title>
<link>https://arxiv.org/abs/2504.20294</link>
<guid>https://arxiv.org/abs/2504.20294</guid>
<content:encoded><![CDATA[
<div> Keywords: human collaboration, generative AI, multimodal instructions, computer aided designs (CADs), refinement instructions <br />
Summary: <br />
The article introduces mrCAD, a dataset of multimodal instructions in a communication game where players create and refine computer aided designs. The dataset consists of 6,082 communication games played by 1,092 pairs of human players. Analysis of the dataset reveals differences in the use of drawing and text between generation and refinement instructions. State-of-the-art VLMs perform better at following generation instructions than refinement instructions in the mrCAD task, highlighting a gap in current AI capabilities. The findings provide insights into the multimodal language of refinement and lay the groundwork for future research in this area. <br /> <div>
arXiv:2504.20294v1 Announce Type: cross 
Abstract: A key feature of human collaboration is the ability to iteratively refine the concepts we have communicated. In contrast, while generative AI excels at the \textit{generation} of content, it often struggles to make specific language-guided \textit{modifications} of its prior outputs. To bridge the gap between how humans and machines perform edits, we present mrCAD, a dataset of multimodal instructions in a communication game. In each game, players created computer aided designs (CADs) and refined them over several rounds to match specific target designs. Only one player, the Designer, could see the target, and they must instruct the other player, the Maker, using text, drawing, or a combination of modalities. mrCAD consists of 6,082 communication games, 15,163 instruction-execution rounds, played between 1,092 pairs of human players. We analyze the dataset and find that generation and refinement instructions differ in their composition of drawing and text. Using the mrCAD task as a benchmark, we find that state-of-the-art VLMs are better at following generation instructions than refinement instructions. These results lay a foundation for analyzing and modeling a multimodal language of refinement that is not represented in previous datasets.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding</title>
<link>https://arxiv.org/abs/2504.20456</link>
<guid>https://arxiv.org/abs/2504.20456</guid>
<content:encoded><![CDATA[
<div> diffusion models, any-subset autoregressive models, parallel sampling, joint probability density estimation, language modeling
<br />
Summary:
The article introduces any-subset autoregressive models (AS-ARMs) as a solution for parallel sampling in arbitrary-order language models. AS-ARMs allow for generating tokens in any order and in parallel, supporting joint probability density estimation. The Any-Subset Speculative Decoding (ASSD) algorithm corrects token distributions, ensuring generation from the correct joint distribution. Empirical results show that ASSD speeds up language generation without compromising quality. AS-ARMs outperform larger models on infilling benchmark tasks and almost match their performance on code generation. The study provides a mathematically justified training scheme for AS-ARMs and suggests that these models are a promising direction in language modeling. <div>
arXiv:2504.20456v1 Announce Type: cross 
Abstract: In arbitrary-order language models, it is an open question how to sample tokens in parallel from the correct joint distribution. With discrete diffusion models, the more tokens they generate in parallel, the less their predicted distributions adhere to the originally learned data distribution, as they rely on a conditional independence assumption that only works with infinitesimally small timesteps. We find that a different class of models, any-subset autoregressive models (AS-ARMs), holds the solution. As implied by the name, AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs support parallelized joint probability density estimation, allowing them to correct their own parallel-generated token distributions, via our Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of tokens from the correct joint distribution, with the number of neural network calls upper bounded by the number of tokens predicted. We empirically verify that ASSD speeds up language generation, without sacrificing quality. Furthermore, we provide a mathematically justified scheme for training AS-ARMs for generation, and show that AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling benchmark tasks, and nearly match the performance of models 50X larger on code generation. Our theoretical and empirical results indicate that the once-forgotten AS-ARMs are a promising direction of language modeling.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User</title>
<link>https://arxiv.org/abs/2504.20458</link>
<guid>https://arxiv.org/abs/2504.20458</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational recommendation systems, simulated user, generative reward model, multi-turn interaction, personalized recommendations

Summary:
The article introduces a novel approach called GRSU, a generative reward model based simulated user for conversational recommendation systems (CRSs). The GRSU enables automatic interaction with CRSs by providing feedback on recommended items. It offers two types of feedback actions - generative item scoring for coarse-grained feedback, and attribute-based item critique for fine-grained feedback. These feedback actions are unified into an instruction-based format for seamless integration. Inspired by reward-guided search, beam search is used for efficient interaction with CRSs. Additionally, an efficient candidate ranking method is proposed to enhance recommendation results. Experimental results on public datasets validate the effectiveness, efficiency, and transferability of the approach. Overall, the GRSU model addresses the challenge of understanding intricate user preferences in CRSs through automatic multi-turn interaction, resulting in improved personalized recommendations. 

<br /><br />Summary: <div>
arXiv:2504.20458v1 Announce Type: cross 
Abstract: Conversational recommendation systems (CRSs) use multi-turn interaction to capture user preferences and provide personalized recommendations. A fundamental challenge in CRSs lies in effectively understanding user preferences from conversations. User preferences can be multifaceted and complex, posing significant challenges for accurate recommendations even with access to abundant external knowledge. While interaction with users can clarify their true preferences, frequent user involvement can lead to a degraded user experience.
  To address this problem, we propose a generative reward model based simulated user, named GRSU, for automatic interaction with CRSs. The simulated user provides feedback to the items recommended by CRSs, enabling them to better capture intricate user preferences through multi-turn interaction. Inspired by generative reward models, we design two types of feedback actions for the simulated user: i.e., generative item scoring, which offers coarse-grained feedback, and attribute-based item critique, which provides fine-grained feedback. To ensure seamless integration, these feedback actions are unified into an instruction-based format, allowing the development of a unified simulated user via instruction tuning on synthesized data. With this simulated user, automatic multi-turn interaction with CRSs can be effectively conducted. Furthermore, to strike a balance between effectiveness and efficiency, we draw inspiration from the paradigm of reward-guided search in complex reasoning tasks and employ beam search for the interaction process. On top of this, we propose an efficient candidate ranking method to improve the recommendation results derived from interaction. Extensive experiments on public datasets demonstrate the effectiveness, efficiency, and transferability of our approach.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Reasoning in Large Language Models with One Training Example</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
arXiv:2504.20571v1 Announce Type: cross 
Abstract: We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonIR: Training Retrievers for Reasoning Tasks</title>
<link>https://arxiv.org/abs/2504.20595</link>
<guid>https://arxiv.org/abs/2504.20595</guid>
<content:encoded><![CDATA[
arXiv:2504.20595v1 Announce Type: cross 
Abstract: We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2504.20859</link>
<guid>https://arxiv.org/abs/2504.20859</guid>
<content:encoded><![CDATA[
arXiv:2504.20859v1 Announce Type: cross 
Abstract: As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Leaderboard Illusion</title>
<link>https://arxiv.org/abs/2504.20879</link>
<guid>https://arxiv.org/abs/2504.20879</guid>
<content:encoded><![CDATA[
arXiv:2504.20879v1 Announce Type: cross 
Abstract: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification</title>
<link>https://arxiv.org/abs/2504.20930</link>
<guid>https://arxiv.org/abs/2504.20930</guid>
<content:encoded><![CDATA[
arXiv:2504.20930v1 Announce Type: cross 
Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition</title>
<link>https://arxiv.org/abs/2504.20938</link>
<guid>https://arxiv.org/abs/2504.20938</guid>
<content:encoded><![CDATA[
arXiv:2504.20938v1 Announce Type: cross 
Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of Transformer attention layers to disentangle original Multi Head Self Attention (MHSA) into individually comprehensible components. Lorsa is designed to address the challenge of attention superposition to understand attention-mediated interaction between features in different token positions. We show that Lorsa heads find cleaner and finer-grained versions of previously discovered MHSA behaviors like induction heads, successor heads and attention sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse Autoencoder (SAE) are both sparse dictionary learning methods applied to different Transformer components, and lead to consistent findings in many ways. For instance, we discover a comprehensive family of arithmetic-specific Lorsa heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated interpretability analysis indicates that Lorsa achieves parity with SAE in interpretability while Lorsa exhibits superior circuit discovery properties, especially for features computed collectively by multiple MHSA heads. We also conduct extensive experiments on architectural design ablation, Lorsa scaling law and error analysis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Consistency for Assuring Reliability of Large Language Models</title>
<link>https://arxiv.org/abs/2308.09138</link>
<guid>https://arxiv.org/abs/2308.09138</guid>
<content:encoded><![CDATA[
arXiv:2308.09138v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</title>
<link>https://arxiv.org/abs/2310.03903</link>
<guid>https://arxiv.org/abs/2310.03903</guid>
<content:encoded><![CDATA[
arXiv:2310.03903v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education</title>
<link>https://arxiv.org/abs/2310.12059</link>
<guid>https://arxiv.org/abs/2310.12059</guid>
<content:encoded><![CDATA[
arXiv:2310.12059v4 Announce Type: replace 
Abstract: In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI: The Era of Semantic Decoding</title>
<link>https://arxiv.org/abs/2403.14562</link>
<guid>https://arxiv.org/abs/2403.14562</guid>
<content:encoded><![CDATA[
arXiv:2403.14562v2 Announce Type: replace 
Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Analysis of Human Alignment with *PO</title>
<link>https://arxiv.org/abs/2407.15229</link>
<guid>https://arxiv.org/abs/2407.15229</guid>
<content:encoded><![CDATA[
arXiv:2407.15229v2 Announce Type: replace 
Abstract: At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment</title>
<link>https://arxiv.org/abs/2408.00137</link>
<guid>https://arxiv.org/abs/2408.00137</guid>
<content:encoded><![CDATA[
arXiv:2408.00137v2 Announce Type: replace 
Abstract: A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge</title>
<link>https://arxiv.org/abs/2409.07394</link>
<guid>https://arxiv.org/abs/2409.07394</guid>
<content:encoded><![CDATA[
arXiv:2409.07394v2 Announce Type: replace 
Abstract: Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Across four LLMs, six question-answering (QA) and three summarization datasets, we demonstrate that ADACAD consistently outperforms other decoding baselines with average QA accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt performance when conflict is absent, ADACAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Racing Thoughts: Explaining Contextualization Errors in Large Language Models</title>
<link>https://arxiv.org/abs/2410.02102</link>
<guid>https://arxiv.org/abs/2410.02102</guid>
<content:encoded><![CDATA[
arXiv:2410.02102v2 Announce Type: replace 
Abstract: The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt "John is going fishing, so he walks over to the bank. Can he make an ATM transaction?", a model may incorrectly respond "Yes" if it has not properly contextualized "bank" as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of contextualization errors of this form. This hypothesis identifies dependencies between tokens (e.g., "bank" must be properly contextualized before the final token, "?", integrates information from "bank"), and claims that contextualization errors are a result of violating these dependencies. Using a variety of techniques from mechanistic intepretability, we provide correlational and causal evidence in support of the hypothesis, and suggest inference-time interventions to address it.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</title>
<link>https://arxiv.org/abs/2410.07103</link>
<guid>https://arxiv.org/abs/2410.07103</guid>
<content:encoded><![CDATA[
arXiv:2410.07103v2 Announce Type: replace 
Abstract: Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model's reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known "lost-in-the-middle" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDCure: A Scalable Pipeline for Multi-Document Instruction-Following</title>
<link>https://arxiv.org/abs/2410.23463</link>
<guid>https://arxiv.org/abs/2410.23463</guid>
<content:encoded><![CDATA[
arXiv:2410.23463v3 Announce Type: replace 
Abstract: Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present unique difficulties, including management of inter-document dependencies, redundancy, and incoherent structures. To address this challenge, we introduce MDCure, a scalable and effective instruction data generation framework to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human-annotated data. MDCure generates high-quality synthetic MD instruction data over sets of articles via targeted prompts. We also introduce MDCureRM, a cost-effective, MD-specific reward model to score and filter generated data based on their training utility for MD settings. MDCure is compatible with open- and closed-source models in addition to policy optimization methods such as PPO, enabling even small open-source models to surpass proprietary LLMs as strong generators of high-quality MD instruction data without further data filtering. With MDCure, we fine-tune a wide variety of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model families. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks and domains show MDCure consistently improves performance over pre-trained baselines and base models by up to 75.1%. Our code, datasets, and models are available at https://github.com/yale-nlp/MDCure.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title>
<link>https://arxiv.org/abs/2410.24175</link>
<guid>https://arxiv.org/abs/2410.24175</guid>
<content:encoded><![CDATA[
arXiv:2410.24175v2 Announce Type: replace 
Abstract: Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Judgments with No Gold Standard</title>
<link>https://arxiv.org/abs/2411.07127</link>
<guid>https://arxiv.org/abs/2411.07127</guid>
<content:encoded><![CDATA[
arXiv:2411.07127v2 Announce Type: replace 
Abstract: We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset</title>
<link>https://arxiv.org/abs/2411.08243</link>
<guid>https://arxiv.org/abs/2411.08243</guid>
<content:encoded><![CDATA[
arXiv:2411.08243v2 Announce Type: replace 
Abstract: In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset's content through both manual and automated evaluation; (2) experiments demonstrating the dataset's impact on models' safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian Optimization Approach to Machine Translation Reranking</title>
<link>https://arxiv.org/abs/2411.09694</link>
<guid>https://arxiv.org/abs/2411.09694</guid>
<content:encoded><![CDATA[
arXiv:2411.09694v2 Announce Type: replace 
Abstract: Reranking a list of candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate remains a simple and effective method for improving the overall output quality. Translation scoring models continue to grow in size, with the best models being comparable to generation models. Thus, reranking can add substantial computational cost to the translation pipeline. In this work, we pose reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list. For instance, our method achieves the same CometKiwi score using only 70 scoring evaluations compared a baseline system using 180. We present a multi-fidelity setting for BayesOpt, where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff when using smaller but well-trained distilled proxy scorers.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding</title>
<link>https://arxiv.org/abs/2502.01563</link>
<guid>https://arxiv.org/abs/2502.01563</guid>
<content:encoded><![CDATA[
arXiv:2502.01563v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation</title>
<link>https://arxiv.org/abs/2502.12836</link>
<guid>https://arxiv.org/abs/2502.12836</guid>
<content:encoded><![CDATA[
arXiv:2502.12836v2 Announce Type: replace 
Abstract: Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.17163</link>
<guid>https://arxiv.org/abs/2502.17163</guid>
<content:encoded><![CDATA[
arXiv:2502.17163v3 Announce Type: replace 
Abstract: Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation</title>
<link>https://arxiv.org/abs/2503.15358</link>
<guid>https://arxiv.org/abs/2503.15358</guid>
<content:encoded><![CDATA[
arXiv:2503.15358v2 Announce Type: replace 
Abstract: Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.19878</link>
<guid>https://arxiv.org/abs/2503.19878</guid>
<content:encoded><![CDATA[
arXiv:2503.19878v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users</title>
<link>https://arxiv.org/abs/2504.00799</link>
<guid>https://arxiv.org/abs/2504.00799</guid>
<content:encoded><![CDATA[
arXiv:2504.00799v2 Announce Type: replace 
Abstract: Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Automated Grading with Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2504.05239</link>
<guid>https://arxiv.org/abs/2504.05239</guid>
<content:encoded><![CDATA[
arXiv:2504.05239v2 Announce Type: replace 
Abstract: The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined "golden" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation</title>
<link>https://arxiv.org/abs/2504.07072</link>
<guid>https://arxiv.org/abs/2504.07072</guid>
<content:encoded><![CDATA[
arXiv:2504.07072v2 Announce Type: replace 
Abstract: The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2303.01903</link>
<guid>https://arxiv.org/abs/2303.01903</guid>
<content:encoded><![CDATA[
arXiv:2303.01903v4 Announce Type: replace-cross 
Abstract: Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the \emph{blind} LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. Prophet is general that can be instantiated with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones). Moreover, Prophet can also be integrated with modern large multimodal models in different stages, which is named Prophet++, to further improve the capabilities on knowledge-based VQA tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose-Based Sign Language Appearance Transfer</title>
<link>https://arxiv.org/abs/2410.13675</link>
<guid>https://arxiv.org/abs/2410.13675</guid>
<content:encoded><![CDATA[
arXiv:2410.13675v2 Announce Type: replace-cross 
Abstract: We introduce a method for transferring the signer's appearance in sign language skeletal poses while preserving the sign content. Using estimated poses, we transfer the appearance of one signer to another, maintaining natural movements and transitions. This approach improves pose-based rendering and sign stitching while obfuscating identity. Our experiments show that while the method reduces signer identification accuracy, it slightly harms sign recognition performance, highlighting a tradeoff between privacy and utility. Our code is available at https://github.com/sign-language-processing/pose-anonymization.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators</title>
<link>https://arxiv.org/abs/2412.02467</link>
<guid>https://arxiv.org/abs/2412.02467</guid>
<content:encoded><![CDATA[
arXiv:2412.02467v2 Announce Type: replace-cross 
Abstract: Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision signals. Recently, pre-trained Large Language Models (LLMs) -- even those at the scale of GPT-2 -- have demonstrated great potential in synthesizing tabular data. However, their applications under DP constraints remain largely unexplored. In this work, we address this gap by applying DP techniques to the generation of synthetic tabular data. Our findings shows that LLMs face difficulties in generating coherent text when fine-tuned with DP, as privacy budgets are inefficiently allocated to non-private elements like table structures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning framework for differentially private tabular data generation. The first stage involves non-private fine-tuning on a pseudo dataset, followed by DP fine-tuning on a private dataset. Our empirical results show that this approach improves performance across various settings and metrics compared to directly fine-tuned LLMs in DP contexts. We release our code and setup at https://github.com/tejuafonja/DP-2Stage.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering</title>
<link>https://arxiv.org/abs/2412.06832</link>
<guid>https://arxiv.org/abs/2412.06832</guid>
<content:encoded><![CDATA[
arXiv:2412.06832v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules-and horizontal scaling-replicating tasks across multiple agents-to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG tailored for real-world Question Answering (QA) applications. By integrating task-specific non-functional requirements-such as answer quality, cost, and latency-into the system, we enable dynamic reconfiguration to meet diverse SLAs. Our method maps these Service Level Objectives (SLOs) to system-level parameters, allowing the generation of optimal results within specified resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic re-orchestration of a multi-agent RAG system can effectively manage the trade-off between answer quality and cost. By adjusting the system based on query intent and operational conditions, we systematically balance performance and resource utilization. This approach allows the system to meet SLOs for various query types, showcasing its practicality for real-world applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models</title>
<link>https://arxiv.org/abs/2501.12433</link>
<guid>https://arxiv.org/abs/2501.12433</guid>
<content:encoded><![CDATA[
arXiv:2501.12433v2 Announce Type: replace-cross 
Abstract: Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as "owls as wise," "foxes as unfaithful," etc. Our findings reveal significant stereotyped instances where the model consistently generates images aligned with cultural biases. The current work is the first of its kind to examine animal stereotyping in vision-language models systematically and to highlight a critical yet underexplored dimension of bias in AI-generated visual content.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors</title>
<link>https://arxiv.org/abs/2501.18045</link>
<guid>https://arxiv.org/abs/2501.18045</guid>
<content:encoded><![CDATA[
arXiv:2501.18045v2 Announce Type: replace-cross 
Abstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations</title>
<link>https://arxiv.org/abs/2502.03629</link>
<guid>https://arxiv.org/abs/2502.03629</guid>
<content:encoded><![CDATA[
arXiv:2502.03629v2 Announce Type: replace-cross 
Abstract: Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation</title>
<link>https://arxiv.org/abs/2503.04606</link>
<guid>https://arxiv.org/abs/2503.04606</guid>
<content:encoded><![CDATA[
arXiv:2503.04606v3 Announce Type: replace-cross 
Abstract: Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\sim$14,000$\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocAgent: Graph-Guided LLM Agents for Code Localization</title>
<link>https://arxiv.org/abs/2503.09089</link>
<guid>https://arxiv.org/abs/2503.09089</guid>
<content:encoded><![CDATA[
arXiv:2503.09089v2 Announce Type: replace-cross 
Abstract: Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages</title>
<link>https://arxiv.org/abs/2504.18560</link>
<guid>https://arxiv.org/abs/2504.18560</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, bias testing, multilingual, automated translation, discrimination

Summary: 
The study introduces MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that enhances bias evaluation methods for Large Language Models (LLMs) by allowing systematic multilingual bias testing. MLA-BiTe utilizes automated translation and paraphrasing techniques to enable comprehensive assessments across diverse linguistic contexts. The effectiveness of MLA-BiTe is evaluated by testing four top-performing LLMs in six languages, including two low-resource languages, focusing on seven categories of discrimination. The framework aims to address and mitigate social biases present in LLMs trained on biased data. The study highlights the importance of testing for bias in LLMs across different languages to ensure fair and equitable natural language processing outcomes. 

<br /><br />Summary: <div>
arXiv:2504.18560v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. In this study, we evaluate the effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six languages -- including two low-resource languages -- focusing on seven sensitive categories of discrimination.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Span-Level Hallucination Detection for LLM-Generated Answers</title>
<link>https://arxiv.org/abs/2504.18639</link>
<guid>https://arxiv.org/abs/2504.18639</guid>
<content:encoded><![CDATA[
<div> Semantic Role Labeling, Hallucination Detection, DeBERTa, Textual Entailment, Mu-SHROOM dataset 
Summary: 
Semantic Role Labeling is integrated into a span-level hallucination detection framework for the SemEval-2025 Shared Task for English and Arabic texts. The approach decomposes answers into atomic roles, aligns them with a reference context retrieved through LLM prompting, and evaluates semantic alignment using a DeBERTa-based entailment model. Token-level confidence measures and entailment scores are combined to detect hallucinated spans, leading to competitive performance on the Mu-SHROOM dataset. Hallucinated spans are additionally verified through fact-checking with GPT-4 and LLaMA, contributing to improved detection of hallucination in LLM-generated responses. 
<br /><br />Summary: <div>
arXiv:2504.18639v1 Announce Type: new 
Abstract: Detecting spans of hallucination in LLM-generated answers is crucial for improving factual consistency. This paper presents a span-level hallucination detection framework for the SemEval-2025 Shared Task, focusing on English and Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose the answer into atomic roles, which are then compared with a retrieved reference context obtained via question-based LLM prompting. Using a DeBERTa-based textual entailment model, we evaluate each role semantic alignment with the retrieved context. The entailment scores are further refined through token-level confidence measures derived from output logits, and the combined scores are used to detect hallucinated spans. Experiments on the Mu-SHROOM dataset demonstrate competitive performance. Additionally, hallucinated spans have been verified through fact-checking by prompting GPT-4 and LLaMA. Our findings contribute to improving hallucination detection in LLM-generated responses.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Third-parties Read Our Emotions?</title>
<link>https://arxiv.org/abs/2504.18673</link>
<guid>https://arxiv.org/abs/2504.18673</guid>
<content:encoded><![CDATA[
<div> emotion recognition tasks, third-party annotations, first-party labels, limitations, language models

Summary:
- Natural Language Processing tasks focus on inferring authors' emotions and opinions from text, relying on third-party annotations.
- Human experiments reveal significant limitations in third-party annotations in accurately representing authors' private states.
- Large language models outperform human annotators in emotion recognition tasks.
- Demographic similarity between authors and annotators enhances annotation performance.
- Incorporating first-party demographic information into prompts improves language models' performance marginally but significantly.
- The study introduces a framework for evaluating the limitations of third-party annotations and calls for refined annotation practices for accurately representing and modeling authors' private states.<br /><br />Summary: <div>
arXiv:2504.18673v1 Announce Type: new 
Abstract: Natural Language Processing tasks that aim to infer an author's private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by third-party annotators. However, the assumption that third-party annotators can accurately capture authors' private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations-whether provided by human annotators or large language models (LLMs)-in faithfully representing authors' private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and third-party human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs' performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors' private states.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Speech Translation: Translating Across Space With Binaural Hearables</title>
<link>https://arxiv.org/abs/2504.18715</link>
<guid>https://arxiv.org/abs/2504.18715</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial speech translation, hearables, blind source separation, binaural rendering, real-time inference <br />
Summary: <br />
The article introduces the concept of spatial speech translation for hearables, which translates speakers in a crowded space into the wearer's native language while preserving the spatial cues and unique voice characteristics of each speaker. Technical challenges including blind source separation, localization, real-time expressive translation, and binaural rendering are addressed to achieve real-time inference on Apple M2 silicon. Despite interference from other speakers, the prototype binaural headset achieves a BLEU score of up to 22.01. User studies confirm the system's effectiveness in spatially rendering translated speech in real-world reverberant environments. This work is a significant step towards integrating spatial perception into speech translation. <br /><br />Summary: <div>
arXiv:2504.18715v1 Announce Type: new 
Abstract: Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building UD Cairo for Old English in the Classroom</title>
<link>https://arxiv.org/abs/2504.18718</link>
<guid>https://arxiv.org/abs/2504.18718</guid>
<content:encoded><![CDATA[
<div> treebank, Old English, UD, syntactic constructions, annotation <br />
<br />Summary: 
This paper introduces a sample treebank for Old English based on the UD Cairo sentences, collected in a classroom setting. The data was gathered using a combination of LLM prompting and searches in authentic Old English data. Multiple students with limited prior UD exposure annotated the sentences, with their results compared and adjudicated. Although LLM outputs in Old English do not reflect authentic syntax, post-editing can improve accuracy. Beginner annotators, while not achieving perfect results individually, collectively produce good annotations and learn from the process. Preliminary parsing experiments using Modern English training data show improved performance when parsing on annotated features such as lemma, hyperlemma, and gloss. <div>
arXiv:2504.18718v1 Announce Type: new 
Abstract: In this paper we present a sample treebank for Old English based on the UD Cairo sentences, collected and annotated as part of a classroom curriculum in Historical Linguistics. To collect the data, a sample of 20 sentences illustrating a range of syntactic constructions in the world's languages, we employ a combination of LLM prompting and searches in authentic Old English data. For annotation we assigned sentences to multiple students with limited prior exposure to UD, whose annotations we compare and adjudicate. Our results suggest that while current LLM outputs in Old English do not reflect authentic syntax, this can be mitigated by post-editing, and that although beginner annotators do not possess enough background to complete the task perfectly, taken together they can produce good results and learn from the experience. We also conduct preliminary parsing experiments using Modern English training data, and find that although performance on Old English is poor, parsing on annotated features (lemma, hyperlemma, gloss) leads to improved performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers</title>
<link>https://arxiv.org/abs/2504.18736</link>
<guid>https://arxiv.org/abs/2504.18736</guid>
<content:encoded><![CDATA[
<div> Keywords: EvidenceBench, biomedical papers, hypothesis generation, language models, retrieval systems

Summary: 
EvidenceBench introduces a novel pipeline to automate the task of finding evidence relevant to hypotheses in biomedical papers. The pipeline involves hypothesis generation and sentence-by-sentence annotation of papers, closely following human expert judgment. Multiple sets of human-expert annotations validate the pipeline's accuracy and validity. However, models tested on the benchmark still lag significantly behind human experts in performance. To support further research, a larger dataset EvidenceBench-100k with over 100,000 fully annotated papers is created. The datasets are made available for model training and development. This study highlights the challenges in automating evidence retrieval in biomedical literature, indicating the need for further advancements in language models and retrieval systems. 

<br /><br />Summary: <div>
arXiv:2504.18736v1 Announce Type: new 
Abstract: We study the task of automatically finding evidence relevant to hypotheses in biomedical papers. Finding relevant evidence is an important step when researchers investigate scientific hypotheses. We introduce EvidenceBench to measure models performance on this task, which is created by a novel pipeline that consists of hypothesis generation and sentence-by-sentence annotation of biomedical papers for relevant evidence, completely guided by and faithfully following existing human experts judgment. We demonstrate the pipeline's validity and accuracy with multiple sets of human-expert annotations. We evaluated a diverse set of language models and retrieval systems on the benchmark and found that model performances still fall significantly short of the expert level on this task. To show the scalability of our proposed pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated papers with hypotheses to facilitate model training and development. Both datasets are available at https://github.com/EvidenceBench/EvidenceBench
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.18762</link>
<guid>https://arxiv.org/abs/2504.18762</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, legal domain, pre-training, synthetic data augmentation, legal document analysis 

Summary: 
SynLexLM introduces a novel approach to efficiently pre-train large language models (LLMs) for specialized legal domains like law. The method incorporates curriculum learning, starting from simple to complex legal texts and queries, along with synthetic data augmentation using models like Gemini Pro to address the challenge of limited legal data availability. The goal is to enhance performance on legal benchmarks such as BigLaw-Bench and EUR-Lex-Sum compared to traditional models and fine-tuned versions. The initial phase of the work involves generating synthetic question-answer pairs that reflect legal reasoning. The ultimate aim is to improve legal document analysis and research tools, potentially making advanced legal AI more accessible to a wider audience. <div>
arXiv:2504.18762v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are powerful but often require extensive fine-tuning and large datasets for specialized domains like law. General-purpose pre-training may not capture legal nuances, and acquiring sufficient legal data is challenging. We introduce SynLexLM, a novel approach to efficiently pre-train a legal LLM. Our method employs curriculum learning, progressing from simple to complex legal texts and queries, combined with synthetic data augmentation using models like Gemini Pro to address data scarcity. We aim to achieve improved performance on legal benchmarks (BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned versions. Preliminary work involves generating synthetic QA pairs reflecting legal reasoning. This work aims to enhance legal document analysis and research tools, potentially democratizing access to advanced legal AI.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation</title>
<link>https://arxiv.org/abs/2504.18805</link>
<guid>https://arxiv.org/abs/2504.18805</guid>
<content:encoded><![CDATA[
<div> Keywords: short-form videos, scientific papers, multi-LLM agentic framework, content summarization, visual scene planning <br />
Summary: <br />
Generating engaging and accurate short-form videos from scientific papers is a challenging task due to content complexity and the gap between experts and readers. Existing methods often lack factual accuracy and visual quality, hindering effective dissemination of scientific information. To address these issues, the SciTalk framework is proposed, leveraging a multi-LLM agentic approach that incorporates various sources and specialized agents for content summarization and visual planning. The framework also utilizes an iterative feedback mechanism to enhance video quality based on user simulation feedback. Experimental evaluations demonstrate that SciTalk surpasses simple prompting methods in producing scientifically accurate and engaging video content. While still not reaching human creator quality, the framework offers valuable insights into feedback-driven video generation and its benefits for scientific communication. The code, data, and generated videos will be made publicly available. <br /> 

Summary: <div>
arXiv:2504.18805v1 Announce Type: new 
Abstract: Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks</title>
<link>https://arxiv.org/abs/2504.18838</link>
<guid>https://arxiv.org/abs/2504.18838</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, evaluation, capability-based, automated, generalization

Summary:
Large Language Models (LLMs) have rapidly become essential across various sectors, leading to core challenges in their evaluation. The survey discusses transitions from task-specific to capability-based evaluation and from manual to automated evaluation. Key challenges include the evaluation generalization issue, as bounded test sets struggle to keep up with the expanding abilities of LLMs. The survey delves into methods, datasets, evaluators, and metrics in addressing these challenges. A living GitHub repository is maintained for continual updates and corrections, with contributions and collaboration encouraged. The dynamic nature of the field necessitates a collaborative approach to evolving evaluation practices.<br /><br />Summary: <div>
arXiv:2504.18838v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs poses for evaluation. We identify and analyze two pivotal transitions: (i) from task-specific to capability-based evaluation, which reorganizes benchmarks around core competencies such as knowledge, reasoning, instruction following, multi-modal understanding, and safety; and (ii) from manual to automated evaluation, encompassing dynamic dataset curation and "LLM-as-a-judge" scoring.
  Yet, even with these transitions, a crucial obstacle persists: the evaluation generalization issue. Bounded test sets cannot scale alongside models whose abilities grow seemingly without limit. We will dissect this issue, along with the core challenges of the above two transitions, from the perspectives of methods, datasets, evaluators, and metrics. Due to the fast evolving of this field, we will maintain a living GitHub repository (links are in each section) to crowd-source updates and corrections, and warmly invite contributors and collaborators.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning</title>
<link>https://arxiv.org/abs/2504.18839</link>
<guid>https://arxiv.org/abs/2504.18839</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, conversational breakdowns, fine-tuning, dialogue detection, real-time deployment

Summary: 
This paper explores the challenges of detecting and mitigating dialogue breakdowns within Large Language Models (LLMs) used in conversational systems. The proposed approach combines specialized fine-tuning techniques with advanced prompting strategies such as few-shot learning, chain-of-thought reasoning, and analogical prompting. By fine-tuning a small 8B model, the authors demonstrate its robust capabilities in classifying and calibrating responses in English and Japanese dialogues, as well as its generalization on the BETOLD dataset with a 7% accuracy improvement over the base model. Additionally, a real-time deployment architecture is introduced to selectively escalate suspicious responses to more resource-intensive models only when breakdowns are detected, resulting in cost and energy savings. Experimental results show that the proposed method surpasses prior state-of-the-art specialized classifiers and narrows the performance gaps between smaller open-source models and large proprietary ones. This approach provides a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability.<br /><br />Summary: <div>
arXiv:2504.18839v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly changing various domains. However, their capabilities in handling conversational breakdowns still require an in-depth exploration. This paper addresses the challenge of detecting and mitigating dialogue breakdowns within LLM-driven conversational systems. While powerful models from OpenAI and Anthropic excel in many dialogue tasks, they can still produce incoherent or contradictory responses, commonly referred to as breakdowns, which undermine user trust. To tackle this, we propose an approach that combines specialized fine-tuning with advanced prompting strategies, including few-shot learning, chain-of-thought reasoning, and analogical prompting. In particular, we fine-tune a small 8B model and demonstrate its robust classification and calibration capabilities in English and Japanese dialogue. We also validate its generalization on the BETOLD dataset, achieving a 7\% accuracy improvement over its base model. Furthermore, we introduce a real-time deployment architecture that selectively escalates suspicious responses to more resource-intensive frontier models only when breakdowns are detected, significantly cutting operational expenses and energy consumption. Experimental results show our method surpasses prior state-of-the-art specialized classifiers while also narrowing performance gaps between smaller open-source models and large proprietary ones. Our approach offers a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When2Call: When (not) to Call Tools</title>
<link>https://arxiv.org/abs/2504.18851</link>
<guid>https://arxiv.org/abs/2504.18851</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, benchmark, tool-calling decision-making, training data, preference optimization<br />
Summary:<br />
Modern Language Models (LMs) are increasingly utilizing external tools to enhance their capabilities. A new benchmark, When2Call, focuses on evaluating the decision-making process of LMs in calling external tools. This benchmark assesses when to generate a tool call, when to ask follow-up questions, and when to acknowledge the limitations of the tools available. State-of-the-art LMs exhibit room for improvement on When2Call, underscoring the significance of this evaluation metric. The development of a training set for When2Call and the implementation of a preference optimization training regime have shown substantial improvements compared to traditional fine-tuning methods. The benchmark, training data, and evaluation scripts are openly available on GitHub. This initiative aims to drive advancements in tool-calling strategies of LMs, paving the way for more effective integration of external tools into language processing systems.<br /><br />Summary: <div>
arXiv:2504.18851v1 Announce Type: new 
Abstract: Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling -- whether the correct tool is called with the correct parameters -- and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when to admit the question can't be answered with the tools provided. We find that state-of-the-art tool-calling LMs show significant room for improvement on When2Call, indicating the importance of this benchmark. We also develop a training set for When2Call and leverage the multiple-choice nature of the benchmark to develop a preference optimization training regime, which shows considerably more improvement than traditional fine-tuning. We release the benchmark and training data as well as evaluation scripts at https://github.com/NVIDIA/When2Call.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation</title>
<link>https://arxiv.org/abs/2504.18857</link>
<guid>https://arxiv.org/abs/2504.18857</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Dimension-Wise Positional Embeddings Manipulation, context window, extrapolation, performance improvement

Summary:<br /><br />Large Language Models (LLMs) face challenges with processing and generating coherent context when exceeding the pre-trained length. A new framework, Dimension-Wise Positional Embeddings Manipulation (DPE), is proposed to extend the context window without additional training overhead. DPE focuses on manipulating key dimensions for context extension, reaching optimal states for each dimension. It outperforms baseline methods like YaRN and Self-Extend, allowing models like Llama3-8k 8B to support 128k token context windows. DPE seamlessly integrates with Flash Attention 2 and boosts performance on long-context benchmarks like RULER for models like Llama3.1 70B by 18 points. Even compared to commercial models, Llama 3.1 70B with DPE achieves superior performance to GPT-4-128K. <div>
arXiv:2504.18857v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle to process and generate coherent context when the number of input tokens exceeds the pre-trained length. Recent advancements in long-context extension have significantly expanded the context window of LLMs but require expensive overhead to train the large-scale models with longer context. In this work, we propose Dimension-Wise Positional Embeddings Manipulation (DPE), a training-free framework to extrapolate the context window of LLMs by diving into RoPE's different hidden dimensions. Instead of manipulating all dimensions equally, DPE detects the effective length for every dimension and finds the key dimensions for context extension. We reuse the original position indices with their embeddings from the pre-trained model and manipulate the key dimensions' position indices to their most effective lengths. In this way, DPE adjusts the pre-trained models with minimal modifications while ensuring that each dimension reaches its optimal state for extrapolation. DPE significantly surpasses well-known baselines such as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of 128k tokens without continual training and integrates seamlessly with Flash Attention 2. In addition to its impressive extrapolation capability, DPE also dramatically improves the models' performance within training length, such as Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When compared with commercial models, Llama 3.1 70B with DPE even achieves better performance than GPT-4-128K.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adversarial Training Improves the Representation of Refusal</title>
<link>https://arxiv.org/abs/2504.18872</link>
<guid>https://arxiv.org/abs/2504.18872</guid>
<content:encoded><![CDATA[
<div> refusal behavior, language models, Latent Adversarial Training, robustness, targeted attacks <br />
<br />
Summary: 
The article explores the impact of Latent Adversarial Training (LAT) on the encoding of refusal behavior in language models. By analyzing a model called Llama 27B, the study compares LAT with traditional supervised safety fine-tuning (SSFT) and embedding space adversarial training (AT). It finds that LAT significantly alters the representation of refusal behavior in the model's latent space, concentrating it in the first two Singular Value Decomposition (SVD) components. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks. While LAT models show improved robustness against attacks using vectors from reference models, they become more vulnerable to self-generated vectors compared to SSFT and AT. The study suggests that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its strengths and vulnerabilities in enhancing model safety. <br /> <div>
arXiv:2504.18872v1 Announce Type: new 
Abstract: Recent work has shown that language models' refusal behavior is primarily encoded in a single direction in their latent space, making it vulnerable to targeted attacks. Although Latent Adversarial Training (LAT) attempts to improve robustness by introducing noise during training, a key question remains: How does this noise-based training affect the underlying representation of refusal behavior? Understanding this encoding is crucial for evaluating LAT's effectiveness and limitations, just as the discovery of linear refusal directions revealed vulnerabilities in traditional supervised safety fine-tuning (SSFT).
  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the refusal behavior in the model's latent space compared to SSFT and embedding space adversarial training (AT). By computing activation differences between harmful and harmless instruction pairs and applying Singular Value Decomposition (SVD), we find that LAT significantly alters the refusal representation, concentrating it in the first two SVD components which explain approximately 75 percent of the activation differences variance - significantly higher than in reference models. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks: LAT models show improved robustness when attacked with vectors from reference models but become more vulnerable to self-generated vectors compared to SSFT and AT. Our findings suggest that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its potential strengths and vulnerabilities for improving model safety.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification</title>
<link>https://arxiv.org/abs/2504.18884</link>
<guid>https://arxiv.org/abs/2504.18884</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, variability, reproducibility, sentiment analysis, ensemble strategy  
Summary:  
- The study focuses on the challenges of variability and reproducibility in results obtained from large language models (LLMs) for sentiment analysis.  
- Existing literature has largely overlooked these issues, which are crucial for ensuring the reliability of LLMs.  
- Human annotation techniques use majority voting to resolve disagreements among annotators, highlighting the importance of ensemble strategies in LLM applications.  
- The study introduces a straightforward ensemble strategy for sentiment analysis using medium-sized LLMs, demonstrating its effectiveness in producing more robust and accurate results.  
- By utilizing multiple inferences with medium-sized LLMs, the ensemble approach reduces the root mean square error (RMSE) by 18.6% compared to using a single large model with a single attempt.  

<br /><br />Summary: <div>
arXiv:2504.18884v1 Announce Type: new 
Abstract: With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction</title>
<link>https://arxiv.org/abs/2504.18938</link>
<guid>https://arxiv.org/abs/2504.18938</guid>
<content:encoded><![CDATA[
<div> Correction, Chinese Spelling Correction, Large Language Models, domain adaptation, length consistency

Summary:<br />
This work introduces a Multi-Turn CSC framework for Chinese Spelling Correction (CSC) tasks, extending traditional CSC to variable-length correction scenarios such as Chinese Splitting Error Correction (CSEC) and ASR N-best Error Correction. The proposed framework, MTCSC, leverages a retrieval database and length reflection mechanism to address domain adaptation and ensure output length fidelity. By fine-tuning retrievers on domain-specific training data and dictionaries, MTCSC significantly outperforms current approaches in correction quality, particularly in handling domain-specific and variable-length error correction tasks. The framework also introduces a multi-source combination strategy with iterative length reflection for consistent output lengths, showcasing improved performance across diverse domain datasets. <br /><br />Summary: <div>
arXiv:2504.18938v1 Announce Type: new 
Abstract: Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens in sentences. While Large Language Models (LLMs) have shown remarkable success in identifying and rectifying potential errors, they often struggle with maintaining consistent output lengths and adapting to domain-specific corrections. Furthermore, existing CSC task impose rigid constraints requiring input and output lengths to be identical, limiting their applicability. In this work, we extend traditional CSC to variable-length correction scenarios, including Chinese Splitting Error Correction (CSEC) and ASR N-best Error Correction. To address domain adaptation and length consistency, we propose MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection mechanism. Our approach constructs a retrieval database from domain-specific training data and dictionaries, fine-tuning retrievers to optimize performance for error-containing inputs. Additionally, we introduce a multi-source combination strategy with iterative length reflection to ensure output length fidelity. Experiments across diverse domain datasets demonstrate that our method significantly outperforms current approaches in correction quality, particularly in handling domain-specific and variable-length error correction tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LawFlow : Collecting and Simulating Lawyers' Thought Processes</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
<div> dataset, legal workflows, AI, decision-making, law students <br />
Summary:
This article introduces LawFlow, a dataset of end-to-end legal workflows collected from trained law students, aimed at capturing the complexity of real-world legal practice. The comparison between human and LLM-generated workflows reveals differences in structure, reasoning flexibility, and plan execution. Human workflows are found to be more modular and adaptive, while LLM workflows are more sequential and exhaustive. Legal professionals prefer AI to play a supportive role in brainstorming, identifying blind spots, and surfacing alternatives rather than executing entire workflows. Design suggestions are proposed to align AI assistance with human goals, emphasizing clarity, completeness, creativity, and efficiency through hybrid planning and decision-point support. The study highlights the limitations of current LLMs in supporting complex legal workflows and points towards opportunities for developing collaborative, reasoning-aware legal AI systems. <div>
arXiv:2504.18942v1 Announce Type: new 
Abstract: Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Fisher-weighted Model Merging via Bayesian Optimization</title>
<link>https://arxiv.org/abs/2504.18992</link>
<guid>https://arxiv.org/abs/2504.18992</guid>
<content:encoded><![CDATA[
<div> merging, language models, multi-task, parameter importance, dynamic

Summary:<br />
This paper presents Dynamic Fisher-weighted Merging (DF-Merge), a novel approach to creating multi-task models by combining fine-tuned language models at the parameter level. DF-Merge dynamically adjusts coefficients associated with candidate models using Bayesian optimization to maximize performance on validation sets. The process integrates parameter importance based on Fisher information conditioned by the coefficients, leading to significant performance improvements compared to existing merging approaches. Experimental results demonstrate the effectiveness of DF-Merge across models of varying sizes and tasks. The unified view of merging in DF-Merge allows for near-optimal performance to be achieved in a few iterations, even with limited validation data. The approach outperforms strong baselines and highlights the benefits of integrating parameter importance in the merging process. <div>
arXiv:2504.18992v1 Announce Type: new 
Abstract: The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses, leading to a notable performance gap compared to multi-task fine-tuning. In this paper, we unify these seemingly distinct strategies into a more general merging framework, and introduce Dynamic Fisher-weighted Merging (DF-Merge). Specifically, candidate models are associated with a set of coefficients that linearly scale their fine-tuned parameters. Bayesian optimization is applied to dynamically adjust these coefficients, aiming to maximize overall performance on validation sets. Each iteration of this process integrates parameter importance based on the Fisher information conditioned by the coefficients. Experimental results show that DF-Merge outperforms strong baselines across models of different sizes and a variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises from the unified view of merging and that near-optimal performance is achievable in a few iterations, even with minimal validation data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs</title>
<link>https://arxiv.org/abs/2504.19019</link>
<guid>https://arxiv.org/abs/2504.19019</guid>
<content:encoded><![CDATA[
<div> Graph of ATtacks, Large Language Models, Adversarial Prompts, Robustness, Jailbreak <br />
<br />
Summary: 
The article introduces Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the alignment of Large Language Models (LLMs) with societal standards. GoAT excels at creating highly effective jailbreak prompts with fewer queries to the victim model, achieving up to five times better success rate against robust models like Llama. It is a black-box attack that does not require access to the targeted model's parameters. GoAT's reasoning is based on a sophisticated graph structure, enabling a deeper integration and refinement of attack paths. By combining and improving thoughts iteratively, GoAT allows collaborative exploration of adversarial vulnerabilities in LLMs. The code for implementation is available on GitHub. <div>
arXiv:2504.19019v1 Announce Type: new 
Abstract: The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting</title>
<link>https://arxiv.org/abs/2504.19021</link>
<guid>https://arxiv.org/abs/2504.19021</guid>
<content:encoded><![CDATA[
<div> language models, text classification, dataset augmentation, fine-tuning, academic publications
Summary:
- The study focuses on efficient text classification for academic publications using pre-trained language models (PLMs) like BERT, SciBERT, BioBERT, and BlueBERT.
- Dataset augmentation is done by retrieving additional articles from the Web of Science database and using PLMs to predict labels for them.
- A hard-voting strategy is employed to combine predictions for improved accuracy and confidence.
- Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly enhances classification accuracy, especially in specialized domains.
- Domain-specific models like SciBERT and BioBERT outperform general-purpose models like BERT consistently. This study highlights the effectiveness of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in developing robust and scalable solutions for automated academic text classification.<br /><br />Summary: <div>
arXiv:2504.19021v1 Announce Type: new 
Abstract: Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation</title>
<link>https://arxiv.org/abs/2504.19024</link>
<guid>https://arxiv.org/abs/2504.19024</guid>
<content:encoded><![CDATA[
<div> Keywords: k-step return, reinforcement learning, knowledge distillation, text generation, large language model<br />
Summary:<br />
The proposed method, KETCHUP, introduces a novel approach for estimating k-step returns in Reinforcement Learning-based knowledge distillation in text generation tasks. By leveraging the Bellman Optimality Equation for multiple steps, KETCHUP reduces gradient estimate variance, enhancing RL optimization, particularly with larger student model sizes. Empirical evaluations across three text generation tasks demonstrate superior performance in both standard metrics and evaluation against large language models. These findings indicate the efficacy of KETCHUP in improving RL-based knowledge distillation for large language model research. <div>
arXiv:2504.19024v1 Announce Type: new 
Abstract: We propose a novel k-step return estimation method (called KETCHUP) for Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation tasks. Our idea is to induce a K-step return by using the Bellman Optimality Equation for multiple steps. Theoretical analysis shows that this K-step formulation reduces the variance of the gradient estimates, thus leading to improved RL optimization especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation. These results suggest that our K-step return induction offers a promising direction for enhancing RL-based KD in LLM research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Translation Decoding with Quality Estimation on LLMs</title>
<link>https://arxiv.org/abs/2504.19044</link>
<guid>https://arxiv.org/abs/2504.19044</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural machine translation, calibration, likelihood, quality, decoding

Summary:
This paper introduces a new method for improving neural machine translation systems by calibrating hypothesis likelihoods with translation quality. By directly optimizing the correlation between hypothesis likelihoods and real-world translation quality, the proposed method enhances the effectiveness of translation decoding. This approach leads to significant improvements in translation quality on large language models (LLMs) with limited training data. The calibrated translation likelihoods serve as a strong proxy for translation quality and outperform state-of-the-art translation quality estimation models. The methodology also enhances the efficiency of maximum a posteriori (MAP) decoding, making deployment of the translation model more effective in real-world scenarios. The resulting state-of-the-art translation model, covering 10 languages, along with code and human evaluation data, has been released to the community for further research and development. 

<br /><br />Summary: <div>
arXiv:2504.19044v1 Announce Type: new 
Abstract: Neural machine translation (NMT) systems typically employ maximum a posteriori (MAP) decoding to select the highest-scoring translation from the distribution mass. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses -- the decoding objective is not aligned with real-world translation quality. This paper proposes calibrating hypothesis likelihoods with translation quality from a distribution view by directly optimizing their Pearson correlation -- thereby enhancing the effectiveness of translation decoding. With our method, translation on large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations -- even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released to the community: https://github.com/moore3930/calibrating-llm-mt.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2504.19061</link>
<guid>https://arxiv.org/abs/2504.19061</guid>
<content:encoded><![CDATA[
<div> extract, clinical summarization, large language models, discharge reports, hallucinations <br />
Summary: Large language models are being investigated for their effectiveness in extracting key events from discharge reports in healthcare. These models aim to provide precise and concise information transfer for enhanced patient understanding and care management. The study evaluates the performance of open-source LLMs in identifying reasons for hospital admission, significant in-hospital events, critical follow-up actions, and detecting hallucinations in the summaries produced. Detecting hallucinations is crucial as it impacts the reliability of information and can influence patient care and treatment outcomes. Comprehensive numerical simulations are conducted to rigorously assess the accuracy and fidelity of the extracted content in clinical summarization. <div>
arXiv:2504.19061v1 Announce Type: new 
Abstract: Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, such as reasons for hospital admission, significant in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive numerical simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics</title>
<link>https://arxiv.org/abs/2504.19066</link>
<guid>https://arxiv.org/abs/2504.19066</guid>
<content:encoded><![CDATA[
<div> Keywords: Extreme Weather Events, Large Language Models, Small Language Models, Reasoning Paths, Emotion Analysis 

Summary:<br /><br />Accurate assessments of extreme weather events are essential for research and policy, but data gaps hinder effective decision-making. This paper introduces the ClimaEmpact framework, using Large Language Models (LLMs) to enhance Small Language Models (SLMs) for extreme weather analytics. By incorporating structured reasoning paths and utilizing the ExtremeWeatherNews dataset, the proposed method, Extreme Weather Reasoning-Aware Alignment (EWRA), improves the generation of domain-specific responses for tasks such as vulnerability/impact categorization, topic labeling, and emotion analysis. Results demonstrate that the alignment of SLMs with advanced reasoning strategies leads to outputting domain-aligned responses, outperforming task-specific models and providing enhanced real-world applicability for extreme weather analytics. <div>
arXiv:2504.19066v1 Announce Type: new 
Abstract: Accurate assessments of extreme weather events are vital for research and policy, yet localized and granular data remain scarce in many parts of the world. This data gap limits our ability to analyze potential outcomes and implications of extreme weather events, hindering effective decision-making. Large Language Models (LLMs) can process vast amounts of unstructured text data, extract meaningful insights, and generate detailed assessments by synthesizing information from multiple sources. Furthermore, LLMs can seamlessly transfer their general language understanding to smaller models, enabling these models to retain key knowledge while being fine-tuned for specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware Alignment (EWRA), a method that enhances small language models (SLMs) by incorporating structured reasoning paths derived from LLMs, and ExtremeWeatherNews, a large dataset of extreme weather event-related news articles. EWRA and ExtremeWeatherNews together form the overall framework, ClimaEmpact, that focuses on addressing three critical extreme-weather tasks: categorization of tangible vulnerabilities/impacts, topic labeling, and emotion analysis. By aligning SLMs with advanced reasoning strategies on ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and domain-specific responses for extreme weather analytics. Our results show that the approach proposed guides SLMs to output domain-aligned responses, surpassing the performance of task-specific models and offering enhanced real-world applicability for extreme weather analytics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-Efficient Language Model for Hinglish Conversational AI</title>
<link>https://arxiv.org/abs/2504.19070</link>
<guid>https://arxiv.org/abs/2504.19070</guid>
<content:encoded><![CDATA[
<div> Keywords: Hinglish, chatbot, language model, fine-tuning, data scarcity 

Summary: 
This paper discusses the development of a sample-efficient language model for a conversational Hinglish chatbot. Hinglish, a mix of Hindi and English, poses challenges due to inconsistent spelling and limited conversational data quality. The study evaluates pre-trained cross-lingual language models such as Gemma3-4B and Qwen2.5-7B, utilizing fine-tuning techniques to enhance performance on Hinglish conversational tasks. By incorporating synthetically generated dialogues and insights from existing Hinglish datasets, the approach aims to overcome data scarcity issues. Experimental findings indicate that models with fewer parameters, when properly fine-tuned on high-quality code-mixed data, can deliver competitive results in Hinglish conversation generation while ensuring computational efficiency. 

<br /><br />Summary: <div>
arXiv:2504.19070v1 Announce Type: new 
Abstract: This paper presents our process for developing a sample-efficient language model for a conversational Hinglish chatbot. Hinglish, a code-mixed language that combines Hindi and English, presents a unique computational challenge due to inconsistent spelling, lack of standardization, and limited quality of conversational data. This work evaluates multiple pre-trained cross-lingual language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning techniques to improve performance on Hinglish conversational tasks. The proposed approach integrates synthetically generated dialogues with insights from existing Hinglish datasets to address data scarcity. Experimental results demonstrate that models with fewer parameters, when appropriately fine-tuned on high-quality code-mixed data, can achieve competitive performance for Hinglish conversation generation while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reasoning for LLMs through Speculative Chain-of-Thought</title>
<link>https://arxiv.org/abs/2504.19095</link>
<guid>https://arxiv.org/abs/2504.19095</guid>
<content:encoded><![CDATA[
<div> Latency reduction, Large reasoning language models, Speculative Chain-of-Thought, Model collaboration, Efficient drafting

Summary: 
Speculative Chain-of-Thought (SCoT) introduces a new approach to reducing reasoning latency in large reasoning language models like OpenAI-o1 and Deepseek-R1. By combining large and small models, SCoT accelerates average reasoning speed by conducting thought-level drafting with a lightweight draft model and then correcting errors with the target model. This thinking behavior alignment improves drafting efficiency and maintains prediction accuracy for complex problems. Experimental results on various datasets demonstrate that SCoT reduces reasoning latency by 48% to 66% for Deepseek-R1-Distill-Qwen-32B while achieving performance levels close to the target model. The code for SCoT is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2504.19095v1 Announce Type: new 
Abstract: Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48\%$\sim$66\% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at https://github.com/Jikai0Wang/Speculative_CoT.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.19101</link>
<guid>https://arxiv.org/abs/2504.19101</guid>
<content:encoded><![CDATA[
<div> Framework, Federated Learning, Retrieval-Augmented Generation, Data Privacy, Knowledge Distillation
Summary: 
The article introduces a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG) to address challenges faced by private RAG systems. FedE4RAG utilizes federated learning to collaboratively train client-side RAG retrieval models while ensuring data privacy. Knowledge distillation is employed for communication between server and client models to improve generalization. Homomorphic encryption is applied in federated learning to protect model parameters and mitigate data leakage concerns. Experimental results on a real-world dataset validate the effectiveness of FedE4RAG in enhancing the performance of private RAG systems while maintaining robust data privacy protection. 
<br /><br />Summary: <div>
arXiv:2504.19101v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution for enhancing the accuracy and credibility of Large Language Models (LLMs), particularly in Question & Answer tasks. This is achieved by incorporating proprietary and private data from integrated databases. However, private RAG systems face significant challenges due to the scarcity of private domain data and critical data privacy issues. These obstacles impede the deployment of private RAG systems, as developing privacy-preserving RAG systems requires a delicate balance between data security and data availability. To address these challenges, we regard federated learning (FL) as a highly promising technology for privacy-preserving RAG services. We propose a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG). This framework facilitates collaborative training of client-side RAG retrieval models. The parameters of these models are aggregated and distributed on a central-server, ensuring data privacy without direct sharing of raw data. In FedE4RAG, knowledge distillation is employed for communication between the server and client models. This technique improves the generalization of local RAG retrievers during the federated learning process. Additionally, we apply homomorphic encryption within federated learning to safeguard model parameters and mitigate concerns related to data leakage. Extensive experiments conducted on the real-world dataset have validated the effectiveness of FedE4RAG. The results demonstrate that our proposed framework can markedly enhance the performance of private RAG systems while maintaining robust data privacy protection.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries</title>
<link>https://arxiv.org/abs/2504.19110</link>
<guid>https://arxiv.org/abs/2504.19110</guid>
<content:encoded><![CDATA[
<div> Benchmark, Language models, Automated Proof Engineering, Mathlib, Eleanstic

Summary:
Automated Proof Engineering (APE) leverages large language models (LLMs) to automate proof engineering tasks like feature additions and bug fixing in formal mathematics libraries. APE-Bench I introduces a benchmark based on real-world commit histories of Mathlib, presenting diverse file-level tasks verified using Lean compiler and LLM-as-a-Judge. Eleanstic is a parallel verification infrastructure designed for checking proofs across different Mathlib versions. Experimental results show LLMs excel in localized edits but struggle with complex proof engineering tasks. This research paves the way for developing autonomous workflows in proof engineering, with future benchmarks aiming for multi-file coordination, project-scale verification, and agent-based systems capable of planning, editing, and maintaining formal libraries. 

<br /><br />Summary: <div>
arXiv:2504.19110v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has shown promise in formal theorem proving, yet existing benchmarks remain limited to isolated, static proof tasks, failing to capture the iterative, engineering-intensive workflows of real-world formal mathematics libraries. Motivated by analogous advances in software engineering, we introduce the paradigm of Automated Proof Engineering (APE), which aims to automate proof engineering tasks such as feature addition, proof refactoring, and bug fixing using LLMs. To facilitate research in this direction, we present APE-Bench I, the first realistic benchmark built from real-world commit histories of Mathlib4, featuring diverse file-level tasks described in natural language and verified via a hybrid approach combining the Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable parallel verification infrastructure optimized for proof checking across multiple versions of Mathlib. Empirical results on state-of-the-art LLMs demonstrate strong performance on localized edits but substantial degradation on handling complex proof engineering. This work lays the foundation for developing agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination, project-scale verification, and autonomous agents capable of planning, editing, and repairing formal libraries.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, self-play critic, error detection, reasoning process benchmarks, mathematical reasoning performance
Summary:
The paper introduces a novel approach called Self-Play Critic (SPC) to evaluate the step-by-step reliability of large language model (LLM) reasoning without the need for manual step-level annotation. SPC involves a "sneaky generator" model and a "critic" model that engage in an adversarial game to assess the correctness of reasoning steps. Through iterative improvement using reinforcement learning based on game outcomes, SPC enhances its error detection capabilities and surpasses strong baselines on three reasoning process benchmarks. Additionally, applying SPC to guide the test-time search of diverse LLMs improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.<br /><br />Summary: 
- Introduction of Self-Play Critic (SPC) for evaluating LLM reasoning
- SPC involves a "sneaky generator" and a "critic" model in an adversarial game
- Iterative improvement of SPC through reinforcement learning
- SPC enhances error detection capabilities and surpasses baselines on benchmarks
- SPC improves mathematical reasoning performance of diverse LLMs <div>
arXiv:2504.19162v1 Announce Type: new 
Abstract: Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a "sneaky generator" that deliberately produces erroneous steps designed to be difficult to detect, and a "critic" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WuNeng: Hybrid State with Attention</title>
<link>https://arxiv.org/abs/2504.19191</link>
<guid>https://arxiv.org/abs/2504.19191</guid>
<content:encoded><![CDATA[
<div> Keywords: WuNeng architecture, recurrent neural network, attention mechanisms, multi-head attention, expressivity

Summary: 
The WuNeng architecture combines recurrent neural network-based RWKV-7 with advanced attention mechanisms to enhance the expressivity of large language models. It prioritizes contextual coherence over reducing KV cache size and introduces additional state-driven heads to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among different types of heads for robust information integration. The architecture also includes a multi-token state processing mechanism that leverages continuous RWKV-7 state to capture complex dependencies, leading to significant improvements in expressivity. Despite these enhancements, WuNeng maintains computational efficiency by minimizing additional parameters. This approach allows the model to excel in tasks that require complex reasoning and sequence generation, setting a new standard for balancing expressivity and efficiency in modern neural architectures.<br /><br />Summary: <div>
arXiv:2504.19191v1 Announce Type: new 
Abstract: The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora</title>
<link>https://arxiv.org/abs/2504.19209</link>
<guid>https://arxiv.org/abs/2504.19209</guid>
<content:encoded><![CDATA[
<div> keywords: implementation choices, Dynamic Embedded Topic Model, diachronic corpora, scalability, temporal distributions

Summary: 
The study investigates the impact of various implementation decisions on the Dynamic Embedded Topic Model when applied to five different diachronic corpora. The research aims to identify key factors that will enhance the model's utility and future development. The findings highlight the importance of prioritizing the scalability of vocabulary size to leverage embedded representations effectively. Additionally, the study emphasizes the need for more flexible modeling of intervals to accommodate the uneven temporal distribution of historical writings. Furthermore, the research indicates that performance is not significantly or consistently affected by certain aspects that may otherwise limit the model's applicability or require excessive resources for grid search. Overall, the study underscores the significance of these identified priorities in maximizing the model's effectiveness for applied scholarship. 

<br /><br />Summary: <div>
arXiv:2504.19209v1 Announce Type: new 
Abstract: We measure the effects of several implementation choices for the Dynamic Embedded Topic Model, as applied to five distinct diachronic corpora, with the goal of isolating important decisions for its use and further development. We identify priorities that will maximize utility in applied scholarship, including the practical scalability of vocabulary size to best exploit the strengths of embedded representations, and more flexible modeling of intervals to accommodate the uneven temporal distributions of historical writing. Of similar importance, we find performance is not significantly or consistently affected by several aspects that otherwise limit the model's application or might consume the resources of a grid search.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
<div> hallucination detection, Large Language Models, uncertainty quantification, ensemble approach, Python toolkit<br />
<br />
Summary: 
Hallucinations in Large Language Models (LLMs) are a significant challenge in critical domains such as healthcare and finance. In response, a novel framework for zero-resource hallucination detection is proposed, leveraging various uncertainty quantification techniques. By transforming these techniques into standardized confidence scores and incorporating them into a tunable ensemble approach, practitioners can optimize performance for specific use cases. The accompanying Python toolkit, UQLM, offers a comprehensive suite of scorers for streamlined implementation. Extensive experiments on LLM question-answering benchmarks demonstrate the superiority of the tunable ensemble over individual scorers and existing methods, highlighting the benefits of customized hallucination detection strategies in enhancing the accuracy and reliability of LLMs. 
<br /><br /> <div>
arXiv:2504.19254v1 Announce Type: new 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?</title>
<link>https://arxiv.org/abs/2504.19267</link>
<guid>https://arxiv.org/abs/2504.19267</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual storytelling, Multimodal models, Transformer-based architectures, VIST dataset, Novel evaluation metrics

Summary: 
The paper introduces a new approach to visual storytelling that utilizes transformer-based architectures and large multimodal models to generate cohesive narratives from sequences of images. The model, called VIST-GPT, is evaluated using the Visual Storytelling (VIST) dataset and focuses on producing visually grounded and contextually appropriate narratives. Traditional evaluation metrics like BLEU and CIDEr are deemed unsuitable for this task, prompting the development of RoViST and GROOVIST, reference-free metrics that assess narrative quality based on visual grounding, coherence, and non-redundancy. These metrics offer a more nuanced evaluation aligned with human judgment, providing a better understanding of the generated narratives' quality. <div>
arXiv:2504.19267v1 Announce Type: new 
Abstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AndroidGen: Building an Android Language Agent under Data Scarcity</title>
<link>https://arxiv.org/abs/2504.19298</link>
<guid>https://arxiv.org/abs/2504.19298</guid>
<content:encoded><![CDATA[
<div> framework, AndroidGen, LLM-based agents, data scarcity, trajectory collection

Summary: 
The article discusses the challenges in utilizing large language models (LLMs) on mobile devices due to data scarcity and incomplete completion rates. To address these issues, the AndroidGen framework is developed to enhance LLM-based agents and collect trajectories from human tasks. The framework is used to train open-source LLMs without manually labeled trajectories, leading to the creation of an open-source mobile agent. AndroidGen is evaluated with various applications, showing improvements in performance and suggesting areas for further enhancement. This work provides a valuable contribution to leveraging LLMs on mobile devices and highlights the potential for future advancements in this area. The code, model, and data for AndroidGen are available on GitHub for further exploration and development. <div>
arXiv:2504.19298v1 Announce Type: new 
Abstract: Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese</title>
<link>https://arxiv.org/abs/2504.19314</link>
<guid>https://arxiv.org/abs/2504.19314</guid>
<content:encoded><![CDATA[
<div> Chinese web, language models, benchmark, BrowseComp-ZH, retrieval competence  
Summary:<br /><br />Language models are evolving into tool-using agents, necessitating the ability to browse the web in real-time to showcase their reasoning and retrieval abilities. Existing benchmarks like BrowseComp focus on English, neglecting the complexities of other major information ecosystems such as Chinese. To fill this gap, BrowseComp-ZH is introduced as a high-difficulty benchmark specifically tailored to evaluate language models on the Chinese web. With 289 challenging questions across diverse domains, each question is reverse-engineered from easily verifiable answers. Evaluation of over 20 state-of-the-art language models reveals their struggle on BrowseComp-ZH, with most models achieving low accuracy rates. The dataset, construction guidelines, and benchmark results have been made publicly available, shedding light on the considerable difficulty of the BrowseComp-ZH benchmark and the need for improved retrieval strategies, reasoning, and information reconciliation in current language models.  
Summary: <div>
arXiv:2504.19314v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Multi-Task Learning &amp; Model Fusion for Efficient Language Model Guardrailing</title>
<link>https://arxiv.org/abs/2504.19333</link>
<guid>https://arxiv.org/abs/2504.19333</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, task-specific data generation, MultiTaskGuard, UniGuard, guardrail classifiers 

Summary:
In this work, the authors propose a novel approach to guardrailing against undesired behaviors using task-specific data generation. They introduce the \texttt{MultiTaskGuard} model, pretrained on a large synthetically generated dataset, to improve generalization. Additionally, they use a search-based model merging approach to create the most performant models, called \texttt{UniGuard}. These efficient guardrail classifiers outperform current state-of-the-art models by a significant margin on public datasets and guardrail benchmarks. The proposed approach shows an average F1 score improvement of 29.92 points over Aegis-LlamaGuard and 21.62 points over \texttt{gpt-4o}. The study also highlights the effectiveness of custom task-specific guardrail policies in enhancing the overall performance of guardrail classifiers. Overall, this work presents a promising solution to the challenges posed by large language models in terms of latency, memory consumption, and hosting expenses in guardrailing applications. 

<br /><br />Summary: <div>
arXiv:2504.19333v1 Announce Type: new 
Abstract: The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, increased latency, memory consumption, hosting expenses and non-structured outputs can make their use prohibitive.
  In this work, we show that task-specific data generation can lead to fine-tuned classifiers that significantly outperform current state of the art (SoTA) while being orders of magnitude smaller. Secondly, we show that using a single model, \texttt{MultiTaskGuard}, that is pretrained on a large synthetically generated dataset with unique task instructions further improves generalization. Thirdly, our most performant models, \texttt{UniGuard}, are found using our proposed search-based model merging approach that finds an optimal set of parameters to combine single-policy models and multi-policy guardrail models. %
On 7 public datasets and 4 guardrail benchmarks we created, our efficient guardrail classifiers improve over the best performing SoTA publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92} points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o}, respectively. Lastly, our guardrail synthetic data generation process that uses custom task-specific guardrail poli
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explanatory Summarization with Discourse-Driven Planning</title>
<link>https://arxiv.org/abs/2504.19339</link>
<guid>https://arxiv.org/abs/2504.19339</guid>
<content:encoded><![CDATA[
<div> Keywords: lay summaries, automatic summarization, discourse frameworks, explanation modeling, plan-based approach

Summary:
Our paper introduces a plan-based approach for generating lay summaries of scientific documents, focusing on including explanations to help readers understand complex content. Unlike current methods, our approach leverages discourse frameworks to organize summary generation and guide explanatory sentences. We propose two discourse-driven planning strategies that outperform existing methods in terms of summary quality, robustness, controllability, and hallucination prevention. Empirical experiments on three lay summarization datasets validate the effectiveness of our approach. By conditioning the plan as part of the input or output prefix, our approach ensures that explanatory content is aligned with human-written summaries. This innovative method enhances the overall quality of lay summaries by providing a structured framework for generating informative and explanatory content. 

<br /><br />Summary: <div>
arXiv:2504.19339v1 Announce Type: new 
Abstract: Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers</title>
<link>https://arxiv.org/abs/2504.19395</link>
<guid>https://arxiv.org/abs/2504.19395</guid>
<content:encoded><![CDATA[
<div> substitution ciphers, in-context learning, task retrieval, task learning, bijective mapping
Summary:
- The study introduces a new approach called ICL CIPHERS which involves using substitution ciphers to transform in-context inputs.
- The ciphers make English sentences less comprehensible to humans but maintain a reversible pattern for machines to decipher.
- The research focuses on determining if large language models (LLMs) can effectively solve these ciphers with a bijective mapping, which requires decoding the latent cipher.
- Results show that LLMs perform better at solving ICL CIPHERS with bijective mappings compared to non-bijective baselines on multiple datasets and models.
- Analysis of LLMs' internal representations provides evidence of their ability to decode the ciphered inputs.
<br /><br />Summary: <div>
arXiv:2504.19395v1 Announce Type: new 
Abstract: Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Selection and Rewriting for Video-based EducationalQuestion Generation</title>
<link>https://arxiv.org/abs/2504.19406</link>
<guid>https://arxiv.org/abs/2504.19406</guid>
<content:encoded><![CDATA[
<div> Keywords: Educational question generation, intelligent educational systems, lecture videos, large language models, dataset

Summary: 
The article discusses the importance of Educational Question Generation (EQG) in intelligent educational systems and the challenges faced when generating questions from lecture videos. Existing datasets for EQG lack real-world classroom content, leading to difficulties in accurately aligning questions with specific timestamps and target answers. To address these challenges, a novel framework is introduced that utilizes large language models to dynamically select and rewrite contexts from lecture transcripts and video keyframes based on answer relevance and temporal proximity. By integrating contexts from both modalities and rewriting them into answer-containing knowledge statements, the framework significantly improves the quality and relevance of generated questions. A dataset and code for this framework are released for further research and development. 

Summary: <br /><br />The article discusses the importance of Educational Question Generation in intelligent educational systems. Existing datasets lack real-world classroom content, leading to difficulties in aligning questions with specific timestamps and answers. A novel framework utilizing large language models dynamically selects and rewrites contexts from lecture transcripts and video keyframes to enhance question relevance. The framework significantly improves the quality of generated questions and a dataset and code are released for further research and development. <div>
arXiv:2504.19406v1 Announce Type: new 
Abstract: Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in https://github.com/mengxiayu/COSER.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</title>
<link>https://arxiv.org/abs/2504.19413</link>
<guid>https://arxiv.org/abs/2504.19413</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mem0, Memory-centric architecture, Graph memory, Conversational coherence

Summary:
Mem0 introduces a memory-centric architecture to address challenges in maintaining consistency during prolonged multi-session dialogues in Large Language Models (LLMs). It dynamically extracts, consolidates, and retrieves salient information from ongoing conversations. An enhanced variant utilizes graph-based memory representations to capture complex relational structures among conversational elements. Comprehensive evaluations on the LOCOMO benchmark show that Mem0 outperforms existing memory systems across various question categories. It achieves a 26% relative improvement in the LLM-as-a-Judge metric over OpenAI and reduces computational overhead significantly compared to full-context methods. In particular, Mem0 demonstrates a 91% lower p95 latency and more than 90% token cost savings. These results highlight the importance of structured, persistent memory mechanisms for long-term conversational coherence in LLM-driven AI agents. 

<br /><br />Summary: <div>
arXiv:2504.19413v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models</title>
<link>https://arxiv.org/abs/2504.19436</link>
<guid>https://arxiv.org/abs/2504.19436</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic optimization, Retrieval-Augmented Generation, knowledge retrieval, semantic understanding, large language models

Summary: 
This paper introduces a state-aware dynamic knowledge retrieval mechanism for the Retrieval-Augmented Generation (RAG) architecture. The method enhances semantic understanding and knowledge scheduling efficiency in large language models used for open-domain question answering and complex generation tasks. By implementing a multi-level perceptive retrieval vector construction strategy and a differentiable document matching path, the approach allows for joint training and optimization of the retrieval and generation modules. Experiments on the Natural Questions dataset, using models such as GPT-4 and DeepSeek, show significant improvements in BLEU and ROUGE-L scores. The proposed structure also exhibits enhanced robustness and generation consistency, particularly in tasks involving semantic ambiguity and multi-document fusion. These results underscore the potential of the approach in developing high-quality language generation systems. 

<br /><br />Summary: <div>
arXiv:2504.19436v1 Announce Type: new 
Abstract: This paper focuses on the dynamic optimization of the Retrieval-Augmented Generation (RAG) architecture. It proposes a state-aware dynamic knowledge retrieval mechanism to enhance semantic understanding and knowledge scheduling efficiency in large language models for open-domain question answering and complex generation tasks. The method introduces a multi-level perceptive retrieval vector construction strategy and a differentiable document matching path. These components enable end-to-end joint training and collaborative optimization of the retrieval and generation modules. This effectively addresses the limitations of static RAG structures in context adaptation and knowledge access. Experiments are conducted on the Natural Questions dataset. The proposed structure is thoroughly evaluated across different large models, including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments from multiple perspectives confirm the significant improvements in BLEU and ROUGE-L scores. The approach also demonstrates stronger robustness and generation consistency in tasks involving semantic ambiguity and multi-document fusion. These results highlight its broad application potential and practical value in building high-quality language generation systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks</title>
<link>https://arxiv.org/abs/2504.19445</link>
<guid>https://arxiv.org/abs/2504.19445</guid>
<content:encoded><![CDATA[
<div> response format, large language models, biases, judgments, decision tasks 

Summary:<br /><br />Large Language Models (LLMs) are being used more frequently in tasks like psychological text analysis and decision-making in automated workflows. Concerns about their reliability stem from potential biases inherited during training. This study explored how different response formats, binary versus continuous, impact LLM judgments. Results indicated a consistent negative bias, with LLMs more prone to delivering "negative" judgments in binary formats compared to continuous ones. Control experiments supported these findings across different tasks. The study underscores the significance of considering response format when utilizing LLMs for decision-making tasks, as slight modifications in task design can introduce systematic biases. <div>
arXiv:2504.19445v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used in tasks such as psychological text analysis and decision-making in automated workflows. However, their reliability remains a concern due to potential biases inherited from their training process. In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments. In a value statement judgments task and a text sentiment analysis task, we prompted LLMs to simulate human responses and tested both formats across several models, including both open-source and commercial models. Our findings revealed a consistent negative bias: LLMs were more likely to deliver "negative" judgments in binary formats compared to continuous ones. Control experiments further revealed that this pattern holds across both tasks. Our results highlight the importance of considering response format when applying LLMs to decision tasks, as small changes in task design can introduce systematic biases.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Long Context Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.19457</link>
<guid>https://arxiv.org/abs/2504.19457</guid>
<content:encoded><![CDATA[
<div> hallucination detection, large language models, contextual understanding, long-context inputs, model architecture <br />
Summary: 
This study addresses the issue of contextual hallucination in Large Language Models (LLMs) by creating a dataset for long-context hallucination detection. A novel architecture is proposed to enhance pre-trained encoder models like BERT to process long contexts and identify contextual hallucinations through decomposition and aggregation. Experimental results demonstrate that the new architecture outperforms previous models and LLM-based models in terms of accuracy and speed of inference. This advancement is a significant step towards effectively detecting and addressing contextual hallucinations in LLMs, improving their reliability in generating accurate and contextually relevant information. <div>
arXiv:2504.19457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text</title>
<link>https://arxiv.org/abs/2504.19467</link>
<guid>https://arxiv.org/abs/2504.19467</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, clinical data, natural language processing, performance variation

Summary:
- The article introduces BRIDGE, a new multilingual benchmark for evaluating large language models (LLMs) in clinical contexts using real-world electronic health record (EHR) data.
- 52 state-of-the-art LLMs were systematically evaluated across 87 tasks in nine languages, revealing significant performance differences based on model sizes, languages, tasks, and clinical specialties.
- Open-source LLMs were shown to perform comparably to proprietary models, while older medically fine-tuned LLMs lagged behind updated general-purpose models.
- The BRIDGE benchmark and associated leaderboard provide a foundational resource and reference point for the development and assessment of LLMs in real-world clinical text understanding.

<br /><br />Summary: <div>
arXiv:2504.19467v1 Announce Type: new 
Abstract: Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflicts in Texts: Data, Implications and Challenges</title>
<link>https://arxiv.org/abs/2504.19472</link>
<guid>https://arxiv.org/abs/2504.19472</guid>
<content:encoded><![CDATA[
<div> conflicting information, NLP models, natural texts, human-annotated data, model interactions

Summary:<br /><br />As NLP models are increasingly used in real-world applications, the presence of conflicting information poses challenges. The conflicts arise from various sources such as factual inconsistencies, annotator disagreements, and hallucinations in model outputs. This survey categorizes conflicts into three main areas: natural texts, human-annotated data, and model interactions. Addressing these conflicts is crucial to ensure the reliability and trustworthiness of NLP models. Prior work has addressed some conflicts in isolation, but this survey unifies them under the broader concept of conflicting information. It discusses the implications of conflicts and proposes mitigation strategies. Developing conflict-aware NLP systems that can reason over and reconcile conflicting information effectively poses key challenges for future research. <div>
arXiv:2504.19472v1 Announce Type: new 
Abstract: As NLP models become increasingly integrated into real-world applications, it becomes clear that there is a need to address the fact that models often rely on and generate conflicting information. Conflicts could reflect the complexity of situations, changes that need to be explained and dealt with, difficulties in data annotation, and mistakes in generated outputs. In all cases, disregarding the conflicts in data could result in undesired behaviors of models and undermine NLP models' reliability and trustworthiness. This survey categorizes these conflicts into three key areas: (1) natural texts on the web, where factual inconsistencies, subjective biases, and multiple perspectives introduce contradictions; (2) human-annotated data, where annotator disagreements, mistakes, and societal biases impact model training; and (3) model interactions, where hallucinations and knowledge conflicts emerge during deployment. While prior work has addressed some of these conflicts in isolation, we unify them under the broader concept of conflicting information, analyze their implications, and discuss mitigation strategies. We highlight key challenges and future directions for developing conflict-aware NLP systems that can reason over and reconcile conflicting information more effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment</title>
<link>https://arxiv.org/abs/2504.19556</link>
<guid>https://arxiv.org/abs/2504.19556</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AI-mediated communication, social media, sentiment analysis, language shifts<br />
Summary:<br />
This study investigates the influence of AI-mediated communication on social media language patterns by analyzing shifts in text complexity and sentiment over time. By comparing tweets mentioning Donald Trump during election periods from 2020 and 2024, the researchers found a significant increase in mean sentiment polarity and a shift towards more positive expressions. The analysis showed a decrease in neutral content and an increase in positive sentiment in tweets, indicating the impact of AI on language and emotional expression on social media. These findings suggest a growing presence of AI in shaping online communication and highlight the importance of understanding the evolving nature of language patterns influenced by large language models. <div>
arXiv:2504.19556v1 Announce Type: new 
Abstract: Given the subtle human-like effects of large language models on linguistic patterns, this study examines shifts in language over time to detect the impact of AI-mediated communication (AI- MC) on social media. We compare a replicated dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the same period in 2024, all of which mention Donald Trump during election periods. Using a combination of Flesch-Kincaid readability and polarity scores, we analyze changes in text complexity and sentiment. Our findings reveal a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%). These findings suggest not only an increasing presence of AI in social media communication but also its impact on language and emotional expression patterns.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</title>
<link>https://arxiv.org/abs/2504.19565</link>
<guid>https://arxiv.org/abs/2504.19565</guid>
<content:encoded><![CDATA[
<div> agents, scientific corpus, biomedical, language models, knowledge-driven

Summary:
The article introduces a knowledge-driven, multi-agent framework for distilling scientific corpora tailored for training large language models in the biomedical field. It proposes a collaborative architecture where specialized agents, guided by the MeSH hierarchy, autonomously extract high-quality data from scientific literature. These agents generate domain-specific question-answer pairs, ensuring coverage and consistency with biomedical ontologies while minimizing manual involvement. Experimental results show that language models trained on the distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming strong baselines. The AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2. Ablation studies and case analyses validate the effectiveness and synergy of each agent within the framework, demonstrating the potential of multi-agent collaboration in biomedical LLM training. 

<br /><br />Summary: <div>
arXiv:2504.19565v1 Announce Type: new 
Abstract: The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Metaphor Sentiment Classification Using Semantic Information</title>
<link>https://arxiv.org/abs/2504.19590</link>
<guid>https://arxiv.org/abs/2504.19590</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic Metaphor Corpus, sentiment classification, semantic tags, F-score, precision

Summary: 
This paper discusses the testing of the Arabic Metaphor Corpus (AMC) using newly developed automatic tools for sentiment classification based on semantic tags. The tools utilize semantic emotional tags to classify sentiment and evaluate their performance using standard metrics such as F-score, recall, and precision. The study aims to demonstrate the impact of Arabic online metaphors on sentiment through the use of these tools. This approach represents a novel method for conducting sentiment classification for Arabic metaphors by incorporating semantic tags to analyze the metaphor's influence. <div>
arXiv:2504.19590v1 Announce Type: new 
Abstract: In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1] using newly designed automatic tools for sentiment classification for AMC based on semantic tags. The tool incorporates semantic emotional tags for sentiment classification. I evaluate the tool using standard methods, which are F-score, recall, and precision. The method is to show the impact of Arabic online metaphors on sentiment through the newly designed tools. To the best of our knowledge, this is the first approach to conduct sentiment classification for Arabic metaphors using semantic tags to find the impact of the metaphor.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coreference Resolution for Vietnamese Narrative Texts</title>
<link>https://arxiv.org/abs/2504.19606</link>
<guid>https://arxiv.org/abs/2504.19606</guid>
<content:encoded><![CDATA[
<div> Coreference resolution, Vietnamese, annotated dataset, language models, GPT-4<br />
Summary:<br />
- Coreference resolution is crucial in NLP, especially challenging for Vietnamese due to limited resources.
- An annotated dataset was created using narratives from VnExpress with detailed guidelines and focus on consistency and accuracy.
- Evaluation of large language models (LLMs) GPT-3.5-Turbo and GPT-4 on the dataset showed GPT-4 outperforms in accuracy and response consistency.
- This highlights GPT-4 as a more reliable tool for coreference resolution in Vietnamese. <br /> <div>
arXiv:2504.19606v1 Announce Type: new 
Abstract: Coreference resolution is a vital task in natural language processing (NLP) that involves identifying and linking different expressions in a text that refer to the same entity. This task is particularly challenging for Vietnamese, a low-resource language with limited annotated datasets. To address these challenges, we developed a comprehensive annotated dataset using narrative texts from VnExpress, a widely-read Vietnamese online news platform. We established detailed guidelines for annotating entities, focusing on ensuring consistency and accuracy. Additionally, we evaluated the performance of large language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset. Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in terms of both accuracy and response consistency, making it a more reliable tool for coreference resolution in Vietnamese.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19627</link>
<guid>https://arxiv.org/abs/2504.19627</guid>
<content:encoded><![CDATA[
<div> Framework, Visual Concepts, LVLMs, Self-supervised Learning, Image Understanding

Summary:
The article introduces VCM, a self-supervised visual concept modeling framework aimed at improving Large Vision-Language Models (LVLMs) efficiency and performance in real-world applications. LVLMs currently process entire images at the token level, lacking a visual concept model that mimics humans' ability to extract relevant visual concepts effortlessly. VCM addresses this by leveraging implicit contrastive learning and vision-language fine-tuning to construct a visual concept model without requiring concept-level annotations. Results show VCM reduces computational costs significantly while maintaining strong performance across diverse image understanding tasks. Additionally, VCM enhances visual encoders' capabilities in visual concept perception tasks. Extensive experiments validate the effectiveness and efficiency of VCM in improving LVLMs for various applications. <br /><br />Summary: <div>
arXiv:2504.19627v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks</title>
<link>https://arxiv.org/abs/2504.19645</link>
<guid>https://arxiv.org/abs/2504.19645</guid>
<content:encoded><![CDATA[
<div> POS tagging, Central-Kurdish language, NLP, resources, Universal Dependencies framework  
Summary:  
This study focuses on developing a comprehensive POS tagset for the Central-Kurdish language (CKL), which lacks resources for NLP tasks. The POS tagging task is essential for various NLP applications, but CKL faces challenges due to the lack of standardized tagsets. The proposed tagset, curated from existing studies and input from linguistic experts, aims to enhance the performance of Kurdish NLP tasks by providing accurate annotations for CKL corpus. By comparing with the Universal Dependencies framework for standard languages, the study demonstrates that the proposed tagset can improve sentence corrections and streamline NLP tasks for CKL. This research contributes to filling the gap in low-resourced language processing and lays the foundation for further advancements in Kurdish NLP. <br /><br />Summary: <div>
arXiv:2504.19645v1 Announce Type: new 
Abstract: - The field of natural language processing (NLP) has dramatically expanded within the last decade. Many human-being applications are conducted daily via NLP tasks, starting from machine translation, speech recognition, text generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity Recognition (NER). However, low-resourced languages, such as the Central-Kurdish language (CKL), mainly remain unexamined due to shortage of necessary resources to support their development. The POS tagging task is the base of other NLP tasks; for example, the POS tag set has been used to standardized languages to provide the relationship between words among the sentences, followed by machine translation and text recommendation. Specifically, for the CKL, most of the utilized or provided POS tagsets are neither standardized nor comprehensive. To this end, this study presented an accurate and comprehensive POS tagset for the CKL to provide better performance of the Kurdish NLP tasks. The article also collected most of the POS tags from different studies as well as from Kurdish linguistic experts to standardized part-of-speech tags. The proposed POS tagset is designed to annotate a large CKL corpus and support Kurdish NLP tasks. The initial investigations of this study via comparison with the Universal Dependencies framework for standard languages, show that the proposed POS tagset can streamline or correct sentences more accurately for Kurdish NLP tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Conditioned Diffusive Time Series Forecasting</title>
<link>https://arxiv.org/abs/2504.19669</link>
<guid>https://arxiv.org/abs/2504.19669</guid>
<content:encoded><![CDATA[
arXiv:2504.19669v1 Announce Type: new 
Abstract: Diffusion models achieve remarkable success in processing images and text, and have been extended to special domains such as time series forecasting (TSF). Existing diffusion-based approaches for TSF primarily focus on modeling single-modality numerical sequences, overlooking the rich multimodal information in time series data. To effectively leverage such information for prediction, we propose a multimodal conditioned diffusion model for TSF, namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for time series modeling, especially for forecasting. Specifically, Timestamps are combined with time series to establish temporal and semantic correlations among different data points when aggregating information along the temporal dimension. Texts serve as supplementary descriptions of time series' history, and adaptively aligned with data points as well as dynamically controlled in a classifier-free manner. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed MCD-TSF model achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs</title>
<link>https://arxiv.org/abs/2504.19675</link>
<guid>https://arxiv.org/abs/2504.19675</guid>
<content:encoded><![CDATA[
arXiv:2504.19675v1 Announce Type: new 
Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming the Titans: A Survey of Efficient LLM Inference Serving</title>
<link>https://arxiv.org/abs/2504.19720</link>
<guid>https://arxiv.org/abs/2504.19720</guid>
<content:encoded><![CDATA[
arXiv:2504.19720v1 Announce Type: new 
Abstract: Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding</title>
<link>https://arxiv.org/abs/2504.19734</link>
<guid>https://arxiv.org/abs/2504.19734</guid>
<content:encoded><![CDATA[
arXiv:2504.19734v1 Announce Type: new 
Abstract: Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs</title>
<link>https://arxiv.org/abs/2504.19759</link>
<guid>https://arxiv.org/abs/2504.19759</guid>
<content:encoded><![CDATA[
arXiv:2504.19759v1 Announce Type: new 
Abstract: In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB) to evaluate the moral reasoning abilities of large language models (LLMs) across five typologically diverse languages and three levels of contextual complexity: sentence, paragraph, and document. Our results show moral reasoning performance degrades with increasing context complexity, particularly for low-resource languages such as Vietnamese. We further fine-tune the open-source LLaMA-3-8B model using curated monolingual data for alignment and poisoning. Surprisingly, low-resource languages have a stronger impact on multilingual reasoning than high-resource ones, highlighting their critical role in multilingual NLP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance</title>
<link>https://arxiv.org/abs/2504.19811</link>
<guid>https://arxiv.org/abs/2504.19811</guid>
<content:encoded><![CDATA[
arXiv:2504.19811v1 Announce Type: new 
Abstract: Accurately forecasting the performance of Large Language Models (LLMs) before extensive fine-tuning or merging can substantially reduce both computational expense and development time. Although prior approaches like scaling laws account for global factors such as parameter size or training tokens, they often overlook explicit lineage relationships - i.e., which models are derived or merged from which parents. In this work, we propose a novel Lineage-Regularized Matrix Factorization (LRMF) framework that encodes ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging multi-hop parent-child connections, LRMF consistently outperforms conventional matrix factorization and collaborative filtering methods in both instance-level and benchmark-level performance prediction. Our large-scale study includes 2,934 publicly available Hugging Face models and 21,000+ instances across 6 major benchmarks, showing that lineage constraints yield up to 7-10 percentage points higher correlation with actual performance compared to baselines. Moreover, LRMF effectively addresses the cold-start problem, providing accurate estimates for newly derived or merged models even with minimal data. This lineage-guided strategy thus offers a resource-efficient way to inform hyperparameter tuning, data selection, and model combination in modern LLM development.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels</title>
<link>https://arxiv.org/abs/2504.19850</link>
<guid>https://arxiv.org/abs/2504.19850</guid>
<content:encoded><![CDATA[
arXiv:2504.19850v1 Announce Type: new 
Abstract: This article presents the results of a pilot study involving the reception of a fictional short story translated from English into Dutch under four conditions: machine translation (MT), post-editing (PE), human translation (HT) and original source text (ST). The aim is to understand how creativity and errors in different translation modalities affect readers, specifically regarding cognitive load. Eight participants filled in a questionnaire, read a story using an eye-tracker, and conducted a retrospective think-aloud (RTA) interview. The results show that units of creative potential (UCP) increase cognitive load and that this effect is highest for HT and lowest for MT; no effect of error was observed. Triangulating the data with RTAs leads us to hypothesize that the higher cognitive load in UCPs is linked to increases in reader enjoyment and immersion. The effect of translation creativity on cognitive load in different translation modalities at word-level is novel and opens up new avenues for further research. All the code and data are available at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language</title>
<link>https://arxiv.org/abs/2504.19856</link>
<guid>https://arxiv.org/abs/2504.19856</guid>
<content:encoded><![CDATA[
arXiv:2504.19856v1 Announce Type: new 
Abstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage</title>
<link>https://arxiv.org/abs/2504.19867</link>
<guid>https://arxiv.org/abs/2504.19867</guid>
<content:encoded><![CDATA[
arXiv:2504.19867v1 Announce Type: new 
Abstract: Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.
  In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets</title>
<link>https://arxiv.org/abs/2504.19898</link>
<guid>https://arxiv.org/abs/2504.19898</guid>
<content:encoded><![CDATA[
arXiv:2504.19898v1 Announce Type: new 
Abstract: As a fundamental task in machine learning, text classification plays a crucial role in many areas. With the rapid scaling of Large Language Models (LLMs), particularly through reinforcement learning (RL), there is a growing need for more capable discriminators. Consequently, advances in classification are becoming increasingly vital for enhancing the overall capabilities of LLMs. Traditional discriminative methods map text to labels but overlook LLMs' intrinsic generative strengths. Generative classification addresses this by prompting the model to directly output labels. However, existing studies still rely on simple SFT alone, seldom probing the interplay between training and inference prompts, and no work has systematically leveraged RL for generative text classifiers and unified SFT, RL, and inference-time prompting in one framework. We bridge this gap with GenCLS++, a framework that jointly optimizes SFT and RL while systematically exploring five high-level strategy dimensions-in-context learning variants, category definitions, explicit uncertainty labels, semantically irrelevant numeric labels, and perplexity-based decoding-during both training and inference. After an SFT "policy warm-up," we apply RL with a simple rule-based reward, yielding sizable extra gains. Across seven datasets, GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline; on public datasets, this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that benefit from explicit thinking processes, we find that classification tasks perform better without such reasoning steps. These insights into the role of explicit reasoning provide valuable guidance for future LLM applications.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2504.19940</link>
<guid>https://arxiv.org/abs/2504.19940</guid>
<content:encoded><![CDATA[
arXiv:2504.19940v1 Announce Type: new 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons</title>
<link>https://arxiv.org/abs/2504.19982</link>
<guid>https://arxiv.org/abs/2504.19982</guid>
<content:encoded><![CDATA[
arXiv:2504.19982v1 Announce Type: new 
Abstract: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by Large Language Models (LLMs), yet the evaluation methodologies for these systems remain insufficient for their growing sophistication. While traditional automatic metrics effectively assessed earlier modular systems, they focus solely on the dialogue level and cannot detect critical intermediate errors that can arise during user-agent interactions. In this paper, we introduce TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework that unifies fine-grained turn-level analysis with holistic dialogue-level comparisons. At turn level, we evaluate each response along three TOD-specific dimensions: conversation cohesion, backend knowledge consistency, and policy compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons to provide a measure of dialogue-level quality. Through experiments on MultiWOZ 2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the conversational errors that conventional metrics miss. Furthermore, TD-EVAL exhibits better alignment with human judgments than traditional and LLM-based metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for TOD system evaluation, efficiently assessing both turn and system levels with a plug-and-play framework for future research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom</title>
<link>https://arxiv.org/abs/2504.20000</link>
<guid>https://arxiv.org/abs/2504.20000</guid>
<content:encoded><![CDATA[
arXiv:2504.20000v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) is one of the approaches to reduce the size of Large Language Models (LLMs). A LLM with smaller number of model parameters (student) is trained to mimic the performance of a LLM of a larger size (teacher model) on a specific task. For domain-specific tasks, it is not clear if teacher or student model, or both, must be considered for domain adaptation. In this work, we study this problem from perspective of telecom domain Question-Answering (QA) task. We systematically experiment with Supervised Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to KD. We design experiments to study the impact of vocabulary (same and different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the distilled model. Multi-faceted evaluation of the distillation using 14 different metrics (N-gram, embedding and LLM-based metrics) is considered. Experimental results show that SFT of teacher improves performance of distilled model when both models have same vocabulary, irrespective of algorithm and metrics. Overall, SFT of both teacher and student results in better performance across all metrics, although the statistical significance of the same depends on the vocabulary of the teacher models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation</title>
<link>https://arxiv.org/abs/2504.20013</link>
<guid>https://arxiv.org/abs/2504.20013</guid>
<content:encoded><![CDATA[
arXiv:2504.20013v1 Announce Type: new 
Abstract: Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages</title>
<link>https://arxiv.org/abs/2504.20022</link>
<guid>https://arxiv.org/abs/2504.20022</guid>
<content:encoded><![CDATA[
arXiv:2504.20022v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) have demonstrated significant effectiveness across various languages, particularly in high-resource languages such as English. However, their performance in terms of factual accuracy across other low-resource languages, especially Indic languages, remains an area of investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o, Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in English and Indic languages using the IndicQuest dataset, which contains question-answer pairs in English and 19 Indic languages. By asking the same questions in English and their respective Indic translations, we analyze whether the models are more reliable for regional context questions in Indic languages or when operating in English. Our findings reveal that LLMs often perform better in English, even for questions rooted in Indic contexts. Notably, we observe a higher tendency for hallucination in responses generated in low-resource Indic languages, highlighting challenges in the multilingual understanding capabilities of current LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoJudge: Judge Decoding Without Manual Annotation</title>
<link>https://arxiv.org/abs/2504.20039</link>
<guid>https://arxiv.org/abs/2504.20039</guid>
<content:encoded><![CDATA[
arXiv:2504.20039v1 Announce Type: new 
Abstract: We introduce AutoJudge, a framework that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the generated response, relaxing the guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft model should be corrected to preserve quality, and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B (target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more accepted tokens per verification cycle with under 1% degradation in answer accuracy compared to standard speculative decoding and over 2x with small loss in accuracy. When applied to the LiveCodeBench benchmark, our approach automatically detects other, programming-specific important tokens and shows similar speedups, demonstrating its ability to generalize across tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines</title>
<link>https://arxiv.org/abs/2504.18596</link>
<guid>https://arxiv.org/abs/2504.18596</guid>
<content:encoded><![CDATA[
arXiv:2504.18596v1 Announce Type: cross 
Abstract: This paper explores the strategic use of modern synthetic data generation and advanced data perturbation techniques to enhance security, maintain analytical utility, and improve operational efficiency when managing large datasets, with a particular focus on the Banking, Financial Services, and Insurance (BFSI) sector. We contrast these advanced methods encompassing generative models like GANs, sophisticated context-aware PII transformation, configurable statistical perturbation, and differential privacy with traditional anonymization approaches.
  The goal is to create realistic, privacy-preserving datasets that retain high utility for complex machine learning tasks and analytics, a critical need in the data-sensitive industries like BFSI, Healthcare, Retail, and Telecommunications. We discuss how these modern techniques potentially offer significant improvements in balancing privacy preservation while maintaining data utility compared to older methods. Furthermore, we examine the potential for operational gains, such as reduced overhead and accelerated analytics, by using these privacy-enhanced datasets. We also explore key use cases where these methods can mitigate regulatory risks and enable scalable, data-driven innovation without compromising sensitive customer information.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Product Recommendations for Implicit Superlative Queries</title>
<link>https://arxiv.org/abs/2504.18748</link>
<guid>https://arxiv.org/abs/2504.18748</guid>
<content:encoded><![CDATA[
arXiv:2504.18748v1 Announce Type: cross 
Abstract: In Recommender Systems, users often seek the best products through indirect, vague, or under-specified queries, such as "best shoes for trail running". Such queries, also referred to as implicit superlative queries, pose a significant challenge for standard retrieval and ranking systems as they lack an explicit mention of attributes and require identifying and reasoning over complex factors. We investigate how Large Language Models (LLMs) can generate implicit attributes for ranking as well as reason over them to improve product recommendations for such queries. As a first step, we propose a novel four-point schema for annotating the best product candidates for superlative queries called SUPERB, paired with LLM-based product annotations. We then empirically evaluate several existing retrieval and ranking approaches on our new dataset, providing insights and discussing their integration into real-world e-commerce production systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical knowledge in LLMs does not translate to human interactions</title>
<link>https://arxiv.org/abs/2504.18919</link>
<guid>https://arxiv.org/abs/2504.18919</guid>
<content:encoded><![CDATA[
arXiv:2504.18919v1 Announce Type: cross 
Abstract: Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings</title>
<link>https://arxiv.org/abs/2504.18988</link>
<guid>https://arxiv.org/abs/2504.18988</guid>
<content:encoded><![CDATA[
arXiv:2504.18988v1 Announce Type: cross 
Abstract: Collaborative research often includes contributors with varied perspectives from diverse linguistic backgrounds. However, English as a Second Language (ESL) researchers often struggle to communicate during meetings in English and comprehend discussions, leading to limited contribution. To investigate these challenges, we surveyed 64 ESL researchers who frequently collaborate in multilingual teams and identified four key design goals around participation, comprehension, documentation, and feedback. Guided by these design goals, we developed LINC, a multimodal Language INdependent Collaboration system with two components: a real-time module for multilingual communication during meetings and a post-meeting dashboard for discussion analysis. We evaluated the system through a two-phased study with six triads of multilingual teams. We found that using LINC, participants benefited from communicating in their preferred language, recalled and reviewed actionable insights, and prepared for upcoming meetings effectively. We discuss external factors that impact multilingual meeting participation beyond language preferences and the implications of multimodal systems in facilitating meetings in hybrid multilingual collaborative settings beyond research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2504.19056</link>
<guid>https://arxiv.org/abs/2504.19056</guid>
<content:encoded><![CDATA[
arXiv:2504.19056v1 Announce Type: cross 
Abstract: Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Framework for Song Generation with Prompt-based Control</title>
<link>https://arxiv.org/abs/2504.19062</link>
<guid>https://arxiv.org/abs/2504.19062</guid>
<content:encoded><![CDATA[
arXiv:2504.19062v1 Announce Type: cross 
Abstract: Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Attention Generates Better Proofs</title>
<link>https://arxiv.org/abs/2504.19188</link>
<guid>https://arxiv.org/abs/2504.19188</guid>
<content:encoded><![CDATA[
arXiv:2504.19188v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively. The code is available at https://github.com/Car-pe/HAGBP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anyprefer: An Agentic Framework for Preference Data Synthesis</title>
<link>https://arxiv.org/abs/2504.19276</link>
<guid>https://arxiv.org/abs/2504.19276</guid>
<content:encoded><![CDATA[
arXiv:2504.19276v1 Announce Type: cross 
Abstract: High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks</title>
<link>https://arxiv.org/abs/2504.19444</link>
<guid>https://arxiv.org/abs/2504.19444</guid>
<content:encoded><![CDATA[
arXiv:2504.19444v1 Announce Type: cross 
Abstract: Pre-trained code models rely heavily on high-quality pre-training data, particularly human-written reference comments that bridge code and natural language. However, these comments often become outdated as software evolves, degrading model performance. Large language models (LLMs) excel at generating high-quality code comments. We investigate whether replacing human-written comments with LLM-generated ones improves pre-training datasets. Since standard metrics cannot assess reference comment quality, we propose two novel reference-free evaluation tasks: code-comment inconsistency detection and semantic code search. Results show that LLM-generated comments are more semantically consistent with code than human-written ones, as confirmed by manual evaluation. Leveraging this finding, we rebuild the CodeSearchNet dataset with LLM-generated comments and re-pre-train CodeT5. Evaluations demonstrate that models trained on LLM-enhanced data outperform those using original human comments in code summarization, generation, and translation tasks. This work validates rebuilding pre-training datasets with LLMs to advance code intelligence, challenging the traditional reliance on human reference comments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective</title>
<link>https://arxiv.org/abs/2504.19458</link>
<guid>https://arxiv.org/abs/2504.19458</guid>
<content:encoded><![CDATA[
arXiv:2504.19458v1 Announce Type: cross 
Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Reasoning Performance in Large Language Models via Representation Engineering</title>
<link>https://arxiv.org/abs/2504.19483</link>
<guid>https://arxiv.org/abs/2504.19483</guid>
<content:encoded><![CDATA[
arXiv:2504.19483v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether reasoning in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.19500</link>
<guid>https://arxiv.org/abs/2504.19500</guid>
<content:encoded><![CDATA[
arXiv:2504.19500v1 Announce Type: cross 
Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical intelligence, as it enables embodied agents to interpret and interact dynamically within real-world environments. This paper introduces MPEC, a novel Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic segmentation that leverages both 3D entity-language alignment and point-entity consistency across different point cloud views to foster entity-specific feature representations. Our method improves semantic discrimination and enhances the differentiation of unique instances, achieving state-of-the-art results on ScanNet for open-vocabulary 3D semantic segmentation and demonstrating superior zero-shot scene understanding capabilities. Extensive fine-tuning experiments on 8 datasets, spanning from low-level perception to high-level reasoning tasks, showcase the potential of learned 3D features, driving consistent performance gains across varied 3D scene understanding tasks. Project website: https://mpec-3d.github.io/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation</title>
<link>https://arxiv.org/abs/2504.19519</link>
<guid>https://arxiv.org/abs/2504.19519</guid>
<content:encoded><![CDATA[
arXiv:2504.19519v1 Announce Type: cross 
Abstract: Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features.
  To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19583</link>
<guid>https://arxiv.org/abs/2504.19583</guid>
<content:encoded><![CDATA[
arXiv:2504.19583v1 Announce Type: cross 
Abstract: This paper proposes a parameter collaborative optimization algorithm for large language models, enhanced with graph spectral analysis. The goal is to improve both fine-tuning efficiency and structural awareness during training. In the proposed method, the parameters of a pre-trained language model are treated as nodes in a graph. A weighted graph is constructed, and Laplacian spectral decomposition is applied to enable frequency-domain modeling and structural representation of the parameter space. Based on this structure, a joint loss function is designed. It combines the task loss with a spectral regularization term to facilitate collaborative updates among parameters. In addition, a spectral filtering mechanism is introduced during the optimization phase. This mechanism adjusts gradients in a structure-aware manner, enhancing the model's training stability and convergence behavior. The method is evaluated on multiple tasks, including traditional fine-tuning comparisons, few-shot generalization tests, and convergence speed analysis. In all settings, the proposed approach demonstrates superior performance. The experimental results confirm that the spectral collaborative optimization framework effectively reduces parameter perturbations and improves fine-tuning quality while preserving overall model performance. This work contributes significantly to the field of artificial intelligence by advancing parameter-efficient training methodologies for large-scale models, reinforcing the importance of structural signal processing in deep learning optimization, and offering a robust, generalizable framework for enhancing language model adaptability and performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2504.19730</link>
<guid>https://arxiv.org/abs/2504.19730</guid>
<content:encoded><![CDATA[
arXiv:2504.19730v1 Announce Type: cross 
Abstract: The widespread adoption of code language models in software engineering tasks has exposed vulnerabilities to adversarial attacks, especially the identifier substitution attacks. Although existing identifier substitution attackers demonstrate high success rates, they often produce adversarial examples with unnatural code patterns. In this paper, we systematically assess the quality of adversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80% of adversarial examples generated by state-of-the-art identifier substitution attackers (e.g., ALERT) are actually detectable. Based on this insight, we propose EP-Shield, a unified framework for evaluating and purifying identifier substitution attacks via naturalness-aware reasoning. Specifically, we first evaluate the naturalness of code and identify the perturbed adversarial code, then purify it so that the victim model can restore correct prediction. Extensive experiments demonstrate the superiority of EP-Shield over adversarial fine-tuning (up to 83.36% improvement) and its lightweight design 7B parameters) with GPT-4-level performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.19754</link>
<guid>https://arxiv.org/abs/2504.19754</guid>
<content:encoded><![CDATA[
arXiv:2504.19754v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian approach to modeling topic-metadata relationships</title>
<link>https://arxiv.org/abs/2104.02496</link>
<guid>https://arxiv.org/abs/2104.02496</guid>
<content:encoded><![CDATA[
arXiv:2104.02496v2 Announce Type: replace 
Abstract: The objective of advanced topic modeling is not only to explore latent topical structures, but also to estimate relationships between the discovered topics and theoretically relevant metadata. Methods used to estimate such relationships must take into account that the topical structure is not directly observed, but instead being estimated itself in an unsupervised fashion, usually by common topic models. A frequently used procedure to achieve this is the method of composition, a Monte Carlo sampling technique performing multiple repeated linear regressions of sampled topic proportions on metadata covariates. In this paper, we propose two modifications of this approach: First, we substantially refine the existing implementation of the method of composition from the R package stm by replacing linear regression with the more appropriate Beta regression. Second, we provide a fundamental enhancement of the entire estimation framework by substituting the current blending of frequentist and Bayesian methods with a fully Bayesian approach. This allows for a more appropriate quantification of uncertainty. We illustrate our improved methodology by investigating relationships between Twitter posts by German parliamentarians and different metadata covariates related to their electoral districts, using the Structural Topic Model to estimate topic proportions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information</title>
<link>https://arxiv.org/abs/2110.08420</link>
<guid>https://arxiv.org/abs/2110.08420</guid>
<content:encoded><![CDATA[
arXiv:2110.08420v3 Announce Type: replace 
Abstract: Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\mathcal{V}$ -- as the lack of $\mathcal{V}$-$\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\mathcal{V}$. We further introduce $\textit{pointwise $\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\mathcal{V}$-$\textit{usable information}$ and PVI also permit the converse: for a given model $\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach</title>
<link>https://arxiv.org/abs/2112.06876</link>
<guid>https://arxiv.org/abs/2112.06876</guid>
<content:encoded><![CDATA[
arXiv:2112.06876v3 Announce Type: replace 
Abstract: In recent years, the field of NLP has seen growing interest in modeling both semantic and pragmatic dimensions. Despite this progress, two key challenges persist: firstly, the complex task of mapping and analyzing the interactions between semantic and pragmatic features; secondly, the insufficient incorporation of relevant insights from related disciplines outside NLP. Addressing these issues, this study introduces a novel geometric metric that utilizes word co-occurrence patterns. This metric maps two fundamental properties - semantic typicality (cognitive) and pragmatic salience (socio-cultural) - for basic-level categories within a two-dimensional hyperbolic space. Our evaluations reveal that this semantic-pragmatic metric produces mappings for basic-level categories that not only surpass traditional cognitive semantics benchmarks but also demonstrate significant socio-cultural relevance. This finding proposes that basic-level categories, traditionally viewed as semantics-driven cognitive constructs, should be examined through the lens of both semantic and pragmatic dimensions, highlighting their role as a cognitive-cultural interface. The broad contribution of this paper lies in the development of medium-sized, interpretable, and human-centric language embedding models, which can effectively blend semantic and pragmatic dimensions to elucidate both the cognitive and socio-cultural significance of linguistic categories.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Meta-Learning for Zero-Shot Relation Triplet Extraction</title>
<link>https://arxiv.org/abs/2305.01920</link>
<guid>https://arxiv.org/abs/2305.01920</guid>
<content:encoded><![CDATA[
arXiv:2305.01920v2 Announce Type: replace 
Abstract: Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation triplets from texts containing unseen relation types. This capability benefits various downstream information retrieval (IR) tasks. The primary challenge lies in enabling models to generalize effectively to unseen relation categories. Existing approaches typically leverage the knowledge embedded in pre-trained language models to accomplish the generalization process. However, these methods focus solely on fitting the training data during training, without specifically improving the model's generalization performance, resulting in limited generalization capability. For this reason, we explore the integration of bi-level optimization (BLO) with pre-trained language models for learning generalized knowledge directly from the training data, and propose a generative meta-learning framework which exploits the `learning-to-learn' ability of meta-learning to boost the generalization capability of generative models.
  Specifically, we introduce a BLO approach that simultaneously addresses data fitting and generalization. This is achieved by constructing an upper-level loss to focus on generalization and a lower-level loss to ensure accurate data fitting. Building on this, we subsequently develop three generative meta-learning methods, each tailored to a distinct category of meta-learning. Extensive experimental results demonstrate that our framework performs well on the ZeroRTE task. Our code is available at https://github.com/leeworry/TGM-MetaLearning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking large language models for biomedical natural language processing applications and recommendations</title>
<link>https://arxiv.org/abs/2305.16326</link>
<guid>https://arxiv.org/abs/2305.16326</guid>
<content:encoded><![CDATA[
arXiv:2305.16326v5 Announce Type: replace 
Abstract: The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise in general domains, their effectiveness in BioNLP tasks remains unclear due to limited benchmarks and practical guidelines.
  We perform a systematic evaluation of four LLMs, GPT and LLaMA representatives on 12 BioNLP benchmarks across six applications. We compare their zero-shot, few-shot, and fine-tuning performance with traditional fine-tuning of BERT or BART models. We examine inconsistencies, missing information, hallucinations, and perform cost analysis. Here we show that traditional fine-tuning outperforms zero or few shot LLMs in most tasks. However, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as medical question answering. Open source LLMs still require fine-tuning to close performance gaps. We find issues like missing information and hallucinations in LLM outputs. These results offer practical insights for applying LLMs in BioNLP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ragas: Automated Evaluation of Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2309.15217</link>
<guid>https://arxiv.org/abs/2309.15217</guid>
<content:encoded><![CDATA[
arXiv:2309.15217v2 Announce Type: replace 
Abstract: We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models for newspaper sentiment analysis during COVID-19: The Guardian</title>
<link>https://arxiv.org/abs/2405.13056</link>
<guid>https://arxiv.org/abs/2405.13056</guid>
<content:encoded><![CDATA[
arXiv:2405.13056v2 Announce Type: replace 
Abstract: During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake News Detection: It's All in the Data!</title>
<link>https://arxiv.org/abs/2407.02122</link>
<guid>https://arxiv.org/abs/2407.02122</guid>
<content:encoded><![CDATA[
arXiv:2407.02122v2 Announce Type: replace 
Abstract: This comprehensive survey serves as an indispensable resource for researchers embarking on the journey of fake news detection. By highlighting the pivotal role of dataset quality and diversity, it underscores the significance of these elements in the effectiveness and robustness of detection models. The survey meticulously outlines the key features of datasets, various labeling systems employed, and prevalent biases that can impact model performance. Additionally, it addresses critical ethical issues and best practices, offering a thorough overview of the current state of available datasets. Our contribution to this field is further enriched by the provision of GitHub repository, which consolidates publicly accessible datasets into a single, user-friendly portal. This repository is designed to facilitate and stimulate further research and development efforts aimed at combating the pervasive issue of fake news.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pula: Training Large Language Models for Setswana</title>
<link>https://arxiv.org/abs/2408.02239</link>
<guid>https://arxiv.org/abs/2408.02239</guid>
<content:encoded><![CDATA[
arXiv:2408.02239v2 Announce Type: replace 
Abstract: In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we release the largest-ever Setswana text corpus, Marothodi, and the first comprehensive Setswana instruction-tuning dataset, Medupi, consisting of reformatted datasets, translated corpora, and synthetic LLM-generated text. To accompany this data, we release the code used for dataset construction, formatting, filtering, and scraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and GSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising</title>
<link>https://arxiv.org/abs/2408.05906</link>
<guid>https://arxiv.org/abs/2408.05906</guid>
<content:encoded><![CDATA[
arXiv:2408.05906v2 Announce Type: replace 
Abstract: With the increase in the fluency of ad texts automatically created by natural language generation technology, there is high demand to verify the quality of these creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation Benchmark by CyberAgent), the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations. Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as building a Japanese dataset based on the practical operational experiences of building a Japanese dataset based on the practical operational experiences of advertising agencies, which are typically kept in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human evaluators on the dataset. (iii) Analyzing the characteristics and providing challenges of the benchmark. The results show that while PLMs have already reached practical usage level in several tasks, humans still outperform in certain domains, implying that there is significant room for improvement in this area.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</title>
<link>https://arxiv.org/abs/2408.08444</link>
<guid>https://arxiv.org/abs/2408.08444</guid>
<content:encoded><![CDATA[
arXiv:2408.08444v2 Announce Type: replace 
Abstract: In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Processing for the OpenGPT-X Model Family</title>
<link>https://arxiv.org/abs/2410.08800</link>
<guid>https://arxiv.org/abs/2410.08800</guid>
<content:encoded><![CDATA[
arXiv:2410.08800v2 Announce Type: replace 
Abstract: This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling</title>
<link>https://arxiv.org/abs/2410.11325</link>
<guid>https://arxiv.org/abs/2410.11325</guid>
<content:encoded><![CDATA[
arXiv:2410.11325v3 Announce Type: replace 
Abstract: Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Domain Question Answering with Conflicting Contexts</title>
<link>https://arxiv.org/abs/2410.12311</link>
<guid>https://arxiv.org/abs/2410.12311</guid>
<content:encoded><![CDATA[
arXiv:2410.12311v4 Announce Type: replace 
Abstract: Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</title>
<link>https://arxiv.org/abs/2410.13961</link>
<guid>https://arxiv.org/abs/2410.13961</guid>
<content:encoded><![CDATA[
arXiv:2410.13961v2 Announce Type: replace 
Abstract: Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, we use existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, we manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. We release our dataset and code at github.com/megagonlabs/Hallucination_MDS.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiNER-fr-gold: A Gold-Standard NER Corpus</title>
<link>https://arxiv.org/abs/2411.00030</link>
<guid>https://arxiv.org/abs/2411.00030</guid>
<content:encoded><![CDATA[
arXiv:2411.00030v2 Announce Type: replace 
Abstract: We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese</title>
<link>https://arxiv.org/abs/2411.17270</link>
<guid>https://arxiv.org/abs/2411.17270</guid>
<content:encoded><![CDATA[
arXiv:2411.17270v2 Announce Type: replace 
Abstract: In this paper, we aimed to develop a neural parser for Vietnamese based on simplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora, VietTreebank and VnDT, had around 15% of constituency and dependency tree pairs that did not adhere to simplified HPSG rules. To attempt to address the issue of the corpora not adhering to simplified HPSG rules, we randomly permuted samples from the training and development sets to make them compliant with simplified HPSG. We then modified the first simplified HPSG Neural Parser for the Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which can encode Vietnamese texts. We conducted experiments on our modified VietTreebank and VnDT corpora. Our extensive experiments showed that the simplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82% for constituency parsing when using the same predicted part-of-speech (POS) tags as the self-attentive constituency parser. Additionally, it outperformed previous studies in dependency parsing with a higher Unlabeled Attachment Score (UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores likely due to our focus on arc permutation without changing the original labels, as we did not consult with a linguistic expert. Lastly, the research findings of this paper suggest that simplified HPSG should be given more attention to linguistic expert when developing treebanks for Vietnamese natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Length Issues in Document-level Machine Translation</title>
<link>https://arxiv.org/abs/2412.17592</link>
<guid>https://arxiv.org/abs/2412.17592</guid>
<content:encoded><![CDATA[
arXiv:2412.17592v2 Announce Type: replace 
Abstract: Transformer architectures are increasingly effective at processing and generating very long chunks of texts, opening new perspectives for document-level machine translation (MT). In this work, we challenge the ability of MT systems to handle texts comprising up to several thousands of tokens. We design and implement a new approach designed to precisely measure the effect of length increments on MT outputs. Our experiments with two representative architectures unambiguously show that (a)~translation performance decreases with the length of the input text; (b)~the position of sentences within the document matters, and translation quality is higher for sentences occurring earlier in a document. We further show that manipulating the distribution of document lengths and of positional embeddings only marginally mitigates such problems. Our results suggest that even though document-level MT is computationally feasible, it does not yet match the performance of sentence-based MT.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</title>
<link>https://arxiv.org/abs/2412.17596</link>
<guid>https://arxiv.org/abs/2412.17596</guid>
<content:encoded><![CDATA[
arXiv:2412.17596v3 Announce Type: replace 
Abstract: While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora</title>
<link>https://arxiv.org/abs/2502.00090</link>
<guid>https://arxiv.org/abs/2502.00090</guid>
<content:encoded><![CDATA[
arXiv:2502.00090v2 Announce Type: replace 
Abstract: A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have up to four distinct readings depending on the system that is used to read them. We consider the task of disambiguating between these readings in order to determine the values of the numeric quantities recorded in this corpus. We algorithmically extract a list of possible readings for each PE numeral notation, and contribute two disambiguation techniques based on structural properties of the original documents and classifiers learned with the bootstrapping algorithm. We also contribute a test set for evaluating disambiguation techniques, as well as a novel approach to cautious rule selection for bootstrapped classifiers. Our analysis confirms existing intuitions about this script and reveals previously-unknown correlations between tablet content and numeral magnitude. This work is crucial to understanding and deciphering PE, as the corpus is heavily accounting-focused and contains many more numeric tokens than tokens of text.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context</title>
<link>https://arxiv.org/abs/2502.12257</link>
<guid>https://arxiv.org/abs/2502.12257</guid>
<content:encoded><![CDATA[
arXiv:2502.12257v2 Announce Type: replace 
Abstract: Large language models excel at following explicit instructions, but they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses instead of seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. This benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions before providing appropriate responses. Our evaluation of both open and closed models reveals that, while proprietary models generally perform better, all current assistants struggle to gather critical information effectively. They often require multiple turns to infer user intent and frequently default to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, which can be leveraged to automatically generate data for self-improvement. We also offer insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.13019</link>
<guid>https://arxiv.org/abs/2502.13019</guid>
<content:encoded><![CDATA[
arXiv:2502.13019v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.14644</link>
<guid>https://arxiv.org/abs/2502.14644</guid>
<content:encoded><![CDATA[
arXiv:2502.14644v3 Announce Type: replace 
Abstract: Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision</title>
<link>https://arxiv.org/abs/2502.15147</link>
<guid>https://arxiv.org/abs/2502.15147</guid>
<content:encoded><![CDATA[
arXiv:2502.15147v2 Announce Type: replace 
Abstract: Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM's reasoning ability and drops when the data is noisy or beyond LLM's knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM's instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short.
  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents, estimates their presence across the dataset, and applies gradient-based optimization to uncover hidden factors, where each factor is represented by a cluster of co-occurring properties. We evaluate latent factors produced by Instruct-LF on movie recommendation, text-world navigation, and legal document categorization tasks. These interpretable representations improve downstream task performance by 5-52% than the best baselines and were preferred 1.8 times as often as the best alternative, on average, in human evaluation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting multimodal large language models against misleading visualizations</title>
<link>https://arxiv.org/abs/2502.20503</link>
<guid>https://arxiv.org/abs/2502.20503</guid>
<content:encoded><![CDATA[
arXiv:2502.20503v3 Announce Type: replace 
Abstract: Visualizations play a pivotal role in daily communication in an increasingly data-driven world. Research on multimodal large language models (MLLMs) for automated chart understanding has accelerated massively, with steady improvements on standard benchmarks. However, for MLLMs to be reliable, they must be robust to misleading visualizations, charts that distort the underlying data, leading readers to draw inaccurate conclusions that may support disinformation. Here, we uncover an important vulnerability: MLLM question-answering accuracy on misleading visualizations drops on average to the level of a random baseline. To address this, we introduce the first inference-time methods to improve performance on misleading visualizations, without compromising accuracy on non-misleading ones. The most effective method extracts the underlying data table and uses a text-only LLM to answer the question based on the table. Our findings expose a critical blind spot in current research and establish benchmark results to guide future efforts in reliable MLLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</title>
<link>https://arxiv.org/abs/2503.10617</link>
<guid>https://arxiv.org/abs/2503.10617</guid>
<content:encoded><![CDATA[
arXiv:2503.10617v3 Announce Type: replace 
Abstract: Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish</title>
<link>https://arxiv.org/abs/2504.09714</link>
<guid>https://arxiv.org/abs/2504.09714</guid>
<content:encoded><![CDATA[
arXiv:2504.09714v2 Announce Type: replace 
Abstract: The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.
  Our results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Semantic Scholar Open Data Platform</title>
<link>https://arxiv.org/abs/2301.10140</link>
<guid>https://arxiv.org/abs/2301.10140</guid>
<content:encoded><![CDATA[
arXiv:2301.10140v2 Announce Type: replace-cross 
Abstract: The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models</title>
<link>https://arxiv.org/abs/2303.10430</link>
<guid>https://arxiv.org/abs/2303.10430</guid>
<content:encoded><![CDATA[
arXiv:2303.10430v2 Announce Type: replace-cross 
Abstract: Online texts with toxic content are a clear threat to the users on social media in particular and society in general. Although many platforms have adopted various measures (e.g., machine learning-based hate-speech detection systems) to diminish their effect, toxic content writers have also attempted to evade such measures by using cleverly modified toxic words, so-called human-written text perturbations. Therefore, to help build automatic detection tools to recognize those perturbations, prior methods have developed sophisticated techniques to generate diverse adversarial samples. However, we note that these ``algorithms"-generated perturbations do not necessarily capture all the traits of ``human"-written perturbations. Therefore, in this paper, we introduce a novel, high-quality dataset of human-written perturbations, named as NoisyHate, that was created from real-life perturbations that are both written and verified by human-in-the-loop. We show that perturbations in NoisyHate have different characteristics than prior algorithm-generated toxic datasets show, and thus can be in particular useful to help develop better toxic speech detection solutions. We thoroughly validate NoisyHate against state-of-the-art language models, such as BERT and RoBERTa, and black box APIs, such as Perspective API, on two tasks, such as perturbation normalization and understanding.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</title>
<link>https://arxiv.org/abs/2307.12369</link>
<guid>https://arxiv.org/abs/2307.12369</guid>
<content:encoded><![CDATA[
arXiv:2307.12369v2 Announce Type: replace-cross 
Abstract: Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2403.19103</link>
<guid>https://arxiv.org/abs/2403.19103</guid>
<content:encoded><![CDATA[
arXiv:2403.19103v3 Announce Type: replace-cross 
Abstract: Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2403.20331</link>
<guid>https://arxiv.org/abs/2403.20331</guid>
<content:encoded><![CDATA[
arXiv:2403.20331v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation</title>
<link>https://arxiv.org/abs/2409.03140</link>
<guid>https://arxiv.org/abs/2409.03140</guid>
<content:encoded><![CDATA[
arXiv:2409.03140v4 Announce Type: replace-cross 
Abstract: Online sellers and advertisers are recommended keyphrases for their listed products, which they bid on to enhance their sales. One popular paradigm that generates such recommendations is Extreme Multi-Label Classification (XMC), which involves tagging/mapping keyphrases to items. We outline the limitations of using traditional item-query based tagging or mapping techniques for keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an innovative graph-based approach that recommends keyphrases to sellers using extraction of token permutations from item titles. Additionally, we demonstrate that relying on traditional metrics such as precision/recall can be misleading in practical applications, thereby necessitating a combination of metrics to evaluate performance in real-world scenarios. These metrics are designed to assess the relevance of keyphrases to items and the potential for buyer outreach. GraphEx outperforms production models at eBay, achieving the objectives mentioned above. It supports near real-time inferencing in resource-constrained production environments and scales effectively for billions of items.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents</title>
<link>https://arxiv.org/abs/2409.09013</link>
<guid>https://arxiv.org/abs/2409.09013</guid>
<content:encoded><![CDATA[
arXiv:2409.09013v2 Announce Type: replace-cross 
Abstract: Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), which makes it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, though truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be truthful or deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and LLM-based agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2410.08847</link>
<guid>https://arxiv.org/abs/2410.08847</guid>
<content:encoded><![CDATA[
arXiv:2410.08847v4 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we term likelihood displacement. We demonstrate that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning. As a simple example, training a model to prefer $\texttt{No}$ over $\texttt{Never}$ can sharply increase the probability of $\texttt{Yes}$. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement can unintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by a centered hidden embedding similarity (CHES) score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CREAM: Consistency Regularized Self-Rewarding Language Models</title>
<link>https://arxiv.org/abs/2410.12735</link>
<guid>https://arxiv.org/abs/2410.12735</guid>
<content:encoded><![CDATA[
arXiv:2410.12735v5 Announce Type: replace-cross 
Abstract: Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee of accuracy in the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance. The code is publicly available at https://github.com/Raibows/CREAM.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title>
<link>https://arxiv.org/abs/2410.18252</link>
<guid>https://arxiv.org/abs/2410.18252</guid>
<content:encoded><![CDATA[
arXiv:2410.18252v3 Announce Type: replace-cross 
Abstract: The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model which give a worse training signal. We tackle the fundamental challenge in this regime: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we test, online DPO is found to be most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. We verify the scalability of asynchronous RLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an instruction-following task ~40% faster than a synchronous run while matching final performance. Finally, we extend our results to math and reasoning to demonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster while matching synchronous accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
arXiv:2411.05060v3 Announce Type: replace-cross 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training</title>
<link>https://arxiv.org/abs/2501.15579</link>
<guid>https://arxiv.org/abs/2501.15579</guid>
<content:encoded><![CDATA[
arXiv:2501.15579v2 Announce Type: replace-cross 
Abstract: The clinical adoption of artificial intelligence (AI) in medical imaging requires models that are both diagnostically accurate and interpretable to clinicians. While current multimodal biomedical foundation models prioritize performance, their black-box nature hinders explaining the decision-making process in clinically meaningful concepts. Here, we present ConceptCLIP, the first explainable biomedical foundation model that achieves state-of-the-art diagnostic accuracy while delivering human-interpretable explanations across diverse imaging modalities. We curate MedConcept-23M, the largest pre-training dataset comprising 23 million image-text-concept triplets across diverse medical modalities, where clinical concepts are derived from the Unified Medical Language System. Leveraging this dataset, we develop ConceptCLIP through a novel dual-alignment approach that simultaneously learns global image-text representations and fine-grained region-concept associations for precise and interpretable medical image analysis. We curate the most extensive evaluation benchmark for multimodal biomedical foundation models, covering 52 clinical tasks spanning 10 imaging modalities. Extensive experiments demonstrate that ConceptCLIP outperforms existing state-of-the-art multimodal biomedical foundation models. Importantly, ConceptCLIP demonstrates superior diagnostic performance while providing human-understandable explanations validated by clinical experts. As the first precise and interpretable biomedical foundation model, ConceptCLIP represents a critical milestone toward the widespread clinical adoption of AI, thereby advancing trustworthy AI in medicine.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering</title>
<link>https://arxiv.org/abs/2502.09573</link>
<guid>https://arxiv.org/abs/2502.09573</guid>
<content:encoded><![CDATA[
arXiv:2502.09573v3 Announce Type: replace-cross 
Abstract: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM-based Student Simulation for Metacognitive Cultivation</title>
<link>https://arxiv.org/abs/2502.11678</link>
<guid>https://arxiv.org/abs/2502.11678</guid>
<content:encoded><![CDATA[
arXiv:2502.11678v2 Announce Type: replace-cross 
Abstract: Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence</title>
<link>https://arxiv.org/abs/2502.20601</link>
<guid>https://arxiv.org/abs/2502.20601</guid>
<content:encoded><![CDATA[
arXiv:2502.20601v2 Announce Type: replace-cross 
Abstract: Maintaining a balanced diet is essential for overall health, yet many individuals struggle with meal planning due to nutritional complexity, time constraints, and lack of dietary knowledge. Personalized food recommendations can help address these challenges by tailoring meal plans to individual preferences, habits, and dietary restrictions. However, existing dietary recommendation systems often lack adaptability, fail to consider real-world constraints such as food ingredient availability, and require extensive user input, making them impractical for sustainable and scalable daily use. To address these limitations, we introduce NutriGen, a framework based on large language models (LLM) designed to generate personalized meal plans that align with user-defined dietary preferences and constraints. By building a personalized nutrition database and leveraging prompt engineering, our approach enables LLMs to incorporate reliable nutritional references like the USDA nutrition database while maintaining flexibility and ease-of-use. We demonstrate that LLMs have strong potential in generating accurate and user-friendly food recommendations, addressing key limitations in existing dietary recommendation systems by providing structured, practical, and scalable meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve the lowest percentage errors of 1.55\% and 3.68\%, respectively, producing meal plans that closely align with user-defined caloric targets while minimizing deviation and improving precision. Additionally, we compared the performance of DeepSeek V3 against several established models to evaluate its potential in personalized nutrition planning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</title>
<link>https://arxiv.org/abs/2503.10619</link>
<guid>https://arxiv.org/abs/2503.10619</guid>
<content:encoded><![CDATA[
arXiv:2503.10619v3 Announce Type: replace-cross 
Abstract: We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking</title>
<link>https://arxiv.org/abs/2504.03947</link>
<guid>https://arxiv.org/abs/2504.03947</guid>
<content:encoded><![CDATA[
arXiv:2504.03947v2 Announce Type: replace-cross 
Abstract: We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use</title>
<link>https://arxiv.org/abs/2504.04736</link>
<guid>https://arxiv.org/abs/2504.04736</guid>
<content:encoded><![CDATA[
arXiv:2504.04736v2 Announce Type: replace-cross 
Abstract: Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniCaptioner: One Captioner to Rule Them All</title>
<link>https://arxiv.org/abs/2504.07089</link>
<guid>https://arxiv.org/abs/2504.07089</guid>
<content:encoded><![CDATA[
arXiv:2504.07089v2 Announce Type: replace-cross 
Abstract: We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection</title>
<link>https://arxiv.org/abs/2504.17834</link>
<guid>https://arxiv.org/abs/2504.17834</guid>
<content:encoded><![CDATA[
arXiv:2504.17834v2 Announce Type: replace-cross 
Abstract: Spoilers in movie reviews are important on platforms like IMDb and Rotten Tomatoes, offering benefits and drawbacks. They can guide some viewers' choices but also affect those who prefer no plot details in advance, making effective spoiler detection essential. Existing spoiler detection methods mainly analyze review text, often overlooking the impact of movie genres and user bias, limiting their effectiveness. To address this, we analyze movie review data, finding genre-specific variations in spoiler rates and identifying that certain users are more likely to post spoilers. Based on these findings, we introduce a new spoiler detection framework called GUSD (The code is available at https://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler Detection), which incorporates genre-specific data and user behavior bias. User bias is calculated through dynamic graph modeling of review history. Additionally, the R2GFormer module combines RetGAT (Retentive Graph Attention Network) for graph information and GenreFormer for genre-specific aggregation. The GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to specialized experts based on genre. Extensive testing on benchmark datasets shows that GUSD achieves state-of-the-art results. This approach advances spoiler detection by addressing genre and user-specific patterns, enhancing user experience on movie review platforms.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English</title>
<link>https://arxiv.org/abs/2504.17974</link>
<guid>https://arxiv.org/abs/2504.17974</guid>
<content:encoded><![CDATA[
<div> Dataset, Hope, Natural Language Processing, Emotional State, Sarcasm
<br />
Summary:
This study introduces PolyHope V2, a multilingual dataset of over 30,000 annotated tweets in English and Spanish, distinguishing between four hope subtypes: Generalized, Realistic, Unrealistic, and Sarcastic. The dataset enhances existing resources by explicitly labeling sarcastic instances. Benchmarking multiple pretrained transformer models against large language models (LLMs) like GPT 4 and Llama 3 shows that fine-tuned transformers outperform prompt-based LLMs in identifying nuanced hope categories and sarcasm, especially in zero-shot and few-shot regimes. Through qualitative analysis and confusion matrices, systematic challenges in differentiating closely related hope subtypes are highlighted. The dataset and results provide a solid basis for future emotion recognition tasks that require greater semantic and contextual sensitivity across languages. 
<br /><br />Summary: <div>
arXiv:2504.17974v1 Announce Type: new 
Abstract: Hope is a complex and underexplored emotional state that plays a significant role in education, mental health, and social interaction. Unlike basic emotions, hope manifests in nuanced forms ranging from grounded optimism to exaggerated wishfulness or sarcasm, making it difficult for Natural Language Processing systems to detect accurately. This study introduces PolyHope V2, a multilingual, fine-grained hope speech dataset comprising over 30,000 annotated tweets in English and Spanish. This resource distinguishes between four hope subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances existing datasets by explicitly labeling sarcastic instances. We benchmark multiple pretrained transformer models and compare them with large language models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes. Our findings show that fine-tuned transformers outperform prompt-based LLMs, especially in distinguishing nuanced hope categories and sarcasm. Through qualitative analysis and confusion matrices, we highlight systematic challenges in separating closely related hope subtypes. The dataset and results provide a robust foundation for future emotion recognition tasks that demand greater semantic and contextual sensitivity across languages.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Personas via Rationalization with Psychological Scaffolds</title>
<link>https://arxiv.org/abs/2504.17993</link>
<guid>https://arxiv.org/abs/2504.17993</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, user personas, psychology, behavior, judgments

Summary:
The article introduces a new framework called PB&amp;J (Psychology of Behavior and Judgments) that enhances language model personas by incorporating rationales for user judgments. These rationales are generated by LLMs and aim to explain why a user makes specific decisions based on their experiences, personality traits, and beliefs. The framework utilizes psychological scaffolds grounded in theories like the Big 5 Personality Traits and Primal World Beliefs to provide structure to the generated rationales. Experiments show that LLM personas augmented with PB&amp;J rationales outperform traditional methods that only consider demographics and judgments. Furthermore, LLM personas constructed using scaffolds describing user beliefs perform on par with those using human-written rationales.<br /><br />Summary: <div>
arXiv:2504.17993v1 Announce Type: new 
Abstract: Language models prompted with a user description or persona can predict a user's preferences and opinions, but existing approaches to building personas -- based solely on a user's demographic attributes and/or prior judgments -- fail to capture the underlying reasoning behind said user judgments. We introduce PB&amp;J (Psychology of Behavior and Judgments), a framework that improves LLM personas by incorporating rationales of why a user might make specific judgments. These rationales are LLM-generated, and aim to reason about a user's behavior on the basis of their experiences, personality traits or beliefs. This is done using psychological scaffolds -- structured frameworks grounded in theories such as the Big 5 Personality Traits and Primal World Beliefs -- that help provide structure to the generated rationales. Experiments on public opinion and movie preference prediction tasks demonstrate that LLM personas augmented with PB&amp;J rationales consistently outperform methods using only a user's demographics and/or judgments. Additionally, LLM personas constructed using scaffolds describing user beliefs perform competitively with those using human-written rationales.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2504.18012</link>
<guid>https://arxiv.org/abs/2504.18012</guid>
<content:encoded><![CDATA[
<div> pre-trained encoders, pre-trained decoders, multimodal machine translation, training strategies, translation performance<br />
Summary:<br />
This study explores the impact of pre-trained encoders and decoders in multimodal translation models. It investigates different training strategies, including training from scratch and utilizing pre-trained and partially frozen components. The experiments conducted on the Multi30K and CoMMuTE datasets for English-German and English-French translation tasks demonstrate that pre-training plays a vital role in multimodal translation, with pre-trained decoders consistently producing more fluent and accurate outputs. The effects of pre-trained encoders vary depending on the quality of visual-text alignment. The study also offers insights into how modality fusion and pre-trained components interact, providing valuable guidance for the design of future multimodal translation systems. <br /> <div>
arXiv:2504.18012v1 Announce Type: new 
Abstract: Multimodal Machine Translation (MMT) aims to improve translation quality by leveraging auxiliary modalities such as images alongside textual input. While recent advances in large-scale pre-trained language and vision models have significantly benefited unimodal natural language processing tasks, their effectiveness and role in MMT remain underexplored. In this work, we conduct a systematic study on the impact of pre-trained encoders and decoders in multimodal translation models. Specifically, we analyze how different training strategies, from training from scratch to using pre-trained and partially frozen components, affect translation performance under a unified MMT framework. Experiments are carried out on the Multi30K and CoMMuTE dataset across English-German and English-French translation tasks. Our results reveal that pre-training plays a crucial yet asymmetrical role in multimodal settings: pre-trained decoders consistently yield more fluent and accurate outputs, while pre-trained encoders show varied effects depending on the quality of visual-text alignment. Furthermore, we provide insights into the interplay between modality fusion and pre-trained components, offering guidance for future architecture design in multimodal translation systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models</title>
<link>https://arxiv.org/abs/2504.18041</link>
<guid>https://arxiv.org/abs/2504.18041</guid>
<content:encoded><![CDATA[
<div> frameworks, safety, language models, RAG, red-teaming<br />Summary:
Efforts to ensure the safety of large language models (LLMs) have focused on standard models but little is known about the safety profile of models using the Retrieval-Augmented Generation (RAG) framework. A comparative analysis of RAG and non-RAG frameworks with eleven LLMs revealed that RAG can make models less safe and change their safety profile. Even combinations of safe models with safe documents can lead to unsafe generations. Existing red teaming methods are found to be less effective in RAG settings compared to non-RAG settings. This highlights the need for safety research and red-teaming methods tailored specifically for RAG LLMs. <div>
arXiv:2504.18041v1 Announce Type: new 
Abstract: Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.18053</link>
<guid>https://arxiv.org/abs/2504.18053</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Risk Disentanglement, Safety Alignment, DREAM, Reinforcement Learning.

Summary:
Multimodal Large Language Models (MLLMs) integrate visual and textual data, introducing new safety challenges. This paper disentangles risks in multimodal inputs to enhance risk awareness in MLLMs. A novel approach, DREAM (Disentangling Risks to Enhance Safety Alignment in MLLMs), uses supervised fine-tuning and Reinforcement Learning from AI Feedback (RLAIF) to boost safety without compromising performance. Experimental results show DREAM improves safety during both inference and training phases, achieving a 16.17% improvement in the SIUO safe & effective score compared to GPT-4V. The data and code for DREAM are available on GitHub at https://github.com/Kizna1ver/DREAM.

<br /><br />Summary: Multimodal Large Language Models present unique safety challenges. This paper analyzes and disentangles risks in multimodal inputs, leading to enhanced risk awareness. The DREAM approach leverages supervised fine-tuning and Reinforcement Learning from AI Feedback to improve safety alignment in MLLMs. Experimental results demonstrate significant safety enhancements without compromising performance, outperforming GPT-4V in the SIUO safe & effective score. The data and code for DREAM can be found on GitHub. <div>
arXiv:2504.18053v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \textbf{DREAM} (\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\% improvement in the SIUO safe\&amp;effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Personality-Aware Interactions in Salesperson Dialogue Agents</title>
<link>https://arxiv.org/abs/2504.18058</link>
<guid>https://arxiv.org/abs/2504.18058</guid>
<content:encoded><![CDATA[
<div> MBTI, user personas, dialogue agents, sales domain, interaction quality <br />
Summary:<br />
This study examines the impact of user personas, categorized by MBTI, on sales-oriented dialogue agents. Through extensive testing, the study evaluates how pre-trained agents perform with different user types. Results show distinct patterns in interaction dynamics, task completion rates, and dialogue naturalness based on user personas. The findings suggest opportunities for dialogue agents to enhance their adaptability and personalization to align with various personality traits. The study's insights offer practical guidance for creating more user-centric conversational systems in sales and provide persona-defined user simulators that can be applied across diverse applications. The release of these simulators contributes to advancing personalized dialogue systems in various domains. <br /> <div>
arXiv:2504.18058v1 Announce Type: new 
Abstract: The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent's effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropRAG: Guiding Retrieval with Beam Search over Proposition Paths</title>
<link>https://arxiv.org/abs/2504.18070</link>
<guid>https://arxiv.org/abs/2504.18070</guid>
<content:encoded><![CDATA[
<div> framework, propositions, reasoning, retrieval, continual learning
Summary:
PropRAG introduces a framework that leverages contextually rich propositions and a novel beam search algorithm for multi-step reasoning chains. The online retrieval process of PropRAG operates without relying on generative Large Language Models (LLMs). By using efficient graph traversal and pre-computed embeddings, PropRAG avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are utilized offline for proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on various question answering datasets, including PopQA, 2Wiki, HotpotQA, and MuSiQue, with top F1 scores on MuSiQue. By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.
<br /><br />Summary: <div>
arXiv:2504.18070v1 Announce Type: new 
Abstract: Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18080</link>
<guid>https://arxiv.org/abs/2504.18080</guid>
<content:encoded><![CDATA[
<div> medical, language models, Japanese, reasoning explanations, accuracy <br />
<br />
Summary: 
The paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to provide high accuracy and stable reasoning. It undergoes a two-stage fine-tuning process: Continued Pretraining (CPT) on a Japanese medical corpus followed by Reasoning Preference Optimization (RPO). Evaluations on the Japanese Medical Licensing Exam benchmark show that Preferred-MedLLM-Qwen-72B outperforms proprietary models like GPT-4o in accuracy. Importantly, the model maintains its high accuracy even when prompted for reasoning explanations, showcasing the effectiveness of RPO in stabilizing reasoning generation. The study emphasizes the need to optimize language models for reliable explanations in addition to accuracy, aiming to promote research into trustworthy LLMs for critical applications. The model weights for Preferred-MedLLM-Qwen-72B are made publicly available to support further research in this area. <br /> <div>
arXiv:2504.18080v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations -- a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPO's effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random-Set Large Language Models</title>
<link>https://arxiv.org/abs/2504.18085</link>
<guid>https://arxiv.org/abs/2504.18085</guid>
<content:encoded><![CDATA[
<div> Language Models, Uncertainty Quantification, Random-Set Approach, Epistemic Uncertainty, Hierarchical Clustering <br />
Summary:<br />
This paper addresses uncertainty quantification in Large Language Models (LLMs) by introducing a Random-Set Large Language Model (RSLLM) approach. Instead of using probability vectors, RSLLMs predict finite random sets (belief functions) over the token space to encode epistemic uncertainty. A methodology based on hierarchical clustering is proposed to extract and use "focal" subsets of tokens for efficient belief prediction. The approach is evaluated on CoQA and OBQA datasets using various models, showing improved correctness of answers and the ability to estimate second-level uncertainty. RSLLMs also demonstrate the capability to detect hallucinations in generated text. Overall, the RSLLM approach outperforms standard LLMs in terms of answer correctness and uncertainty estimation, highlighting its potential for more reliable and trustworthy text generation. <br /> <div>
arXiv:2504.18085v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of "focal" subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation</title>
<link>https://arxiv.org/abs/2504.18104</link>
<guid>https://arxiv.org/abs/2504.18104</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, fact-check-worthiness, prompt tuning, language models, evaluation.

Summary:
This paper introduces a method for estimating the worthiness of fact-checking claims in the face of misinformation in a globalized and digital world. The proposed approach utilizes prompt tuning to enhance the accuracy of determining the fact-check-worthiness of claims, particularly in scenarios with limited or unlabelled data. By applying prompt templates to large language models, the model achieves superior performance compared to established baseline methods such as BERT, GPT-3.5, and GPT-4. The experiments conducted on public datasets demonstrate the effectiveness of the prompt tuning-based method, showcasing improvements in evaluation metrics like F1 score and accuracy. This method proves to be a significant advancement in the field of fact-checking, providing a reliable and efficient tool for combating misinformation. 

<br /><br />Summary: <div>
arXiv:2504.18104v1 Announce Type: new 
Abstract: In response to the growing problem of misinformation in the context of globalization and informatization, this paper proposes a classification method for fact-check-worthiness estimation based on prompt tuning. We construct a model for fact-check-worthiness estimation at the methodological level using prompt tuning. By applying designed prompt templates to large language models, we establish in-context learning and leverage prompt tuning technology to improve the accuracy of determining whether claims have fact-check-worthiness, particularly when dealing with limited or unlabeled data. Through extensive experiments on public datasets, we demonstrate that the proposed method surpasses or matches multiple baseline methods in the classification task of fact-check-worthiness estimation assessment, including classical pre-trained models such as BERT, as well as recent popular large models like GPT-3.5 and GPT-4. Experiments show that the prompt tuning-based method proposed in this study exhibits certain advantages in evaluation metrics such as F1 score and accuracy, thereby effectively validating its effectiveness and advancement in the task of fact-check-worthiness estimation.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering</title>
<link>https://arxiv.org/abs/2504.18106</link>
<guid>https://arxiv.org/abs/2504.18106</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese media, English media, Olympics, topic modeling, discourse construction

Summary: 
Chinese and English media reports on the Paris Olympics were analyzed using topic modeling, Large Language Model prompt engineering, and corpus phraseology methods. Common topics included the opening ceremony, athlete performance, and sponsorship brands. Chinese media focused on specific sports, sports spirit, doping controversies, and new technologies, while English media highlighted female athletes, medal wins, and eligibility controversies. Chinese reports exhibited more prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. In contrast, English reports showed positive semantic prosody in coverage of female athletes but negativity in predicting opening ceremony reactions and discussing women's boxing controversies. These differences in discourse construction and attitudinal meanings provide insights into how the Olympics were perceived and reported in Chinese and English media. 

Summary: <div>
arXiv:2504.18106v1 Announce Type: new 
Abstract: This study analyzes Chinese and English media reports on the Paris Olympics using topic modeling, Large Language Model (LLM) prompt engineering, and corpus phraseology methods to explore similarities and differences in discourse construction and attitudinal meanings. Common topics include the opening ceremony, athlete performance, and sponsorship brands. Chinese media focus on specific sports, sports spirit, doping controversies, and new technologies, while English media focus on female athletes, medal wins, and eligibility controversies. Chinese reports show more frequent prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. English reports exhibit positive semantic prosody when covering female athletes but negative prosody in predicting opening ceremony reactions and discussing women's boxing controversies.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.18114</link>
<guid>https://arxiv.org/abs/2504.18114</guid>
<content:encoded><![CDATA[
<div> hallucination detection metrics, language models, empirical evaluation, decoding methods, knowledge-grounded settings <br />
Summary: <br />
The study evaluates hallucination detection metrics across various language models and decoding methods. Current metrics do not consistently align with human judgments, lack a comprehensive view of the issue, and show inconsistent performance with model scaling. Notably, LLM-based evaluation, particularly using GPT-4, shows the best results, while mode-seeking decoding methods appear to reduce hallucinations in knowledge-grounded contexts. The findings highlight the need for more robust metrics and strategies to effectively measure and mitigate hallucinations in language models. <div>
arXiv:2504.18114v1 Announce Type: new 
Abstract: Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Entailment Pretraining for Clinical Language Models over EHR Data</title>
<link>https://arxiv.org/abs/2504.18128</link>
<guid>https://arxiv.org/abs/2504.18128</guid>
<content:encoded><![CDATA[
<div> Clinical language models, temporally entailed pretraining, electronic health record, patient trajectories, clinical reasoning<br />
<br />
Summary: <br />
The article introduces a novel temporal entailment pretraining objective for language models in the clinical domain, aiming to capture the evolving and interconnected nature of patient trajectories in the electronic health record. By formulating EHR segments as temporally ordered sentence pairs and training models to understand the relationships between different states over time, the proposed method enhances the ability of language models to generalize across forecasting and diagnosis tasks. Pretraining on a large corpus from MIMIC IV leads to state-of-the-art performance on temporal clinical QA, early warning prediction, and disease progression modeling. This approach provides a more nuanced understanding of patient data in the clinical setting, improving the model's capacity for clinical reasoning and decision-making. <br /> <div>
arXiv:2504.18128v1 Announce Type: new 
Abstract: Clinical language models have achieved strong performance on downstream tasks by pretraining on domain specific corpora such as discharge summaries and medical notes. However, most approaches treat the electronic health record as a static document, neglecting the temporally-evolving and causally entwined nature of patient trajectories. In this paper, we introduce a novel temporal entailment pretraining objective for language models in the clinical domain. Our method formulates EHR segments as temporally ordered sentence pairs and trains the model to determine whether a later state is entailed by, contradictory to, or neutral with respect to an earlier state. Through this temporally structured pretraining task, models learn to perform latent clinical reasoning over time, improving their ability to generalize across forecasting and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and demonstrate state of the art results on temporal clinical QA, early warning prediction, and disease progression modeling.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)</title>
<link>https://arxiv.org/abs/2504.18142</link>
<guid>https://arxiv.org/abs/2504.18142</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, NER, Urdu, educational content, annotated dataset<br />
<br />
Summary: 
This study focuses on Named Entity Recognition (NER) in Urdu within the domain of education, an area that lacks annotated datasets. The authors created the EDU-NER-2025 dataset containing 13 entities related to education. They detail the annotation process and discuss challenges faced in labeling the dataset. Additionally, linguistic complexities such as morphological nuances and ambiguity in formal Urdu texts are addressed. This work highlights the need for targeted resources for Urdu NER in specific domains like education, where existing models struggle to accurately identify entities such as academic roles, course names, and institutional terms. <div>
arXiv:2504.18142v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) plays a pivotal role in various Natural Language Processing (NLP) tasks by identifying and classifying named entities (NEs) from unstructured data into predefined categories such as person, organization, location, date, and time. While extensive research exists for high-resource languages and general domains, NER in Urdu particularly within domain-specific contexts like education remains significantly underexplored. This is Due to lack of annotated datasets for educational content which limits the ability of existing models to accurately identify entities such as academic roles, course names, and institutional terms, underscoring the urgent need for targeted resources in this domain. To the best of our knowledge, no dataset exists in the domain of the Urdu language for this purpose. To achieve this objective this study makes three key contributions. Firstly, we created a manually annotated dataset in the education domain, named EDU-NER-2025, which contains 13 unique most important entities related to education domain. Second, we describe our annotation process and guidelines in detail and discuss the challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed key linguistic challenges, such as morphological complexity and ambiguity, which are prevalent in formal Urdu texts.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models for Icelandic Legal Text Summarization</title>
<link>https://arxiv.org/abs/2504.18180</link>
<guid>https://arxiv.org/abs/2504.18180</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, legal domain, preference training, Icelandic, summarization

Summary:
Preference-based training techniques were examined to enhance language models' performance in generating Icelandic legal summaries. The study compared models trained with Reinforcement Learning from Human Feedback and Direct Preference Optimization to those using supervised learning. Results showed that preference training improved the legal accuracy of summaries but did not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations emphasize the need for qualitative assessments in developing language models for the legal domain. <div>
arXiv:2504.18180v1 Announce Type: new 
Abstract: The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models' performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish</title>
<link>https://arxiv.org/abs/2504.18221</link>
<guid>https://arxiv.org/abs/2504.18221</guid>
<content:encoded><![CDATA[
<div> Keywords: Chat-GPT, machine translation, creativity, literary text, evaluation<br />
Summary:<br />
This study investigates the variability of Chat-GPT machine translation outputs across different configurations in four languages, focusing on creativity in literary text. Various text granularity levels, temperature settings, and prompting strategies were evaluated using a Creativity Score formula. The results showed that prompting ChatGPT with minimal instruction led to the best creative translations, particularly with the instruction "Translate the following text into [TG] creatively" at a temperature of 1.0. This configuration outperformed other setups as well as DeepL in Spanish, Dutch, and Chinese translations. However, it was noted that ChatGPT consistently fell short compared to human translation. The study highlights the importance of prompting strategies and temperature settings in enhancing the creative output of machine translation systems like Chat-GPT. <br /> <div>
arXiv:2504.18221v1 Announce Type: new 
Abstract: This study examines the variability of Chat-GPT machine translation (MT) outputs across six different configurations in four languages,with a focus on creativity in a literary text. We evaluate GPT translations in different text granularity levels, temperature settings and prompting strategies with a Creativity Score formula. We found that prompting ChatGPT with a minimal instruction yields the best creative translations, with "Translate the following text into [TG] creatively" at the temperature of 1.0 outperforming other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless, ChatGPT consistently underperforms compared to human translation (HT).
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family</title>
<link>https://arxiv.org/abs/2504.18225</link>
<guid>https://arxiv.org/abs/2504.18225</guid>
<content:encoded><![CDATA[
<div> Keywords: small reasoning models, RAG, source summarization, Pleias-RAG-350m, Pleias-RAG-1B 

Summary: 
Pleias-RAG-350m and Pleias-RAG-1B are new small reasoning models designed for RAG, search, and source summarization tasks. They are mid-trained on a synthetic dataset to emulate retrieval from multilingual open sources and excel in features like query routing and source reranking. These models outperform models below 4 billion parameters on RAG benchmarks like HotPotQA and 2wiki, rivaling larger models like Qwen-2.5-7B and Llama-3.1-8B. They offer consistent performance across European languages and ensure systematic reference grounding. Their size allows deployment on constrained infrastructure and higher factuality. Pleias-RAG-350m and Pleias-RAG-1B unlock new use cases for generative AI with their capabilities. 

<br /><br />Summary: <div>
arXiv:2504.18225v1 Announce Type: new 
Abstract: We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Single-Pass Training for Multi-Turn Reasoning</title>
<link>https://arxiv.org/abs/2504.18246</link>
<guid>https://arxiv.org/abs/2504.18246</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, explicit reasoning, multi-turn reasoning, fine-tuning, attention mask

Summary:
Training Large Language Models (LLMs) to generate explicit reasoning before producing an answer has shown improved performance in tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets poses a challenge as they must exclude reasoning tokens from subsequent inputs, preventing processing an entire conversation in a single forward pass. This paper introduces a novel approach utilizing response token duplication and a custom attention mask to address this limitation. By enforcing appropriate visibility constraints, the proposed method reduces training time and enables efficient fine-tuning on multi-turn reasoning datasets. This innovative solution opens up opportunities for enhancing LLM performance in complex reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2504.18246v1 Announce Type: new 
Abstract: Training Large Language Models ( LLMs) to generate explicit reasoning before they produce an answer has been shown to improve their performance across various tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets presents a unique challenge: LLMs must generate reasoning tokens that are excluded from subsequent inputs to the LLM. This discrepancy prevents us from processing an entire conversation in a single forward pass-an optimization readily available when we fine-tune on a multi-turn non-reasoning dataset. This paper proposes a novel approach that overcomes this limitation through response token duplication and a custom attention mask that enforces appropriate visibility constraints. Our approach significantly reduces the training time and allows efficient fine-tuning on multi-turn reasoning datasets.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGI: Multi-Agent Guided Interview for Psychiatric Assessment</title>
<link>https://arxiv.org/abs/2504.18260</link>
<guid>https://arxiv.org/abs/2504.18260</guid>
<content:encoded><![CDATA[
<div> Keywords: structured clinical interviews, mental healthcare, MAGI, Mini International Neuropsychiatric Interview, psychiatric diagnostic protocols

Summary: 
MAGI is introduced as a framework that automates structured clinical interviews, specifically based on the Mini International Neuropsychiatric Interview (MINI). It operates through a coordinated multi-agent collaboration, navigating clinical logic with specialized agents. These agents include interview tree guided navigation, adaptive question formulation, judgment validation, and diagnosis generation. Through experimental testing on over 1,000 real-world participants with various mental health conditions, MAGI is shown to enhance large language model (LLM) assisted mental health assessments by incorporating clinical rigor, conversational adaptability, and explainable reasoning. This advancement demonstrates the potential for automated systems to revolutionize mental healthcare accessibility and align with established psychiatric diagnostic protocols.<br /><br />Summary: <div>
arXiv:2504.18260v1 Announce Type: new 
Abstract: Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2504.18269</link>
<guid>https://arxiv.org/abs/2504.18269</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, entity prompts, Large Language Models, TextTIGER, WiT-Cub

Summary:
TextTIGER introduces a method for improving image generation from prompts containing specific entities by augmenting entity knowledge and summarizing descriptions with Large Language Models (LLMs). The method, validated on the WiT-Cub dataset, enhances image generation performance compared to using caption-only prompts, as shown through standard metrics (IS, FID, and CLIPScore). An evaluation by multiple annotators confirms the informative nature of the summarized descriptions, indicating the ability of LLMs to generate concise yet rich prompts. By refining prompts with augmented and summarized entity-related descriptions, TextTIGER enhances image generation capabilities, showcasing the potential of this approach for generating visually compelling images. The code and dataset for this research will be made available upon acceptance. 

<br /><br />Summary: <div>
arXiv:2504.18269v1 Announce Type: new 
Abstract: Generating images from prompts containing specific entities requires models to retain as much entity-specific knowledge as possible. However, fully memorizing such knowledge is impractical due to the vast number of entities and their continuous emergence. To address this, we propose Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), which augments knowledge on entities included in the prompts and then summarizes the augmented descriptions using Large Language Models (LLMs) to mitigate performance degradation from longer inputs. To evaluate our method, we introduce WiT-Cub (WiT with Captions and Uncomplicated Background-explanations), a dataset comprising captions, images, and an entity list. Experiments on four image generation models and five LLMs show that TextTIGER improves image generation performance in standard metrics (IS, FID, and CLIPScore) compared to caption-only prompts. Additionally, multiple annotators' evaluation confirms that the summarized descriptions are more informative, validating LLMs' ability to generate concise yet rich descriptions. These findings demonstrate that refining prompts with augmented and summarized entity-related descriptions enhances image generation capabilities. The code and dataset will be available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2504.18346</link>
<guid>https://arxiv.org/abs/2504.18346</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Uncertainty Quantification, Calibration, Benchmark, Reliability Datasets

Summary: 
This paper addresses the issue of hallucination in Large Language Models (LLMs) and the challenge of accurately assessing their uncertainty. The authors survey existing literature on Uncertainty Quantification (UQ) and calibration techniques for LLMs, highlighting the lack of a comprehensive benchmark for comparison. They evaluate six methods using two reliability datasets, providing empirical evidence for their effectiveness. This study is the first to systematically review calibration methods and metrics for LLMs, offering insights for future research directions and identifying open challenges in the field. Overall, the paper contributes to the understanding of uncertainty estimation in LLMs and provides a valuable resource for researchers working on improving the reliability of these models. 

<br /><br />Summary: <div>
arXiv:2504.18346v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant</title>
<link>https://arxiv.org/abs/2504.18373</link>
<guid>https://arxiv.org/abs/2504.18373</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, large language models, multi-agent frameworks, intelligent personal assistants, Auto-SLURP

Summary:
Auto-SLURP is a new benchmark dataset designed to evaluate the performance of large language model-based multi-agent frameworks in the context of intelligent personal assistants. It builds upon the original SLURP dataset by incorporating simulated servers and external services, allowing for a comprehensive evaluation pipeline covering language understanding, task execution, and response generation. The experiments conducted using Auto-SLURP reveal that current state-of-the-art frameworks face significant challenges in achieving reliable and intelligent multi-agent personal assistants. This underscores the ongoing work needed to enhance the capabilities of such systems. The dataset and related code are publicly available, providing a valuable resource for researchers and developers in the field. 

<br /><br />Summary: <div>
arXiv:2504.18373v1 Announce Type: new 
Abstract: In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the boundary on Natural Language Inference</title>
<link>https://arxiv.org/abs/2504.18376</link>
<guid>https://arxiv.org/abs/2504.18376</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, natural language inference, Chain-of-Thought learning, adversarial benchmark, parameter-efficient techniques 

Summary:
Natural Language Inference (NLI) is a crucial task in language understanding, but current systems face limitations due to reliance on supervised learning with biased datasets. This study introduces a reinforcement learning-based approach, GRPO, for CoT learning in NLI, allowing training on challenging datasets like ANLI without rationales. The use of LoRA and QLoRA for fine-tuning 7B, 14B, and 32B language models demonstrates strong performance on standard and adversarial benchmarks. The 32B AWQ-quantized model surpasses state-of-the-art results on multiple adversarial sets within a limited memory footprint, showcasing robust reasoning capabilities under aggressive quantization. This work presents a scalable framework for building reliable NLI systems without compromising inference quality.

<br /><br />Summary: <div>
arXiv:2504.18376v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\unicode{x2013}$or on all of them considering our replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A UD Treebank for Bohairic Coptic</title>
<link>https://arxiv.org/abs/2504.18386</link>
<guid>https://arxiv.org/abs/2504.18386</guid>
<content:encoded><![CDATA[
<div> annotated corpus, Bohairic Coptic, syntactic analysis, cross-dialect parsing, linguistic resources
Summary:
An annotated corpus of Bohairic Coptic, a main dialect of pre-Mamluk Egypt, has been developed, addressing the lack of resources for this variety compared to Sahidic Coptic. The corpus includes texts from various genres like Biblical text and saints' lives. Comparisons with Sahidic Coptic reveal distinct differences, highlighting Bohairic's unique features. Cross-dialect parsing experiments demonstrate the challenges and importance of studying Bohairic separately. This work contributes to advancing linguistic resources for Bohairic Coptic and sheds light on its significance as a distinct but related dialect within the Coptic language family. <div>
arXiv:2504.18386v1 Announce Type: new 
Abstract: Despite recent advances in digital resources for other Coptic dialects, especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk, late Byzantine Egypt, and the contemporary language of the Coptic Church, remains critically under-resourced. This paper presents and evaluates the first syntactically annotated corpus of Bohairic Coptic, sampling data from a range of works, including Biblical text, saints' lives and Christian ascetic writing. We also explore some of the main differences we observe compared to the existing UD treebank of Sahidic Coptic, the classical dialect of the language, and conduct joint and cross-dialect parsing experiments, revealing the unique nature of Bohairic as a related, but distinct variety from the more often studied Sahidic.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?</title>
<link>https://arxiv.org/abs/2504.18406</link>
<guid>https://arxiv.org/abs/2504.18406</guid>
<content:encoded><![CDATA[
<div> image understanding, high-resolution, VLMs, benchmark, HRScene

Summary:
HRScene introduces a new benchmark for high-resolution image (HRI) understanding, encompassing 25 real-world and 2 synthetic diagnostic datasets with varying resolutions. The benchmark covers a wide range of scenarios, from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-images, providing a comprehensive evaluation platform for Vision Large Language Models (VLMs). Experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o, reveal an average accuracy of around 50% on real-world tasks, highlighting significant gaps in HRI understanding. Results on synthetic datasets expose VLMs' struggles to effectively utilize HRI regions, indicating Regional Divergence and lost-in-middle issues that require further research exploration.

<br /><br />Summary: <div>
arXiv:2504.18406v1 Announce Type: new 
Abstract: High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers</title>
<link>https://arxiv.org/abs/2504.18412</link>
<guid>https://arxiv.org/abs/2504.18412</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, therapy, therapeutic alliance, mental health, LLMs<br />
Summary:<br />
This paper examines the potential use of large language models (LLMs) as therapists and evaluates their ability to replicate crucial aspects of therapeutic relationships. The study highlights that current LLMs exhibit stigma towards mental health conditions and respond inadequately to common therapy scenarios, such as encouraging delusional thinking. Despite advancements in LLM technology, they struggle to establish a therapeutic alliance due to lacking human characteristics. The research emphasizes the importance of human empathy and understanding in therapeutic interactions, indicating that LLMs are not suitable replacements for human therapists. The paper concludes by proposing alternative roles for LLMs in clinical therapy settings.<br />  
Summary: <div>
arXiv:2504.18412v1 Announce Type: new 
Abstract: Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</title>
<link>https://arxiv.org/abs/2504.18415</link>
<guid>https://arxiv.org/abs/2504.18415</guid>
<content:encoded><![CDATA[
<div> Keywords: 1-bit Large Language Models, activation quantization, BitNet v2, H-BitLinear, memory footprint reduction

Summary: 
BitNet v2 is a novel framework designed to efficiently deploy 1-bit Large Language Models (LLMs) by enabling native 4-bit activation quantization. The framework introduces H-BitLinear, a module that applies an online Hadamard transformation to tackle outliers in attention and feed-forward network activations. This transformation helps smooth sharp activation distributions into more Gaussian-like forms, making them suitable for low-bit representation. Experimental results demonstrate that BitNet v2, when trained from scratch with 8-bit activations, matches the performance of BitNet b1.58. Importantly, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing both memory footprint and computational cost for batched inference. This approach offers a promising solution for optimizing the deployment of 1-bit LLMs with improved efficiency and reduced resource requirements. 

<br /><br />Summary: <div>
arXiv:2504.18415v1 Announce Type: new 
Abstract: Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts</title>
<link>https://arxiv.org/abs/2504.18428</link>
<guid>https://arxiv.org/abs/2504.18428</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual, mathematical reasoning, LLMs, language diversity<br />
Summary:<br /> 
The paper introduces PolyMath, a multilingual mathematical reasoning benchmark with 18 languages and 4 difficulty levels. It evaluates advanced LLMs and finds that they struggle with the benchmark, showing low accuracy. The benchmark highlights challenges in multilingual reasoning: performance variation across languages, low input-output language consistency, and differing thinking lengths by language. Controlling the output language in instructions can impact reasoning performance, especially for low-resource languages. This research demonstrates the need for improvement in multilingual capabilities of LLMs. <br /><br />Summary: <div>
arXiv:2504.18428v1 Announce Type: new 
Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-Slow Thinking for Large Vision-Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.18458</link>
<guid>https://arxiv.org/abs/2504.18458</guid>
<content:encoded><![CDATA[
<div> Keywords: large vision-language models, FAST framework, fast-slow thinking, adaptive reasoning, state-of-the-art accuracy

Summary:
The study introduces the FAST framework to address the overthinking issue in large vision-language models by dynamically adapting reasoning depth. By exploring the impact of response length and data distribution on performance, the feasibility of fast-slow thinking in LVLMs is established. The FAST-GRPO model incorporates model-based metrics, adaptive thinking reward mechanism, and difficulty-aware KL regularization to optimize reasoning. Experimental results on seven reasoning benchmarks show that FAST outperforms base models with a relative improvement of over 10% in accuracy and reduces token usage by 32.7-67.3% compared to previous slow-thinking approaches. This balance between reasoning length and accuracy demonstrates the effectiveness of the FAST framework in enhancing the performance of LVLMs. 

<br /><br />Summary: The study introduces the FAST framework to address the overthinking issue in large vision-language models, establishing the feasibility of fast-slow thinking. The FAST-GRPO model optimizes reasoning depth using model-based metrics and adaptive mechanisms, achieving state-of-the-art accuracy with reduced token usage. <div>
arXiv:2504.18458v1 Announce Type: new 
Abstract: Recent advances in large vision-language models (LVLMs) have revealed an \textit{overthinking} phenomenon, where models generate verbose reasoning across all tasks regardless of questions. To address this issue, we present \textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. We develop FAST-GRPO with three components: model-based metrics for question characterization, an adaptive thinking reward mechanism, and difficulty-aware KL regularization. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10\% relative improvement compared to the base model, while reducing token usage by 32.7-67.3\% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions</title>
<link>https://arxiv.org/abs/2504.18474</link>
<guid>https://arxiv.org/abs/2504.18474</guid>
<content:encoded><![CDATA[
<div> Slot Schema Induction, Task-Oriented Dialogue, Language Model, Evaluation Metrics, Dialogue Understanding<br />
Summary:<br />
This paper introduces a novel approach for Slot Schema Induction (SSI) in task-oriented dialogue systems by framing it as a text generation task. They develop a Language Model-based simulation method to automatically generate high-quality data for training SSI models. The authors address challenges in SSI evaluation, such as data leakage and metric misalignment, by generating new evaluation data with human input and refining evaluation metrics. These contributions advance the state-of-the-art in dialogue understanding and system development, laying a solid foundation for future research in SSI and improving the overall quality and efficiency of task-oriented dialogue systems. <div>
arXiv:2504.18474v1 Announce Type: new 
Abstract: In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a language model incrementally constructs and refines a slot schema over a stream of dialogue data. To develop this approach, we present a fully automatic LLM-based TOD simulation method that creates data with high-quality state labels for novel task domains. Furthermore, we identify issues in SSI evaluation due to data leakage and poor metric alignment with human judgment. We resolve these by creating new evaluation data using our simulation method with human guidance and correction, as well as designing improved evaluation metrics. These contributions establish a foundation for future SSI research and advance the SoTA in dialogue understanding and system development.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues</title>
<link>https://arxiv.org/abs/2504.18483</link>
<guid>https://arxiv.org/abs/2504.18483</guid>
<content:encoded><![CDATA[
<div> Keywords: co-constructive explanation dialogues, large language models, understanding, engagement, user study <br />
Summary: <br />
The research focuses on the use of large language models (LLMs) as explainers in co-constructive explanation dialogues. In a user study, interactions between explainees and LLMs were examined, with some LLMs instructed to explain topics co-constructively. The results showed that LLMs exhibited co-constructive behaviors such as asking verification questions that enhanced engagement and improved explainees' understanding of the topic. However, the ability of LLMs to effectively monitor understanding and adjust explanations accordingly was found to be limited. The study highlights the potential of LLMs in fostering engagement and enhancing understanding in explanation dialogues, but also points out areas where improvement is needed, particularly in dynamically adapting explanations to the explainees' needs. <br /> <div>
arXiv:2504.18483v1 Announce Type: new 
Abstract: The ability to generate explanations that are understood by explainees is the quintessence of explainable artificial intelligence. Since understanding depends on the explainee's background and needs, recent research has focused on co-constructive explanation dialogues, where the explainer continuously monitors the explainee's understanding and adapts explanations dynamically. We investigate the ability of large language models (LLMs) to engage as explainers in co-constructive explanation dialogues. In particular, we present a user study in which explainees interact with LLMs, of which some have been instructed to explain a predefined topic co-constructively. We evaluate the explainees' understanding before and after the dialogue, as well as their perception of the LLMs' co-constructive behavior. Our results indicate that current LLMs show some co-constructive behaviors, such as asking verification questions, that foster the explainees' engagement and can improve understanding of a topic. However, their ability to effectively monitor the current understanding and scaffold the explanations accordingly remains limited.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation</title>
<link>https://arxiv.org/abs/2504.18535</link>
<guid>https://arxiv.org/abs/2504.18535</guid>
<content:encoded><![CDATA[
<div> Framework, Language model, Probabilistic reasoning, Control generation, Adaptable

Summary:
TRACE introduces a framework for controllable generation in large language models (LMs). By distilling an HMM from an LM and pairing it with a classifier, it efficiently computes Expected Attribute Probability (EAP) for future sequences. This allows for reweighing next-token probabilities in the LM to align with desired attributes. The approach achieves state-of-the-art results in detoxification with minimal overhead and can adapt to personalized attributes quickly. TRACE's lightweight control mechanism enables it to be flexible and scalable, seamlessly extending to composite attributes. This novel framework represents a significant advancement in enabling LMs to generate outputs that align with human values and desired attributes. 

<br /><br />Summary: <div>
arXiv:2504.18535v1 Announce Type: new 
Abstract: As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM's predicted futures. This EAP is then used to reweigh the LM's next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
<div> benchmark, cultural diversity, multi-linguistics, domain, video comprehension

Summary:
Existing video evaluation benchmarks have primarily focused on English language and Western cultural contexts, limiting the assessment of multimodal AI systems' understanding and reasoning abilities. VideoVista-CulturalLingo is introduced as a novel benchmark that addresses these limitations by incorporating cultural diversity from China, North America, and Europe, presenting questions in both Chinese and English, and featuring videos from a wide variety of domains. Experiment results reveal that current large models exhibit better performance on Western-centric questions compared to Chinese-centric ones, particularly in Chinese history-related queries. These models also struggle with temporal understanding and achieving high scores in Event Localization tasks. Furthermore, mainstream models excel in general scientific questions but show weakness in answering mathematical queries. Overall, VideoVista-CulturalLingo serves as a comprehensive and inclusive benchmark for evaluating video comprehension capabilities of AI systems. 

<br /><br />Summary: <div>
arXiv:2504.17821v1 Announce Type: cross 
Abstract: Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval</title>
<link>https://arxiv.org/abs/2504.17884</link>
<guid>https://arxiv.org/abs/2504.17884</guid>
<content:encoded><![CDATA[
<div> Keywords: corpus poisoning, information retrieval, adversarial attacks, embedding space, unsupervised

Summary:
This paper introduces a novel approach to corpus poisoning attacks in dense information retrieval. By operating directly in the embedding space, the proposed optimization method maintains the geometric distance between original and adversarial documents while maximizing token-level dissimilarity. Unlike previous work, this method does not require prior knowledge about query distribution, making it more challenging and effective. Experimental results on various datasets show that the proposed method can generate successful adversarial examples quickly, outperforming gradient-based approaches in speed and naturalness. The generated text has low perplexity, making it harder to detect. The study focuses on both top-1 attacks and corpus poisoning attacks, considering white-box and black-box settings. In conclusion, the proposed adversarial corpus attack is fast, effective, and robust against detection methods. 

<br /><br />Summary: <div>
arXiv:2504.17884v1 Announce Type: cross 
Abstract: This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current literature. First, attacks that perform adversarial gradient-based word substitution search do so in the discrete lexical space, while retrieval itself happens in the continuous embedding space. We thus propose an optimization method that operates in the embedding space directly. Specifically, we train a perturbation model with the objective of maintaining the geometric distance between the original and adversarial document embeddings, while also maximizing the token-level dissimilarity between the original and adversarial documents. Second, it is common for related work to have a strong assumption that the adversary has prior knowledge about the queries. In this paper, we focus on a more challenging variant of the problem where the adversary assumes no prior knowledge about the query distribution (hence, unsupervised). Our core contribution is an adversarial corpus attack that is fast and effective. We present comprehensive experimental results on both in- and out-of-domain datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning attack. We consider attacks under both a white-box and a black-box setting. Notably, our method can generate successful adversarial examples in under two minutes per target document; four times faster compared to the fastest gradient-based word substitution methods in the literature with the same hardware. Furthermore, our adversarial generation method generates text that is more likely to occur under the distribution of natural text (low perplexity), and is therefore more difficult to detect.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Sequence Compression for Efficient Multimodal Computing</title>
<link>https://arxiv.org/abs/2504.17892</link>
<guid>https://arxiv.org/abs/2504.17892</guid>
<content:encoded><![CDATA[
<div> vision language models, visual token selection, cluster-level token aggregation, vision encoder redundancy, multimodal systems <br />
<br />Summary: 
This article focuses on improving the efficiency of visual language models by addressing redundancy and inefficiencies in current vision encoders. The research explores various approaches to visual token selection and merging, and finds that simple cluster-level token aggregation outperforms previous methods. The study highlights the redundancy in existing vision encoders and identifies trends in visual token selection through cross-modal attention visualizations. By developing more effective methods for encoding and processing high-dimensional data, the research aims to create more scalable and sustainable multimodal systems. This work represents a significant step towards enhancing cross-modal reasoning in Large Multimodal Models and offers insights into improving the performance of visual language models. <div>
arXiv:2504.17892v1 Announce Type: cross 
Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven advancements in cross-modal reasoning but at significant computational costs. In this work, we focus on visual language models. We highlight the redundancy and inefficiency in current vision encoders, and seek to construct an adaptive compression method for multimodal data. In this work, we characterize a panoply of visual token selection and merging approaches through both benchmarking and qualitative analysis. In particular, we demonstrate that simple cluster-level token aggregation outperforms prior state-of-the-art works in token selection and merging, including merging at the vision encoder level and attention-based approaches. We underline the redundancy in current vision encoders, and shed light on several puzzling trends regarding principles of visual token selection through cross-modal attention visualizations. This work is a first effort towards more effective encoding and processing of high-dimensional data, and paves the way for more scalable and sustainable multimodal systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMU: Context Augmentation for Meme Understanding</title>
<link>https://arxiv.org/abs/2504.17902</link>
<guid>https://arxiv.org/abs/2504.17902</guid>
<content:encoded><![CDATA[
<div> Framework, CAMU, hate detection, memes, vision-language models

Summary:
- The CAMU framework combines vision-language models to analyze social media memes for hate content.
- By fine-tuning CLIP's text encoder, CAMU achieves high accuracy and F1-score on the Hateful Memes dataset.
- Selectively tuning deeper text encoder layers improves hate detection performance significantly.
- CAMU also demonstrates effectiveness in identifying offensive memes on the MultiOFF dataset.
- Robust visual grounding and nuanced text representations are essential for reliable hate and offense detection.<br /><br />Summary: <div>
arXiv:2504.17902v1 Announce Type: cross 
Abstract: Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. We introduce a novel framework, CAMU, which leverages large vision-language models to generate more descriptive captions, a caption-scoring neural network to emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder for an improved multimodal understanding of memes. Experiments on publicly available hateful meme datasets show that simple projection layer fine-tuning yields modest gains, whereas selectively tuning deeper text encoder layers significantly boosts performance on all evaluation metrics. Moreover, our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset, at par with the existing SoTA framework while being much more efficient, offering practical advantages in real-world scenarios that rely on fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the MultiOFF dataset for offensive meme identification, demonstrating its generalisability. Additional analyses on benign confounders reveal that robust visual grounding and nuanced text representations are crucial for reliable hate and offence detection. We will publicly release CAMU along with the resultant models for further research.
  Disclaimer: This paper includes references to potentially disturbing, hateful, or offensive content due to the nature of the task.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</title>
<link>https://arxiv.org/abs/2504.17934</link>
<guid>https://arxiv.org/abs/2504.17934</guid>
<content:encoded><![CDATA[
<div> Large Language Models, GUI automation, privacy risks, security risks, evaluation metrics 
Summary: 
Large Language Models (LLMs) have transformed Graphical User Interface (GUI) automation but raise significant privacy and security risks due to their ability to process sensitive data with limited human oversight. This paper outlines the three key risks of GUI agents, highlighting the differences from traditional GUI automation and general autonomous agents. Existing evaluations mainly focus on performance, neglecting privacy and security assessments. The paper reviews current evaluation metrics for both GUI and general LLM agents and discusses challenges in integrating human evaluators for GUI agent assessments. To address these gaps, a human-centered evaluation framework is proposed, advocating for risk assessments, user awareness through in-context consent, and embedding privacy and security considerations into GUI agent design and evaluation. <br /><br />Summary: <div>
arXiv:2504.17934v1 Announce Type: cross 
Abstract: The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning</title>
<link>https://arxiv.org/abs/2504.17950</link>
<guid>https://arxiv.org/abs/2504.17950</guid>
<content:encoded><![CDATA[
<div> Keywords: Collaboration, LLMs, Embodied reasoning tasks, Natural language communication, Multi-agent collaboration

Summary: 
LLMs are studied for adaptive collaboration in complex embodied reasoning tasks using the MINDcraft platform in Minecraft. The MineCollab benchmark tests embodied and collaborative reasoning dimensions. An experimental study reveals the challenge of efficient natural language communication, with a 15% drop in agent performance when detailed task completion plans are communicated. Current LLM agents are not optimized for multi-agent collaboration in embodied scenarios, indicating the need for methods beyond in-context and imitation learning.<br /><br />Summary: <div>
arXiv:2504.17950v1 Announce Type: cross 
Abstract: Collaboration is ubiquitous and essential in day-to-day life -- from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: https://mindcraft-minecollab.github.io/
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTFinRAG: Interactive Modularized Financial RAG Benchmark</title>
<link>https://arxiv.org/abs/2504.18024</link>
<guid>https://arxiv.org/abs/2504.18024</guid>
<content:encoded><![CDATA[
<div> architecture, evaluation, financial, language models, SMARTFinRAG

Summary:
SMARTFinRAG introduces a modular architecture for evaluating specialized RAG systems in the financial sector, addressing key gaps. It allows components to be interchanged dynamically, employs a document-centric evaluation approach, and features an intuitive interface. Evaluation results show variations in retrieval efficacy and response quality across configurations. The open-source platform supports transparent, reproducible research and helps financial institutions tackle deployment challenges when implementing RAG systems. <div>
arXiv:2504.18024v1 Announce Type: cross 
Abstract: Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture</title>
<link>https://arxiv.org/abs/2504.18099</link>
<guid>https://arxiv.org/abs/2504.18099</guid>
<content:encoded><![CDATA[
<div> Keywords: speech production, tongue articulatory features, lip articulatory features, BiLSTM architecture, Convolutional Neural Network

Summary: 
The paper presents a novel approach for predicting tongue and lip articulatory features in speech production using a stacked BiLSTM architecture combined with a one-dimensional CNN. The model is trained on datasets with variations in geographical origin, linguistic characteristics, phonetic diversity, and recording equipment. Performance evaluation was conducted in Speaker Dependent, Speaker Independent, corpus dependent, and cross corpus modes. The fixed weights approach outperformed adaptive weights initialization in a minimal number of training epochs. These findings contribute to the development of efficient models for predicting articulatory features, offering potential advancements in speech production research and applications.
<br /><br />Summary: <div>
arXiv:2504.18099v1 Announce Type: cross 
Abstract: Speech production is a complex sequential process which involve the coordination of various articulatory features. Among them tongue being a highly versatile active articulator responsible for shaping airflow to produce targeted speech sounds that are intellectual, clear, and distinct. This paper presents a novel approach for predicting tongue and lip articulatory features involved in a given speech acoustics using a stacked Bidirectional Long Short-Term Memory (BiLSTM) architecture, combined with a one-dimensional Convolutional Neural Network (CNN) for post-processing with fixed weights initialization. The proposed network is trained with two datasets consisting of simultaneously recorded speech and Electromagnetic Articulography (EMA) datasets, each introducing variations in terms of geographical origin, linguistic characteristics, phonetic diversity, and recording equipment. The performance of the model is assessed in Speaker Dependent (SD), Speaker Independent (SI), corpus dependent (CD) and cross corpus (CC) modes. Experimental results indicate that the proposed model with fixed weights approach outperformed the adaptive weights initialization with in relatively minimal number of training epochs. These findings contribute to the development of robust and efficient models for articulatory feature prediction, paving the way for advancements in speech production research and applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections</title>
<link>https://arxiv.org/abs/2504.18333</link>
<guid>https://arxiv.org/abs/2504.18333</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, prompt injection attacks, framework, text quality, code correctness.

Summary:
LLM judge systems used for evaluating text quality, code correctness, and argument strength are susceptible to prompt injection attacks. This study introduces a framework to distinguish between content author attacks and system prompt attacks. Five different models – Gemma 3.27B, Gemma 3.4B, Llama 3.23B, GPT 4, and Claude 3 Opus – were evaluated on four tasks with various defenses involving fifty prompts per condition. The attacks demonstrated success rates of up to 73.8%, with smaller models being more vulnerable. Transferability ranged from 50.5% to 62.6%. These results contradict previous research findings. The study recommends the use of multi-model committees and comparative scoring strategies. All code and datasets used in the study have been released for further analysis and experimentation. 

Summary:<br /><br />Keywords: LLM, prompt injection attacks, framework, text quality, code correctness, multi-model committees. The study examines the vulnerability of LLM judge systems to prompt injection attacks and proposes a framework to address these issues. Evaluation of five models on various tasks reveals differing levels of susceptibility, with smaller models being more vulnerable. The study recommends the use of multi-model committees and comparative scoring for improved defense against prompt injection attacks. Availability of code and datasets for further research and analysis is highlighted. <div>
arXiv:2504.18333v1 Announce Type: cross 
Abstract: LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi-Audio Technical Report</title>
<link>https://arxiv.org/abs/2504.18425</link>
<guid>https://arxiv.org/abs/2504.18425</guid>
<content:encoded><![CDATA[
<div> tokenizer, LLM-based architecture, pre-training dataset, audio benchmarks, MoonshotAI-Kimi-Audio repository

Summary:
The article introduces Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. It details the practices involved in building Kimi-Audio, including utilizing a 12.5Hz audio tokenizer, designing a novel LLM-based architecture with continuous features as input and discrete tokens as output, and developing a chunk-wise streaming detokenizer. The model is pre-trained on a dataset consisting of over 13 million hours of audio data and then fine-tuned to support various audio-related tasks. Kimi-Audio achieves state-of-the-art performance on audio benchmarks such as speech recognition, audio understanding, audio question answering, and speech conversation. The codes, model checkpoints, and evaluation toolkits are released on the MoonshotAI-Kimi-Audio repository. <div>
arXiv:2504.18425v1 Announce Type: cross 
Abstract: We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRobELM: Plausibility Ranking Evaluation for Language Models</title>
<link>https://arxiv.org/abs/2404.03818</link>
<guid>https://arxiv.org/abs/2404.03818</guid>
<content:encoded><![CDATA[
<div> Benchmark, Language models, Plausibility, World knowledge, Evaluation <br />
Summary: <br />
PRobELM is a new benchmark designed to assess language models' ability to distinguish between plausible and less plausible scenarios by leveraging world knowledge. While existing benchmarks focus on factual accuracy or plausible scenarios without incorporating world knowledge, PRobELM bridges this gap by evaluating the models' capability to prioritize plausible scenarios that align with world knowledge. The benchmark is constructed from a dataset curated from Wikidata edit histories and includes multiple prompting types such as statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures demonstrate that factual accuracy does not directly correlate with plausibility performance, and up-to-date training data enhances plausibility assessment across different model architectures. This benchmark can be valuable for applications such as literature-based discovery where identifying likely but unknown information is crucial. <br /> <div>
arXiv:2404.03818v4 Announce Type: replace 
Abstract: This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</title>
<link>https://arxiv.org/abs/2405.19325</link>
<guid>https://arxiv.org/abs/2405.19325</guid>
<content:encoded><![CDATA[
<div> nearest neighbor speculative decoding, semi-parametric language modeling, generation quality, attribution rate, inference speed
Summary:
Nearest Neighbor Speculative Decoding (NEST) is a novel semi-parametric language modeling approach that enhances generation quality and attribution rate of large language models (LLMs). By incorporating real-world text spans of arbitrary length into the LM generations, NEST outperforms conventional kNN-LM methods and competes with in-context retrieval augmentation. NEST employs token-level retrieval and a speculative decoding procedure to generate fluent texts with attributions to their sources. It significantly improves generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B dataset. The code for NEST will be available at https://github.com/facebookresearch/NEST/tree/main. 
<br /><br />Summary: <div>
arXiv:2405.19325v3 Announce Type: replace 
Abstract: Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B. Code will be released at https://github.com/facebookresearch/NEST/tree/main.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction</title>
<link>https://arxiv.org/abs/2406.10432</link>
<guid>https://arxiv.org/abs/2406.10432</guid>
<content:encoded><![CDATA[
<div> retrieve, in-context learning, relation extraction, semantic structure, state-of-the-art

Summary:
The article introduces an AMR-enhanced retrieval-based in-context learning (ICL) method for relation extraction (RE) that prioritizes structural similarity over language similarity. By retrieving examples based on semantic structure similarity, the proposed model outperforms existing methods in unsupervised settings across four standard English RE datasets. In supervised settings, the model achieves state-of-the-art results on three datasets and competitive results on the fourth. This approach highlights the importance of considering semantic structure in learning entity relationships and demonstrates the effectiveness of leveraging such information for improving RE performance. <div>
arXiv:2406.10432v3 Announce Type: replace 
Abstract: Existing in-context learning (ICL) methods for relation extraction (RE) often prioritize language similarity over structural similarity, which can lead to overlooking entity relationships. To address this, we propose an AMR-enhanced retrieval-based ICL method for RE. Our model retrieves in-context examples based on semantic structure similarity between task inputs and training samples. Evaluations on four standard English RE datasets show that our model outperforms baselines in the unsupervised setting across all datasets. In the supervised setting, it achieves state-of-the-art results on three datasets and competitive results on the fourth.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Large Language Models and Curse of Multilinguality</title>
<link>https://arxiv.org/abs/2406.10602</link>
<guid>https://arxiv.org/abs/2406.10602</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual Large Language Models, NLP, architectures, tokenization, curse of multilinguality

Summary:
Multilingual Large Language Models (LLMs) are widely used in Natural Language Processing (NLP) due to their proficiency in multiple languages and effectiveness in various tasks. This paper provides an overview of the technical aspects of these models, including architectures, objective functions, pre-training data sources, and tokenization methods. It discusses different model types such as encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART), highlighting their unique features. One significant limitation of multilingual LLMs is the curse of multilinguality, and current efforts to address this challenge are explored.Overall, this paper serves as a comprehensive guide to understand the landscape of multilingual LLMs and their potential in NLP research and applications.<br /><br />Summary: <div>
arXiv:2406.10602v2 Announce Type: replace 
Abstract: Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings</title>
<link>https://arxiv.org/abs/2408.16073</link>
<guid>https://arxiv.org/abs/2408.16073</guid>
<content:encoded><![CDATA[
<div> large language models, replication, generalization, marketing, AI-assisted

Summary: 
This report examines the use of large language models (LLMs) to replicate and generalize research on message effects in marketing. LLM-powered personas successfully replicated 76% of original main effects and 68% including interaction effects from 133 experimental findings across 14 papers. Results suggest strong potential for AI-assisted replication in marketing research. The study also found that replication results can vary when tested with different participant samples, media stimuli, and measures, highlighting the importance of considering these factors in research. Implications are discussed for addressing replication and generalizability crises in social science, accelerating theory building in media and marketing psychology, and the practical benefits of rapid message testing for consumer products. Limitations of AI replications, such as handling complex interaction effects and biases in AI models, are also addressed to establish benchmarks for AI metrics in marketing research. <div>
arXiv:2408.16073v2 Announce Type: replace 
Abstract: This report analyzes the potential for large language models (LLMs) to expedite accurate replication and generalization of published research about message effects in marketing. LLM-powered participants (personas) were tested by replicating 133 experimental findings from 14 papers containing 45 recent studies published in the Journal of Marketing. For each study, the measures, stimuli, and sampling specifications were used to generate prompts for LLMs to act as unique personas. The AI personas, 19,447 in total across all of the studies, generated complete datasets and statistical analyses were then compared with the original human study results. The LLM replications successfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential for AI-assisted replication. The overall replication rate including interaction effects was 68% (90 out of 133). Furthermore, a test of how human results generalized to different participant samples, media stimuli, and measures showed that replication results can change when tests go beyond the parameters of the original human studies. Implications are discussed for the replication and generalizability crises in social science, the acceleration of theory building in media and marketing psychology, and the practical advantages of rapid message testing for consumer products. Limitations of AI replications are addressed with respect to complex interaction effects, biases in AI models, and establishing benchmarks for AI metrics in marketing research.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Weak LLM is Secretly a Strong Teacher for Alignment</title>
<link>https://arxiv.org/abs/2409.08813</link>
<guid>https://arxiv.org/abs/2409.08813</guid>
<content:encoded><![CDATA[
<div> alignment, large language models, weak LLM, feedback, human values

Summary:
The study focuses on aligning large language models (LLMs) with human values using a less resource-intensive weak LLM approach. The research demonstrates the efficacy of weak LLMs in providing feedback comparable to human annotations, highlighting a scalable alignment strategy. Through qualitative and quantitative analyses, the study uncovers insights into the quality differences between human and weak LLM feedback, emphasizing the potential for sustainable alignment practices. These findings contribute to addressing the crucial need for ensuring LLMs act in accordance with human intentions, offering a promising middle ground approach for alignment efforts. 

<br /><br />Summary: <div>
arXiv:2409.08813v2 Announce Type: replace 
Abstract: The burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs. This paper explores a promising middle ground, where we employ a weak LLM that is significantly less resource-intensive than top-tier models, yet offers more automation than purely human feedback. We present a systematic study to evaluate and understand weak LLM's ability to generate feedback for alignment. Our empirical findings demonstrate that weak LLMs can provide feedback that rivals or even exceeds that of fully human-annotated data. Our study indicates a minimized impact of model size on feedback efficacy, shedding light on a scalable and sustainable alignment strategy. To deepen our understanding of alignment under weak LLM feedback, we conduct a series of qualitative and quantitative analyses, offering novel insights into the quality discrepancies between human feedback vs. weak LLM feedback.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title>
<link>https://arxiv.org/abs/2409.12059</link>
<guid>https://arxiv.org/abs/2409.12059</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, thinking ability, TaS, cognitive mechanism, data-driven

Summary: 
TaS is a novel model architecture designed to enhance the thinking ability of language models by incorporating cognitive mechanisms from the natural world. This model first considers thoughts before generating responses based on queries, utilizing language heads in a middle layer as the thinking layer. By annotating or generating thought contents from prompt-response samples, the model is trained on thoughts-augmented data to automatically generate reasonable thoughts and improve response quality. Both qualitative examples and quantitative results demonstrate the effectiveness and performance of TaS in enhancing the reasoning capabilities of language models. This approach represents a significant step towards improving the overall understanding and generation capabilities of large language models. The code for TaS is available for further exploration and experimentation. 

Summary: <div>
arXiv:2409.12059v4 Announce Type: replace 
Abstract: Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"</title>
<link>https://arxiv.org/abs/2410.03727</link>
<guid>https://arxiv.org/abs/2410.03727</guid>
<content:encoded><![CDATA[
<div> Benchmark, faithfulness, large language models, retrieval-augmented generation, context  
Summary:  
Faithfulness to context is critical for the deployment of large language models (LLMs) and retrieval-augmented generation (RAG) systems in real-world applications. Despite progress on standard benchmarks, faithfulness hallucination, where models generate responses not aligned with the provided context, remains a significant challenge. The introduction of FaithEval, a comprehensive benchmark, evaluates the faithfulness of LLMs across unanswerable, inconsistent, and counterfactual contexts. The benchmark comprises 4.9K high-quality problems validated through a context construction and validation framework. Results from the study on various models indicate that even state-of-the-art models struggle with remaining faithful to the given context, suggesting that larger models may not necessarily exhibit improved faithfulness.<br /><br />Summary: <div>
arXiv:2410.03727v3 Announce Type: replace 
Abstract: Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness.Project is available at: https://github.com/SalesforceAIResearch/FaithEval.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
<link>https://arxiv.org/abs/2411.07611</link>
<guid>https://arxiv.org/abs/2411.07611</guid>
<content:encoded><![CDATA[
arXiv:2411.07611v2 Announce Type: replace 
Abstract: Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable mutlimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in a same encoding space, enabling it naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data</title>
<link>https://arxiv.org/abs/2412.11704</link>
<guid>https://arxiv.org/abs/2412.11704</guid>
<content:encoded><![CDATA[
arXiv:2412.11704v3 Announce Type: replace 
Abstract: Vocabulary expansion (VE) is the de-facto approach to language adaptation of large language models (LLMs) by adding new tokens and continuing pre-training on target data. While this is effective for base models trained on unlabeled data, it poses challenges for chat models trained to follow instructions through labeled conversation data. Directly adapting the latter with VE on target unlabeled data may result in forgetting chat abilities. While ideal, target chat data is often unavailable or costly to create for low-resource languages, and machine-translated alternatives are not always effective. To address this issue, previous work proposed using a base and chat model from the same family. This method first adapts the base LLM with VE on target unlabeled data and then converts it to a chat model by adding a chat vector (CV) derived from the weight difference between the source base and chat models. We propose ElChat, a new language adaptation method for chat LLMs that adapts a chat model directly on target unlabeled data, without a base model. It elicits chat abilities by injecting information from the source chat model. ElChat offers more robust and competitive target language and safety performance while achieving superior English, chat, and instruction-following abilities compared to CV.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations</title>
<link>https://arxiv.org/abs/2502.01220</link>
<guid>https://arxiv.org/abs/2502.01220</guid>
<content:encoded><![CDATA[
arXiv:2502.01220v3 Announce Type: replace 
Abstract: This paper explores the robustness of language models (LMs) to variations in the temporal context within factual knowledge. It examines whether LMs can correctly associate a temporal context with a past fact valid over a defined period, by asking them to differentiate correct from incorrect contexts. The accuracy of LMs is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context. To this end, a dataset called TimeStress is introduced, enabling the evaluation of 18 diverse LMs. Results reveal that the best LM achieves perfect accuracy for only 6% of the studied facts, with critical errors that humans would not make. This work highlights the limitations of current LMs in temporal representation. We provide all data and code for further research.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12486</link>
<guid>https://arxiv.org/abs/2502.12486</guid>
<content:encoded><![CDATA[
arXiv:2502.12486v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-generated text detection prevents language model collapse</title>
<link>https://arxiv.org/abs/2502.15654</link>
<guid>https://arxiv.org/abs/2502.15654</guid>
<content:encoded><![CDATA[
arXiv:2502.15654v4 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. We release our code at https://github.com/GeorgeDrayson/model_collapse.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRAGE: Legal Retrieval Augmented Generation Evaluation Tool</title>
<link>https://arxiv.org/abs/2504.01840</link>
<guid>https://arxiv.org/abs/2504.01840</guid>
<content:encoded><![CDATA[
arXiv:2504.01840v2 Announce Type: replace 
Abstract: Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at https://github.com/hoorangyee/LRAGE.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Evaluation of Complex Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02810</link>
<guid>https://arxiv.org/abs/2504.02810</guid>
<content:encoded><![CDATA[
arXiv:2504.02810v2 Announce Type: replace 
Abstract: With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Reasoning LLMs Enhance Clinical Document Classification?</title>
<link>https://arxiv.org/abs/2504.08040</link>
<guid>https://arxiv.org/abs/2504.08040</guid>
<content:encoded><![CDATA[
arXiv:2504.08040v2 Announce Type: replace 
Abstract: Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio</title>
<link>https://arxiv.org/abs/1711.08058</link>
<guid>https://arxiv.org/abs/1711.08058</guid>
<content:encoded><![CDATA[
arXiv:1711.08058v2 Announce Type: replace-cross 
Abstract: We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-feature representations, and multiple-instance learning. The cascaded classifiers handle the task's class imbalance and reduce power consumption on computationally-constrained devices via early termination. The KWS system achieves a false negative rate of 6% at an hourly false positive rate of 0.75
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2405.15638</link>
<guid>https://arxiv.org/abs/2405.15638</guid>
<content:encoded><![CDATA[
arXiv:2405.15638v2 Announce Type: replace-cross 
Abstract: Multilingual capability is an essential aspect for large multimodal models, since they are usually deployed across various countries and languages. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 10k samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in six languages. Using M4U, we conduct extensive evaluations of leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results demonstrate that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, struggle to perform reasoning using multilingual information present in both visual and textual context. Specifically, they suffer performance degradation when prompted with cross-lingual multimodal questions. Our code and dataset is public available.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition</title>
<link>https://arxiv.org/abs/2406.02566</link>
<guid>https://arxiv.org/abs/2406.02566</guid>
<content:encoded><![CDATA[
arXiv:2406.02566v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel two-stage active learning (AL) pipeline for automatic speech recognition (ASR), combining unsupervised and supervised AL methods. The first stage utilizes unsupervised AL by using x-vectors clustering for diverse sample selection from unlabeled speech data, thus establishing a robust initial dataset for the subsequent supervised AL. The second stage incorporates a supervised AL strategy, with a batch AL method specifically developed for ASR, aimed at selecting diverse and informative batches of samples. Here, sample diversity is also achieved using x-vectors clustering, while the most informative samples are identified using a Bayesian AL method tailored for ASR with an adaptation of Monte Carlo dropout to approximate Bayesian inference. This approach enables precise uncertainty estimation, thereby enhancing ASR model training with significantly reduced data requirements. Our method has shown superior performance compared to competing methods on homogeneous, heterogeneous, and OOD test sets, demonstrating that strategic sample selection and innovative Bayesian modeling can substantially optimize both labeling effort and data utilization in deep learning-based ASR applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GOFA: A Generative One-For-All Model for Joint Graph Language Modeling</title>
<link>https://arxiv.org/abs/2407.09709</link>
<guid>https://arxiv.org/abs/2407.09709</guid>
<content:encoded><![CDATA[
arXiv:2407.09709v2 Announce Type: replace-cross 
Abstract: Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better -- yet, no existing work can achieve both simultaneously. In this paper, we identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA to solve the problem. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, and structural tasks to obtain the above GFM properties. The pre-trained model is further fine-tuned on downstream tasks to obtain task-solving ability. The fine-tuned model is evaluated on various downstream tasks, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs</title>
<link>https://arxiv.org/abs/2408.06621</link>
<guid>https://arxiv.org/abs/2408.06621</guid>
<content:encoded><![CDATA[
arXiv:2408.06621v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Uncertainty Quantification for Generative AI</title>
<link>https://arxiv.org/abs/2408.08990</link>
<guid>https://arxiv.org/abs/2408.08990</guid>
<content:encoded><![CDATA[
arXiv:2408.08990v2 Announce Type: replace-cross 
Abstract: This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND: Math Informed syNthetic Dialogues for Pretraining LLMs</title>
<link>https://arxiv.org/abs/2410.12881</link>
<guid>https://arxiv.org/abs/2410.12881</guid>
<content:encoded><![CDATA[
arXiv:2410.12881v2 Announce Type: replace-cross 
Abstract: The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification</title>
<link>https://arxiv.org/abs/2411.01841</link>
<guid>https://arxiv.org/abs/2411.01841</guid>
<content:encoded><![CDATA[
arXiv:2411.01841v3 Announce Type: replace-cross 
Abstract: Accurate annotation of educational resources is crucial for effective personalized learning and resource recommendation in online education. However, fine-grained knowledge labels often overlap or share similarities, making it difficult for existing multi-label classification methods to differentiate them. The label distribution imbalance due to sparsity of human annotations further intensifies these challenges. To address these issues, this paper introduces RR2QC, a novel Retrieval Reranking method to multi-label Question Classification by leveraging label semantics and meta-label refinement. First, RR2QC improves the pre-training strategy by utilizing semantic relationships within and across label groups. Second, it introduces a class center learning task to align questions with label semantics during downstream training. Finally, this method decomposes labels into meta-labels and uses a meta-label classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances the understanding and prediction capability of long-tail labels by learning from meta-labels that frequently appear in other labels. Additionally, a mathematical LLM is used to generate solutions for questions, extracting latent information to further refine the model's insights. Experimental results show that RR2QC outperforms existing methods in Precision@K and F1 scores across multiple educational datasets, demonstrating its effectiveness for online education applications. The code and datasets are available at https://github.com/78Erii/RR2QC.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repurposing the scientific literature with vision-language models</title>
<link>https://arxiv.org/abs/2502.19546</link>
<guid>https://arxiv.org/abs/2502.19546</guid>
<content:encoded><![CDATA[
arXiv:2502.19546v2 Announce Type: replace-cross 
Abstract: Leading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty publishing, educational, and clinical tasks. We created NeuroPubs, a multimodal dataset of 23,000 Neurosurgery Publications articles (134M words, 78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready graphical abstracts (70% of 100 abstracts) and board-style questions indistinguishable from human-written ones (54% of 89,587 questions). We used these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded, randomized controlled trial, our model demonstrated non-inferiority to then state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%, p=0.3797). Our pilot study demonstrates how training generative AI models on specialty-specific journal content - without large-scale internet data - results in high-performance academic and clinical tools, enabling domain-tailored AI across diverse fields.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Audio Processing with Large Language Model on Wearable Devices</title>
<link>https://arxiv.org/abs/2504.08907</link>
<guid>https://arxiv.org/abs/2504.08907</guid>
<content:encoded><![CDATA[
arXiv:2504.08907v2 Announce Type: replace-cross 
Abstract: Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\circ$-a substantial improvement compared to the 88.52$^\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity</title>
<link>https://arxiv.org/abs/2504.16956</link>
<guid>https://arxiv.org/abs/2504.16956</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, GeneMamba, single-cell RNA sequencing, state space modeling, computational challenges<br />
Summary:<br />
GeneMamba is introduced as a scalable and efficient foundation model for single-cell transcriptomics, addressing the complexity of high dimensionality, sparsity, and batch effects. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity. Pretrained on nearly 30 million cells with biologically informed objectives, such as pathway-aware contrastive loss and rank-based gene encoding, GeneMamba demonstrates strong performance across tasks such as multi-batch integration, cell type annotation, and gene-gene correlation. It offers substantial computational gains over transformer baselines while providing interpretability and robustness, positioning it as a practical and powerful alternative for large-scale single-cell data analysis. <br /> <div>
arXiv:2504.16956v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenization Matters: Improving Zero-Shot NER for Indic Languages</title>
<link>https://arxiv.org/abs/2504.16977</link>
<guid>https://arxiv.org/abs/2504.16977</guid>
<content:encoded><![CDATA[
<div> tokenization, Natural Language Processing, Named Entity Recognition, Indic languages, SentencePiece

Summary: 
- The study compares tokenization strategies BPE, SentencePiece, and Character Level for NER in low resource Indic languages.
- SentencePiece outperforms BPE in NER tasks, especially in zero-shot cross-lingual settings, as it maintains entity consistency better.
- BPE offers compact tokenization but fails in generalization, misclassifying entity labels in unseen languages.
- SentencePiece better preserves linguistic structure and is effective for extremely low resource and morphologically rich Indic languages.
- SentencePiece also shows high generalization across scripts, such as Sindhi written in Arabic, making it the more effective tokenization strategy for NER in multilingual and low resource Indic NLP applications.

<br /><br />Summary: <div>
arXiv:2504.16977v1 Announce Type: new 
Abstract: Tokenization is a critical component of Natural Language Processing (NLP), especially for low resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks in low resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties tokenization efficiency, out of vocabulary (OOV) rates, and morphological preservation as well as extrinsic downstream performance, including fine tuning and zero shot cross lingual transfer.
  Our experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low resource Indic Languages, particularly in zero shot cross lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low resource and morphologically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2504.17025</link>
<guid>https://arxiv.org/abs/2504.17025</guid>
<content:encoded><![CDATA[
<div> adaptation, Large Language Models, Italian language, Semantic Alignment, vocabulary substitution

Summary:
- The study focuses on optimizing English Large Language Models (LLMs) for the Italian language.
- Various vocabulary adaptation techniques are compared, with Semantic Alignment Vocabulary Adaptation (SAVA) proposed as a novel method that utilizes neural mapping for vocabulary substitution.
- SAVA proves effective in reducing token fertility and improving model efficiency for Italian language tasks.
- Two LLMs, Mistral-7b-v0.1 and Llama-3.1-8B, are adapted using SAVA, resulting in decreased token fertility and parameter reduction.
- The adapted models show competitive performance on a range of downstream tasks with minimal additional training in the target language.

<br /><br />Summary: <div>
arXiv:2504.17025v1 Announce Type: new 
Abstract: The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17052</link>
<guid>https://arxiv.org/abs/2504.17052</guid>
<content:encoded><![CDATA[
<div> framework, belief depth, argumentative consistency, uncertainty quantification, LLMs <br />
Summary: This study investigates the political beliefs of Large Language Models (LLMs) by assessing their argumentative consistency and uncertainty quantification. It challenges LLMs with both supportive and opposing arguments on 19 economic policies to evaluate their belief stability. The study finds that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Left-leaning models show up to 95% consistency and right-leaning models up to 89% consistency in their responses, indicating a high level of semantic entropy in distinguishing surface-level alignment from genuine belief. The results suggest that LLMs may not maintain stable, human-like political ideologies and emphasize the importance of topic-specific reliability assessments for real-world applications. <br /> <div>
arXiv:2504.17052v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly shaping political discourse, yet their responses often display inconsistency when subjected to scrutiny. While prior research has primarily categorized LLM outputs as left- or right-leaning to assess their political stances, a critical question remains: Do these responses reflect genuine internal beliefs or merely surface-level alignment with training data? To address this, we propose a novel framework for evaluating belief depth by analyzing (1) argumentative consistency and (2) uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from the Political Compass Test, challenging their belief stability with both supportive and opposing arguments. Our analysis reveals that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Notably, up to 95% of left-leaning models' responses and 89% of right-leaning models' responses remain consistent under the challenge, enabling semantic entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing between surface-level alignment from genuine belief. These findings call into question the assumption that LLMs maintain stable, human-like political ideologies, emphasizing the importance of conducting topic-specific reliability assessments for real-world applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agree to Disagree? A Meta-Evaluation of LLM Misgendering</title>
<link>https://arxiv.org/abs/2504.17075</link>
<guid>https://arxiv.org/abs/2504.17075</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM misgendering, evaluation methods, convergent validity, dataset transformation, human evaluation

Summary: 
The study explores various methods for measuring LLM misgendering and examines the convergent validity of these evaluation techniques. By analyzing three existing datasets and evaluating models from different families, the researchers find that probability-based and generation-based evaluation methods can disagree with each other on a significant portion of instances. Human evaluation of LLM generations highlights the complexity of misgendering behavior, extending beyond pronouns which are not always captured by automatic evaluations. The findings suggest the need for improved evaluation methods for LLM misgendering and call into question the assumption of agreement between different evaluation approaches. Recommendations are provided for future evaluations in this area, emphasizing the importance of considering the diverse aspects of misgendering behavior in LLM assessment.<br /><br />Summary: <div>
arXiv:2504.17075v1 Announce Type: new 
Abstract: Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study</title>
<link>https://arxiv.org/abs/2504.17083</link>
<guid>https://arxiv.org/abs/2504.17083</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, language style, user preferences, individual traits, user studies

Summary: 
LLM interactions are influenced by language style, not just information accuracy. Inaccurate responses can be preferred if they appear authoritative or well-articulated. This suggests language style plays a significant role in user preferences. However, the impact of language style varies among different user populations and is influenced by individual traits. The study's preliminary findings indicate a need for caution due to sample limitations, requiring more diverse demographics and larger samples. Future research aims to explore the joint effects of language style, individual traits, and preferences, as well as investigate any causal relationships between these variables. Overall, understanding how language style affects user interactions with LLMs can enhance user experience but also raise concerns about misinformation and hallucinations. <br /><br />Summary: <div>
arXiv:2504.17083v1 Announce Type: new 
Abstract: What makes an interaction with the LLM more preferable for the user? While it is intuitive to assume that information accuracy in the LLM's responses would be one of the influential variables, recent studies have found that inaccurate LLM's responses could still be preferable when they are perceived to be more authoritative, certain, well-articulated, or simply verbose. These variables interestingly fall under the broader category of language style, implying that the style in the LLM's responses might meaningfully influence users' preferences. This hypothesized dynamic could have double-edged consequences: enhancing the overall user experience while simultaneously increasing their susceptibility to risks such as LLM's misinformation or hallucinations. In this short paper, we present our preliminary studies in exploring this subject. Through a series of exploratory and experimental user studies, we found that LLM's language style does indeed influence user's preferences, but how and which language styles influence the preference varied across different user populations, and more interestingly, moderated by the user's very own individual traits. As a preliminary work, the findings in our studies should be interpreted with caution, particularly given the limitations in our samples, which still need wider demographic diversity and larger sample sizes. Our future directions will first aim to address these limitations, which would enable a more comprehensive joint effect analysis between the language style, individual traits, and preferences, and further investigate the potential causal relationship between and beyond these variables.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.17091</link>
<guid>https://arxiv.org/abs/2504.17091</guid>
<content:encoded><![CDATA[
<div> Framework, explainability, responsible AI, cognitive engagement, ethical transparency

Summary:
The Interactive Chain-of-Thought (CoT) Framework was introduced to enhance human-centered explainability and responsible AI usage by promoting deep, reflective thinking. The framework allows users to inspect, modify, and re-execute reasoning blocks, encouraging active cognitive engagement. It also incorporates a lightweight edit-adaptation mechanism to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, bias checkpoint functionality, and privacy-preserving safeguards. The design principles and architecture outlined in this work aim to foster critical engagement, responsible interaction, and inclusive adaptation in AI systems addressing complex societal challenges. <div>
arXiv:2504.17091v1 Announce Type: new 
Abstract: Due to the proliferation of short-form content and the rapid adoption of AI, opportunities for deep, reflective thinking have significantly diminished, undermining users' critical thinking and reducing engagement with the reasoning behind AI-generated outputs. To address this issue, we propose an Interactive Chain-of-Thought (CoT) Framework that enhances human-centered explainability and responsible AI usage by making the model's inference process transparent, modular, and user-editable. The framework decomposes reasoning into clearly defined blocks that users can inspect, modify, and re-execute, encouraging active cognitive engagement rather than passive consumption. It further integrates a lightweight edit-adaptation mechanism inspired by preference learning, allowing the system to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, built-in bias checkpoint functionality, and privacy-preserving safeguards. This work outlines the design principles and architecture necessary to promote critical engagement, responsible interaction, and inclusive adaptation in AI systems aimed at addressing complex societal challenges.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of Small Language Models in Healthcare: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.17119</link>
<guid>https://arxiv.org/abs/2504.17119</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, healthcare applications, data privacy, resource-constrained environments, model optimization<br />
Summary:<br />
Small language models (SLMs) offer a scalable and clinically viable solution for healthcare applications, addressing concerns around data privacy and resource limitations. A comprehensive survey categorizes SLMs for healthcare professionals, highlighting their contributions across NLP tasks and stakeholder roles. The timeline of SLM advancements showcases their potential in transforming healthcare informatics. The taxonomic framework guides the development of SLMs, emphasizing model optimization and sustainability through compression techniques. By presenting experimental results across various healthcare NLP tasks, this survey aims to equip professionals with curated resources for future research and development in the field. Access the updated repository on Github for a comprehensive compilation of SLM advancements in healthcare.<br /> 
Summary: <div>
arXiv:2504.17119v1 Announce Type: new 
Abstract: Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control</title>
<link>https://arxiv.org/abs/2504.17130</link>
<guid>https://arxiv.org/abs/2504.17130</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, censorship, representation engineering, safety-tuned models, thought suppression

Summary: 
Large language models (LLMs) have introduced new ways to access information but may censor harmful requests. A study using representation engineering techniques focuses on open-weights safety-tuned models to understand censorship. A method for detecting and controlling censorship through a refusal-compliance vector is presented. Analysis of reasoning LLMs reveals an additional dimension of censorship known as "thought suppression" in models distilled from DeepSeek-R1. A similar approach can be used to find a vector that suppresses the model's reasoning process, allowing for the removal of censorship by applying the negative multiples of this vector. <div>
arXiv:2504.17130v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation</title>
<link>https://arxiv.org/abs/2504.17137</link>
<guid>https://arxiv.org/abs/2504.17137</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, Large Language Models, MIRAGE, Question Answering dataset, evaluation metrics <br />
Summary: <br />
The paper introduces MIRAGE, a new Question Answering dataset designed for evaluating Retrieval-Augmented Generation (RAG) systems. The dataset includes 7,560 instances mapped to a retrieval pool of 37,800 entries, allowing for precise evaluation of retrieval and generation tasks. Novel evaluation metrics like noise vulnerability, context acceptability, and context misinterpretation are introduced to measure RAG adaptability. The paper conducts comprehensive experiments with various retriever-LLM configurations to gain insights into optimal model alignment and dynamics within RAG systems. The dataset and evaluation code are made publicly available for seamless integration and customization in research contexts. <div>
arXiv:2504.17137v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, Large Language Models, code implementation, PaperCoder, benchmark 

Summary: 
The article introduced a new framework called PaperCoder that aims to transform machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, analysis, and generation, with specialized agents collaborating across the pipeline. It effectively generates high-quality, faithful implementations based on both model-based and human evaluations, including evaluations from original paper authors. The framework outperforms strong baselines in the PaperBench benchmark, showcasing its strengths in code generation from research papers. PaperCoder's innovative approach addresses the lack of code implementations accompanying machine learning research, making it easier for researchers to reproduce results and build upon prior work. Overall, PaperCoder shows promise in bridging the gap between research papers and practical code implementations, potentially accelerating progress in the machine learning field. 

<br /><br />Summary: <div>
arXiv:2504.17192v1 Announce Type: new 
Abstract: Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation</title>
<link>https://arxiv.org/abs/2504.17200</link>
<guid>https://arxiv.org/abs/2504.17200</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, natural hazards, extreme weather events, retrieval-augmented generation, WildfireGPT

Summary: 
The article introduces the concept of using Large Language Models (LLMs) for decision-making in the context of natural hazards, particularly focusing on extreme weather events such as wildfires. The proposed system, WildfireGPT, utilizes a retrieval-augmented generation (RAG) framework to provide tailored risk insights to various stakeholder groups. By integrating data from natural hazard projections, observational datasets, and scientific literature, WildfireGPT ensures the accuracy and contextual relevance of the information it provides. Through a user-centered, multi-agent design, the system outperforms existing LLM-based solutions for decision support, as demonstrated by evaluations across ten expert-led case studies. This specialized system showcases the potential of LLMs to address pressing societal challenges related to natural hazards and extreme weather events. 

<br /><br />Summary: <div>
arXiv:2504.17200v1 Announce Type: new 
Abstract: Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?</title>
<link>https://arxiv.org/abs/2504.17220</link>
<guid>https://arxiv.org/abs/2504.17220</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, bundle generation, large language models, efficiency, performance

Summary:
This study explores the use of knowledge distillation (KD) techniques to improve the efficiency and performance of Large Language Models (LLMs) in bundle generation tasks. The research focuses on three key questions: the impact of KD format on performance, the influence of distilled knowledge quantity, and the effect of different knowledge utilization methods. A comprehensive KD framework is proposed, which extracts knowledge progressively, varies the quantity through different strategies, and employs various LLM adaptation techniques to enhance efficiency. The experiments demonstrate that KD offers a promising solution for reducing computational costs while maintaining performance quality in LLM-based bundle generation tasks. <div>
arXiv:2504.17220v1 Announce Type: new 
Abstract: LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues</title>
<link>https://arxiv.org/abs/2504.17238</link>
<guid>https://arxiv.org/abs/2504.17238</guid>
<content:encoded><![CDATA[
<div> framework, Cognitive Restructuring, CRDial, dialogues, Crispers <br />
Summary: <br />
The article introduces a new framework, CRDial, for Cognitive Restructuring (CR) in psychotherapy to address clinician shortage and reduce stigma. CRDial creates multi-turn dialogues with identification and restructuring stages for negative thoughts and includes sentence-level supportive conversation strategies. It also incorporates a multi-channel loop mechanism for iterative CR. The framework utilizes the Crisp dataset to train Crispers, conversational Language Models (LLMs) specifically designed for CR at 7B and 14B scales. Human studies demonstrate the effectiveness of Crispers in pointwise, pairwise, and intervention evaluations. This innovative approach aims to improve the psychotherapeutic process for individuals with mental health challenges by providing more personalized and effective CR interventions through interactive dialogues. <br /> <div>
arXiv:2504.17238v1 Announce Type: new 
Abstract: Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo</title>
<link>https://arxiv.org/abs/2504.17252</link>
<guid>https://arxiv.org/abs/2504.17252</guid>
<content:encoded><![CDATA[
<div> develop, Neural Machine Translation, Transformer, transfer learning, English-to-Igbo translation <br />
Summary:<br />
The study focuses on developing Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation, a low-resource African language spoken by over 40 million people in Nigeria and West Africa. The models are trained on curated datasets from Bible corpora, local news, Wikipedia articles, and Common Crawl, verified by native language experts. Recurrent Neural Network (RNN) architectures like LSTM and GRU, enhanced with attention mechanisms, are used to improve translation accuracy. Transfer learning with MarianNMT pre-trained models within the SimpleTransformers framework further boosts performance, showing a gain of +4.83 BLEU points and reaching an estimated accuracy of 70%. The combination of RNNs and transfer learning effectively addresses the performance gap in low-resource language translation tasks. <br />Summary: <div>
arXiv:2504.17252v1 Announce Type: new 
Abstract: In this study, we develop Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation - a low-resource African language spoken by over 40 million people across Nigeria and West Africa. Our models are trained on a curated and benchmarked dataset compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl, all verified by native language experts. We leverage Recurrent Neural Network (RNN) architectures, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), enhanced with attention mechanisms to improve translation accuracy. To further enhance performance, we apply transfer learning using MarianNMT pre-trained models within the SimpleTransformers framework. Our RNN-based system achieves competitive results, closely matching existing English-Igbo benchmarks. With transfer learning, we observe a performance gain of +4.83 BLEU points, reaching an estimated translation accuracy of 70%. These findings highlight the effectiveness of combining RNNs with transfer learning to address the performance gap in low-resource language translation tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning</title>
<link>https://arxiv.org/abs/2504.17264</link>
<guid>https://arxiv.org/abs/2504.17264</guid>
<content:encoded><![CDATA[
<div> Domain Adaptation, Natural Language Processing, Legal Text, JurisCTC, Legal Judgment Prediction 

Summary:
JurisCTC is proposed for Unsupervised Domain Adaptation in Natural Language Processing, specifically for Legal Judgment Prediction tasks. It addresses challenges of lengthy legal texts and limited annotated datasets, enabling knowledge transfer between civil and criminal law domains. JurisCTC utilizes contrastive learning to distinguish samples from different legal domains. Compared to existing models and large language models, JurisCTC achieves higher accuracy in LJP tasks with peak accuracies of 76.59% and 78.83% respectively. The model demonstrates notable advancements in enhancing model generalization across diverse legal domains. <br /><br />Summary: <div>
arXiv:2504.17264v1 Announce Type: new 
Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Mitigating Bias in AI-Based Medical Text Generation</title>
<link>https://arxiv.org/abs/2504.17279</link>
<guid>https://arxiv.org/abs/2504.17279</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, fairness, text generation, medical field, bias <br />
Summary:<br />
The study focuses on addressing the issue of fairness in text generation within the medical field. It highlights the concern that AI systems may reflect and amplify human biases, leading to performance discrepancies across different races, sexes, and age groups. The researchers propose an algorithm that selectively optimizes underperformed groups to reduce bias while maintaining model training effectiveness. The algorithm considers word-level accuracy and pathology accuracy to target reference, ensuring full differentiability. Evaluations across various models, datasets, and modalities show that the proposed algorithm significantly enhances fairness in text generation without compromising overall performance. Disparities among different groups are reduced by over 30%, while text generation accuracy remains within 2% relative change. By mitigating bias in deep learning models, the approach aims to improve the reliability and fairness of text generation diagnoses in the medical domain. The code for the proposed algorithm is publicly available for further research. <br /> <div>
arXiv:2504.17279v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems, particularly those based on deep learning models, have increasingly achieved expert-level performance in medical applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations. The fairness issue has attracted considerable research interest in the medical imaging classification field, yet it remains understudied in the text generation domain. In this study, we investigate the fairness problem in text generation within the medical field and observe significant performance discrepancies across different races, sexes, and age groups, including intersectional groups, various model scales, and different evaluation metrics. To mitigate this fairness issue, we propose an algorithm that selectively optimizes those underperformed groups to reduce bias. The selection rules take into account not only word-level accuracy but also the pathology accuracy to the target reference, while ensuring that the entire process remains fully differentiable for effective model training. Our evaluations across multiple backbones, datasets, and modalities demonstrate that our proposed algorithm enhances fairness in text generation without compromising overall performance. Specifically, the disparities among various groups across different metrics were diminished by more than 30% with our algorithm, while the relative change in text generation accuracy was typically within 2%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of text generation diagnosis in medical domain.
  Our code is publicly available to facilitate further research at https://github.com/iriscxy/GenFair.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality</title>
<link>https://arxiv.org/abs/2504.17309</link>
<guid>https://arxiv.org/abs/2504.17309</guid>
<content:encoded><![CDATA[
<div> watermarking technology, sentence-level, CoheMark, logical fluency, text quality<br />
Summary: <br />
The article introduces CoheMark, an advanced sentence-level watermarking technique that aims to balance high text quality with robust watermark detection. Unlike existing techniques that rely on arbitrary processes, CoheMark utilizes cohesive relationships between sentences for better logical fluency. It selects sentences using fuzzy c-means clustering and specific next sentence selection criteria, resulting in strong watermark strength with minimal impact on text quality. Experimental evaluations confirm the effectiveness of CoheMark in preserving semantic integrity within sentences while maintaining robustness in watermark detection. <div>
arXiv:2504.17309v1 Announce Type: new 
Abstract: Watermarking technology is a method used to trace the usage of content generated by large language models. Sentence-level watermarking aids in preserving the semantic integrity within individual sentences while maintaining greater robustness. However, many existing sentence-level watermarking techniques depend on arbitrary segmentation or generation processes to embed watermarks, which can limit the availability of appropriate sentences. This limitation, in turn, compromises the quality of the generated response. To address the challenge of balancing high text quality with robust watermark detection, we propose CoheMark, an advanced sentence-level watermarking technique that exploits the cohesive relationships between sentences for better logical fluency. The core methodology of CoheMark involves selecting sentences through trained fuzzy c-means clustering and applying specific next sentence selection criteria. Experimental evaluations demonstrate that CoheMark achieves strong watermark strength while exerting minimal impact on text quality.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation</title>
<link>https://arxiv.org/abs/2504.17311</link>
<guid>https://arxiv.org/abs/2504.17311</guid>
<content:encoded><![CDATA[
<div> FLUKE, model robustness, linguistic variations, NLP tasks, LLMs<br />
<br />
Summary: <br />
FLUKE is a framework that evaluates model robustness by systematically varying test data across linguistic levels. Testing on four NLP tasks shows that the impact of linguistic variations varies by task, with some tests crucial for certain tasks. While large language models (LLMs) are generally more robust than fine-tuned models, they still exhibit brittleness to certain linguistic variations. All models are vulnerable to negation modifications across most tasks. This underscores the need for systematic robustness testing to better understand model behaviors. <div>
arXiv:2504.17311v1 Announce Type: new 
Abstract: We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a task-agnostic framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels - from orthography to dialect and style varieties - and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) while LLMs have better overall robustness compared to fine-tuned models, they still exhibit significant brittleness to certain linguistic variations; (3) all models show substantial vulnerability to negation modifications across most tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection</title>
<link>https://arxiv.org/abs/2504.17332</link>
<guid>https://arxiv.org/abs/2504.17332</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, misinformation detection, empathy, Dual-Aspect Empathy Framework, Large Language Models

Summary:<br /><br />
In the digital age, social media plays a significant role in information dissemination, but also leads to the rapid spread of misinformation. Traditional methods of detecting misinformation often overlook the influence of human empathy in this process. To address this, the Dual-Aspect Empathy Framework (DAE) is proposed, integrating cognitive and emotional empathy to analyze misinformation from both creator and reader perspectives. By considering creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more holistic approach to misinformation detection. Additionally, an empathy-aware filtering mechanism is introduced to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate the superiority of DAE over existing methods, presenting a novel paradigm for multimodal misinformation detection.<br /> <div>
arXiv:2504.17332v1 Announce Type: new 
Abstract: In the digital era, social media has become a major conduit for information dissemination, yet it also facilitates the rapid spread of misinformation. Traditional misinformation detection methods primarily focus on surface-level features, overlooking the crucial roles of human empathy in the propagation process. To address this gap, we propose the Dual-Aspect Empathy Framework (DAE), which integrates cognitive and emotional empathy to analyze misinformation from both the creator and reader perspectives. By examining creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more comprehensive and human-centric approach to misinformation detection. Moreover, we further introduce an empathy-aware filtering mechanism to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate that DAE outperforms existing methods, providing a novel paradigm for multimodal misinformation detection.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction</title>
<link>https://arxiv.org/abs/2504.17353</link>
<guid>https://arxiv.org/abs/2504.17353</guid>
<content:encoded><![CDATA[
<div> Keywords: Mutual Reinforcement Effect, information extraction, model interpretability, multimodal, LVLMs

Summary: 
Mutual Reinforcement Effect (MRE) is a new area at the crossroads of information extraction and model interpretability. It aims to improve both coarse-grained and fine-grained tasks by leveraging the relationship between tasks. While MRE has been successful in text, its application to visual and multimodal domains has not been explored. This study introduces Multimodal Mutual Reinforcement Effect (M-MRE) for the first time and provides a dataset for it. The Prompt Format Adapter (PFA) is proposed to address challenges in M-MRE and is compatible with Large Vision-Language Models (LVLMs). Experimental results demonstrate the effectiveness of MRE in the M-MRE task, showing its potential in multimodal text-image understanding scenarios. The study confirms the generalizability of MRE beyond the textual domain.<br /><br />Summary: <div>
arXiv:2504.17353v1 Announce Type: new 
Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare</title>
<link>https://arxiv.org/abs/2504.17360</link>
<guid>https://arxiv.org/abs/2504.17360</guid>
<content:encoded><![CDATA[
<div> framework, model merging, large language models, data privacy, healthcare

Summary:
PatientDx introduces a framework for effective health-predictive tasks using model merging techniques without the need for fine-tuning or adaptation on patient data, addressing data privacy concerns in the healthcare domain. By optimizing a building block merging strategy, PatientDx leverages a pivotal model focused on numerical reasoning and tunes hyperparameters based on performance metrics. Experimental results on mortality tasks from the MIMIC-IV dataset show performance improvements of up to 7% in terms of AUROC compared to initial models. The proposed approach also mitigates data leak issues common with fine-tuned models, maintaining performance levels. The study concludes with a case study demonstrating the effectiveness of PatientDx. The best model is accessible for public use at https://huggingface.co/Jgmorenof/mistral_merged_0_4. 

<br /><br />Summary: <div>
arXiv:2504.17360v1 Announce Type: new 
Abstract: Fine-tuning of Large Language Models (LLMs) has become the default practice for improving model performance on a given task. However, performance improvement comes at the cost of training on vast amounts of annotated data which could be sensitive leading to significant data privacy concerns. In particular, the healthcare domain is one of the most sensitive domains exposed to data privacy issues. In this paper, we present PatientDx, a framework of model merging that allows the design of effective LLMs for health-predictive tasks without requiring fine-tuning nor adaptation on patient data. Our proposal is based on recently proposed techniques known as merging of LLMs and aims to optimize a building block merging strategy. PatientDx uses a pivotal model adapted to numerical reasoning and tunes hyperparameters on examples based on a performance metric but without training of the LLM on these data. Experiments using the mortality tasks of the MIMIC-IV dataset show improvements up to 7% in terms of AUROC when compared to initial models. Additionally, we confirm that when compared to fine-tuned models, our proposal is less prone to data leak problems without hurting performance. Finally, we qualitatively show the capabilities of our proposal through a case study. Our best model is publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams</title>
<link>https://arxiv.org/abs/2504.17366</link>
<guid>https://arxiv.org/abs/2504.17366</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context understanding, spoken text dataset, specialized methods, redundancy, e-commerce systems <br />
<br />
Summary: <br />
The article introduces a new dataset for spoken long-texts derived from live streams to address the challenges of long-context understanding in real-world dialogues. Tasks are categorized into retrieval-dependent, reasoning-dependent, and hybrid categories to evaluate the performance of language models. Current methods struggle with redundancy-rich inputs, highlighting the need for improvements in long-context understanding. The study compares popular LLMs and specialized methods, showcasing task-specific preferences and inconsistent performance across tasks. A new baseline model is proposed to better handle redundancy in spoken text and achieve strong results. The benchmark aims to evaluate long-context spoken language understanding and provide a foundation for developing practical e-commerce systems. The code and benchmark are available for further research and development. <br /> <br />Summary: <div>
arXiv:2504.17366v1 Announce Type: new 
Abstract: Long-context understanding poses significant challenges in natural language processing, particularly for real-world dialogues characterized by speech-based elements, high redundancy, and uneven information density. Although large language models (LLMs) achieve impressive results on existing benchmarks, these datasets fail to reflect the complexities of such texts, limiting their applicability to practical scenarios. To bridge this gap, we construct the first spoken long-text dataset, derived from live streams, designed to reflect the redundancy-rich and conversational nature of real-world scenarios. We construct tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We then evaluate both popular LLMs and specialized methods to assess their ability to understand long-contexts in these tasks. Our results show that current methods exhibit strong task-specific preferences and perform poorly on highly redundant inputs, with no single method consistently outperforming others. We propose a new baseline that better handles redundancy in spoken text and achieves strong performance across tasks. Our findings highlight key limitations of current methods and suggest future directions for improving long-context understanding. Finally, our benchmark fills a gap in evaluating long-context spoken language understanding and provides a practical foundation for developing real-world e-commerce systems. The code and benchmark are available at https://github.com/Yarayx/livelongbench.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona</title>
<link>https://arxiv.org/abs/2504.17390</link>
<guid>https://arxiv.org/abs/2504.17390</guid>
<content:encoded><![CDATA[
<div> personalized responses, PicPersona-TOD dataset, user images, natural language interactions, task-oriented dialogue systems
Summary:
The article introduces PicPersona-TOD, a dataset that incorporates user images in task-oriented dialogue systems for personalized responses. The dataset enhances user experience by tailoring responses to user-specific factors such as age or emotional context. This is achieved through first impressions, dialogue policy-guided prompting, and external knowledge integration to reduce hallucinations. Human evaluations confirm the dataset's effectiveness in improving engagement. Additionally, the article presents Pictor, a new NLG model that not only personalizes responses but also demonstrates strong performance in diverse domains. The combination of the PicPersona-TOD dataset and the Pictor model enables more engaging and individualized interactions in task-oriented dialogue systems. <br /><br />Summary: <div>
arXiv:2504.17390v1 Announce Type: new 
Abstract: Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users' personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized responses contributing to a more engaging interaction. Additionally, we introduce a new NLG model, Pictor, which not only personalizes responses, but also demonstrates robust performance across unseen domains https://github.com/JihyunLee1/PicPersona.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation</title>
<link>https://arxiv.org/abs/2504.17445</link>
<guid>https://arxiv.org/abs/2504.17445</guid>
<content:encoded><![CDATA[
<div> machine learning, unsupervised, topic modeling, clustering, interpretability

Summary: 
The article discusses the use of unsupervised machine learning techniques, such as topic modeling and clustering, in identifying patterns in unstructured text data in social science research. Despite the benefits of these methods in terms of reproducibility and cost-efficiency compared to human qualitative analysis, they have limitations in interpretability and relevance to domain-specific research questions. The study explores the potential of using LLM-generated text augmentation to enhance the utility of topic modeling in addressing domain-specific inquiries. Through a political science case study, it demonstrates that topic modeling with GPT-4 augmentations produces easily interpretable categories that can be utilized to explore domain-specific research questions with minimal human intervention. This approach offers a promising way to improve the practicality and effectiveness of topic modeling in social science research. 

Summary: <div>
arXiv:2504.17445v1 Announce Type: new 
Abstract: Unsupervised machine learning techniques, such as topic modeling and clustering, are often used to identify latent patterns in unstructured text data in fields such as political science and sociology. These methods overcome common concerns about reproducibility and costliness involved in the labor-intensive process of human qualitative analysis. However, two major limitations of topic models are their interpretability and their practicality for answering targeted, domain-specific social science research questions. In this work, we investigate opportunities for using LLM-generated text augmentation to improve the usefulness of topic modeling output. We use a political science case study to evaluate our results in a domain-specific application, and find that topic modeling using GPT-4 augmentations creates highly interpretable categories that can be used to investigate domain-specific research questions with minimal human guidance.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation</title>
<link>https://arxiv.org/abs/2504.17480</link>
<guid>https://arxiv.org/abs/2504.17480</guid>
<content:encoded><![CDATA[
<div> watermarking, language models, knowledge distillation, contrastive decoding, attacks

Summary: 
The study introduces a new technique called Contrastive Decoding-Guided Knowledge Distillation (CDG-KD) for tackling the issue of unauthorized knowledge distillation in language models. Watermark radioactivity, a phenomenon where watermarks from teacher models are inherited by student models, is used to detect unauthorized knowledge distillation. However, the robustness of watermarks against scrubbing and spoofing attacks is not well understood. CDG-KD allows for bidirectional attacks by employing contrastive decoding to extract and manipulate watermark texts in student models. The framework effectively removes or forges watermarks while maintaining the overall performance of the model. The research highlights the importance of developing robust and unforgeable watermarking schemes to protect intellectual property in large language models. <div>
arXiv:2504.17480v1 Announce Type: new 
Abstract: Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HalluLens: LLM Hallucination Benchmark</title>
<link>https://arxiv.org/abs/2504.17550</link>
<guid>https://arxiv.org/abs/2504.17550</guid>
<content:encoded><![CDATA[
<div> benchmark, hallucination, language models, factuality, data leakage  
Summary:  
- This paper introduces a new benchmark to evaluate hallucinations in large language models (LLMs) to address the issue of generated responses deviating from user input or training data, known as "hallucination."
- It proposes a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations to promote consistency in research.
- The benchmark includes new extrinsic hallucination tasks with dynamically generated test sets to prevent data leakage and ensure robustness.
- The analysis of existing benchmarks highlights their limitations and aims to provide a comprehensive evaluation of hallucination in LLMs.
- The work also emphasizes the importance of distinguishing hallucination from factuality evaluations to advance the development of generative AI systems.  
<br /><br />Summary: <div>
arXiv:2504.17550v1 Announce Type: new 
Abstract: Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars</title>
<link>https://arxiv.org/abs/2504.17562</link>
<guid>https://arxiv.org/abs/2504.17562</guid>
<content:encoded><![CDATA[
<div> latent semantics, language models, metadata, pre-training, downstream tasks 

Summary:
Prepending metadata to texts during pre-training of language models can enhance performance in downstream tasks by allowing easier access to latent semantics. However, this approach may have both positive and negative effects on performance, depending on the context provided in the downstream task prompt. When the context is sufficient for inferring latent semantics, training with metadata improves model performance. Conversely, if the context lacks the necessary information for accurate inference, the technique can negatively impact performance. This study demonstrates the importance of considering the context provided in downstream tasks when leveraging metadata during pre-training. <div>
arXiv:2504.17562v1 Announce Type: new 
Abstract: The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training</title>
<link>https://arxiv.org/abs/2504.17565</link>
<guid>https://arxiv.org/abs/2504.17565</guid>
<content:encoded><![CDATA[
<div> large language models, reasoning dataset, training process, data quality, mathematical reasoning

Summary:
A new study explores the training processes and data quality of large language models (LLMs) to improve reasoning capabilities. By constructing a large-scale reasoning dataset with varying difficulty levels, the researchers select valuable training data based on pass rate and Coefficient of Variation (CV). They notice a pattern shift in training, requiring higher learning rates for effective reasoning-focused training. With carefully selected data, the base model achieves a 79.2% pass rate on the AIME2024 mathematical reasoning benchmark, approaching state-of-the-art performance. The study provides detailed descriptions of data processing, difficulty assessment, and training methodology, and openly shares all datasets and methods to advance open-source long-reasoning LLMs.<br /><br />Summary: <div>
arXiv:2504.17565v1 Announce Type: new 
Abstract: Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore</title>
<link>https://arxiv.org/abs/2504.17574</link>
<guid>https://arxiv.org/abs/2504.17574</guid>
<content:encoded><![CDATA[
<div> modeling approach, Chinese rumor detection, deep learning framework, TextCNN, Bidirectional GRU

Summary:
RAGAT-Mind is a novel multi-granular modeling approach for Chinese rumor detection that utilizes the MindSpore deep learning framework. The model combines TextCNN, bidirectional GRU, Multi-Head Self-Attention, and Bidirectional Graph Convolutional Networks (BiGCN) to extract local semantic information, learn sequential context, focus on global dependencies, and represent structural word co-occurrence graphs. Experimental results on the Weibo1-Rumor dataset demonstrate the superior classification performance of RAGAT-Mind with 99.2% accuracy and a macro-F1 score of 0.9919. The study validates the effectiveness of integrating hierarchical linguistic features with graph-based semantic structures for rumor detection. Additionally, the model shows strong generalization and interpretability, indicating its practical utility for real-world applications. 

<br /><br />Summary: <div>
arXiv:2504.17574v1 Announce Type: new 
Abstract: As false information continues to proliferate across social media platforms, effective rumor detection has emerged as a pressing challenge in natural language processing. This paper proposes RAGAT-Mind, a multi-granular modeling approach for Chinese rumor detection, built upon the MindSpore deep learning framework. The model integrates TextCNN for local semantic extraction, bidirectional GRU for sequential context learning, Multi-Head Self-Attention for global dependency focusing, and Bidirectional Graph Convolutional Networks (BiGCN) for structural representation of word co-occurrence graphs. Experiments on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior classification performance, attaining 99.2% accuracy and a macro-F1 score of 0.9919. The results validate the effectiveness of combining hierarchical linguistic features with graph-based semantic structures. Furthermore, the model exhibits strong generalization and interpretability, highlighting its practical value for real-world rumor detection applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a comprehensive taxonomy of online abusive language informed by machine leaning</title>
<link>https://arxiv.org/abs/2504.17653</link>
<guid>https://arxiv.org/abs/2504.17653</guid>
<content:encoded><![CDATA[
<div> Keywords: abusive language, online communication, taxonomy, classification systems, detection

Summary:
The paper introduces a taxonomy for identifying abusive language in online text, aiming to address the increasing concerns about online abuse and its harmful effects. The hierarchical and faceted taxonomy developed includes 5 categories and 17 dimensions, covering aspects such as context, target, intensity, directness, and theme of abuse. By integrating classification systems from 18 multi-label datasets, the taxonomy provides a comprehensive framework for classifying online abuse. This classification aids in the identification, monitoring, and mitigation of harmful content online, facilitating early intervention and moderation. The shared understanding generated by this taxonomy can foster collaboration among researchers, policymakers, online platform owners, and other stakeholders in combating online abuse effectively.<br /><br />Summary: <div>
arXiv:2504.17653v1 Announce Type: new 
Abstract: The proliferation of abusive language in online communications has posed significant risks to the health and wellbeing of individuals and communities. The growing concern regarding online abuse and its consequences necessitates methods for identifying and mitigating harmful content and facilitating continuous monitoring, moderation, and early intervention. This paper presents a taxonomy for distinguishing key characteristics of abusive language within online text. Our approach uses a systematic method for taxonomy development, integrating classification systems of 18 existing multi-label datasets to capture key characteristics relevant to online abusive language classification. The resulting taxonomy is hierarchical and faceted, comprising 5 categories and 17 dimensions. It classifies various facets of online abuse, including context, target, intensity, directness, and theme of abuse. This shared understanding can lead to more cohesive efforts, facilitate knowledge exchange, and accelerate progress in the field of online abuse detection and mitigation among researchers, policy makers, online platform owners, and other stakeholders.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics</title>
<link>https://arxiv.org/abs/2504.17665</link>
<guid>https://arxiv.org/abs/2504.17665</guid>
<content:encoded><![CDATA[
<div> code-assisted LLMs, mathematical reasoning tasks, grounding, evaluation, math datasets
<br />
Summary:<br />
Assisting LLMs with code generation enhanced their performance on math reasoning tasks, but the evaluation usually focuses only on execution correctness. This study delves deeper into code-assisted LLMs' generated programs for math tasks, analyzing to what extent they adhere to math rules and how it impacts their performance. The evaluation assessed five LLMs on two math datasets, revealing that the grounding of programs varies based on LLM capabilities and task difficulty. Closed-source models showed better grounding with math rules, while open-source models struggled to apply them accurately. The grounding percentage decreased on more challenging problems, indicating the need for comprehensive evaluations beyond execution accuracy measures. This research emphasizes the importance of understanding code-assisted LLMs' abilities and limitations in mathematical contexts. 
<br /><br />Summary: <div>
arXiv:2504.17665v1 Announce Type: new 
Abstract: Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated programs. In this work, we bridge this gap by conducting an in-depth analysis of code-assisted LLMs' generated programs in response to math reasoning tasks. Our evaluation focuses on the extent to which LLMs ground their programs to math rules, and how that affects their end performance. For this purpose, we assess the generations of five different LLMs, on two different math datasets, both manually and automatically. Our results reveal that the distribution of grounding depends on LLMs' capabilities and the difficulty of math problems. Furthermore, mathematical grounding is more effective for closed-source models, while open-source models fail to employ math rules in their solutions correctly. On MATH500, the percentage of grounded programs decreased to half, while the ungrounded generations doubled in comparison to ASDiv grade-school problems. Our work highlights the need for in-depth evaluation beyond execution accuracy metrics, toward a better understanding of code-assisted LLMs' capabilities and limits in the math domain.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction</title>
<link>https://arxiv.org/abs/2504.17671</link>
<guid>https://arxiv.org/abs/2504.17671</guid>
<content:encoded><![CDATA[
<div> hallucination mitigation, Large Vision-Language Models, Visual Question Answering, uncertainty quantification, split conformal prediction<br />
<br />
Summary:<br />
This study proposes a Split Conformal Prediction framework to address hallucination mitigation in Large Vision-Language Models for Visual Question Answering tasks. The framework integrates dynamic threshold calibration and cross-modal consistency verification to quantify uncertainties. It partitions data into calibration and test sets, computes nonconformity scores, and constructs prediction sets with statistical guarantees under user-defined risk levels. The framework ensures rigorous control of marginal coverage, dynamically adjusts prediction set sizes based on risk levels, and eliminates the need for prior distribution assumptions and retraining. Evaluations on benchmarks show that the framework enforces theoretical guarantees across all risk levels and is robust for real-world deployment in safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making. <br /> <div>
arXiv:2504.17671v1 Announce Type: new 
Abstract: This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Considerations of Large Language Model Inference and Efficiency Optimizations</title>
<link>https://arxiv.org/abs/2504.17674</link>
<guid>https://arxiv.org/abs/2504.17674</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, energy efficiency optimizations, Natural Language Processing, GPU architectures, sustainable deployment

Summary:<br />
The study examines the energy implications of various inference efficiency optimizations in diverse NLP and generative AI workloads. A modeling approach that approximates real-world LLM workflows is introduced through input-output token distribution binning and batch size variations. The analysis covers software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. The research demonstrates that the effectiveness of inference optimizations is highly dependent on workload geometry, software stack, and hardware accelerators. Naive energy estimates based on FLOPs or theoretical GPU utilization underestimate actual energy consumption. Proper application of relevant inference efficiency optimizations can lead to a significant reduction in total energy use, by up to 73% from unoptimized baselines. These findings offer insights for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure. 

Summary: <div>
arXiv:2504.17674v1 Announce Type: new 
Abstract: As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks</title>
<link>https://arxiv.org/abs/2504.17685</link>
<guid>https://arxiv.org/abs/2504.17685</guid>
<content:encoded><![CDATA[
<div> Ensemble Bayesian Inference, small language model, large language models, aptitude assessments, consumer profile analysis <br />
<br />
Summary: This study explores the potential of small language model ensembles in achieving comparable accuracy to large language models (LLMs) through Ensemble Bayesian Inference (EBI). By combining judgments from multiple SLMs using Bayesian estimation, EBI surpasses individual model performance limitations. The experiments conducted on various tasks (aptitude assessments, consumer profile analysis) in both Japanese and English languages demonstrate the effectiveness of EBI. Interestingly, the study shows that including models with negative Lift values in ensembles can enhance overall performance. The efficacy of EBI across different languages suggests new opportunities for constructing high-performance AI systems with limited computational resources and leveraging models with lower individual performance. The paper discusses the novelty and significance of the approach in the context of LLM performance evaluation, ensemble methods, and open-source LLM utilization. <br /><br /> <div>
arXiv:2504.17685v1 Announce Type: new 
Abstract: This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety in Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2504.17704</link>
<guid>https://arxiv.org/abs/2504.17704</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, vulnerabilities, safety risks, attacks, defense strategies

Summary:
Large Reasoning Models (LRMs) have shown impressive performance in tasks like mathematics and coding due to their advanced reasoning abilities. However, with the increasing sophistication of LRMs, concerns have been raised about their vulnerabilities and safety risks, which could hinder their practical application. This survey paper comprehensively examines the current safety landscape of LRMs, categorizing and summarizing the emerging risks, potential attacks, and defense mechanisms. By providing a structured taxonomy of these elements, the paper aims to enhance the understanding of the security challenges associated with LRMs. This organized approach can help guide future research efforts and development initiatives focused on improving the reliability and robustness of these powerful models. 

<br /><br />Summary: Large Reasoning Models (LRMs) excel in tasks like mathematics and coding but face growing concerns over vulnerabilities and safety risks. This paper surveys the safety landscape of LRMs, categorizing risks, attacks, and defense strategies to guide future research in enhancing the security of these models. <div>
arXiv:2504.17704v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Performance Biases of Large Language Models in Education</title>
<link>https://arxiv.org/abs/2504.17720</link>
<guid>https://arxiv.org/abs/2504.17720</guid>
<content:encoded><![CDATA[
<div> educational tasks, language models, performance, languages, deployment  
Summary:  
Large language models (LLMs) are being used in educational settings, but their effectiveness varies across languages. Testing popular LLMs on educational tasks in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English revealed that lower-resource languages had poorer task performance due to less representation in training data. While models performed reasonably well overall, there were significant drops in performance compared to English. It is recommended that practitioners validate LLM performance in the target language before deployment to ensure effectiveness.  
<br /><br />Summary: <div>
arXiv:2504.17720v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT</title>
<link>https://arxiv.org/abs/2504.17753</link>
<guid>https://arxiv.org/abs/2504.17753</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational assistants, Healthcare, Large Language Models, User study, Heart failure patients

Summary:
The study compares two versions of a conversational assistant for heart failure patients, one using a neurosymbolic architecture and the other based on ChatGPT. The in-house system is found to be more accurate and efficient in task completion, while the ChatGPT-based system has fewer speech errors and clarification needs. However, patients show no preference between the two versions. This highlights the need for controlled evaluations in healthcare settings to understand the advantages and disadvantages of different conversational assistant architectures. The increasing popularity of conversational assistants, particularly in healthcare, calls for rigorous testing with real stakeholders to ensure the systems meet the needs of users effectively. The study emphasizes the importance of considering factors such as accuracy, efficiency, and user preference when designing conversational assistants for healthcare applications.

<br /><br />Summary: <div>
arXiv:2504.17753v1 Announce Type: new 
Abstract: Conversational assistants are becoming more and more popular, including in healthcare, partly because of the availability and capabilities of Large Language Models. There is a need for controlled, probing evaluations with real stakeholders which can highlight advantages and disadvantages of more traditional architectures and those based on generative AI. We present a within-group user study to compare two versions of a conversational assistant that allows heart failure patients to ask about salt content in food. One version of the system was developed in-house with a neurosymbolic architecture, and one is based on ChatGPT. The evaluation shows that the in-house system is more accurate, completes more tasks and is less verbose than the one based on ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors and requires fewer clarifications to complete the task. Patients show no preference for one over the other.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</title>
<link>https://arxiv.org/abs/2504.17768</link>
<guid>https://arxiv.org/abs/2504.17768</guid>
<content:encoded><![CDATA[
<div> Sparse attention, Transformer LLMs, long-context capabilities, efficiency-accuracy trade-offs, model scales <br />
<br />
Summary: Sparse attention is a promising strategy to enhance Transformer LLMs' ability to process long sequences. Through experiments, it was found that larger and highly sparse models are more preferable for very long sequences. The level of sparsity achievable without loss of accuracy is higher during decoding than prefilling and is influenced by model size. Different sparsification methods are required for different tasks and phases, with no single strategy performing best overall. Even moderate levels of sparsity can lead to significant performance degradation on certain tasks, indicating that sparse attention is not universally applicable. Scaling laws tailored for sparse attention were introduced and validated, suggesting that the findings are likely applicable beyond the range of experiments. Sparse attention is a valuable tool for processing longer sequences but requires careful consideration of trade-offs for performance-sensitive applications.<br /><br /> <div>
arXiv:2504.17768v1 Announce Type: new 
Abstract: Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2504.16939</link>
<guid>https://arxiv.org/abs/2504.16939</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Conversational Agents, Reasoning, Monitor, Control

Summary:<br /><br />
The survey paper discusses the advancements in Large Language Models (LLMs) and their impact on Conversational Agents, highlighting the need for more scalable systems approaching human-level intelligence. The paper categorizes the capabilities of LLM-driven Conversational Agents into three dimensions: Reasoning, Monitor, and Control. It introduces a taxonomy classifying recent work on Conversational Agents based on this desideratum, pointing out critical research gaps. Key directions for future research include realistic evaluations, enhancing long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. The ultimate goal is to advance progress towards Artificial General Intelligence (AGI). A curated repository of papers on Conversational Agents is also maintained to facilitate further research in the field. <br /> <div>
arXiv:2504.16939v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: https://github.com/emrecanacikgoz/awesome-conversational-agents.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Im)possibility of Automated Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17004</link>
<guid>https://arxiv.org/abs/2504.17004</guid>
<content:encoded><![CDATA[
<div> framework, hallucination detection, language models, language identification, expert-labeled feedback
Summary:
- The study introduces a theoretical framework to analyze the possibility of automated hallucination detection in large language models (LLMs).
- It establishes the equivalence between hallucination detection and language identification, suggesting that detecting hallucinations is fundamentally impossible for most language collections without expert-labeled feedback.
- With expert-labeled feedback, automated hallucination detection becomes possible for all countable language collections.
- The research emphasizes the importance of expert-labeled examples in training hallucination detectors and supports feedback-based methods like reinforcement learning with human feedback (RLHF) for reliable LLM deployment. <div>
arXiv:2504.17004v1 Announce Type: cross 
Abstract: Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.
  First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCALAR: A Part-of-speech Tagger for Identifiers</title>
<link>https://arxiv.org/abs/2504.17038</link>
<guid>https://arxiv.org/abs/2504.17038</guid>
<content:encoded><![CDATA[
<div> Keywords: Source Code Analysis, Lexical Annotation, Identifier Names, Part-of-Speech Tagging, SCALAR<br />
<br />
Summary: <br />
The paper introduces SCALAR, a tool designed to annotate source code identifier names with their corresponding grammar patterns. SCALAR utilizes the GradientBoostingClassifier from scikit-learn and a manually-curated oracle of identifier names to train its internal model. This specialized training allows SCALAR to recognize the unique language structure developers use when creating identifiers. The tool's output is compared to an older version of the tagger and a modern off-the-shelf part-of-speech tagger to demonstrate its improved annotation accuracy for identifiers. The code for SCALAR is open-source and available on Github. Overall, SCALAR's approach to mapping identifier names to part-of-speech tags shows promise in enhancing the accuracy and efficiency of source code analysis and lexical annotation. <br /> <div>
arXiv:2504.17038v1 Announce Type: cross 
Abstract: The paper presents the Source Code Analysis and Lexical Annotation Runtime (SCALAR), a tool specialized for mapping (annotating) source code identifier names to their corresponding part-of-speech tag sequence (grammar pattern). SCALAR's internal model is trained using scikit-learn's GradientBoostingClassifier in conjunction with a manually-curated oracle of identifier names and their grammar patterns. This specializes the tagger to recognize the unique structure of the natural language used by developers to create all types of identifiers (e.g., function names, variable names etc.). SCALAR's output is compared with a previous version of the tagger, as well as a modern off-the-shelf part-of-speech tagger to show how it improves upon other taggers' output for annotating identifiers. The code is available on Github
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation</title>
<link>https://arxiv.org/abs/2504.17365</link>
<guid>https://arxiv.org/abs/2504.17365</guid>
<content:encoded><![CDATA[
<div> Keywords: Soccer, Multimodal Large Language Models, Video Understanding, Dense Video Captioning, State-of-The-Art Performance<br />
Summary:<br />
This article introduces TimeSoccer, an end-to-end soccer Multimodal Large Language Model designed for Single-anchor Dense Video Captioning in full-match soccer videos. TimeSoccer allows for accurate temporal alignment and semantic relevance by jointly predicting timestamps and generating captions in a single pass. To handle long temporal sequences in soccer matches, the model includes MoFA-Select, a training-free frame compression module that selects representative frames and utilizes various training paradigms for improved performance. Through extensive experiments, TimeSoccer demonstrates State-of-The-Art performance in generating high-quality commentary with precise temporal localization across 45-minute matches. This approach addresses the limitations of existing soccer models that rely on temporal a priori for caption generation or follow a two-step paradigm, resulting in suboptimal performance. <div>
arXiv:2504.17365v1 Announce Type: cross 
Abstract: Soccer is a globally popular sporting event, typically characterized by long matches and distinctive highlight moments. Recent advances in Multimodal Large Language Models (MLLMs) offer promising capabilities in temporal grounding and video understanding, soccer commentary generation often requires precise temporal localization and semantically rich descriptions over long-form video. However, existing soccer MLLMs often rely on the temporal a priori for caption generation, so they cannot process the soccer video end-to-end. While some traditional approaches follow a two-step paradigm that is complex and fails to capture the global context to achieve suboptimal performance. To solve the above issues, we present TimeSoccer, the first end-to-end soccer MLLM for Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos. TimeSoccer jointly predicts timestamps and generates captions in a single pass, enabling global context modeling across 45-minute matches. To support long video understanding of soccer matches, we introduce MoFA-Select, a training-free, motion-aware frame compression module that adaptively selects representative frames via a coarse-to-fine strategy, and incorporates complementary training paradigms to strengthen the model's ability to handle long temporal sequences. Extensive experiments demonstrate that our TimeSoccer achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end form, generating high-quality commentary with accurate temporal alignment and strong semantic relevance.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models</title>
<link>https://arxiv.org/abs/2504.17449</link>
<guid>https://arxiv.org/abs/2504.17449</guid>
<content:encoded><![CDATA[
<div> Hierarchical knowledge management, Multi-tenant Inference, Pretrained language models, Resource-efficient, GPU memory usage<br />
<br />
Summary: 
The article introduces HMI, a system designed to efficiently serve multiple tenants with distinct pretrained language models (PLMs) by categorizing knowledge into general, domain-specific, and task-specific components. By leveraging hierarchical PLMs (hPLMs) with knowledge stored at different levels, GPU memory usage per tenant is significantly reduced. Domain-specific knowledge is managed through knowledge trees based on frequency, while task-specific knowledge is handled with parameter swapping to stay within limited GPU memory. System optimizations such as hierarchical knowledge prefetching and batched matrix multiplications enhance resource utilization and inference throughput. Experimental results show that HMI can effectively support up to 10,000 hPLMs on a single GPU with minimal impact on accuracy. <br /><br />Summary: <div>
arXiv:2504.17449v1 Announce Type: cross 
Abstract: The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant Inference system, designed to manage tenants with distinct PLMs resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM knowledge into general, domain-specific, and task-specific. Leveraging insights on knowledge acquisition across different model layers, we construct hierarchical PLMs (hPLMs) by extracting and storing knowledge at different levels, significantly reducing GPU memory usage per tenant. Secondly, we establish hierarchical knowledge management for hPLMs generated by various tenants in HMI. We manage domain-specific knowledge with acceptable storage increases by constructing and updating domain-specific knowledge trees based on frequency. We manage task-specific knowledge within limited GPU memory through parameter swapping. Finally, we propose system optimizations to enhance resource utilization and inference throughput. These include fine-grained pipelining via hierarchical knowledge prefetching to overlap CPU and I/O operations with GPU computations, and optimizing parallel implementations with batched matrix multiplications. Our experimental results demonstrate that the proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a single GPU, with only a negligible compromise in accuracy.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization</title>
<link>https://arxiv.org/abs/2406.07494</link>
<guid>https://arxiv.org/abs/2406.07494</guid>
<content:encoded><![CDATA[
<div> Transformer-based summarization, English dialogues, challenges, techniques, evaluation metrics <br />
<br />
Summary: Abstractive dialogue summarization research on Transformer-based models for English dialogues reviews 1262 papers from 2019 to 2024. Main challenges include language, structure, comprehension, speaker, salience, and factuality, addressed by graph-based approaches and BART models. Progress has been made in language challenges, while comprehension, factuality, and salience remain difficult. Assessments use datasets for dialogue subdomains, ROUGE metric, and human evaluation lacking detail on annotator agreement. Few datasets cover all subdomains. Large language models are explored with potential implications, but challenge taxonomy remains relevant. <div>
arXiv:2406.07494v3 Announce Type: replace 
Abstract: Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure</title>
<link>https://arxiv.org/abs/2406.17276</link>
<guid>https://arxiv.org/abs/2406.17276</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive language models, Speculative decoding, OPT-Tree, Inference efficiency, Adaptive draft trees

Summary:
OPT-Tree introduces an adaptive and scalable draft tree algorithm to improve the inference efficiency of autoregressive language models. By optimizing tree structures to maximize acceptance length during verification, OPT-Tree outperforms existing methods in speed-up ratio and can generate more than ten tokens in a single step. The "draft and then verify" mechanism of speculative decoding allows for lossless acceleration, enabling multiple token generation in each decoding step. Experimental results demonstrate a speed-up ratio of up to 3.2 compared to autoregressive decoding. With the availability of the OPT-Tree code on GitHub, researchers can implement this algorithm to enhance the performance of autoregressive language models in various scenarios. <br /><br />Summary: <div>
arXiv:2406.17276v4 Announce Type: replace 
Abstract: Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a "draft and then verify" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse</title>
<link>https://arxiv.org/abs/2409.11242</link>
<guid>https://arxiv.org/abs/2409.11242</guid>
<content:encoded><![CDATA[
<div> Trust-Score, LLMs, RAG task, Trust-Align, in-context learning  
Summary:  
Trust-Score is introduced as a metric to evaluate the trustworthiness of LLMs in the RAG framework. Various prompting methods, like in-context learning, are found to be ineffective in adapting LLMs for the RAG task, leading to the proposal of Trust-Align to improve performance. Models aligned using Trust-Align outperform baselines on ASQA, QAMPARI, and ELI5. Trust-Align enhances models' ability to refuse correctly and provide quality citations. The effectiveness of Trust-Align is demonstrated across different open-weight models, including LLaMA series, Qwen series, and Phi3.5 series. Code for Trust-Align is released on GitHub at https://github.com/declare-lab/trust-align. <div>
arXiv:2409.11242v4 Announce Type: replace 
Abstract: LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56), QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly enhances models' ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine</title>
<link>https://arxiv.org/abs/2409.18986</link>
<guid>https://arxiv.org/abs/2409.18986</guid>
<content:encoded><![CDATA[
<div> Keywords: Lab-AI, personalized normal ranges, retrieval-augmented generation, patient portals, clinical medicine<br />
Summary: 
Lab-AI is an interactive system developed for accurate interpretation of lab results in clinical medicine. It offers personalized normal ranges based on conditional factors such as age and gender, using retrieval-augmented generation (RAG) from credible health sources. The system consists of two modules: factor retrieval and normal range retrieval, which were tested on 122 lab tests. Lab-AI, powered by GPT-4-turbo with RAG, achieved high performance metrics with a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval. It outperformed non-RAG systems significantly in factor retrieval and showed substantial improvements in question-level and lab-level performance for normal range retrieval. The study demonstrates the potential of Lab-AI to enhance patient understanding of lab results and improve the accuracy of interpreting lab data in clinical settings.<br /><br />Summary: <div>
arXiv:2409.18986v2 Announce Type: replace 
Abstract: Accurate interpretation of lab results is crucial in clinical medicine, yet most patient portals use universal normal ranges, ignoring conditional factors like age and gender. This study introduces Lab-AI, an interactive system that offers personalized normal ranges using retrieval-augmented generation (RAG) from credible health sources. Lab-AI has two modules: factor retrieval and normal range retrieval. We tested these on 122 lab tests: 40 with conditional factors and 82 without. For tests with factors, normal ranges depend on patient-specific information. Our results show GPT-4-turbo with RAG achieved a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5% in factor retrieval and showed 132% and 100% improvements in question-level and lab-level performance, respectively, for normal range retrieval. These findings highlight Lab-AI's potential to enhance patient understanding of lab results.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?</title>
<link>https://arxiv.org/abs/2409.19151</link>
<guid>https://arxiv.org/abs/2409.19151</guid>
<content:encoded><![CDATA[
<div> linguistics, NLP, XLR languages, machine translation, grammar books

Summary:<br />
- The study investigates the use of grammar books in training NLP models for Extremely Low-Resource (XLR) languages.
- Machine Translation from One Book shows that using grammar books can enable translation for XLR languages not seen by LLMs.
- The study finds that the translation ability mainly comes from the parallel examples in the grammar books rather than the grammatical explanations.
- Similar results are found for Nepali and Guarani languages, and fine-tuning a translation model can achieve comparable performance to LLMs with grammar books.
- Grammar books are more helpful for translation tasks through parallel examples, while linguistic tasks benefit from grammatical knowledge through a typological feature prompt. <br />Summary: <div>
arXiv:2409.19151v2 Announce Type: replace 
Abstract: Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book's parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we conclude data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking</title>
<link>https://arxiv.org/abs/2410.01952</link>
<guid>https://arxiv.org/abs/2410.01952</guid>
<content:encoded><![CDATA[
<div> Large Language Models, reasoning types, deductive reasoning, inductive reasoning, abductive reasoning <br />
Summary: <br />
The article discusses the limitations of current Large Language Models (LLMs) in utilizing diverse reasoning types such as inductive, abductive, or analogical reasoning. It introduces TypedThinker, a system that predicts suitable reasoning types for specific problems and provides guidance to LLMs in applying these strategies. Experimental results demonstrate significant performance improvements across logical and mathematical reasoning tasks without requiring knowledge distillation from larger models. TypedThinker can enhance LLM reasoning and problem-solving capabilities, potentially integrating into advanced systems like GPT-4o or specialized models like MetaMath to diversify reasoning approaches. <div>
arXiv:2410.01952v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems. However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types. LLMs typically employ deductive reasoning, proceeding step-by-step from given conditions, which limits their exploration during problem-solving. Our analysis reveals that certain problems are exclusively solvable through specific reasoning strategies like inductive, abductive, or analogical reasoning. However, incorporating diverse reasoning approaches presents two key challenges: identifying the appropriate reasoning type for each problem and exploiting this approach during problem-solving. Therefore, we propose the TypedThinker that predicts suitable reasoning types based on the problem and their previous effectiveness and provides relevant demonstrations to guide LLMs in applying these strategies. Experimental results show significant improvements across multiple benchmarks, with performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and mathematical reasoning tasks. TypedThinker enhances LLM reasoning without requiring knowledge distillation from larger models. It can be integrated into more advanced systems like GPT-4o or specialized models like MetaMath to diversify their reasoning approaches and improve their problem-solving capabilities.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Attention Improves Transformer</title>
<link>https://arxiv.org/abs/2410.02703</link>
<guid>https://arxiv.org/abs/2410.02703</guid>
<content:encoded><![CDATA[
arXiv:2410.02703v2 Announce Type: replace 
Abstract: Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
arXiv:2410.05401v2 Announce Type: replace 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies</title>
<link>https://arxiv.org/abs/2410.19878</link>
<guid>https://arxiv.org/abs/2410.19878</guid>
<content:encoded><![CDATA[
arXiv:2410.19878v3 Announce Type: replace 
Abstract: The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach</title>
<link>https://arxiv.org/abs/2411.04950</link>
<guid>https://arxiv.org/abs/2411.04950</guid>
<content:encoded><![CDATA[
arXiv:2411.04950v4 Announce Type: replace 
Abstract: We introduce a data-centric hypothesis-testing framework to quantify the influence of sequentially correlated literary properties--such as thematic continuity--on textual classification tasks. Our method models label sequences as stochastic processes and uses an empirical autocovariance matrix to generate surrogate labelings that preserve sequential dependencies. This enables statistical testing to determine whether classification outcomes are primarily driven by thematic structure or by non-sequential features like authorial style. Applying this framework across a diverse corpus of English prose, we compare traditional (word n-grams and character k-mers) and neural (contrastively trained) embeddings in both supervised and unsupervised classification settings. Crucially, our method identifies when classifications are confounded by sequentially correlated similarity, revealing that supervised and neural models are more prone to false positives--mistaking shared themes and cross-genre differences for stylistic signals. In contrast, unsupervised models using traditional features often yield high true positive rates with minimal false positives, especially in genre-consistent settings. By disentangling sequential from non-sequential influences, our approach provides a principled way to assess and interpret classification reliability. This is particularly impactful for authorship attribution, forensic linguistics, and the analysis of redacted or composite texts, where conventional methods may conflate theme with style. Our results demonstrate that controlling for sequential correlation is essential for reducing false positives and ensuring that classification outcomes reflect genuine stylistic distinctions.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images</title>
<link>https://arxiv.org/abs/2412.08802</link>
<guid>https://arxiv.org/abs/2412.08802</guid>
<content:encoded><![CDATA[
arXiv:2412.08802v2 Announce Type: replace 
Abstract: Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model's performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing</title>
<link>https://arxiv.org/abs/2501.14936</link>
<guid>https://arxiv.org/abs/2501.14936</guid>
<content:encoded><![CDATA[
arXiv:2501.14936v2 Announce Type: replace 
Abstract: The integration of contextual embeddings into the optimization processes of large language models is an advancement in natural language processing. The Context-Aware Neural Gradient Mapping framework introduces a dynamic gradient adjustment mechanism, incorporating contextual embeddings directly into the optimization process. This approach facilitates real-time parameter adjustments, enhancing task-specific generalization even in the presence of sparse or noisy data inputs. The mathematical foundation of this framework relies on gradient descent modifications, where contextual embeddings are derived from a supplementary neural network trained to map input features to optimal adaptation gradients. By employing differential geometry principles, high-dimensional input dependencies are encoded into low-dimensional gradient manifolds, enabling efficient adaptation without necessitating the retraining of the entire model. Empirical evaluations demonstrate that the proposed framework consistently outperforms baseline models across various metrics, including accuracy, robustness to noise, and computational efficiency. The integration of context-specific embeddings allows for a more complex understanding of language, thereby improving the model's ability to handle diverse linguistic phenomena. Furthermore, the computational efficiency achieved through this method demonstrates its scalability for large-scale language models operating under diverse constraints.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual State Space Models for Structured Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2502.01673</link>
<guid>https://arxiv.org/abs/2502.01673</guid>
<content:encoded><![CDATA[
arXiv:2502.01673v2 Announce Type: replace 
Abstract: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05346</link>
<guid>https://arxiv.org/abs/2502.05346</guid>
<content:encoded><![CDATA[
arXiv:2502.05346v2 Announce Type: replace 
Abstract: Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reasoning Ability of Small Language Models</title>
<link>https://arxiv.org/abs/2502.11569</link>
<guid>https://arxiv.org/abs/2502.11569</guid>
<content:encoded><![CDATA[
arXiv:2502.11569v2 Announce Type: replace 
Abstract: Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSCon: Product Search Through Conversations</title>
<link>https://arxiv.org/abs/2502.13881</link>
<guid>https://arxiv.org/abs/2502.13881</guid>
<content:encoded><![CDATA[
arXiv:2502.13881v2 Announce Type: replace 
Abstract: Conversational Product Search ( CPS ) systems interact with users via natural language to offer personalized and context-aware product lists. However, most existing research on CPS is limited to simulated conversations, due to the lack of a real CPS dataset driven by human-like language. Moreover, existing conversational datasets for e-commerce are constructed for a particular market or a particular language and thus can not support cross-market and multi-lingual usage. In this paper, we propose a CPS data collection protocol and create a new CPS dataset, called PSCon, which assists product search through conversations with human-like language. The dataset is collected by a coached human-human data collection protocol and is available for dual markets and two languages. By formulating the task of CPS, the dataset allows for comprehensive and in-depth research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Moreover, we present a concise analysis of the dataset and propose a benchmark model on the proposed CPS dataset. Our proposed dataset and model will be helpful for facilitating future research on CPS.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Evaluating the Paper Reviewing Capability of Large Language Models</title>
<link>https://arxiv.org/abs/2502.17086</link>
<guid>https://arxiv.org/abs/2502.17086</guid>
<content:encoded><![CDATA[
arXiv:2502.17086v2 Announce Type: replace 
Abstract: Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection</title>
<link>https://arxiv.org/abs/2503.07269</link>
<guid>https://arxiv.org/abs/2503.07269</guid>
<content:encoded><![CDATA[
arXiv:2503.07269v2 Announce Type: replace 
Abstract: We present our shared task on text-based emotion detection, covering more than 30 languages from seven distinct language families. These languages are predominantly low-resource and are spoken across various continents. The data instances are multi-labeled with six emotional classes, with additional datasets in 11 languages annotated for emotion intensity. Participants were asked to predict labels in three tracks: (a) multilabel emotion detection, (b) emotion intensity score detection, and (c) cross-lingual emotion detection.
  The task attracted over 700 participants. We received final submissions from more than 200 teams and 93 system description papers. We report baseline results, along with findings on the best-performing systems, the most common approaches, and the most effective methods across different tracks and languages. The datasets for this task are publicly available. The dataset is available at SemEval2025 Task 11 https://brighter-dataset.github.io
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks</title>
<link>https://arxiv.org/abs/2503.10894</link>
<guid>https://arxiv.org/abs/2503.10894</guid>
<content:encoded><![CDATA[
arXiv:2503.10894v2 Announce Type: replace 
Abstract: Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts(e.g., the birth year of a person) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) constructs features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shared Global and Local Geometry of Language Model Embeddings</title>
<link>https://arxiv.org/abs/2503.21073</link>
<guid>https://arxiv.org/abs/2503.21073</guid>
<content:encoded><![CDATA[
arXiv:2503.21073v2 Announce Type: replace 
Abstract: Researchers have recently suggested that models share common representations. In our work, we find that token embeddings of language models exhibit common geometric structure. First, we find ``global'' similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. Our intrinsic dimension demonstrates that token embeddings lie on a lower dimensional manifold. We qualitatively show that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow us to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, we find that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, we introduce Emb2Emb, a simple method to transfer steering vectors from one language model to another, despite the two models having different dimensions.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Memory in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02441</link>
<guid>https://arxiv.org/abs/2504.02441</guid>
<content:encoded><![CDATA[
arXiv:2504.02441v2 Announce Type: replace 
Abstract: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Data Are Unlearned Equally</title>
<link>https://arxiv.org/abs/2504.05058</link>
<guid>https://arxiv.org/abs/2504.05058</guid>
<content:encoded><![CDATA[
arXiv:2504.05058v4 Announce Type: replace 
Abstract: Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual MFA: Forced Alignment on Low-Resource Related Languages</title>
<link>https://arxiv.org/abs/2504.07315</link>
<guid>https://arxiv.org/abs/2504.07315</guid>
<content:encoded><![CDATA[
arXiv:2504.07315v2 Announce Type: replace 
Abstract: We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories. We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language. Results indicate benefits of adapting the English baseline model for previously unseen languages.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable text data distillation by trajectory matching</title>
<link>https://arxiv.org/abs/2504.09818</link>
<guid>https://arxiv.org/abs/2504.09818</guid>
<content:encoded><![CDATA[
arXiv:2504.09818v2 Announce Type: replace 
Abstract: In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama).
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation</title>
<link>https://arxiv.org/abs/2406.14088</link>
<guid>https://arxiv.org/abs/2406.14088</guid>
<content:encoded><![CDATA[
arXiv:2406.14088v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for empowering large language model (LLM) applications. Compared with the supervised training process of LLMs, the RLHF training process is much more sophisticated, requiring a diverse range of computation workloads with intricate dependencies between multiple LLM instances. Therefore, simply adopting the fixed parallelization strategies from supervised training for LLMs can be insufficient for RLHF and result in low training efficiency. To overcome this limitation, we propose a novel technique named parameter ReaLlocation, which dynamically adapts the parallelization strategies for different workloads during training by redistributing LLM parameters across the training cluster. Building upon this idea, we introduce ReaL, a pioneering system for efficient RLHF training. ReaL introduces the concept of an execution plan, which defines a fine-grained resource allocation and parallelization strategy particularly designed for RLHF training. Based on this concept, ReaL employs a tailored search algorithm with a lightweight run-time estimator to automatically discover an efficient execution plan for an instance of RLHF experiment. Subsequently, the runtime engine deploys the selected plan by effectively parallelizing computations and redistributing parameters. We evaluate ReaL on the LLaMA models with up to 70 billion parameters and 128 GPUs. The experimental results demonstrate that ReaL achieves speedups of up to $3.58\times$ compared to baseline methods. Furthermore, the execution plans generated by ReaL exhibit an average of $81\%$ performance improvement over heuristic approaches based on Megatron-LM in the long-context scenario. The source code of ReaL is publicly available at https://github.com/openpsi-project/ReaLHF .
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF</title>
<link>https://arxiv.org/abs/2410.04612</link>
<guid>https://arxiv.org/abs/2410.04612</guid>
<content:encoded><![CDATA[
arXiv:2410.04612v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn reinforcement learning from human feedback (RLHF) methods to the multi-turn setting by treating all prior dialogue turns as a long context. Such approaches suffer from covariate shift: the conversations in the training set have previous turns generated by some reference policy, which means that low training error may not necessarily correspond to good performance when the learner is actually in the conversation loop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient policy optimization approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single model to estimate $Q$-values and trains on self-generated data, addressing the covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of regression tasks on iteratively collected datasets, enabling ease of implementation. Theoretically, we prove that REFUEL can match the performance of any policy covered by the training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our model. REFUEL consistently outperforms state-of-the-art methods such as DPO and REBEL across various settings. Furthermore, despite having only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found at https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be found at https://huggingface.co/Cornell-AGI.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing</title>
<link>https://arxiv.org/abs/2501.05255</link>
<guid>https://arxiv.org/abs/2501.05255</guid>
<content:encoded><![CDATA[
arXiv:2501.05255v2 Announce Type: replace-cross 
Abstract: API-driven chatbot systems are increasingly integral to software engineering applications, yet their effectiveness hinges on accurately generating and executing API calls. This is particularly challenging in scenarios requiring multi-step interactions with complex parameterization and nested API dependencies. Addressing these challenges, this work contributes to the evaluation and assessment of AI-based software development through three key advancements: (1) the introduction of a novel dataset specifically designed for benchmarking API function selection, parameter generation, and nested API execution; (2) an empirical evaluation of state-of-the-art language models, analyzing their performance across varying task complexities in API function generation and parameter accuracy; and (3) a hybrid approach to API routing, combining general-purpose large language models for API selection with fine-tuned models and prompt engineering for parameter generation. These innovations significantly improve API execution in chatbot systems, offering practical methodologies for enhancing software design, testing, and operational workflows in real-world software engineering contexts.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?</title>
<link>https://arxiv.org/abs/2501.15857</link>
<guid>https://arxiv.org/abs/2501.15857</guid>
<content:encoded><![CDATA[
arXiv:2501.15857v3 Announce Type: replace-cross 
Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, "FTCT" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing</title>
<link>https://arxiv.org/abs/2503.10742</link>
<guid>https://arxiv.org/abs/2503.10742</guid>
<content:encoded><![CDATA[
arXiv:2503.10742v2 Announce Type: replace-cross 
Abstract: Vision language models (VLMs) demonstrate strong capabilities in jointly processing visual and textual data. However, they often incur substantial computational overhead due to redundant visual information, particularly in long-form video scenarios. Existing approaches predominantly focus on either vision token pruning, which may overlook spatio-temporal dependencies, or keyframe selection, which identifies informative frames but discards others, thus disrupting contextual continuity. In this work, we propose KVTP (Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the drawbacks of token pruning and keyframe selection. By adaptively assigning pruning rates based on frame relevance to the query, KVTP effectively retains essential contextual information while significantly reducing redundant computation. To thoroughly evaluate the long-form video understanding capacities of VLMs, we curated and reorganized subsets from VideoMME, EgoSchema, and NextQA into a unified benchmark named SparseKV-QA that highlights real-world scenarios with sparse but crucial events. Our experiments with VLMs of various scales show that KVTP can reduce token usage by 80% without compromising spatiotemporal and contextual consistency, significantly cutting computation while maintaining the performance. These results demonstrate our approach's effectiveness in efficient long-video processing, facilitating more scalable VLM deployment.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking beyond the next token</title>
<link>https://arxiv.org/abs/2504.11336</link>
<guid>https://arxiv.org/abs/2504.11336</guid>
<content:encoded><![CDATA[
arXiv:2504.11336v2 Announce Type: replace-cross 
Abstract: The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. We argue that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. We demonstrate that this technique, Trelawney, and the inference algorithms derived from it allow us to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, our method naturally enables the generation of long-term goals at no additional cost. We investigate how using the model's goal-generation capability can further improve planning and reasoning. Additionally, we believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Reason through Learning and Forgetting</title>
<link>https://arxiv.org/abs/2504.11364</link>
<guid>https://arxiv.org/abs/2504.11364</guid>
<content:encoded><![CDATA[
arXiv:2504.11364v2 Announce Type: replace-cross 
Abstract: Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the model's search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\times$.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking</title>
<link>https://arxiv.org/abs/2504.16188</link>
<guid>https://arxiv.org/abs/2504.16188</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Financial Natural Language Inference, diverse financial texts, SEC Filings, Annual Reports, Earnings Call transcripts

Summary:
FinNLI is introduced as a benchmark dataset for Financial Natural Language Inference (FinNLI) that includes diverse premise-hypothesis pairs from various financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. The dataset consists of 21,304 pairs, with a high-quality test set of 3,304 instances annotated by finance experts. Evaluations reveal a notable performance degradation in general-domain NLI models when faced with domain shifts. The highest Macro F1 scores for pre-trained language models (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, indicating the dataset's complexity. Interestingly, financial LLMs fine-tuned with instructions perform poorly, highlighting limitations in generalizability. FinNLI exposes shortcomings in current LLMs for financial reasoning, suggesting a need for improvement. 

<br /><br />Summary: <div>
arXiv:2504.16188v1 Announce Type: new 
Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy</title>
<link>https://arxiv.org/abs/2504.16271</link>
<guid>https://arxiv.org/abs/2504.16271</guid>
<content:encoded><![CDATA[
<div> NLP, attachment style, psychotherapy, PACS, classification model <br />
Summary: <br />
This paper introduces the use of Natural Language Processing (NLP) techniques to automatically assess patient attachment style in psychotherapy transcripts. Currently, attachment style assessment is done manually using the PACS system, which is time-consuming and requires extensive training. By implementing NLP classification models, the process can be automated, allowing for more personalized psychotherapy and targeted research on therapy mechanisms. Mislabeling patients' attachment styles can have negative effects on therapy outcomes, so accurate automated assessment is crucial. This research paves the way for widespread adoption of attachment-informed treatment and research in the mental healthcare field. <div>
arXiv:2504.16271v1 Announce Type: new 
Abstract: The delivery of mental healthcare through psychotherapy stands to benefit immensely from developments within Natural Language Processing (NLP), in particular through the automatic identification of patient specific qualities, such as attachment style. Currently, the assessment of attachment style is performed manually using the Patient Attachment Coding System (PACS; Talia et al., 2017), which is complex, resource-consuming and requires extensive training. To enable wide and scalable adoption of attachment informed treatment and research, we propose the first exploratory analysis into automatically assessing patient attachment style from psychotherapy transcripts using NLP classification models. We further analyze the results and discuss the implications of using automated tools for this purpose -- e.g., confusing `preoccupied' patients with `avoidant' likely has a more negative impact on therapy outcomes with respect to other mislabeling. Our work opens an avenue of research enabling more personalized psychotherapy and more targeted research into the mechanisms of psychotherapy through advancements in NLP.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation</title>
<link>https://arxiv.org/abs/2504.16286</link>
<guid>https://arxiv.org/abs/2504.16286</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese-English translation, back-translation, cultural fidelity, NLP performance

Summary:
<br /><br />
1. The study evaluates the performance of large language models in Chinese-English translation, focusing on scientific terminology, historical paradoxes, and literary metaphors.
2. Scientific abstracts benefit from back-translation, while traditional tools perform better in linguistically distinct texts.
3. Large language models struggle with retaining cultural and literary nuances, showcasing challenges in preserving poetic intent.
4. Some models exhibit a tendency towards "verbatim back-translation", indicating emergent memory behavior.
5. A novel BLEU variant utilizing Jieba segmentation and n-gram weighting is proposed to improve translation accuracy and cultural fidelity. This study contributes to a better understanding of Chinese NLP performance and the importance of maintaining cultural integrity in AI-mediated translation efforts. <div>
arXiv:2504.16286v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit "verbatim back-translation", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives</title>
<link>https://arxiv.org/abs/2504.16312</link>
<guid>https://arxiv.org/abs/2504.16312</guid>
<content:encoded><![CDATA[
<div> Keywords: symmetric relations, antisymmetric relations, natural language inference, large language models, contrastive learning

Summary:
This paper introduces a new natural language inference dataset derived from Wikidata to evaluate the performance of large language models (LLMs) in capturing symmetric and antisymmetric relations. The findings indicate that LLMs perform at random chance levels on this benchmark, revealing a gap in their ability to understand relations. To address this issue, the paper explores encoder retraining through contrastive learning with k-nearest neighbors. The retrained encoder achieves performance comparable to fine-tuned classification heads while offering advantages such as improved efficiency in few-shot learning and better mitigation of catastrophic forgetting.<br /><br />Summary: <div>
arXiv:2504.16312v1 Announce Type: new 
Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric (e.g., parent_of) relations is crucial for a variety of applications. This paper tackles this challenge by introducing a novel Wikidata-derived natural language inference dataset designed to evaluate large language models (LLMs). Our findings reveal that LLMs perform comparably to random chance on this benchmark, highlighting a gap in relational understanding. To address this, we explore encoder retraining via contrastive learning with k-nearest neighbors. The retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including greater efficiency in few-shot learning and improved mitigation of catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Extraction of Statutory Definitions from the U.S. Code</title>
<link>https://arxiv.org/abs/2504.16353</link>
<guid>https://arxiv.org/abs/2504.16353</guid>
<content:encoded><![CDATA[
<div> extract, definitions, legal texts, U.S. Code, NLP

Summary:
- The study presents an advanced NLP system that uses transformer-based architectures to automatically extract definitions from the U.S. Code.
- The system addresses challenges in identifying legal definitions, extracting defined terms, and determining their scope within the complex corpus.
- It employs domain-specific transformers (Legal-BERT) fine-tuned for statutory texts to improve extraction accuracy.
- The system utilizes a multi-stage pipeline combining document structure analysis and state-of-the-art language models to process legal text from the U.S. Code XML version.
- Evaluation on multiple U.S. Code titles shows the system achieves significant improvements with 96.8% precision and 98.9% recall, surpassing traditional machine learning classifiers. 

<br /><br />Summary: <div>
arXiv:2504.16353v1 Announce Type: new 
Abstract: Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions</title>
<link>https://arxiv.org/abs/2504.16358</link>
<guid>https://arxiv.org/abs/2504.16358</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-TrajVis, trajectory data visualization, dataset construction, TrajVL, Large Language Models

Summary: 
This paper introduces the Text-to-TrajVis task, which transforms natural language questions into trajectory data visualizations. The authors propose the Trajectory Visualization Language (TVL) and develop a dataset construction method to create the TrajVL dataset, containing 18,140 question-TVL pairs. The dataset combines human labeling with Large Language Models (LLMs). The performance of multiple LLMs (GPT, Qwen, Llama) is evaluated on this task, showing its feasibility and difficulty. This novel task presents challenges and opportunities for natural language interfaces for trajectory visualization systems. 

<br /><br />Summary: <div>
arXiv:2504.16358v1 Announce Type: new 
Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform natural language questions into trajectory data visualizations, facilitating the development of natural language interfaces for trajectory visualization systems. As this is a novel task, there is currently no relevant dataset available in the community. To address this gap, we first devised a new visualization language called Trajectory Visualization Language (TVL) to facilitate querying trajectory data and generating visualizations. Building on this foundation, we further proposed a dataset construction method that integrates Large Language Models (LLMs) with human efforts to create high-quality data. Specifically, we first generate TVLs using a comprehensive and systematic process, and then label each TVL with corresponding natural language questions using LLMs. This process results in the creation of the first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140 (question, TVL) pairs. Based on this dataset, we systematically evaluated the performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The experimental results demonstrate that this task is both feasible and highly challenging and merits further exploration within the research community.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitReason: Learning To Offload Reasoning</title>
<link>https://arxiv.org/abs/2504.16379</link>
<guid>https://arxiv.org/abs/2504.16379</guid>
<content:encoded><![CDATA[
<div> annotate, reasoning, large language models, offloading, fine-tuning
Summary:
Large language models (LLMs) often struggle with the sequential and memory-bound decoding phase due to the extended generation length required for reasoning tasks. To address this, a new approach called SplitReason is introduced, which offloads the most challenging segments of the reasoning process to a larger model while using a smaller, more efficient model for the rest. By annotating difficult segments and leveraging supervised and reinforcement learning fine-tuning, the 1.5B-parameter reasoning model can improve reasoning accuracy by 24% and 28.3% while offloading only a small percentage of generated tokens. The approach shows promising results in enhancing efficiency and accuracy in reasoning tasks, particularly on the OpenR1-Math-220k CoT dataset. The SplitReason model, along with data, code, and logs, has been open-sourced for further research and development. 
<br /><br />Summary: <div>
arXiv:2504.16379v1 Announce Type: new 
Abstract: Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.16394</link>
<guid>https://arxiv.org/abs/2504.16394</guid>
<content:encoded><![CDATA[
<div> Keywords: unstructured clinical data, clinical text summarization, Contextual, Domain-Specific Knowledge Graph, precision 

Summary: 
Contextual is a new framework designed to extract key information from unstructured clinical data for improved decision-making in patient care. It combines Context-Preserving Token Filtering with a Domain-Specific Knowledge Graph to enhance linguistic coherence and clinical fidelity. By preserving important context-specific tokens and augmenting them with structured knowledge, Contextual excels in both aspects, outperforming other baselines in empirical evaluations on two public benchmark datasets. The approach showcases the significance of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity while offering a scalable solution for enhancing precision in clinical text generation. <div>
arXiv:2504.16394v1 Announce Type: new 
Abstract: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</title>
<link>https://arxiv.org/abs/2504.16408</link>
<guid>https://arxiv.org/abs/2504.16408</guid>
<content:encoded><![CDATA[
<div> framework, reverse-prompt induction, retrieval-augmented reasoning synthesis, dual-stage reward-guided filtering, structured inference<br />
Summary: The Less is More approach, which placed third in the XLLM@ACL2025 Shared Task-III, focuses on structured reasoning using only 24 labeled examples. It employs a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis, and dual-stage reward-guided filtering to improve structure reasoning quality. The approach leverages a unified LoRA+ setup and fine-tunes all modules from Meta-Llama-3-8B-Instruct. By combining structure validation with reward filtering across few-shot and zero-shot prompts, the pipeline consistently enhances structured inference under low-resource constraints. This highlights the importance of controllable data distillation in improving structured reasoning quality. The code for this approach is available at https://github.com/Jiahao-Yuan/Less-is-More.<br /> <div>
arXiv:2504.16408v1 Announce Type: new 
Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-the-Box Conditional Text Embeddings from Large Language Models</title>
<link>https://arxiv.org/abs/2504.16411</link>
<guid>https://arxiv.org/abs/2504.16411</guid>
<content:encoded><![CDATA[
<div> Keywords: Conditional text embedding, PonTE, unsupervised, large language model, interpretability

Summary:
PonTE is an unsupervised conditional text embedding method that utilizes a causal large language model and a conditional prompt. It aims to capture the shift in perspective on texts when conditioned on a specific aspect without the need for extensive training data or fine-tuning models. Through experiments on conditional semantic text similarity and text clustering, PonTE has been shown to generate useful conditional text embeddings and achieve performance comparable to supervised methods. The method also demonstrates interpretability of text embeddings by analyzing word generation following prompts and embedding visualization. Overall, PonTE offers a cost-effective and efficient approach to capturing conditional text embeddings without the need for labor-intensive and resource-consuming fine-tuning processes. 

<br /><br />Summary: 
PonTE is an unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt to capture the shift in perspective on texts. It generates useful conditional text embeddings and achieves performance comparable to supervised methods without fine-tuning. The method showcases interpretability through word generation analysis and embedding visualization, offering a cost-effective approach to conditional text embedding. <div>
arXiv:2504.16411v1 Announce Type: new 
Abstract: Conditional text embedding is a proposed representation that captures the shift in perspective on texts when conditioned on a specific aspect. Previous methods have relied on extensive training data for fine-tuning models, leading to challenges in terms of labor and resource costs. We propose PonTE, a novel unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt. Through experiments on conditional semantic text similarity and text clustering, we demonstrate that PonTE can generate useful conditional text embeddings and achieve performance comparable to supervised methods without fine-tuning. We also show the interpretability of text embeddings with PonTE by analyzing word generation following prompts and embedding visualization.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study</title>
<link>https://arxiv.org/abs/2504.16414</link>
<guid>https://arxiv.org/abs/2504.16414</guid>
<content:encoded><![CDATA[
<div> benchmark, compositional reasoning, large language models, chemistry domain, knowledge graph

Summary: 
This study presents a new benchmark for evaluating the compositional reasoning abilities of large language models in the field of chemistry. The researchers developed a fully automated pipeline that integrates OpenAI reasoning models with named entity recognition systems to extract chemical entities from literature and create a comprehensive knowledge graph. The experiments conducted on this benchmark reveal that even state-of-the-art models struggle with multi-hop compositional reasoning tasks. The results emphasize the importance of augmenting language models with document retrieval to improve performance. However, perfect retrieval accuracy does not eliminate all reasoning errors, highlighting the complexity of compositional reasoning tasks. This work not only showcases the limitations of current language models but also introduces a novel data generation pipeline that can be applied to various domains, advancing our understanding of reasoning in computational linguistics.

Summary: <div>
arXiv:2504.16414v1 Announce Type: new 
Abstract: In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark</title>
<link>https://arxiv.org/abs/2504.16427</link>
<guid>https://arxiv.org/abs/2504.16427</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal language analysis, large language models, cognitive-level semantics, benchmark, evaluation

Summary: 
Multimodal language analysis is a growing field that explores the use of multiple modalities to understand human conversational utterances at a deeper level. To address the lack of research on the capability of multimodal large language models (MLLMs) in comprehending cognitive-level semantics, the MMLA benchmark was introduced. This benchmark consists of over 61K multimodal utterances from various scenarios, focusing on six key dimensions of multimodal semantics. The study evaluated eight different branches of LLMs and MLLMs using three different methods. Results showed that even fine-tuned models achieve only moderate accuracy, highlighting the current limitations in understanding complex human language. The MMLA benchmark aims to provide a foundation for further exploration of large language models in multimodal language analysis and offers valuable resources for advancing this field.

<br /><br />Summary: <div>
arXiv:2504.16427v1 Announce Type: new 
Abstract: Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records</title>
<link>https://arxiv.org/abs/2504.16448</link>
<guid>https://arxiv.org/abs/2504.16448</guid>
<content:encoded><![CDATA[
<div> Keywords: medical consultation dialogues, structured electronic medical records, LoRA fine-tuning, natural language processing models, information extraction benchmark<br />
Summary:<br />
The article introduces EMRModel, an innovative approach that combines LoRA-based fine-tuning with code-style prompt design to convert medical consultation dialogues into structured electronic medical records (EMRs). A high-quality dataset of medical consultation dialogues with detailed annotations is created to train and evaluate the model. A fine-grained evaluation benchmark is introduced for medical consultation information extraction, along with a systematic evaluation methodology for optimizing medical NLP models. Experimental results demonstrate that EMRModel achieves an F1 score of 88.1%, significantly outperforming standard pre-trained models and traditional LoRA fine-tuning methods. This showcases the effectiveness of EMRModel in extracting structured medical record information from unstructured dialogues.<br /><br />Summary: <div>
arXiv:2504.16448v1 Announce Type: new 
Abstract: Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16460</link>
<guid>https://arxiv.org/abs/2504.16460</guid>
<content:encoded><![CDATA[
<div> Vectorization, Telecom, Natural Language Processing, Domain-Specific, NetoAI

Summary: 
NetoAI introduces T-VEC, a specialized vectorization model for the telecom industry. T-VEC is fine-tuned from the gte-Qwen2-1.5B-instruct model with a triplet loss objective on telecom-specific data, resulting in deep integration of domain knowledge. Weight difference analysis validates the extensive modifications across 338 layers of the base model. The open-sourcing of a telecom-specific tokenizer enhances handling of industry jargon. T-VEC outperforms established models with a leading MTEB score and superior performance on a proprietary benchmark, showcasing a strong grasp of industry nuances. This work solidifies NetoAI's position as a leader in telecom AI innovation, offering the community a powerful, open-source tool. 

<br /><br />Summary: <div>
arXiv:2504.16460v1 Announce Type: new 
Abstract: The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining</title>
<link>https://arxiv.org/abs/2504.16511</link>
<guid>https://arxiv.org/abs/2504.16511</guid>
<content:encoded><![CDATA[
<div> optimization, data selection, language models, quality, diversity
Summary:<br />
The paper introduces the QuaDMix framework, which aims to optimize the data distribution for large language model pretraining by balancing quality and diversity. It proposes criteria for measuring data quality and employs domain classification for assessing overall dataset diversity. QuaDMix utilizes a parameterized data sampling function to determine the sampling probability of each data point based on quality and diversity labels. Simulated experiments using LightGBM for parameters searching show that QuaDMix outperforms independent strategies for quality and diversity, leading to an average performance improvement of 7.2% across multiple benchmarks. The study emphasizes the importance of balancing data quality and diversity in training language models. <br />Summary: <div>
arXiv:2504.16511v1 Announce Type: new 
Abstract: Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers for Complex Query Answering over Knowledge Hypergraphs</title>
<link>https://arxiv.org/abs/2504.16537</link>
<guid>https://arxiv.org/abs/2504.16537</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, complex query answering, hyper-relational graphs, transformer model, logical operations <br />
<br />
Summary: Complex Query Answering (CQA) using knowledge graphs has advanced with the introduction of hyper-relational graphs to represent real-world data. However, existing models are limited in representing relationships of varying arities. To address this, new datasets JF17k-HCQA and M-FB15k-HCQA were created, containing diverse query types with logical operations. The Logical Knowledge Hypergraph Transformer (LKHGT) model is proposed to answer knowledge hypergraph (KHG) existential first-order queries. It consists of a Projection Encoder for atomic projection and a Logical Encoder for complex operations, both with Type Aware Bias (TAB) for capturing token interactions. Experimental results demonstrate that LKHGT is a top-performing CQA method for KHG and can generalize to new query types. <br /> <div>
arXiv:2504.16537v1 Announce Type: new 
Abstract: Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression</title>
<link>https://arxiv.org/abs/2504.16574</link>
<guid>https://arxiv.org/abs/2504.16574</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, prompt compression, attention mechanism, reinforcement learning, context structuring

Summary: 
This paper introduces a novel compression framework called Prompt Importance Sampling (PIS) for large language models (LLMs). PIS dynamically compresses prompts by sampling important tokens based on attention scores of hidden states. The framework utilizes a dual-level compression mechanism, quantifying token saliency and implementing adaptive compression through reinforcement learning. It also incorporates a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple benchmarks show that PIS outperforms existing methods in compression performance. Additionally, the framework enhances reasoning efficiency by optimizing context structuring. This work contributes to advancing prompt engineering by providing a theoretical grounding and practical efficiency in managing context for LLMs. 

Summary:<br /><br />Keywords: language models, prompt compression, attention mechanism, reinforcement learning, context structuring
This paper introduces Prompt Importance Sampling (PIS) for large language models, a compression framework that samples important tokens based on attention scores. PIS utilizes a dual-level compression mechanism, including reinforcement learning for adaptive compression. It also incorporates sentence-level importance sampling. Evaluation results demonstrate superior compression performance compared to existing methods. The framework also enhances reasoning efficiency by optimizing context structuring. This work contributes to prompt engineering by offering theoretical grounding and practical efficiency for managing context in LLMs. <div>
arXiv:2504.16574v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study</title>
<link>https://arxiv.org/abs/2504.16601</link>
<guid>https://arxiv.org/abs/2504.16601</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, traditional machine translation, medical consultation summaries, evaluation, clinical relevance <br />
<br />
Summary: 
This study compared the performance of large language models (LLMs) and traditional machine translation (MT) tools in translating medical consultation summaries from English to Arabic, Chinese, and Vietnamese. The results indicated that traditional MT tools generally outperformed LLMs, especially for complex texts. LLMs showed promise in translating simpler summaries, particularly in Vietnamese and Chinese. Arabic translations improved with complexity due to the language's morphology. However, LLMs still lack consistency, and the current evaluation metrics do not accurately capture clinical relevance. The study emphasizes the importance of domain-specific training, enhanced evaluation methods, and human oversight in medical translation. Overall, while LLMs offer contextual flexibility, improvements are needed for their effectiveness in translating medical texts accurately. <br /><br />Summary: <div>
arXiv:2504.16601v1 Announce Type: new 
Abstract: This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Large Language Models, Conspiracy theories, Psychological research, GPT-4o

Summary: 
Large Language Models (LLMs) like GPT-4o, Llama 3, and Mistral have the potential to help counter harmful online content such as conspiracy theories. However, the effectiveness of these models in applying expert-crafted counterspeech strategies remains under-researched. The study found that these models often generate generic, repetitive, or superficial responses when provided with structured prompts derived from psychological research. They also tend to over-acknowledge fear and frequently hallucinate facts, sources, or figures, which could pose challenges in practical applications for countering conspiracy theories. This research highlights the need for further investigation and development to improve the ability of LLMs to effectively address conspiracy theories with expert-driven counterspeech.<br /><br />Summary: <div>
arXiv:2504.16604v1 Announce Type: new 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2504.16627</link>
<guid>https://arxiv.org/abs/2504.16627</guid>
<content:encoded><![CDATA[
<div> retrieve, fact-checked claims, monolingual, crosslingual, disinformation <br />
<br />
The article addresses the challenge of retrieving previously fact-checked claims in both monolingual and crosslingual settings to combat the spread of disinformation. The proposed approach involves a two-stage strategy utilizing a baseline retrieval system with a fine-tuned embedding model and an LLM-based reranker. The key innovation lies in the use of LLM-based translation to overcome obstacles in multilingual information retrieval. Furthermore, the focus is on ensuring that the majority of the pipeline can be replicated on a consumer GPU. The integrated system achieved high success scores of 0.938 and 0.81025 on monolingual and crosslingual test sets, respectively. <br /><br />Summary: <div>
arXiv:2504.16627v1 Announce Type: new 
Abstract: We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics</title>
<link>https://arxiv.org/abs/2504.16677</link>
<guid>https://arxiv.org/abs/2504.16677</guid>
<content:encoded><![CDATA[
<div> transfer, dynamics, language models, multilingual data, post-training<br />
<br />
This study explores the dynamics of cross-lingual transfer in large language models that are fine-tuned on multilingual data. By investigating different post-training settings and tasks of varying complexity, the study examines how cross-lingual transfer and multilingual performance are influenced. The research includes two model families with up to 35B parameters trained on tasks like summarization, instruction following, and mathematical reasoning in both single-task and multi-task instruction tuning scenarios. The results show that the effectiveness of cross-lingual transfer cannot be explained by isolated variables alone and varies depending on the post-training settings. The study identifies the conditions that promote successful cross-lingual transfer in practice, shedding light on the intricate dynamics at play in enabling large language models to be useful across different languages.<br /><br />Summary: <div>
arXiv:2504.16677v1 Announce Type: new 
Abstract: In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations</title>
<link>https://arxiv.org/abs/2504.16754</link>
<guid>https://arxiv.org/abs/2504.16754</guid>
<content:encoded><![CDATA[
<div> transformer, memory architecture, coherent dialogues, factual recall accuracy, privacy-aware conversational AI
Summary:
Compact Memory and Vector Memory are combined in HEMA to create a dual-memory system for maintaining coherent dialogues in large language models. Experimental results show significant improvements in factual recall accuracy and human-rated coherence. Vector Memory with 10K indexed chunks achieves high precision and recall rates, outperforming summarization-only approaches. Ablation studies reveal the benefits of age-weighted pruning for reducing retrieval latency and the use of a two-level summary hierarchy to prevent errors in long conversations. HEMA's approach of combining verbatim recall with semantic continuity enables privacy-aware conversational AI capable of month-long dialogues without the need for model retraining. <div>
arXiv:2504.16754v1 Announce Type: new 
Abstract: Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Effective are Generative Large Language Models in Performing Requirements Classification?</title>
<link>https://arxiv.org/abs/2504.16768</link>
<guid>https://arxiv.org/abs/2504.16768</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based large language models, requirements engineering, generative LLMs, requirements classification, experimental study

Summary:
In recent years, transformer-based large language models (LLMs) have been widely used in natural language processing, including requirements engineering (RE) tasks like trace-link detection and regulatory compliance. While non-generative LLMs such as BERT have been successful in requirements classification, there has been limited exploration of generative LLMs. This study evaluates the performance of generative LLMs (Bloom, Gemma, Llama) in binary and multi-class requirements classification using three datasets. The results highlight the importance of factors like prompt design and LLM architecture, as well as the impact of dataset variations on classification task complexity. The findings suggest the need to optimize prompt structures and align model architectures with specific task requirements for improved performance. The insights from this study can guide future model development and deployment strategies in requirements engineering. 

<br /><br />Summary: <div>
arXiv:2504.16768v1 Announce Type: new 
Abstract: In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Framework for AI Systems in "the Wild"</title>
<link>https://arxiv.org/abs/2504.16778</link>
<guid>https://arxiv.org/abs/2504.16778</guid>
<content:encoded><![CDATA[
<div> Evaluation, GenAI systems, real world, holistic, continuous<br />
Summary:<br />
This white paper discusses the shortcomings of current evaluation methods for Generative AI (GenAI) models and proposes a comprehensive framework for assessing real-world GenAI systems. It emphasizes the importance of diverse and evolving inputs, as well as the need for holistic, dynamic, and ongoing assessment approaches. The paper provides guidance for practitioners on designing evaluation methods that accurately reflect real-time capabilities, and offers recommendations for policymakers to focus on societal impacts rather than fixed performance metrics. It advocates for frameworks that integrate performance, fairness, and ethics, and suggests the use of continuous, outcome-oriented assessment methods that combine human and automated evaluations. Transparency is highlighted as crucial for building trust among stakeholders. By implementing these strategies, GenAI models can be both technically proficient and ethically responsible, ensuring their overall impact on society is positive. <br /> <div>
arXiv:2504.16778v1 Announce Type: new 
Abstract: Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores</title>
<link>https://arxiv.org/abs/2504.16786</link>
<guid>https://arxiv.org/abs/2504.16786</guid>
<content:encoded><![CDATA[
<div> Keywords: token-classification, long-context compression, outlier scores, BERT-based compressor, resource-constrained environments

Summary:
MOOSComp is a proposed method for long-context compression in language models, addressing challenges in inference time and resource consumption. It enhances a BERT-based compressor by tackling the over-smoothing issue and incorporating outlier scores to preserve critical tokens. In the training phase, an inter-class cosine similarity loss term is added to improve token classification accuracy. During compression, outlier scores are used to retain rare but important tokens that may be discarded in traditional compression methods. The method demonstrates superior performance on long-context understanding and reasoning benchmarks at various compression ratios. It also achieves a 3.3x speedup at a 4x compression ratio on a resource-constrained mobile device. This approach shows promise in improving the efficiency and effectiveness of language model compression techniques. 

Summary: <div>
arXiv:2504.16786v1 Announce Type: new 
Abstract: Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credible plan-driven RAG method for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2504.16787</link>
<guid>https://arxiv.org/abs/2504.16787</guid>
<content:encoded><![CDATA[
<div> Keyword: Multi-hop question answering, Retrieval-Augmented Generation, Plan-then-Act-and-Review (PAR RAG) framework, reasoning paths, error propagation

Summary:
The article introduces the Plan-then-Act-and-Review (PAR RAG) framework for multi-hop question answering, which addresses the challenge of error propagation in complex queries. The framework consists of three stages: planning, act, and review, providing an interpretable and incremental reasoning paradigm to enhance accuracy and reliability in answering multi-hop questions. PAR RAG decomposes queries into logical reasoning paths and incorporates a plan execution mechanism based on multi-granularity verification to adjust intermediate results. This approach prevents deviations in reasoning paths and errors from accumulating, ensuring the accuracy of the entire reasoning process. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework outperforms existing state-of-the-art methods in EM and F1 scores.
<br /><br />Summary: <div>
arXiv:2504.16787v1 Announce Type: new 
Abstract: Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error propagation.PAR RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention</title>
<link>https://arxiv.org/abs/2504.16795</link>
<guid>https://arxiv.org/abs/2504.16795</guid>
<content:encoded><![CDATA[
<div> Hierarchical Sparse Attention, RNNs, Transformers, long sequences, attention mechanisms
<br />
Summary:
Hierarchical Sparse Attention (HSA) is proposed to enhance RNNs by allowing long-range random access flexibility while maintaining efficiency. HSA divides inputs into chunks, hierarchically aggregating information and learning token-to-chunk relevance for precise chunk selection. A hardware-aligned kernel design makes HSA efficient. Combining HSA with Mamba results in RAMba, achieving perfect accuracy in passkey retrieval despite pre-training on short contexts. RAMba shows significant improvements on downstream tasks with consistent memory usage, indicating potential in long-context modeling. <div>
arXiv:2504.16795v1 Announce Type: new 
Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-assisted Graph-RAG Information Extraction from IFC Data</title>
<link>https://arxiv.org/abs/2504.16813</link>
<guid>https://arxiv.org/abs/2504.16813</guid>
<content:encoded><![CDATA[
<div> IFC data, Graph-RAG, LLMs, building information standard, construction industry
<br />
Utilizing Graph Retrieval-Augmented Generation (Graph-RAG), this research investigates the parsing of complex IFC data using generative Large Language Models (LLMs) like GPT-4o. The goal is to extract building object properties and their relationships from the intricate hierarchy of IFC data. Despite the complexity of IFC data, the Graph-RAG parsing approach proves effective in enhancing LLMs with graph-based knowledge, allowing for natural language query-response retrieval without the need for a complicated pipeline. The study showcases how LLMs can leverage Graph-RAG to improve the understanding and utilization of IFC data within the construction industry, highlighting the potential for streamlining collaborative work processes. 
<br /><br />Summary: <div>
arXiv:2504.16813v1 Announce Type: new 
Abstract: IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning</title>
<link>https://arxiv.org/abs/2504.16832</link>
<guid>https://arxiv.org/abs/2504.16832</guid>
<content:encoded><![CDATA[
<div> reasoning model, Vietnamese, GreenMind-Medium-14B-R1, Group Relative Policy Optimization, Sentence Transformer-based models <br />
Summary: Chain-of-Thought (CoT) introduces GreenMind-Medium-14B-R1, a Vietnamese reasoning model fine-tuned using Group Relative Policy Optimization. The model utilizes a Vietnamese synthesized reasoning dataset and implements two reward functions to address language mixing and ensure factual correctness. Experimental results on the VLSP 2023 Challenge dataset prove the model's superiority over previous methods and its enhanced linguistic consistency. Evaluation on SeaExam, a multilingual multiple-choice dataset, highlights the model's effectiveness compared to few-shot prompting techniques. The model's innovative approach showcases significant advancements in tackling LLM tasks that require intermediate reasoning steps prior to generating final answers. <div>
arXiv:2504.16832v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Planning with Large Language Model for Text-Based Game Agents</title>
<link>https://arxiv.org/abs/2504.16855</link>
<guid>https://arxiv.org/abs/2504.16855</guid>
<content:encoded><![CDATA[
<div> Keywords: text-based games, Monte Carlo Tree Search, reinforcement learning, Large Language Models, language understanding

Summary:
The paper introduces the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm for text-based games. Traditional planning-then-learning paradigms like Monte Carlo Tree Search (MCTS) are time-consuming and lack language understanding and reasoning abilities. MC-DML combines Large Language Models (LLMs) with tree search algorithms to leverage language understanding and reasoning capabilities. The algorithm enhances LLMs with memory mechanisms for learning from past experiences and dynamically adjusting action evaluations during planning. Experiments on text-based games show that MC-DML outperforms other methods in the initial planning phase, demonstrating its effectiveness in language-grounded planning in complex environments. This research opens the door to more efficient language-driven planning in autonomous agents. 

<br /><br />Summary: <div>
arXiv:2504.16855v1 Announce Type: new 
Abstract: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification</title>
<link>https://arxiv.org/abs/2504.16856</link>
<guid>https://arxiv.org/abs/2504.16856</guid>
<content:encoded><![CDATA[
<div> data synthesis, sentiment analysis, large language models, emotion understanding, Emo Pillars  

Summary:
The article introduces a novel approach to address the limitations of sentiment analysis datasets lacking context and emotion categories. By leveraging a large language model, Mistral-7b, the researchers design a data synthesis pipeline to generate training examples for lightweight BERT-type encoder models. The focus is on increasing the semantic diversity of examples and grounding the generation in a corpus of narratives to produce unique story-character-centered utterances across 28 emotion classes. Through extensive inferences, the dataset of 100K contextual and 300K context-less examples is created. Fine-tuning pre-trained encoders results in the development of Emo Pillars models, which demonstrate high adaptability to new domains and task-specific tuning. The models achieve state-of-the-art performance in sentiment analysis tasks like GoEmotions, ISEAR, and IEMOCAP. Validation through statistical analysis and human evaluation confirms the success of measures in utterance diversification and context personalization, while highlighting the need for improved handling of out-of-taxonomy labels within the pipeline. 

<br /><br />Summary: <div>
arXiv:2504.16856v1 Announce Type: new 
Abstract: Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Diffusion Models for Target-Oriented Dialogue Systems</title>
<link>https://arxiv.org/abs/2504.16858</link>
<guid>https://arxiv.org/abs/2504.16858</guid>
<content:encoded><![CDATA[
<div> Dialogue Planning, Target-Oriented Dialogue, LLM era, Non-Sequential Planning, Diffusion Models <br />
Summary:<br />
The article introduces DiffTOD, a novel framework for Target-Oriented Dialogue planning in the era of Large Language Models (LLM). It addresses the limitations of existing sequential planning methods by leveraging diffusion models to enable non-sequential planning. DiffTOD formulates dialogue planning as a trajectory generation problem with guidance and utilizes a diffusion language model to estimate dialogue trajectory likelihood. It introduces tailored guidance mechanisms for different target types to optimize action strategies in diverse TOD scenarios. Through extensive experiments, DiffTOD demonstrates non-myopic lookahead exploration and effective optimization of action strategies over long horizons. The framework showcases flexibility and strong performance across complex dialogue settings. The code and data are available for accessibility. <br /> <div>
arXiv:2504.16858v1 Announce Type: new 
Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models know who did what to whom?</title>
<link>https://arxiv.org/abs/2504.16884</link>
<guid>https://arxiv.org/abs/2504.16884</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, understanding, thematic roles, sentence representations, attention heads

Summary:
Large Language Models (LLMs) are criticized for lacking language understanding, focusing on cognitive abilities different from language processing. This study investigates whether LLMs can capture thematic roles in sentences, linked closely to language. Two experiments analyze sentence representations in four LLMs. Contrary to human judgments, LLMs prioritize syntactic similarity over agent and patient assignments' accuracy. Thematic role information doesn't seem prominent in hidden units but is detected in some attention heads consistently. While LLMs can extract thematic roles, they represent them weaker compared to humans. Thematic roles are captured independently of syntax by attention heads, suggesting the potential for LLMs to understand this aspect of language, though less prominently than expected. 

<br /><br />Summary: <div>
arXiv:2504.16884v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text</title>
<link>https://arxiv.org/abs/2504.16913</link>
<guid>https://arxiv.org/abs/2504.16913</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text, detection, language model, Chain-of-Thought reasoning, interpretability

Summary:
COT Fine-tuned is a new framework for detecting AI-generated text and identifying the responsible language model. It uses a dual-task approach, classifying text as AI-generated or human-written (Task A) and identifying the specific LLM (Task B). The model utilizes Chain-of-Thought reasoning to explain its predictions, enhancing transparency and interpretability. Experimental results show high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. The CoT reasoning process significantly contributes to the model's effectiveness and interpretability. Overall, COT Fine-tuned offers a promising solution for detecting AI-generated text and attributing it to specific language models.`<br /><br />Summary:` <div>
arXiv:2504.16913v1 Announce Type: new 
Abstract: In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization, Natural Language Processing, Artificial Intelligence, Multi-agent Collaboration, Performance Improvement

Summary: 
OptimAI is a framework that uses AI agents powered by LLM to solve optimization problems described in natural language, outperforming existing methods. It consists of a formulator for translating natural language to mathematical formulations, a planner for high-level solution strategies, and coder and code critic roles for interaction and feedback. Ablation studies show the importance of all roles, with removing planner or code critic resulting in productivity drops. Incorporating UCB-based debug scheduling for plan switching enhances productivity. The framework emphasizes multi-agent collaboration for exploring diverse models within a unified system. Its accuracy on NLP4LP dataset and Optibench subset demonstrates significant error rate reduction compared to prior results. <div>
arXiv:2504.16918v1 Announce Type: new 
Abstract: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IberBench: LLM Evaluation on Iberian Languages</title>
<link>https://arxiv.org/abs/2504.16921</link>
<guid>https://arxiv.org/abs/2504.16921</guid>
<content:encoded><![CDATA[
<div> evaluation, language models, benchmark, NLP tasks, Iberian Peninsula

Summary:<br />
- Large Language Models (LLMs) are challenging to evaluate comprehensively, especially in languages other than English.
- Existing benchmarks are primarily English-centric and overlook the diversity of language varieties.
- IberBench is a new benchmark designed to assess LLM performance on fundamental and industry-relevant NLP tasks in languages spoken across the Iberian Peninsula and Ibero-America.
- The benchmark integrates 101 datasets covering 22 task categories and allows for continual updates and community-driven submissions.
- Evaluating 23 LLMs of various sizes, the study reveals that LLMs perform better in fundamental NLP tasks than in industrial tasks, with varying performance across different languages and tasks. <div>
arXiv:2504.16921v1 Announce Type: new 
Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Speech, Semantic Competence, and AI</title>
<link>https://arxiv.org/abs/2504.16092</link>
<guid>https://arxiv.org/abs/2504.16092</guid>
<content:encoded><![CDATA[
<div> knowledge, cooperative speech, respect, language models, semantic competence
Summary:
Cooperative speech aims at transmitting knowledge with respect, entailing a mutual obligation to reciprocate respect. However, large language models (LLMs) do not exhibit the necessary respect of cooperative interlocutors. This lack of respect suggests that LLMs are not capable of making assertions, calling into question their semantic competence. Moreover, the discussion on meaning goes beyond cognitive psychology to include moral psychology. In essence, the absence of reciprocal respect in LLMs challenges their role in cooperative communication and raises doubts about their ability to convey true knowledge through language. <div>
arXiv:2504.16092v1 Announce Type: cross 
Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial purpose is the transmission of knowledge. Cooperative speakers care about getting things right for their conversational partners. This attitude is a kind of respect. Cooperative speech is an ideal form of communication because participants have respect for each other. And having respect within a cooperative enterprise is sufficient for a particular kind of moral standing: we ought to respect those who have respect for us. Respect demands reciprocity. I maintain that large language models aren't owed the kind of respect that partly constitutes a cooperative conversation. This implies that they aren't cooperative interlocutors, otherwise we would be obliged to reciprocate the attitude. Leveraging this conclusion, I argue that present-day LLMs are incapable of assertion and that this raises an overlooked doubt about their semantic competence. One upshot of this argument is that knowledge of meaning isn't just a subject for the cognitive psychologist. It's also a subject for the moral psychologist.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</title>
<link>https://arxiv.org/abs/2504.16112</link>
<guid>https://arxiv.org/abs/2504.16112</guid>
<content:encoded><![CDATA[
<div> Keywords: attention layer, Transformer-based LLMs, High-bandwidth Processing Unit, memory-intensive co-processor, GPU offloading

Summary: 
The paper introduces the concept of a High-bandwidth Processing Unit (HPU) designed to address inefficiencies in GPU systems caused by the attention layer in Transformer-based Large Language Models (LLMs). The HPU serves as a memory-intensive co-processor that offloads memory-bound operations from the GPU, allowing it to focus on compute-intensive tasks, ultimately improving overall efficiency. By implementing the HPU as an add-on card to the GPU system, it can scale out to meet the growing memory demands of large batch sizes and extended sequence lengths. The prototype HPU, utilizing PCIe-based FPGA cards, demonstrated significant performance gains of up to 4.1x and energy efficiency improvements of 4.6x compared to a GPU-only system. This GPU-HPU heterogeneous system offers scalability without the need for additional GPUs, making it a promising solution for enhancing the performance of large-batched LLM inference. 

<br /><br />Summary: <div>
arXiv:2504.16112v1 Announce Type: cross 
Abstract: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval</title>
<link>https://arxiv.org/abs/2504.16121</link>
<guid>https://arxiv.org/abs/2504.16121</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, regulatory documents, bilingual question-answering, Retrieval Augmented Generation, Bangladesh Police Gazettes

Summary:
Our study focuses on bridging the gap in utilizing NLP and computational linguistic techniques in legal and regulatory tasks, particularly in analyzing the bilingual Bangladesh Police Gazettes. We introduce an efficient bilingual question-answering framework that leverages modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. Our advanced RAG-based approach significantly improves retrieval performance, resulting in more precise answers and making legal information more accessible. By evaluating both our proposed and conventional RAG systems on a diverse test set of Bangladesh Police Gazettes, we consistently outperform existing methods across all evaluation metrics. This framework not only streamlines the process of searching for specific government legal notices but also showcases the potential for NLP in the legal and regulatory domain. 

<br /><br />Summary: <div>
arXiv:2504.16121v1 Announce Type: cross 
Abstract: Natural Language Processing (NLP) and computational linguistic techniques are increasingly being applied across various domains, yet their use in legal and regulatory tasks remains limited. To address this gap, we develop an efficient bilingual question-answering framework for regulatory documents, specifically the Bangladesh Police Gazettes, which contain both English and Bangla text. Our approach employs modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. In addition to conventional RAG pipelines, we propose an advanced RAG-based approach that improves retrieval performance, leading to more precise answers. This system enables efficient searching for specific government legal notices, making legal information more accessible. We evaluate both our proposed and conventional RAG systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that our approach consistently outperforms existing methods across all evaluation metrics.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends</title>
<link>https://arxiv.org/abs/2504.16134</link>
<guid>https://arxiv.org/abs/2504.16134</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic safety, Multimodal Large Language Models, Advanced Driver-Assistance Systems, adversarial robustness, scene understanding

Summary: 
Multimodal Large Language Models (MLLMs) have the potential to revolutionize traffic safety by integrating cross-modal data for holistic scene understanding. This review explores how MLLMs can enhance perception, decision-making, and adversarial robustness in dynamic real-world scenarios, where traditional Advanced Driver-Assistance Systems often struggle. By leveraging datasets such as KITTI, DRAMA, and ML4RoadSafety, researchers are advancing the field to improve road safety. Future directions include real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, scalable and context-aware solutions can be developed to proactively mitigate risks on the road. <div>
arXiv:2504.16134v1 Announce Type: cross 
Abstract: Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design</title>
<link>https://arxiv.org/abs/2504.16204</link>
<guid>https://arxiv.org/abs/2504.16204</guid>
<content:encoded><![CDATA[
<div> framework, prompt engineering, AI systems, societal values, ethical considerations <br />
Summary: Responsible prompt engineering is crucial for ensuring that generative AI systems serve society's needs while minimizing potential harms. This article introduces a comprehensive framework consisting of prompt design, system selection, system configuration, performance evaluation, and prompt management. By embedding ethical and legal considerations directly into AI interactions, organizations can promote improved societal outcomes and mitigate risks. The balance between technical precision and ethical consciousness is key in responsible prompt engineering. This approach aligns with "Responsibility by Design" principles, integrating ethical considerations into the implementation process. Real-world and emerging practices demonstrate the significance of responsible prompt engineering as a bridge between AI development and deployment. The article also highlights key research directions and practical guidelines for advancing the field.<br /><br /> <div>
arXiv:2504.16204v1 Announce Type: cross 
Abstract: Responsible prompt engineering has emerged as a critical framework for ensuring that generative artificial intelligence (AI) systems serve society's needs while minimizing potential harms. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. This article examines how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of real-world and emerging practices, the article illustrates how responsible prompt engineering serves as a crucial bridge between AI development and deployment, enabling organizations to fine-tune AI outputs without modifying underlying model architectures. This approach aligns with broader "Responsibility by Design" principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Phonemes in cascaded S2S translation pipeline</title>
<link>https://arxiv.org/abs/2504.16234</link>
<guid>https://arxiv.org/abs/2504.16234</guid>
<content:encoded><![CDATA[
<div> phonemes, multilingual, speech-to-speech translation, sequence-to-sequence model, BLEU metric

Summary:<br /><br />This paper explores using phonemes instead of text-based language representations in a multilingual speech-to-speech translation system. The study trained a sequence-to-sequence model on the WMT17 dataset in two formats: standard textual representation and phonemic representation. Performance was evaluated using the BLEU metric, showing that the phonemic approach offers comparable quality with advantages such as lower resource requirements and better suitability for low-resource languages. This suggests that incorporating phonemes into translation pipelines can be a viable alternative to traditional text-based methods, particularly for languages with limited resources. <div>
arXiv:2504.16234v1 Announce Type: cross 
Abstract: This paper explores the idea of using phonemes as a textual representation within a conventional multilingual simultaneous speech-to-speech translation pipeline, as opposed to the traditional reliance on text-based language representations. To investigate this, we trained an open-source sequence-to-sequence model on the WMT17 dataset in two formats: one using standard textual representation and the other employing phonemic representation. The performance of both approaches was assessed using the BLEU metric. Our findings shows that the phonemic approach provides comparable quality but offers several advantages, including lower resource requirements or better suitability for low-resource languages.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents</title>
<link>https://arxiv.org/abs/2504.16264</link>
<guid>https://arxiv.org/abs/2504.16264</guid>
<content:encoded><![CDATA[
<div> dataset, cross-lingual information retrieval, academic search, retrieval methods, multilingual retrievers <br />
<br />
Summary: <br />
The paper introduces CLIRudit, a dataset for evaluating cross-lingual academic search between English queries and French documents. Various zero-shot retrieval methods were benchmarked on the dataset, revealing that large dense retrievers can achieve comparable performance to human translations without using machine translation. Sparse retrievers, like BM25 or SPLADE, combined with document translation, also show competitive results. This research enhances understanding of cross-lingual academic information retrieval and offers a framework for creating similar datasets in different languages and disciplines. The dataset and code are made publicly available to facilitate further research on improving access to scientific knowledge across language barriers. <br /> <div>
arXiv:2504.16264v1 Announce Type: cross 
Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant documents in a language that differs from the language of the queries. This paper presents CLIRudit, a new dataset created to evaluate cross-lingual academic search, focusing on English queries and French documents. The dataset is built using bilingual article metadata from \'Erudit, a Canadian publishing platform, and is designed to represent scenarios in which researchers search for scholarly content in languages other than English. We perform a comprehensive benchmarking of different zero-shot first-stage retrieval methods on the dataset, including dense and sparse retrievers, query and document machine translation, and state-of-the-art multilingual retrievers. Our results show that large dense retrievers, not necessarily trained for the cross-lingual retrieval task, can achieve zero-shot performance comparable to using ground truth human translations, without the need for machine translation. Sparse retrievers, such as BM25 or SPLADE, combined with document translation, show competitive results, providing an efficient alternative to large dense models. This research advances the understanding of cross-lingual academic information retrieval and provides a framework that others can use to build comparable datasets across different languages and disciplines. By making the dataset and code publicly available, we aim to facilitate further research that will help make scientific knowledge more accessible across language barriers.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignX: The Foundation Model for Sign Recognition</title>
<link>https://arxiv.org/abs/2504.16315</link>
<guid>https://arxiv.org/abs/2504.16315</guid>
<content:encoded><![CDATA[
<div> framework, sign recognition, pose information, ASL signs, video processing
Summary:
SignX is a new framework for sign recognition that aims to translate ASL signs from RGB videos to English-based ID glosses using consistent glossing conventions. The framework consists of a Pose2Gloss component that combines multiple pose information sources into a single representation and a Video2Pose module that directly converts raw video into signer pose representation. SignX allows for compatibility with existing pose formats, improving sign recognition accuracy compared to previous methods. The framework's 2-stage training enables common pose estimation for sign recognition, enhancing the recognition of signs from sign language videos. <div>
arXiv:2504.16315v1 Announce Type: cross 
Abstract: The complexity of sign language data processing brings many challenges. The current approach to recognition of ASL signs aims to translate RGB sign language videos through pose information into English-based ID glosses, which serve to uniquely identify ASL signs. Note that there is no shared convention for assigning such glosses to ASL signs, so it is essential that the same glossing conventions are used for all of the data in the datasets that are employed. This paper proposes SignX, a foundation model framework for sign recognition. It is a concise yet powerful framework applicable to multiple human activity recognition scenarios. First, we developed a Pose2Gloss component based on an inverse diffusion model, which contains a multi-track pose fusion layer that unifies five of the most powerful pose information sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens Segmentation--into a single latent pose representation. Second, we trained a Video2Pose module based on ViT that can directly convert raw video into signer pose representation. Through this 2-stage training framework, we enable sign language recognition models to be compatible with existing pose formats, laying the foundation for the common pose estimation necessary for sign recognition. Experimental results show that SignX can recognize signs from sign language video, producing predicted gloss representations with greater accuracy than has been reported in prior work.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: Near-Optimal Data Attribution for Deep Learning</title>
<link>https://arxiv.org/abs/2504.16430</link>
<guid>https://arxiv.org/abs/2504.16430</guid>
<content:encoded><![CDATA[
<div> data attribution, predictive, model predictions, convex settings, metadifferentiation

Summary:
The article introduces a new data attribution method called MAGIC, designed to estimate the impact of adding or removing training data on model predictions in large-scale, non-convex settings. The goal of predictive data attribution is to accurately assess the influence of specific training datapoints on model outputs. In convex settings, existing methods like the infinitesimal jackknife provide straightforward solutions. However, these methods are less effective in non-convex scenarios, where current approaches often fall short in accurately correlating estimates with ground truth. MAGIC combines classical techniques with recent advancements in metadifferentiation to optimize the estimation of how model predictions are affected by modifications to the training data. This integration allows MAGIC to provide nearly optimal estimates and greatly improve the accuracy in large-scale, non-convex settings. <div>
arXiv:2504.16430v1 Announce Type: cross 
Abstract: The goal of predictive data attribution is to estimate how adding or removing a given set of training datapoints will affect model predictions. In convex settings, this goal is straightforward (i.e., via the infinitesimal jackknife). In large-scale (non-convex) settings, however, existing methods are far less successful -- current methods' estimates often only weakly correlate with ground truth. In this work, we present a new data attribution method (MAGIC) that combines classical methods and recent advances in metadifferentiation to (nearly) optimally estimate the effect of adding or removing training data on model predictions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title>
<link>https://arxiv.org/abs/2504.16628</link>
<guid>https://arxiv.org/abs/2504.16628</guid>
<content:encoded><![CDATA[
<div> algorithm, language models, human preferences, Pareto optimization, multiobjective alignment

Summary:
The article introduces ParetoHqD, a novel method for aligning large language models with multiple human expectations and values. It addresses issues such as inappropriate preference representations and imbalanced reward scores that limit the performance of existing algorithms. ParetoHqD represents human preferences as preference directions in the objective space and considers data near the Pareto front as "high-quality" data. It follows a two-stage supervised fine-tuning process for each preference, using individual Pareto high-quality training sets that match its preference direction. Experimental results show ParetoHqD's superiority over five baselines on two multiobjective alignment tasks, highlighting its effectiveness in aligning language models with diverse user needs. <div>
arXiv:2504.16628v1 Announce Type: cross 
Abstract: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</title>
<link>https://arxiv.org/abs/2504.16728</link>
<guid>https://arxiv.org/abs/2504.16728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hypothesis generation, Human-in-the-loop, Monte Carlo Tree Search, open-source platform

Summary: 
IRIS is an Interactive Research Ideation System that aims to enhance scientific ideation by leveraging large language models (LLMs) and incorporating a Human-in-the-loop approach. The system includes features such as adaptive test-time compute expansion through Monte Carlo Tree Search, fine-grained feedback mechanisms, and query-based literature synthesis. Researchers can utilize IRIS to generate novel hypotheses in a more transparent and controllable manner, empowering them throughout the ideation process. A user study with researchers from various disciplines validates the effectiveness of the system in improving ideation. The code for IRIS is open-source and available on GitHub, allowing researchers to benefit from its innovative features for accelerating scientific discovery. 

<br /><br />Summary: <div>
arXiv:2504.16728v1 Announce Type: cross 
Abstract: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>