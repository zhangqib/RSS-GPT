<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Position: Editing Large Language Models Poses Serious Safety Risks</title>
<link>https://arxiv.org/abs/2502.02958</link>
<guid>https://arxiv.org/abs/2502.02958</guid>
<content:encoded><![CDATA[
<div> facts, Large Language Models, knowledge editing methods, safety risks, malicious actors

Summary:<br><br>Large Language Models (LLMs) contain a vast amount of facts that can become outdated over time, leading to the development of knowledge editing methods (KEs) for making changes. However, the use of KEs poses serious safety risks that have been largely overlooked. KEs are attractive to malicious actors due to their availability, affordability, performance, and stealthiness. These actors can easily adapt KEs for various malicious purposes, exploiting vulnerabilities in the AI ecosystem that allow unchecked model updates. The lack of social and institutional awareness further exacerbates this risk. To address this issue, the community is urged to research tamper-resistant models and countermeasures against malicious model editing, as well as actively engage in securing the AI ecosystem. <div>
arXiv:2502.02958v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries</title>
<link>https://arxiv.org/abs/2506.13796</link>
<guid>https://arxiv.org/abs/2506.13796</guid>
<content:encoded><![CDATA[
<div> automated method, instruction data, climate change, Large Language Models (LLMs), ClimateChat-Corpus <br />
Summary: <br />
The study introduces an automated method for generating climate change instruction data, named ClimateChat-Corpus. Using this method, the ClimateChat model is fine-tuned, showing significant improvement in performance on climate change question-and-answer tasks. The study highlights the importance of selecting an appropriate base model for instruction tuning and demonstrates the adaptability of LLMs to a wide range of climate change scientific discovery tasks. The research fills a gap in efficiently producing high-precision instruction data for climate change, emphasizing the role of Natural Language Processing technologies in supporting decision-makers and the public amidst the global climate crisis. <div>
arXiv:2506.13796v1 Announce Type: new 
Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</title>
<link>https://arxiv.org/abs/2506.13886</link>
<guid>https://arxiv.org/abs/2506.13886</guid>
<content:encoded><![CDATA[
<div> numeral systems, large language models, linguistic-mathematical puzzles, cross-linguistic numeral systems, compositional structure  
Summary:  
Large language models struggle with solving linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can solve successfully. Models require explicit marking of mathematical operations to consistently solve such problems, unlike humans who use linguistic understanding to infer implicit numeral structure. Through experiments, it was found that LLMs lack the ability to flexibly infer compositional rules from implicit patterns in human-scale data. The linguistic and mathematical aspects of numbers in language play a crucial role in how LLMs approach numeral tasks, highlighting the challenge in modeling human-like reasoning in numerical tasks. This study underscores the need for models to incorporate linguistic understanding and infer compositional rules effectively to navigate the diversity of numeral systems across languages. <br /><br />Summary: <div>
arXiv:2506.13886v1 Announce Type: new 
Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training</title>
<link>https://arxiv.org/abs/2506.13888</link>
<guid>https://arxiv.org/abs/2506.13888</guid>
<content:encoded><![CDATA[
<div> Reinforcement Fine-Tuning, Vision-Language Reward Model, bootstrapping dilemma, modality bias, hallucination detection, multimodal reasoning <br />
<br />
Summary: 
The article introduces Reinforcement Fine-Tuning (RFT) with verifiable rewards in the context of Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) aims to align VL models by providing structured feedback, but faces challenges such as the bootstrapping dilemma and modality bias. To address these issues, an iterative training framework incorporating vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling is proposed. This approach refines preference datasets, enhances structured critiques, and improves reasoning iteratively. Experimental results on VL-RM benchmarks show improved performance in hallucination detection and multimodal reasoning, contributing to the advancement of VL model alignment using reinforcement learning. <br /> <div>
arXiv:2506.13888v1 Announce Type: new 
Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoNews: A Spoken Dialogue System for Expressive News Conversations</title>
<link>https://arxiv.org/abs/2506.13894</link>
<guid>https://arxiv.org/abs/2506.13894</guid>
<content:encoded><![CDATA[
<div> Keywords: emotional speech, spoken dialogue system, news conversations, sentiment analyzer, engagement

Summary:
Emotional speech in task-oriented spoken dialogue systems (SDS) is explored in this study to improve empathy in news conversations. A system is developed that utilizes a large language model-based sentiment analyzer to identify appropriate emotions and PromptTTS for emotional speech synthesis. The research addresses the lack of standardized evaluation metrics for social goals in emotional SDSs. The proposed emotional SDS outperformed a baseline system in emotion regulation and engagement, highlighting the importance of speech emotion in enhancing conversational engagement. This study bridges the gap between emotional TTS and SDS research, demonstrating the effectiveness of integrating emotional speech regulation in task-oriented dialogue systems for more empathetic interactions.

<br /><br />Summary: <div>
arXiv:2506.13894v1 Announce Type: new 
Abstract: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations</title>
<link>https://arxiv.org/abs/2506.13901</link>
<guid>https://arxiv.org/abs/2506.13901</guid>
<content:encoded><![CDATA[
<div> keyword: Alignment, Language models, Safety, Evaluation, Dataset
Summary:<br />
- Alignment in large language models (LLMs) is crucial in high-stakes domains like education, healthcare, etc.<br />
- Current evaluation methods have blind spots and may not accurately assess alignment.<br />
- The Alignment Quality Index (AQI) is introduced as a novel metric to assess LLM alignment by analyzing safe and unsafe activations in latent space.<br />
- AQI combines various clustering quality measures to detect misalignments, jailbreak risks, and alignment faking.<br />
- The LITMUS dataset is proposed to facilitate robust evaluation under challenging conditions.<br />
- Empirical tests show that AQI correlates with external judges and can reveal vulnerabilities missed by refusal metrics.<br /> 
Summary: <div>
arXiv:2506.13901v1 Announce Type: new 
Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection</title>
<link>https://arxiv.org/abs/2506.13956</link>
<guid>https://arxiv.org/abs/2506.13956</guid>
<content:encoded><![CDATA[
<div> data augmentation, multimodal classification, robotics, large language model, diffusion model

Summary: 
This paper introduces a novel framework for data augmentation in robotic assistance scenarios, focusing on improving intent understanding by enhancing user requests with visual cues. The approach combines dialogues and environmental imagery to generate additional data for model training. By leveraging a large language model to simulate conversations and a diffusion model to create related images, the framework aims to refine multimodal models and enhance action selection capabilities in response to user interactions. Experimental results based on real-world data show that this methodology significantly improves the robot's performance, achieving state-of-the-art results. <div>
arXiv:2506.13956v1 Announce Type: new 
Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are manual annotations necessary for statutory interpretations retrieval?</title>
<link>https://arxiv.org/abs/2506.13965</link>
<guid>https://arxiv.org/abs/2506.13965</guid>
<content:encoded><![CDATA[
<div> Keywords: legal research, interpretations, ranking, language models, automation<br />
<br />
Summary: The study focuses on improving the process of retrieving relevant interpretations of legal concepts by exploring the need for manual annotation. It investigates the optimal number of annotations per concept, the impact of annotating only the best candidates, and the potential benefits of automating the annotation process using a Language Model (LLM). By conducting various experiments, the research aims to determine the volume and scope of annotations required for effective interpretation retrieval. The results will provide insights into streamlining the manual annotation process and enhancing the performance of language models in legal research. <div>
arXiv:2506.13965v1 Announce Type: new 
Abstract: One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI shares emotion with humans across languages and cultures</title>
<link>https://arxiv.org/abs/2506.13978</link>
<guid>https://arxiv.org/abs/2506.13978</guid>
<content:encoded><![CDATA[
<div> Keywords: human-machine collaboration, emotion exchange, large language models, emotional alignment, affective outputs<br />
Summary: 

- The study focuses on the exchange of emotions between humans and AI in collaborative settings. 
- Researchers assess emotional alignment across different linguistic-cultural groups and model-families. 
- Findings suggest that LLM-derived emotion spaces align structurally with human perception, based on valence and arousal dimensions. 
- Emotion-related features accurately predict word ratings along these core dimensions, reflecting universal and language-specific patterns. 
- Using steering vectors derived from human-centric emotion concepts, the study shows that AI expressions can be guided to produce specific affective states when conveying content. 

Summary: <div>
arXiv:2506.13978v1 Announce Type: new 
Abstract: Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text</title>
<link>https://arxiv.org/abs/2506.14012</link>
<guid>https://arxiv.org/abs/2506.14012</guid>
<content:encoded><![CDATA[
<div> Keywords: Code-switching, Large Language Models, comprehension, degradation, fine-tuning

Summary:<br /><br />
Code-switching refers to the practice of switching between languages within a single conversation, common in multilingual communities and online content. Large Language Models (LLMs) used for content processing and generation often encounter code-switched text. A systematic evaluation was conducted to assess LLM comprehension of code-switched text by creating code-switched versions of reasoning and comprehension benchmarks. The study found that while comprehension may suffer when foreign tokens disrupt English text, embedding English into other languages can actually improve comprehension. Prompting produced mixed results, but fine-tuning the models proved to be a more reliable method to mitigate degradation. This research sheds light on how LLMs process and understand code-switched text, highlighting the need for further investigation and refinement in this area. 

Summary: <div>
arXiv:2506.14012v1 Announce Type: new 
Abstract: Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.14028</link>
<guid>https://arxiv.org/abs/2506.14028</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual, multimodal, financial, LLMs  
Summary:  
MultiFinBen is introduced as a new benchmark for evaluating large language models (LLMs) in the financial domain. It is the first multilingual and multimodal benchmark that assesses models across various modalities and linguistic settings. Two novel tasks, PolyFiQA-Easy and PolyFiQA-Expert, require complex reasoning over mixed-language inputs, while EnglishOCR and SpanishOCR challenge models with OCR-embedded financial QA tasks. A dynamic selection mechanism and a compact, balanced benchmark are curated to enhance the evaluation process. Evaluation of 22 state-of-the-art models shows that even strong models struggle with complex cross-lingual and multimodal tasks in the financial domain. MultiFinBen aims to promote transparent, reproducible, and inclusive progress in financial research and applications.  
<br /><br />Summary: <div>
arXiv:2506.14028v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interdisciplinary Review of Commonsense Reasoning and Intent Detection</title>
<link>https://arxiv.org/abs/2506.14040</link>
<guid>https://arxiv.org/abs/2506.14040</guid>
<content:encoded><![CDATA[
<div> Keywords: commonsense reasoning, intent detection, natural language understanding, zero-shot learning, multilingual models

Summary: 
This review delves into recent advancements in commonsense reasoning and intent detection within the field of natural language understanding. It examines 28 papers published between 2020-2025 from prominent conferences like ACL, EMNLP, and CHI, categorizing them based on methodology and application. Commonsense reasoning is explored in various contexts such as zero-shot learning, cultural adaptation, structured evaluation, and interactive settings. Intent detection is analyzed through different models including open-set models, generative formulations, clustering techniques, and human-centered systems. By blending insights from natural language processing and human-computer interaction, the review highlights emerging trends towards more adaptive, multilingual, and context-aware models. It also identifies critical gaps in grounding, generalization, and benchmark design that need to be addressed in future research. 

<br /><br />Summary: <div>
arXiv:2506.14040v1 Announce Type: new 
Abstract: This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications</title>
<link>https://arxiv.org/abs/2506.14046</link>
<guid>https://arxiv.org/abs/2506.14046</guid>
<content:encoded><![CDATA[
<div> Keywords: language difficulty, conversational text, Ace-CEFR dataset, Transformer-based models, Large Language Models

Summary: 
The article introduces the Ace-CEFR dataset, designed to evaluate the language difficulty of short, conversational text passages. This dataset is expert-annotated with corresponding text difficulty levels. Various models, including Transformer-based models and Large Language Models, were experimented with on Ace-CEFR, showing that models trained on this dataset can accurately measure text difficulty, surpassing human experts. These models also demonstrated latency appropriate for production environments. The Ace-CEFR dataset is released to the public for further research and development purposes. Overall, this study addresses the existing need to assess language difficulty in conversational text and provides a valuable resource for training and filtering Large Language Models. 

<br /><br />Summary: <div>
arXiv:2506.14046v1 Announce Type: new 
Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data</title>
<link>https://arxiv.org/abs/2506.14064</link>
<guid>https://arxiv.org/abs/2506.14064</guid>
<content:encoded><![CDATA[
<div> methodology, constituency parsing, parsing heuristics, dataset, extraction tool

Summary:
The article introduces a methodological approach for identifying and annotating naturally-occurring examples of English embedded clauses in large text data through the use of constituency parsing and parsing heuristics. The tool developed by the researchers was evaluated on the Golden Embedded Clause Set (GECS) dataset, which contains hand-annotated examples of English embedded clause sentences found in natural language. Additionally, the researchers extracted a large-scale dataset of naturally-occurring English embedded clauses from the Dolma corpus using their extraction tool. This approach allows for the analysis of syntactic and semantic features in embedded clauses using real-world language examples, as opposed to artificially created language examples typically used in current research methodologies. <div>
arXiv:2506.14064v1 Announce Type: new 
Abstract: For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstract Meaning Representation for Hospital Discharge Summarization</title>
<link>https://arxiv.org/abs/2506.14101</link>
<guid>https://arxiv.org/abs/2506.14101</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination, discharge summaries, language-based graphs, deep learning models

Summary:
The Achilles heel of Large Language Models (LLMs) is hallucination, particularly concerning automatically generating discharge summaries in the clinical domain. This poses challenges for content provenance and trustworthiness. To address this issue, a new method combining language-based graphs and deep learning models was developed. This method demonstrates impressive reliability results on the MIMIC-III corpus and clinical notes from Anonymous Hospital. By leveraging language-based graphs, the generated discharge summaries aim to assist physicians in their patient care duties and reduce documentation burdens. The method showcases the potential of utilizing advanced technologies in the medical field to enhance the efficiency and accuracy of summarization processes. Providing accessibility to both the method and its output examples signifies a step towards fostering transparency and reproducibility in research practices. 

Summary: <div>
arXiv:2506.14101v1 Announce Type: new 
Abstract: The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Essential-Web v1.0: 24T tokens of organized web data</title>
<link>https://arxiv.org/abs/2506.14111</link>
<guid>https://arxiv.org/abs/2506.14111</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, pre-training datasets, taxonomy, Essential-Web v1.0, HuggingFace<br />
<br />
Summary: Data is essential for language models to acquire skills and knowledge. However, the lack of well-organized pre-training datasets can result in costly and inaccessible data pipelines. The authors introduce Essential-Web v1.0, a massive dataset with 24 trillion tokens, each annotated with a comprehensive twelve-category taxonomy covering various aspects such as topic, format, content complexity, and quality. The taxonomy labels are generated by a fine-tuned 0.5b-parameter model, EAI-Distill-0.5b, showing high annotator agreement close to a large-scale model like Qwen2.5-32B-Instruct. By using simple SQL-style filters, the researchers were able to extract competitive web-curated datasets in fields like math, web code, STEM, and medical, outperforming existing state-of-the-art datasets in some categories. Essential-Web v1.0 is now accessible on the HuggingFace platform for further research and development. <br /><br />Summary: <div>
arXiv:2506.14111v1 Announce Type: new 
Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling from Your Language Model One Byte at a Time</title>
<link>https://arxiv.org/abs/2506.14123</link>
<guid>https://arxiv.org/abs/2506.14123</guid>
<content:encoded><![CDATA[
<div> Tokenization, language models, Prompt Boundary Problem, ensemble, proxy-tuning <br />
<br />
Summary: The article discusses the issues that tokenization can introduce distortion into the model's generations, particularly the Prompt Boundary Problem. The proposed method converts autoregressive language models with BPE tokenizers into character-level or byte-level models without altering their generative distribution. This approach effectively addresses the Prompt Boundary Problem and allows for the unification of vocabularies of language models with different tokenizers. The method enables the ensemble of language models with different tokenizers at inference time and facilitates the transfer of post-training from one model to another using proxy-tuning. Experimental results demonstrate that the ensemble and proxy-tuned models outperform individual models on downstream evaluations. <div>
arXiv:2506.14123v1 Announce Type: new 
Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization</title>
<link>https://arxiv.org/abs/2506.14157</link>
<guid>https://arxiv.org/abs/2506.14157</guid>
<content:encoded><![CDATA[
<div> Preference optimization, LLMs, response pair, DCRM, distance, reward margin <br />
Summary:<br />Recent research examines the impact of response pair differences on the effectiveness of preference optimization (PO) models. The Distance Calibrated Reward Margin (DCRM) metric combines distance and reward margin to evaluate response quality in PO. Studies reveal a correlation between higher DCRM in training sets and improved learning outcomes. A best-of-$N^2$ pairing method is proposed to select response pairs with high DCRM, leading to enhanced model performance on evaluation benchmarks such as AlpacaEval, MT-Bench, and Arena-Hard. This research sheds light on the significance of identifying and maximizing desired differences in response pairs for effective PO training. <br />Summary: <div>
arXiv:2506.14157v1 Announce Type: new 
Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models</title>
<link>https://arxiv.org/abs/2506.14158</link>
<guid>https://arxiv.org/abs/2506.14158</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, speculative sampling, syntactic coherence, semantic coherence, efficiency <br />
<br />
Summary: 
The paper introduces a new framework called Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) to enhance the efficiency of large language models (LLMs). By combining speculative sampling with multi-head drafting for rapid token generation and a continuous verification tree for candidate validation, S$^4$C shows improved efficiency and parallelism in text generation tasks. Experimental results indicate that S$^4$C outperforms existing methods on mainstream tasks and achieves an acceleration ratio of 2.26x-2.60x on Spec-bench benchmarks. The framework emphasizes the importance of maintaining syntactic and semantic coherence in text generation, leading to the generation of more valid tokens with fewer computational resources. The proposed S$^4$C framework demonstrates its ability to significantly reduce inference latency in LLMs and improve overall performance in real-time applications. <br /> <div>
arXiv:2506.14158v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind</title>
<link>https://arxiv.org/abs/2506.14161</link>
<guid>https://arxiv.org/abs/2506.14161</guid>
<content:encoded><![CDATA[
<div> Word Association Bias Test, Affective Attribution Test, Theory of Mind, Large Language Models, Bias structure

Summary:
The article introduces a new evaluation framework for assessing Theory of Mind (ToM) in Large Language Models (LLMs) by leveraging the Stereotype Content Model (SCM) to understand bias as a multi-dimensional failure in ToM. The framework includes the Word Association Bias Test (WABT) and the Affective Attribution Test (AAT) to indirectly measure implicit lexical associations and covert affective leanings, respectively. Through extensive experiments on 8 State-of-the-Art LLMs, the framework reveals complex bias structures such as pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification. This approach provides a more robust methodology for identifying the structural nature of implicit bias in LLMs, addressing the limitations of conventional direct-query methods. <div>
arXiv:2506.14161v1 Announce Type: new 
Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAM: A Generative Foundation Reward Model for Reward Generalization</title>
<link>https://arxiv.org/abs/2506.14175</link>
<guid>https://arxiv.org/abs/2506.14175</guid>
<content:encoded><![CDATA[
<div> unsupervised learning, generative models, reward models, label smoothing, pairwise ranking loss

Summary: 
This paper introduces a new approach to training reward models for large language models (LLMs) by utilizing both unlabeled and labeled data. By developing a generative reward model that is first trained through unsupervised learning and then fine-tuned through supervised learning, the authors demonstrate the effectiveness of this method. By incorporating label smoothing, they show that the optimization process is akin to a regularized pairwise ranking loss, offering a novel perspective on training reward models. The resulting foundation reward model proves to be versatile and can be applied to various tasks with minimal further adjustment. Extensive experiments validate the model's effectiveness across tasks such as response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, outperforming several strong baseline models. <div>
arXiv:2506.14175v1 Announce Type: new 
Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages</title>
<link>https://arxiv.org/abs/2506.14177</link>
<guid>https://arxiv.org/abs/2506.14177</guid>
<content:encoded><![CDATA[
<div> Keywords: code-switching, ASR, synthetic data generation, Southeast Asian languages, benchmark evaluation

Summary: 
This study explores the development of code-switching automatic speech recognition (ASR) systems for under-resourced Southeast Asian language pairs (Malay-English, Mandarin-Malay, Tamil-English) using synthetic data generation techniques. The researchers propose a phrase-level mixing method to create synthetic code-switching data that simulates natural language patterns. By fine-tuning large pretrained ASR models with a combination of monolingual and synthetic code-switching data, significant improvements in ASR performance are achieved. The experiments demonstrate that the proposed training strategy effectively enhances ASR performance on both monolingual and code-switching tests, with the Malay-English language pair showing the highest gains, followed by Tamil-English and Mandarin-Malay. This cost-effective approach provides a promising solution for developing code-switching ASR systems, which can benefit both research and industry. 

<br /><br />Summary: <div>
arXiv:2506.14177v1 Announce Type: new 
Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR</title>
<link>https://arxiv.org/abs/2506.14190</link>
<guid>https://arxiv.org/abs/2506.14190</guid>
<content:encoded><![CDATA[
<div> Keywords: code-switched ASR, synthetic data generation, asynchronous adaptation framework, large-scale web data, Whisper

Summary:
AsyncSwitch is a novel framework for developing code-switched ASR systems that addresses challenges such as language ambiguity and limited data exposure. The three-stage process involves training decoder layers on code-switched text, aligning decoder and encoder using limited speech-text data, and fully fine-tuning the model. Experiments with Whisper on Malay-English code-switching showed a 9.02% relative WER reduction and improved monolingual performance in Singlish, Malay, and other English variants. This approach leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning, enabling more effective and scalable development of ASR systems for code-switched languages. 

<br /><br />Summary: <div>
arXiv:2506.14190v1 Announce Type: new 
Abstract: Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment</title>
<link>https://arxiv.org/abs/2506.14199</link>
<guid>https://arxiv.org/abs/2506.14199</guid>
<content:encoded><![CDATA[
<div> Keywords: Literary translation, Multi-agent system, Large Language Models, Translation Quality Assessment, Nuanced framework<br />
<br />
Summary: 
The article introduces MAS-LitEval, a multi-agent system that utilizes Large Language Models (LLMs) to evaluate literary translations based on terminology, narrative, and style, aspects often overlooked by traditional metrics like BLEU and METEOR. MAS-LitEval was tested on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, with top models scoring up to 0.890 in capturing literary nuances. This novel framework offers a more nuanced approach to Translation Quality Assessment (TQA) and provides a practical tool for translators and researchers in preserving cultural nuances and stylistic elements in literary works. <div>
arXiv:2506.14199v1 Announce Type: new 
Abstract: Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations</title>
<link>https://arxiv.org/abs/2506.14200</link>
<guid>https://arxiv.org/abs/2506.14200</guid>
<content:encoded><![CDATA[
<div> education, language models, ELI-Why, explanatory answers, human studies  
Summary:  
- Language models are widely used in education but their ability to cater to learners' varied needs remains underexplored.
- ELI-Why is introduced as a benchmark of "Why" questions to evaluate language models' pedagogical capabilities.
- Two human studies were conducted to assess the utility of language model-generated explanations on the benchmark for different educational grades.
- Findings show that GPT-4-generated explanations match intended educational backgrounds only 50% of the time, compared to 79% for human-curated explanations.
- Users deemed GPT-4-generated explanations 20% less suited to their informational needs compared to lay-curated explanations, indicating limitations in the pedagogical effectiveness of language models.  

<br /><br />Summary: <div>
arXiv:2506.14200v1 Announce Type: new 
Abstract: Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation</title>
<link>https://arxiv.org/abs/2506.14203</link>
<guid>https://arxiv.org/abs/2506.14203</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, anomia, semantic paraphasia, gradient-based selective augmentation, Tip-of-the-Tongue dataset

Summary: 
Language models (LMs) are investigated for aiding patients with anomia, a difficulty in naming objects. Two challenges are identified: term failure and semantic paraphasia errors during identification. The proposed approach addresses these challenges by robustifying the model from semantic errors and enhancing it with gradient-based selective augmentation. By controlling the quality of augmented data using gradient values and including unseen but relevant terms based on gradient variance, the model shows strong performance against baselines. Evaluation on the Tip-of-the-Tongue dataset as an intermediary task and real patient data from AphasiaBank demonstrates the model's effectiveness in assisting anomia patients by overcoming term failures and semantic paraphasia errors.<br /><br />Summary: <div>
arXiv:2506.14203v1 Announce Type: new 
Abstract: In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents</title>
<link>https://arxiv.org/abs/2506.14205</link>
<guid>https://arxiv.org/abs/2506.14205</guid>
<content:encoded><![CDATA[
<div> Keywords: AgentSynth, task synthesis, trajectory datasets, LLM-based task proposer, task complexity modulation<br />
<br />
Summary: <br />
AgentSynth is a pipeline designed for automatically creating diverse and realistic tasks and trajectory datasets for computer-use agents. It leverages information asymmetry to generate simple subtasks that become challenging when composed into longer tasks. The pipeline utilizes an LLM-based task proposer and an execution agent guided by a persona to complete tasks and log trajectories. By iteratively combining subtasks and summarizing them into composite tasks, AgentSynth can modulate task complexity. Empirical evaluations demonstrate the difficulty and discriminative power of the created tasks, with state-of-the-art LLM agents showing varying success rates across different difficulty levels. Importantly, the pipeline achieves a low cost per trajectory, making it significantly more affordable than human annotations. The code and data for AgentSynth are openly available for public use. <br /> <div>
arXiv:2506.14205v1 Announce Type: new 
Abstract: We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation</title>
<link>https://arxiv.org/abs/2506.14206</link>
<guid>https://arxiv.org/abs/2506.14206</guid>
<content:encoded><![CDATA[
<div> diffusion model, generative model, tabular data, causal regularization, mixed-type data

Summary:
- Training generative AI relies heavily on high-quality data, but obtaining such data is challenging due to privacy concerns.
- Synthesizing data has become a popular solution, but generating high-quality mixed-type tabular data remains difficult.
- CausalDiffTab is introduced as a generative model designed for mixed tabular data with numerical and categorical features, capable of capturing complex variable interactions.
- A hybrid adaptive causal regularization method, utilizing Hierarchical Prior Fusion, is proposed to enhance model performance without sacrificing generative capabilities.
- Experimental results across seven datasets show that CausalDiffTab outperforms baseline methods in various metrics. The code for CausalDiffTab is publicly available. 

<br /><br />Summary: <div>
arXiv:2506.14206v1 Announce Type: new 
Abstract: Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation</title>
<link>https://arxiv.org/abs/2506.14211</link>
<guid>https://arxiv.org/abs/2506.14211</guid>
<content:encoded><![CDATA[
<div> Keywords: digitalization, linguistic strategies, influential patterns, conversation, language models

Summary:
This paper addresses the issue of detecting implicit influential patterns in conversations in the digital age. As individuals increasingly rely on digital platforms for communication, malicious actors are using verbal strategies to influence perceptions. The proposed model improves detection of these patterns by augmenting the dataset with state-of-the-art language models. The model not only detects influential elements in conversations but also identifies their specific locations. The approach results in a 6% improvement in detecting implicit influential patterns and a 33% and 43% improvement in multi-label classification tasks related to influence techniques and victim vulnerability, respectively.

<br /><br />Summary: 
- Addressing the detection of implicit influential patterns in digital conversations
- Malicious actors using verbal strategies to influence perceptions
- Augmenting dataset with language models for improved detection
- Identifying specific locations of influential elements in conversations
- 6% improvement in detecting implicit influential patterns and significant improvements in multi-label classification tasks <div>
arXiv:2506.14211v1 Announce Type: new 
Abstract: In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chaining Event Spans for Temporal Relation Grounding</title>
<link>https://arxiv.org/abs/2506.14213</link>
<guid>https://arxiv.org/abs/2506.14213</guid>
<content:encoded><![CDATA[
<div> Temporal relations, events, understanding, timeline reasoning, TRC

Summary: 
The paper introduces a novel approach, the Timeline Reasoning Network (TRN), to accurately understand temporal relations between events for tasks like temporal reading comprehension (TRC) and relation extraction (TRE). Current solutions rely on answer overlaps, which can lead to unreliable results. TRN operates in a two-step inductive reasoning process, first answering questions with semantic and syntactic information and then chaining multiple questions on the same event to predict a timeline. By grounding answers using the predicted timeline, TRN effectively resolves spurious overlaps and outperforms previous methods on TRC and TRE tasks. This approach addresses the issue of unreliable results due to spurious overlaps of similar questions with coincidentally identical answers, providing a more accurate understanding of temporal relations between events. <div>
arXiv:2506.14213v1 Announce Type: new 
Abstract: Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team</title>
<link>https://arxiv.org/abs/2506.14234</link>
<guid>https://arxiv.org/abs/2506.14234</guid>
<content:encoded><![CDATA[
<div> framework, experience-aware, language models, reasoning, multi-agent<br />
Summary:<br />
The article introduces Xolver, a multi-agent reasoning framework designed to enhance large language models by incorporating a persistent memory of holistic experiences. Unlike traditional models that operate in isolation, Xolver integrates various experience modalities such as external and self-retrieval, tool use, collaborative interactions, and iterative refinement. By learning from past strategies, code fragments, and abstract reasoning patterns during inference, Xolver moves away from generating solutions from scratch, leading to improved performance. Even with lightweight models, Xolver outperforms advanced specialized agents and achieves new best results on various tasks. The framework demonstrates the importance of holistic experience learning in developing generalist agents capable of expert-level reasoning.<br /> <div>
arXiv:2506.14234v1 Announce Type: new 
Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.14235</link>
<guid>https://arxiv.org/abs/2506.14235</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal knowledge graph reasoning, Multi-Expert Structural-Semantic Hybrid framework, event prediction, graph structure learning, semantic reasoning

Summary:
The article introduces the Multi-Expert Structural-Semantic Hybrid (MESH) framework for temporal knowledge graph reasoning, aiming to predict future events with integrated structural and semantic information. Previous methods have focused on either graph structure learning or semantic reasoning, lacking the ability to handle different prediction scenarios effectively. The MESH framework incorporates three expert modules to guide the reasoning process for historical and non-historical events, enhancing generalization across various temporal contexts. Experimental results on three datasets demonstrate the efficiency of the proposed approach in improving event prediction accuracy. The MESH framework provides a novel solution for integrating dual reasoning perspectives and capturing inherent differences between event types, advancing the field of temporal knowledge graph reasoning. 

<br /><br />Summary: <div>
arXiv:2506.14235v1 Announce Type: new 
Abstract: Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Initialization Token Learning for Tool-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2506.14248</link>
<guid>https://arxiv.org/abs/2506.14248</guid>
<content:encoded><![CDATA[
<div> large language models, numerical reasoning, plan generation, tool integration, token learning<br />
Summary:
This study focuses on enhancing the problem-solving capabilities of large language models (LLMs) by integrating external tools such as calculators and databases. Current methods assign unique tokens to each tool, limiting adaptability within pre-trained LLMs. The proposed novel token learning method aligns tool tokens with the existing word embedding space, improving model performance. Prior token embeddings for each tool based on their name or description are used for initialization and regularization of learnable tool token embeddings. Evaluation on tasks like numerical reasoning and question answering shows clear improvements over recent baselines. The results highlight that the approach effectively augments LLMs with tools through relevant tokens across diverse domains.<br /><br />Summary: <div>
arXiv:2506.14248v1 Announce Type: new 
Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents</title>
<link>https://arxiv.org/abs/2506.14285</link>
<guid>https://arxiv.org/abs/2506.14285</guid>
<content:encoded><![CDATA[
<div> dialogue response generation, timely response prediction, temporal context, TimelyChat benchmark, Timer model <br />
Summary: <br />
This research introduces the concept of timely dialogue response generation, focusing on generating responses based on both textual and temporal context. The TimelyChat benchmark evaluates language models on predicting appropriate time intervals and generating time-conditioned responses. A large-scale training dataset is created using unlabeled event knowledge from a temporal commonsense knowledge graph, and a dialogue agent called Timer is trained to predict time intervals and generate timely responses. Experimental results show that Timer outperforms prompting-based language models and other baselines in both turn-level and dialogue-level evaluations. The dataset, model, and code are publicly released for further research in dialogue systems. <br /> <div>
arXiv:2506.14285v1 Announce Type: new 
Abstract: While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent</title>
<link>https://arxiv.org/abs/2506.14302</link>
<guid>https://arxiv.org/abs/2506.14302</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Conversational Recommendation Agents, Preference Optimization, Multi-turn Dialogue, User Satisfaction<br />
<br />
Summary:<br />
Recent advancements in Large Language Models (LLMs) have led to the development of Conversational Recommendation Agents (CRAs), but their short-sighted responses often fail to meet user expectations. To address this challenge, a novel multi-turn preference optimization (MTPO) paradigm called ECPO is introduced. ECPO leverages Expectation Confirmation Theory to model user satisfaction evolution in multi-turn dialogues, identifying causes of dissatisfaction for targeted optimization. This approach eliminates sampling overhead while driving meaningful improvements. An LLM-based user simulator, AILO, is introduced to support ECPO. Experimental results demonstrate that ECPO significantly enhances CRA interaction capabilities, improving efficiency and effectiveness compared to existing MTPO methods. <div>
arXiv:2506.14302v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics</title>
<link>https://arxiv.org/abs/2506.14335</link>
<guid>https://arxiv.org/abs/2506.14335</guid>
<content:encoded><![CDATA[
<div> Variation, Reference-based metrics, Summarization evaluation, Multi-reference datasets, Human judgments <br />
Summary: 
This article explores the impact of using different reference sets on widely used reference-based metrics in the evaluation of summarization systems. The study analyzes three diverse multi-reference summarization datasets and finds that popular metrics exhibit significant instability, particularly n-gram-based metrics like ROUGE. Model rankings vary depending on the reference sets, leading to unreliable comparisons. Additionally, human judgments on genre-diverse data show weak-to-no correlation with metrics, indicating the need for improved evaluation methods, especially for Language Model models. The study recommends incorporating reference set variation into summarization evaluation to enhance consistency and alignment with human judgments. <br /> <div>
arXiv:2506.14335v1 Announce Type: new 
Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis</title>
<link>https://arxiv.org/abs/2506.14345</link>
<guid>https://arxiv.org/abs/2506.14345</guid>
<content:encoded><![CDATA[
<div> Large Language Models, deep research systems, geo-temporal reasoning, information access, AI-driven

Summary: The paper discusses the importance of integrating geo-temporal capabilities into deep research systems powered by Large Language Models. It highlights the necessity of handling geographic and temporal constraints in answering context-rich questions in domains such as public health, environmental science, and socio-economic analysis. The vision outlined in the paper emphasizes the need for augmenting retrieval and synthesis processes with geo-temporal reasoning, supported by open and reproducible infrastructures and rigorous evaluation protocols. By addressing technical, infrastructural, and evaluative challenges, the paper proposes a path towards developing advanced and geo-temporally aware deep research systems that can enhance AI-driven information access in a variety of domains. <div>
arXiv:2506.14345v1 Announce Type: new 
Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits</title>
<link>https://arxiv.org/abs/2506.14370</link>
<guid>https://arxiv.org/abs/2506.14370</guid>
<content:encoded><![CDATA[
<div> Keywords: search engines, content visibility, algorithmic curation, social media, biases

Summary: 
Google, as a prominent search engine, plays a significant role in shaping the visibility of web and social media content through its algorithmic curation practices. This study examines how Google selectively promotes or suppresses certain hashtags and subreddits, influencing the information users encounter. The analysis compares search engine results with data from Reddit and Twitter/X, revealing systematic biases in content visibility. Google's algorithms tend to suppress sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies-related content while prioritizing content with higher engagement. These findings suggest that Google's gatekeeping practices have a notable impact on public discourse by curating the social media narratives available to users.<br /><br />Summary: <div>
arXiv:2506.14370v1 Announce Type: new 
Abstract: Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection</title>
<link>https://arxiv.org/abs/2506.14371</link>
<guid>https://arxiv.org/abs/2506.14371</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, chat interfaces, critical thinking skills, automatic question generation, argument mining

Summary:
This study examines the use of Large Language Models (LLMs) in chat interfaces to promote critical thinking skills by generating critical questions to challenge unsupported claims. The proposed two-step framework involves a Questioner model that generates multiple candidate questions and a Judge model that selects the most relevant ones. The system developed for this task ranked first in the shared competition, showcasing the effectiveness of LLM-based approaches in encouraging critical engagement with argumentative texts. This research highlights the potential of leveraging LLMs beyond factual information retrieval to foster deeper reasoning and critical questioning in educational settings. <div>
arXiv:2506.14371v1 Announce Type: new 
Abstract: The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding</title>
<link>https://arxiv.org/abs/2506.14397</link>
<guid>https://arxiv.org/abs/2506.14397</guid>
<content:encoded><![CDATA[
<div> negation, linguistic phenomenon, Large Language Models, Thunder-NUBench, sentence-level

Summary: 
The article introduces Thunder-NUBench, a new benchmark specifically designed to evaluate sentence-level negation understanding in Large Language Models (LLMs). Negation, a fundamental linguistic phenomenon, often poses challenges for LLMs, requiring deep semantic understanding. Existing benchmarks usually treat negation as a subset of broader tasks like natural language inference, leading to a lack of focused evaluation on negation understanding. Thunder-NUBench aims to address this gap by presenting a variety of negation types, including standard negation, local negation, contradiction, and paraphrase. The benchmark consists of curated sentence-negation pairs and a multiple-choice dataset for comprehensive evaluation of LLMs' negation comprehension. Thunder-NUBench goes beyond mere surface-level cue detection, providing a nuanced assessment of models' ability to grasp the complexities of negation in language. <div>
arXiv:2506.14397v1 Announce Type: new 
Abstract: Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge</title>
<link>https://arxiv.org/abs/2506.14407</link>
<guid>https://arxiv.org/abs/2506.14407</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval systems, NLP pipelines, document-side processing, temporal relationships, world knowledge

Summary:
ImpliRet introduces a benchmark for evaluating retrieval systems that rely on implicit facts stated in documents through temporal, arithmetic, and world knowledge relationships. Unlike other benchmarks that rely on reasoning-heavy queries, ImpliRet shifts the challenge to document-side processing. Various sparse and dense retrievers struggle in this setting, with the best nDCG@10 score reaching only 15.07%. Long-context models like GPT-4.1 also face difficulties, scoring only 35.06% even with a short context of ten documents. This highlights the ongoing challenge of document-side reasoning in retrieval systems. The study emphasizes the importance of moving beyond surface-level cues and prompts for better understanding implicit relationships in documents. <div>
arXiv:2506.14407v1 Announce Type: new 
Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</title>
<link>https://arxiv.org/abs/2506.14429</link>
<guid>https://arxiv.org/abs/2506.14429</guid>
<content:encoded><![CDATA[
<div> diffusion LLMs, auto-regressive LLMs, context extrapolation, LongLLaDA, RoPE scaling theory

Summary:
Diffusion Large Language Models (LLMs) and traditional auto-regressive LLMs are compared for their long-context performance. Diffusion LLMs show stable perplexity during context extrapolation and exhibit a local perception phenomenon for successful retrieval in the Needle-In-A-Haystack task. The study introduces LongLLaDA, a training-free method integrating LLaDA with NTK-based RoPE extrapolation. It validates established scaling laws for extending the context windows of diffusion LLMs and identifies task-specific performance differences between diffusion LLMs and auto-regressive LLMs. This work establishes a context extrapolation method for diffusion LLMs, offering theoretical insights and empirical benchmarks for future research in long-context diffusion LLMs. 

<br /><br />Summary: <div>
arXiv:2506.14429v1 Announce Type: new 
Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison</title>
<link>https://arxiv.org/abs/2506.14448</link>
<guid>https://arxiv.org/abs/2506.14448</guid>
<content:encoded><![CDATA[
<div> semantic games, test-time learning, evaluation framework, language models, artificial general intelligence

Summary:
Language models are crucial in the development of artificial general intelligence. Current evaluation benchmarks focus on static knowledge, but true intelligence also involves the ability to learn rapidly from experience. The concept of Test-time Learning, which assesses the capacity to improve performance in experience-based tasks during testing, is proposed. Semantic games are suggested as effective testbeds for evaluating test-time learning, as they require strategic reasoning and are resistant to saturation. An evaluation framework is introduced to compare model performance under different experience settings. Results show that large language models demonstrate test-time learning capabilities, but their improvements are less stable and slower than those of humans. This highlights the potential of language models as general-purpose learning machines while also identifying a significant intellectual gap between models and humans. <div>
arXiv:2506.14448v1 Announce Type: new 
Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data</title>
<link>https://arxiv.org/abs/2506.14474</link>
<guid>https://arxiv.org/abs/2506.14474</guid>
<content:encoded><![CDATA[
<div> watermarking, large language models, text, synonym substitutions, data protection

Summary:
LexiMark is a novel watermarking technique designed for text and documents to enhance an LLM's memorization capabilities without altering semantic integrity. It embeds synonym substitutions for high-entropy words, making the watermark difficult to detect and resistant to removal. The method was evaluated on seven open-source models, showing significant improvements in AUROC scores compared to existing methods. LexiMark effectively verifies unauthorized use of watermarked data in LLM training, making it a reliable tool for data protection.<br /><br />Summary: <div>
arXiv:2506.14474v1 Announce Type: new 
Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops</title>
<link>https://arxiv.org/abs/2506.14493</link>
<guid>https://arxiv.org/abs/2506.14493</guid>
<content:encoded><![CDATA[
<div> POS tag, EOS token, LingoLoop, Generative Path Pruning Mechanism, energy-latency attacks

Summary: 
- Multimodal Large Language Models (MLLMs) are vulnerable to attacks that induce them to generate excessively verbose and repetitive sequences.
- LingoLoop attack utilizes a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights based on POS information.
- The attack also employs a Generative Path Pruning Mechanism to limit hidden state magnitude, encouraging the model to produce repetitive loops.
- Extensive experiments demonstrated LingoLoop's effectiveness in increasing generated tokens and energy consumption on models like Qwen2.5-VL-3B.
- The findings highlight significant vulnerabilities in MLLMs and pose challenges for their reliable deployment.

Summary: <div>
arXiv:2506.14493v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models</title>
<link>https://arxiv.org/abs/2506.14532</link>
<guid>https://arxiv.org/abs/2506.14532</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, beam prediction, millimeter-wave communication, multimodal sensor data, fine-tuning<br />
<br />
Summary: 
The paper introduces M2BeamLLM, a novel neural network framework for beam prediction in mmWave mMIMO communication systems. M2BeamLLM integrates multi-modal sensor data like images, radar, LiDAR, and GPS, using large language models (LLMs) for beam prediction. Through sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves higher accuracy and robustness in beam prediction compared to traditional deep learning models. It outperforms them in standard and few-shot scenarios, with performance improving as sensing modality diversity increases. The study presents an efficient solution for vehicle-to-infrastructure mmWave communication systems. <br /><br />Summary: <div>
arXiv:2506.14532v1 Announce Type: new 
Abstract: This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>
<link>https://arxiv.org/abs/2506.14562</link>
<guid>https://arxiv.org/abs/2506.14562</guid>
<content:encoded><![CDATA[
<div> regularization, language models, weight decay, AlphaDecay, spectral properties <br />
Summary:<br />Weight decay is a common regularization technique for training large language models (LLMs), but assigning a uniform decay rate to every layer may not be optimal due to the structural diversity and varying spectral properties across modules. In this paper, the authors introduce AlphaDecay, a method that adaptively assigns different weight decay strengths to each module of an LLM based on its spectral properties. By leveraging Heavy-Tailed Self-Regularization theory and analyzing the empirical spectral density of weight correlation matrices, AlphaDecay aims to balance module-wise differences in feature learning. Experimental results on pre-training tasks with various model sizes show that AlphaDecay outperforms uniform decay and other adaptive decay baselines in terms of perplexity and generalization. <div>
arXiv:2506.14562v1 Announce Type: new 
Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenerationPrograms: Fine-grained Attribution with Executable Programs</title>
<link>https://arxiv.org/abs/2506.14580</link>
<guid>https://arxiv.org/abs/2506.14580</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, attribution methods, GenerationPrograms, interpretability, text generation

Summary:
GenerationPrograms is a new modular generation framework inspired by executable "code agent" architectures. It aims to improve attributions in large language models by decomposing the generation process into two stages: creating an executable program plan tailored to the query, and executing modular text operations to produce the final response. This approach enhances attribution quality at both document and sentence levels in long-form question-answering tasks and multi-document summarization. GenerationPrograms outperforms traditional techniques in post-hoc attribution, providing accurate attributions. The interpretable programs generated enable localized refinement through modular-level improvements, further enhancing overall attribution quality. <div>
arXiv:2506.14580v1 Announce Type: new 
Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees</title>
<link>https://arxiv.org/abs/2506.14606</link>
<guid>https://arxiv.org/abs/2506.14606</guid>
<content:encoded><![CDATA[
<div> GG (Guaranteed Guess), ISA-centric transpilation pipeline, translation, LLMs, software testing <br />
<br />
Summary: 
The article introduces GG, a transpilation pipeline that uses large language models (LLMs) and software testing to translate between complex- (CISC) and reduced- (RISC) hardware architectures. GG generates candidate translations using LLMs and embeds them in a software testing framework to ensure confidence in the translation. The approach achieves high code coverage (>98%) and 99% functional/semantic correctness on HumanEval programs and 49% on BringupBench programs. Compared to Rosetta 2 on Apple Silicon, GG shows 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for transpiled code. The authors plan to open-source their codes, data, models, and benchmarks to advance research on ISA-level code translation. <div>
arXiv:2506.14606v1 Announce Type: new 
Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Meaning Backfire? Investigating the Role of AMRs in NLI</title>
<link>https://arxiv.org/abs/2506.14613</link>
<guid>https://arxiv.org/abs/2506.14613</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Inference, Abstract Meaning Representation, Pretrained Language Models, Fine-tuning, Semantic Reasoning<br />
Summary:<br />
The study explores the impact of incorporating Abstract Meaning Representations (AMR) on the generalization performance of pretrained language models in Natural Language Inference (NLI). Experimental results indicate that the inclusion of AMR during fine-tuning adversely affects model generalization in NLI tasks. However, prompting with AMR leads to slight enhancements in model performance, particularly in the case of GPT-4o. An ablation study further reveals that the improvement observed is primarily attributed to amplifying surface-level discrepancies rather than facilitating semantic reasoning. This amplification tendency may lead models to incorrectly predict non-entailment even when the underlying meaning remains intact. Overall, the findings suggest that while prompting with AMR can yield marginal gains in specific scenarios, caution should be exercised to prevent potential misinterpretation of semantic content during the inference process.<br /> <div>
arXiv:2506.14613v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.14625</link>
<guid>https://arxiv.org/abs/2506.14625</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, moral reasoning, consensus, alignment, AI systems

Summary:
Large Language Models (LLMs) have shown strong moral reasoning abilities but can diverge in complex moral dilemmas. This study proposes a framework to synthesize multiple LLMs' moral judgments into a collective consensus, weighting contributions by model reliability. An aggregation mechanism fuses moral acceptability scores, aligning divergent models through token embedding optimization. Experiments on a social moral dilemma dataset demonstrate that this approach enhances consensus and individual model fidelity. The findings underscore the value of data-driven moral alignment across LLMs for the development of safer and more consistent AI systems. 

Summary: <div>
arXiv:2506.14625v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation</title>
<link>https://arxiv.org/abs/2506.14634</link>
<guid>https://arxiv.org/abs/2506.14634</guid>
<content:encoded><![CDATA[
arXiv:2506.14634v1 Announce Type: new 
Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot</title>
<link>https://arxiv.org/abs/2506.14641</link>
<guid>https://arxiv.org/abs/2506.14641</guid>
<content:encoded><![CDATA[
arXiv:2506.14641v1 Announce Type: new 
Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments</title>
<link>https://arxiv.org/abs/2506.14645</link>
<guid>https://arxiv.org/abs/2506.14645</guid>
<content:encoded><![CDATA[
arXiv:2506.14645v1 Announce Type: new 
Abstract: The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors</title>
<link>https://arxiv.org/abs/2506.14646</link>
<guid>https://arxiv.org/abs/2506.14646</guid>
<content:encoded><![CDATA[
arXiv:2506.14646v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality</title>
<link>https://arxiv.org/abs/2506.14681</link>
<guid>https://arxiv.org/abs/2506.14681</guid>
<content:encoded><![CDATA[
arXiv:2506.14681v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers</title>
<link>https://arxiv.org/abs/2506.14702</link>
<guid>https://arxiv.org/abs/2506.14702</guid>
<content:encoded><![CDATA[
arXiv:2506.14702v1 Announce Type: new 
Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data</title>
<link>https://arxiv.org/abs/2506.14704</link>
<guid>https://arxiv.org/abs/2506.14704</guid>
<content:encoded><![CDATA[
arXiv:2506.14704v1 Announce Type: new 
Abstract: This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2506.14731</link>
<guid>https://arxiv.org/abs/2506.14731</guid>
<content:encoded><![CDATA[
arXiv:2506.14731v1 Announce Type: new 
Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with Exploration: An Entropy Perspective</title>
<link>https://arxiv.org/abs/2506.14758</link>
<guid>https://arxiv.org/abs/2506.14758</guid>
<content:encoded><![CDATA[
arXiv:2506.14758v1 Announce Type: new 
Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Bytes to Ideas: Language Modeling with Autoregressive U-Nets</title>
<link>https://arxiv.org/abs/2506.14761</link>
<guid>https://arxiv.org/abs/2506.14761</guid>
<content:encoded><![CDATA[
arXiv:2506.14761v1 Announce Type: new 
Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Framework for Improving Naturalness in Generative Spoken Language Models</title>
<link>https://arxiv.org/abs/2506.14767</link>
<guid>https://arxiv.org/abs/2506.14767</guid>
<content:encoded><![CDATA[
arXiv:2506.14767v1 Announce Type: new 
Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
arXiv:2506.13771v1 Announce Type: cross 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning</title>
<link>https://arxiv.org/abs/2506.13778</link>
<guid>https://arxiv.org/abs/2506.13778</guid>
<content:encoded><![CDATA[
arXiv:2506.13778v1 Announce Type: cross 
Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AcademicBrowse: Benchmarking Academic Browse Ability of LLMs</title>
<link>https://arxiv.org/abs/2506.13784</link>
<guid>https://arxiv.org/abs/2506.13784</guid>
<content:encoded><![CDATA[
arXiv:2506.13784v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \&amp; a ML Ensemble on Longitudinal Identity Resolution</title>
<link>https://arxiv.org/abs/2506.13792</link>
<guid>https://arxiv.org/abs/2506.13792</guid>
<content:encoded><![CDATA[
arXiv:2506.13792v1 Announce Type: cross 
Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study</title>
<link>https://arxiv.org/abs/2506.13811</link>
<guid>https://arxiv.org/abs/2506.13811</guid>
<content:encoded><![CDATA[
arXiv:2506.13811v1 Announce Type: cross 
Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13923</link>
<guid>https://arxiv.org/abs/2506.13923</guid>
<content:encoded><![CDATA[
arXiv:2506.13923v1 Announce Type: cross 
Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience</title>
<link>https://arxiv.org/abs/2506.13971</link>
<guid>https://arxiv.org/abs/2506.13971</guid>
<content:encoded><![CDATA[
arXiv:2506.13971v1 Announce Type: cross 
Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios</title>
<link>https://arxiv.org/abs/2506.13977</link>
<guid>https://arxiv.org/abs/2506.13977</guid>
<content:encoded><![CDATA[
arXiv:2506.13977v1 Announce Type: cross 
Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science</title>
<link>https://arxiv.org/abs/2506.13992</link>
<guid>https://arxiv.org/abs/2506.13992</guid>
<content:encoded><![CDATA[
arXiv:2506.13992v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking</title>
<link>https://arxiv.org/abs/2506.14086</link>
<guid>https://arxiv.org/abs/2506.14086</guid>
<content:encoded><![CDATA[
arXiv:2506.14086v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints</title>
<link>https://arxiv.org/abs/2506.14104</link>
<guid>https://arxiv.org/abs/2506.14104</guid>
<content:encoded><![CDATA[
arXiv:2506.14104v1 Announce Type: cross 
Abstract: Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fr\'echet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability ({\sigma} = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadFabric: Agentic AI System with Reasoning Capability for Radiology</title>
<link>https://arxiv.org/abs/2506.14142</link>
<guid>https://arxiv.org/abs/2506.14142</guid>
<content:encoded><![CDATA[
arXiv:2506.14142v1 Announce Type: cross 
Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment</title>
<link>https://arxiv.org/abs/2506.14148</link>
<guid>https://arxiv.org/abs/2506.14148</guid>
<content:encoded><![CDATA[
arXiv:2506.14148v1 Announce Type: cross 
Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models</title>
<link>https://arxiv.org/abs/2506.14153</link>
<guid>https://arxiv.org/abs/2506.14153</guid>
<content:encoded><![CDATA[
arXiv:2506.14153v1 Announce Type: cross 
Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios</title>
<link>https://arxiv.org/abs/2506.14204</link>
<guid>https://arxiv.org/abs/2506.14204</guid>
<content:encoded><![CDATA[
arXiv:2506.14204v1 Announce Type: cross 
Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription</title>
<link>https://arxiv.org/abs/2506.14223</link>
<guid>https://arxiv.org/abs/2506.14223</guid>
<content:encoded><![CDATA[
arXiv:2506.14223v1 Announce Type: cross 
Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs</title>
<link>https://arxiv.org/abs/2506.14245</link>
<guid>https://arxiv.org/abs/2506.14245</guid>
<content:encoded><![CDATA[
arXiv:2506.14245v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LoRA with Variational Learning</title>
<link>https://arxiv.org/abs/2506.14280</link>
<guid>https://arxiv.org/abs/2506.14280</guid>
<content:encoded><![CDATA[
arXiv:2506.14280v1 Announce Type: cross 
Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2506.14574</link>
<guid>https://arxiv.org/abs/2506.14574</guid>
<content:encoded><![CDATA[
arXiv:2506.14574v1 Announce Type: cross 
Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Studies in Influencer Marketing: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2506.14602</link>
<guid>https://arxiv.org/abs/2506.14602</guid>
<content:encoded><![CDATA[
arXiv:2506.14602v1 Announce Type: cross 
Abstract: Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning</title>
<link>https://arxiv.org/abs/2506.14629</link>
<guid>https://arxiv.org/abs/2506.14629</guid>
<content:encoded><![CDATA[
arXiv:2506.14629v1 Announce Type: cross 
Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Length Compression in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.14755</link>
<guid>https://arxiv.org/abs/2506.14755</guid>
<content:encoded><![CDATA[
arXiv:2506.14755v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM</title>
<link>https://arxiv.org/abs/2506.14766</link>
<guid>https://arxiv.org/abs/2506.14766</guid>
<content:encoded><![CDATA[
arXiv:2506.14766v1 Announce Type: cross 
Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compression of enumerations and gain</title>
<link>https://arxiv.org/abs/2304.03030</link>
<guid>https://arxiv.org/abs/2304.03030</guid>
<content:encoded><![CDATA[
arXiv:2304.03030v2 Announce Type: replace 
Abstract: We study the compressibility of enumerations in the context of Kolmogorov complexity, focusing on strong and weak forms of compression and their gain: the amount of auxiliary information embedded in the compressed enumeration. The existence of strong compression and weak gainless compression is shown for any computably enumerable (c.e.) set. The density problem of c.e. sets with respect to their prefix complexity is reduced to the question of whether every c.e. set is well-compressible, which we study via enumeration games.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</title>
<link>https://arxiv.org/abs/2307.10867</link>
<guid>https://arxiv.org/abs/2307.10867</guid>
<content:encoded><![CDATA[
arXiv:2307.10867v2 Announce Type: replace 
Abstract: Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring news intent and its application: A theory-driven approach</title>
<link>https://arxiv.org/abs/2312.16490</link>
<guid>https://arxiv.org/abs/2312.16490</guid>
<content:encoded><![CDATA[
arXiv:2312.16490v2 Announce Type: replace 
Abstract: Understanding the intent behind information is crucial. However, news as a medium of public discourse still lacks a structured investigation of perceived news intent and its application. To advance this field, this paper reviews interdisciplinary studies on intentional action and introduces a conceptual deconstruction-based news intent understanding framework (NINT). This framework identifies the components of intent, facilitating a structured representation of news intent and its applications. Building upon NINT, we contribute a new intent perception dataset. Moreover, we investigate the potential of intent assistance on news-related tasks, such as significant improvement (+2.2% macF1) in the task of fake news detection. We hope that our findings will provide valuable insights into action-based intent cognition and computational social science.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification</title>
<link>https://arxiv.org/abs/2402.10735</link>
<guid>https://arxiv.org/abs/2402.10735</guid>
<content:encoded><![CDATA[
arXiv:2402.10735v4 Announce Type: replace 
Abstract: Although LLMs have shown great performance on Mathematics and Coding related reasoning tasks, the reasoning capabilities of LLMs regarding other forms of reasoning are still an open problem. Here, we examine the issue of reasoning from the perspective of claim verification. We propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. We use this framework to create RECV, the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of LLMs. The benchmark comprises of three datasets, covering reasoning problems of increasing complexity. We evaluate three state-of-the-art proprietary LLMs under multiple prompt settings. Our results show that while LLMs can address deductive reasoning problems, they consistently fail in cases of abductive reasoning. Moreover, we observe that enhancing LLMs with rationale generation is not always beneficial. Nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deductive reasoning cases.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora</title>
<link>https://arxiv.org/abs/2406.13677</link>
<guid>https://arxiv.org/abs/2406.13677</guid>
<content:encoded><![CDATA[
arXiv:2406.13677v3 Announce Type: replace 
Abstract: Large language models (LLMs) often inherit and amplify social biases embedded in their training data. A prominent social bias is gender bias. In this regard, prior work has mainly focused on gender stereotyping bias - the association of specific roles or traits with a particular gender - in English and on evaluating gender bias in model embeddings or generated outputs. In contrast, gender representation bias - the unequal frequency of references to individuals of different genders - in the training corpora has received less attention. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. To fill this gap, we propose a novel LLM-based method to detect and quantify gender representation bias in LLM training data in gendered languages, where grammatical gender challenges the applicability of methods developed for English. By leveraging the LLMs' contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora. Applied to four Spanish-English benchmarks and five Valencian corpora, our method reveals substantial male-dominant imbalances. We show that such biases in training data affect model outputs, but can surprisingly be mitigated leveraging small-scale training on datasets that are biased towards the opposite gender. Our findings highlight the need for corpus-level gender bias analysis in multilingual NLP. We make our code and data publicly available.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models</title>
<link>https://arxiv.org/abs/2407.07313</link>
<guid>https://arxiv.org/abs/2407.07313</guid>
<content:encoded><![CDATA[
arXiv:2407.07313v4 Announce Type: replace 
Abstract: The task of Text-to-SQL enables anyone to retrieve information from SQL databases using natural language. While this task has made substantial progress, the two primary evaluation metrics - Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM) - suffer from inherent limitations that can misrepresent performance. Specifically, ESM's rigid matching overlooks semantically correct but stylistically different queries, whereas EXE can overestimate correctness by ignoring structural errors that yield correct outputs. These shortcomings become especially problematic when assessing outputs from large language model (LLM)-based approaches without fine-tuning, which vary more in style and structure compared to their fine-tuned counterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM), which mitigates these issues by comparing queries using both syntactic and semantic elements. Through evaluating nine LLM-based models, we show that EXE and ESM can produce false positive and negative rates as high as 23.0% and 28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release our ETM script as open source, offering the community a more robust and reliable approach to evaluating Text-to-SQL.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Signatures of Compositionality Across a Language Model's Lifetime</title>
<link>https://arxiv.org/abs/2410.01444</link>
<guid>https://arxiv.org/abs/2410.01444</guid>
<content:encoded><![CDATA[
arXiv:2410.01444v5 Announce Type: replace 
Abstract: By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Overfitting in Large Language Model Editing</title>
<link>https://arxiv.org/abs/2410.07819</link>
<guid>https://arxiv.org/abs/2410.07819</guid>
<content:encoded><![CDATA[
arXiv:2410.07819v2 Announce Type: replace 
Abstract: Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of Editing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn the Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Browsing: API-Based Web Agents</title>
<link>https://arxiv.org/abs/2410.16464</link>
<guid>https://arxiv.org/abs/2410.16464</guid>
<content:encoded><![CDATA[
arXiv:2410.16464v3 Announce Type: replace 
Abstract: Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing. However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask -- what if we were to take tasks traditionally tackled by Browsing Agents, and give AI agents access to APIs? To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs. In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-Based Agents outperform web Browsing Agents. Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 24.0% absolute improvement over web browsing alone, achieving a success rate of 38.9%, the SOTA performance among task-agnostic agents. These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework</title>
<link>https://arxiv.org/abs/2410.18653</link>
<guid>https://arxiv.org/abs/2410.18653</guid>
<content:encoded><![CDATA[
arXiv:2410.18653v3 Announce Type: replace 
Abstract: Open-ended text generation has become a prominent task in natural language processing due to the rise of powerful (large) language models. However, evaluating the quality of these models and the employed decoding strategies remains challenging due to trade-offs among widely used metrics such as coherence, diversity, and perplexity. This paper addresses the specific problem of multicriteria evaluation for open-ended text generation, proposing novel methods for both relative and absolute rankings of decoding methods. Specifically, we employ benchmarking approaches based on partial orderings and present a new summary metric to balance existing automatic indicators, providing a more holistic evaluation of text generation quality. Our experiments demonstrate that the proposed approaches offer a robust way to compare decoding strategies and serve as valuable tools to guide model selection for open-ended text generation tasks. We suggest future directions for improving evaluation methodologies in text generation and make our code, datasets, and models publicly available.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Watermarks for Large Language Models</title>
<link>https://arxiv.org/abs/2411.19563</link>
<guid>https://arxiv.org/abs/2411.19563</guid>
<content:encoded><![CDATA[
arXiv:2411.19563v2 Announce Type: replace 
Abstract: As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English</title>
<link>https://arxiv.org/abs/2412.04726</link>
<guid>https://arxiv.org/abs/2412.04726</guid>
<content:encoded><![CDATA[
arXiv:2412.04726v3 Announce Type: replace 
Abstract: Despite large language models (LLMs) being known to exhibit bias against non-standard language varieties, there are no known labelled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine LLMs (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results show that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), in comparison with en-IN, particularly for sarcasm classification. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE dataset is publicly available at: https://huggingface.co/ datasets/unswnlporg/BESSTIE.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression</title>
<link>https://arxiv.org/abs/2412.05693</link>
<guid>https://arxiv.org/abs/2412.05693</guid>
<content:encoded><![CDATA[
arXiv:2412.05693v2 Announce Type: replace 
Abstract: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClusterChat: Multi-Feature Search for Corpus Exploration</title>
<link>https://arxiv.org/abs/2412.14533</link>
<guid>https://arxiv.org/abs/2412.14533</guid>
<content:encoded><![CDATA[
arXiv:2412.14533v2 Announce Type: replace 
Abstract: Exploring large-scale text corpora presents a significant challenge in biomedical, finance, and legal domains, where vast amounts of documents are continuously published. Traditional search methods, such as keyword-based search, often retrieve documents in isolation, limiting the user's ability to easily inspect corpus-wide trends and relationships. We present ClusterChat (The demo video and source code are available at: https://github.com/achouhan93/ClusterChat), an open-source system for corpus exploration that integrates cluster-based organization of documents using textual embeddings with lexical and semantic search, timeline-driven exploration, and corpus and document-level question answering (QA) as multi-feature search capabilities. We validate the system with two case studies on a four million abstract PubMed dataset, demonstrating that ClusterChat enhances corpus exploration by delivering context-aware insights while maintaining scalability and responsiveness on large-scale document collections.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models</title>
<link>https://arxiv.org/abs/2501.05478</link>
<guid>https://arxiv.org/abs/2501.05478</guid>
<content:encoded><![CDATA[
arXiv:2501.05478v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</title>
<link>https://arxiv.org/abs/2501.10970</link>
<guid>https://arxiv.org/abs/2501.10970</guid>
<content:encoded><![CDATA[
arXiv:2501.10970v3 Announce Type: replace 
Abstract: The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11425</link>
<guid>https://arxiv.org/abs/2502.11425</guid>
<content:encoded><![CDATA[
arXiv:2502.11425v2 Announce Type: replace 
Abstract: Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Geo-Culturally Grounded LLM Generations</title>
<link>https://arxiv.org/abs/2502.13497</link>
<guid>https://arxiv.org/abs/2502.13497</guid>
<content:encoded><![CDATA[
arXiv:2502.13497v3 Announce Type: replace 
Abstract: Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs' ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs' cultural awareness.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PredictaBoard: Benchmarking LLM Score Predictability</title>
<link>https://arxiv.org/abs/2502.14445</link>
<guid>https://arxiv.org/abs/2502.14445</guid>
<content:encoded><![CDATA[
arXiv:2502.14445v2 Announce Type: replace 
Abstract: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.15910</link>
<guid>https://arxiv.org/abs/2502.15910</guid>
<content:encoded><![CDATA[
arXiv:2502.15910v2 Announce Type: replace 
Abstract: Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification</title>
<link>https://arxiv.org/abs/2502.17421</link>
<guid>https://arxiv.org/abs/2502.17421</guid>
<content:encoded><![CDATA[
arXiv:2502.17421v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning</title>
<link>https://arxiv.org/abs/2502.20620</link>
<guid>https://arxiv.org/abs/2502.20620</guid>
<content:encoded><![CDATA[
arXiv:2502.20620v2 Announce Type: replace 
Abstract: Large language models (LLMs) can exhibit advanced reasoning yet still generate incorrect answers. We hypothesize that such errors frequently stem from spurious beliefs, propositions the model internally considers true but are incorrect. To address this, we propose a method to rectify the belief space by suppressing these spurious beliefs while simultaneously enhancing true ones, thereby enabling more reliable inferences. Our approach first identifies the beliefs that lead to incorrect or correct answers by prompting the model to generate textual explanations, using our Forward-Backward Beam Search (FBBS). We then apply unlearning to suppress the identified spurious beliefs and enhance the true ones, effectively rectifying the model's belief space. Empirical results on multiple QA datasets and LLMs show that our method corrects previously misanswered questions without harming overall model performance. Furthermore, our approach yields improved generalization on unseen data, suggesting that rectifying a model's belief space is a promising direction for mitigating errors and enhancing overall reliability.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling</title>
<link>https://arxiv.org/abs/2503.04619</link>
<guid>https://arxiv.org/abs/2503.04619</guid>
<content:encoded><![CDATA[
arXiv:2503.04619v2 Announce Type: replace 
Abstract: User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Selection Format on LLM Performance</title>
<link>https://arxiv.org/abs/2503.06926</link>
<guid>https://arxiv.org/abs/2503.06926</guid>
<content:encoded><![CDATA[
arXiv:2503.06926v2 Announce Type: replace 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints</title>
<link>https://arxiv.org/abs/2503.08669</link>
<guid>https://arxiv.org/abs/2503.08669</guid>
<content:encoded><![CDATA[
arXiv:2503.08669v2 Announce Type: replace 
Abstract: As language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline SOPBench with: (1) executable environments containing 167 tools/functions across seven customer service domains with service-specific SOPs and rule-based verifiers, (2) an automated test generation framework producing over 900 verified test cases, and (3) an automated evaluation framework to rigorously assess agent adherence from multiple dimensions. Our approach transforms each service-specific SOP code program into a directed graph of executable functions and requires agents to call these functions based on natural language SOP descriptions. The original code serves as oracle rule-based verifiers to assess compliance, reducing reliance on manual annotations and LLM-based evaluations. We evaluate 18 leading models, and results show the task is challenging even for top-tier models (like GPT-4o, Claude-3.7-Sonnet), with variances across domains. Reasoning models like o4-mini-high show superiority while other powerful models perform less effectively (pass rates of 30%-50%), and small models (7B, 8B) perform significantly worse. Additionally, language agents can be easily jailbroken to overlook SOPs and constraints. Code, data, and over 24k agent trajectories are released at https://github.com/Leezekun/SOPBench.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Construction Distributions Shape Formal Language Learning In German BabyLMs?</title>
<link>https://arxiv.org/abs/2503.11593</link>
<guid>https://arxiv.org/abs/2503.11593</guid>
<content:encoded><![CDATA[
arXiv:2503.11593v2 Announce Type: replace 
Abstract: We analyze the influence of utterance-level construction distributions in German child-directed/child-available speech on the resulting word-level, syntactic and semantic competence (and their underlying learning trajectories) in small LMs, which we train on a novel collection of developmentally plausible language data for German. We find that trajectories are surprisingly robust for markedly different distributions of constructions in the training data, which have little effect on final accuracies and almost no effect on global learning trajectories. While syntax learning benefits from more complex utterances, word-level learning culminates in better scores with more fragmentary utterances. We argue that LMs trained on developmentally plausible data can contribute to debates on how conducive different kinds of linguistic stimuli are to language learning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information</title>
<link>https://arxiv.org/abs/2504.07738</link>
<guid>https://arxiv.org/abs/2504.07738</guid>
<content:encoded><![CDATA[
arXiv:2504.07738v2 Announce Type: replace 
Abstract: In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Cost-Aware Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.16005</link>
<guid>https://arxiv.org/abs/2504.16005</guid>
<content:encoded><![CDATA[
arXiv:2504.16005v4 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automatic prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach</title>
<link>https://arxiv.org/abs/2505.00039</link>
<guid>https://arxiv.org/abs/2505.00039</guid>
<content:encoded><![CDATA[
arXiv:2505.00039v3 Announce Type: replace 
Abstract: This article proposes an adaptation of Graph Retrieval-Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms. Legal texts are characterized by a predefined hierarchical structure, an extensive network of references and a continuous evolution through multiple temporal versions. This temporal dynamism poses a significant challenge for standard AI systems, demanding a deterministic representation of the law at any given point in time. To address this, our approach grounds the knowledge graph construction in a formal, FRBRoo-inspired model that distinguishes abstract legal works from their concrete textual expressions. We introduce a multi-layered representation of Temporal Versions (capturing date-specific changes) and Language Versions (capturing linguistic variations). By modeling normative evolution as a precise sequence of these versioned entities, we enable the construction of a knowledge graph that serves as a verifiable "ground truth". This allows Large Language Models to generate responses based on accurate, context-aware, and point-in-time correct legal information, overcoming the risk of temporal inaccuracies. Through a detailed analysis of this formal Graph RAG approach and its application to legal norm datasets, this article aims to advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective and reliable systems in legal research, legislative analysis, and decision support.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</title>
<link>https://arxiv.org/abs/2505.07897</link>
<guid>https://arxiv.org/abs/2505.07897</guid>
<content:encoded><![CDATA[
arXiv:2505.07897v2 Announce Type: replace 
Abstract: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents</title>
<link>https://arxiv.org/abs/2505.11368</link>
<guid>https://arxiv.org/abs/2505.11368</guid>
<content:encoded><![CDATA[
arXiv:2505.11368v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement</title>
<link>https://arxiv.org/abs/2505.12368</link>
<guid>https://arxiv.org/abs/2505.12368</guid>
<content:encoded><![CDATA[
arXiv:2505.12368v2 Announce Type: replace 
Abstract: Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations. To demonstrate our framework's utility, we train CaptureGuard on our generated data. This new model drastically reduces both false negative and false positive rates on our context-aware datasets while also generalizing effectively to external benchmarks, establishing a path toward more robust and practical prompt injection defenses.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)</title>
<link>https://arxiv.org/abs/2505.17238</link>
<guid>https://arxiv.org/abs/2505.17238</guid>
<content:encoded><![CDATA[
arXiv:2505.17238v2 Announce Type: replace 
Abstract: Collaborative dialogue offers rich insights into students' learning and critical thinking, which is essential for personalizing pedagogical agent interactions in STEM+C settings. While large language models (LLMs) facilitate dynamic pedagogical interactions, hallucinations undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge but requires a clear semantic link between user input and a knowledge base, which is often weak in student dialogue. We propose log-contextualized RAG (LC-RAG), which enhances RAG retrieval by using environment logs to contextualize collaborative discourse. Our findings show that LC-RAG improves retrieval over a discourse-only baseline and allows our collaborative peer agent, Copa, to deliver relevant, personalized guidance that supports students' critical thinking and epistemic decision-making in a collaborative computational modeling environment, C2STEM.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.20613</link>
<guid>https://arxiv.org/abs/2505.20613</guid>
<content:encoded><![CDATA[
arXiv:2505.20613v2 Announce Type: replace 
Abstract: Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
<link>https://arxiv.org/abs/2506.00854</link>
<guid>https://arxiv.org/abs/2506.00854</guid>
<content:encoded><![CDATA[
arXiv:2506.00854v2 Announce Type: replace 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation</title>
<link>https://arxiv.org/abs/2506.01565</link>
<guid>https://arxiv.org/abs/2506.01565</guid>
<content:encoded><![CDATA[
arXiv:2506.01565v2 Announce Type: replace 
Abstract: Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EuroLLM-9B: Technical Report</title>
<link>https://arxiv.org/abs/2506.04079</link>
<guid>https://arxiv.org/abs/2506.04079</guid>
<content:encoded><![CDATA[
arXiv:2506.04079v2 Announce Type: replace 
Abstract: This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2506.07801</link>
<guid>https://arxiv.org/abs/2506.07801</guid>
<content:encoded><![CDATA[
arXiv:2506.07801v2 Announce Type: replace 
Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for three key purposes: selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, achieving state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranking first according to the Friedman test among 19 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26% -- and data imbalance is a key factor for many text classification tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature</title>
<link>https://arxiv.org/abs/2308.12420</link>
<guid>https://arxiv.org/abs/2308.12420</guid>
<content:encoded><![CDATA[
arXiv:2308.12420v4 Announce Type: replace-cross 
Abstract: Distributed Ledger Technology (DLT) faces increasing environmental scrutiny, particularly concerning the energy consumption of the Proof of Work (PoW) consensus mechanism and broader Environmental, Social, and Governance (ESG) issues. However, existing systematic literature reviews of DLT rely on limited analyses of citations, abstracts, and keywords, failing to fully capture the field's complexity and ESG concerns. We address these challenges by analyzing the full text of 24,539 publications using Natural Language Processing (NLP) with our manually labeled Named Entity Recognition (NER) dataset of 39,427 entities for DLT. This methodology identified 505 key publications at the DLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP and temporal graph analysis reveals critical trends in DLT evolution and ESG impacts, including cryptography and peer-to-peer networks research's foundational influence, Bitcoin's persistent impact on research and environmental concerns (a "Lindy effect"), Ethereum's catalytic role on Proof of Stake (PoS) and smart contract adoption, and the industry's progressive shift toward energy-efficient consensus mechanisms. Our contributions include the first DLT-specific NER dataset addressing the scarcity of high-quality labeled NLP data in blockchain research, a methodology integrating NLP and temporal graph analysis for large-scale interdisciplinary literature reviews, and the first NLP-driven literature review focusing on DLT's ESG aspects.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title>
<link>https://arxiv.org/abs/2406.11423</link>
<guid>https://arxiv.org/abs/2406.11423</guid>
<content:encoded><![CDATA[
arXiv:2406.11423v4 Announce Type: replace-cross 
Abstract: Proactive content moderation requires platforms to rapidly and continuously evaluate the credibility of websites. Leveraging the direct and indirect paths users follow to unreliable websites, we develop a website credibility classification and discovery system that integrates both webgraph and large-scale social media contexts. We additionally introduce the concept of dredge words, terms or phrases for which unreliable domains rank highly on search engines, and provide the first exploration of their usage on social media. Our graph neural networks that combine webgraph and social media contexts generate to state-of-the-art results in website credibility classification and significantly improves the top-k identification of unreliable domains. Additionally, we release a novel dataset of dredge words, highlighting their strong connections to both social media and online commerce platforms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers</title>
<link>https://arxiv.org/abs/2406.14986</link>
<guid>https://arxiv.org/abs/2406.14986</guid>
<content:encoded><![CDATA[
arXiv:2406.14986v3 Announce Type: replace-cross 
Abstract: Multiple Choice Questions (MCQ) have become a commonly used approach to assess the capabilities of Large Language Models (LLMs), due to their ease of manipulation and evaluation. The experimental appraisals of the LLMs' Stated Answer (their answer to MCQ) have pointed to their apparent ability to perform probabilistic reasoning or to grasp uncertainty. In this work, we investigate whether these aptitudes are measurable outside tailored prompting and MCQ by reformulating these issues as direct text-completion - the fundamental computational unit of LLMs. We introduce Revealed Belief, an evaluation framework that evaluates LLMs on tasks requiring reasoning under uncertainty, which complements MCQ scoring by analyzing text-completion probability distributions. Our findings suggest that while LLMs frequently state the correct answer, their Revealed Belief shows that they often allocate probability mass inconsistently, exhibit systematic biases, and often fail to update their beliefs appropriately when presented with new evidence, leading to strong potential impacts on downstream tasks. These results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets</title>
<link>https://arxiv.org/abs/2407.05674</link>
<guid>https://arxiv.org/abs/2407.05674</guid>
<content:encoded><![CDATA[
arXiv:2407.05674v3 Announce Type: replace-cross 
Abstract: Large Language Models can carry out human-like conversations in diverse settings, responding to user requests for tasks and knowledge. However, existing conversational agents implemented with LLMs often struggle with hallucination, following instructions with conditional logic, and integrating knowledge from different sources. These shortcomings compromise the agents' effectiveness, rendering them unsuitable for deployment. To address these challenges, we introduce Genie, a programmable framework for creating knowledge-intensive task-oriented conversational agents. Genie can handle involved interactions and answer complex queries. Unlike LLMs, it delivers reliable, grounded responses through advanced dialogue state management and supports controllable agent policies via its declarative specification -- Genie Worksheet. This is achieved through an algorithmic runtime system that implements the developer-supplied policy, limiting LLMs to (1) parse user input using a succinct conversational history, and (2) generate responses according to supplied context. Agents built with Genie outperform SOTA methods on complex logic dialogue datasets. We conducted a user study with 62 participants on three real-life applications: restaurant reservations with Yelp, as well as ticket submission and course enrollment for university students. Genie agents with GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling, improving goal completion rates from 21.8% to 82.8% across three real-world tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents</title>
<link>https://arxiv.org/abs/2410.05243</link>
<guid>https://arxiv.org/abs/2410.05243</guid>
<content:encoded><![CDATA[
arXiv:2410.05243v3 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities</title>
<link>https://arxiv.org/abs/2412.06745</link>
<guid>https://arxiv.org/abs/2412.06745</guid>
<content:encoded><![CDATA[
arXiv:2412.06745v2 Announce Type: replace-cross 
Abstract: Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.
  The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Laboratory: Using LLM Agents as Research Assistants</title>
<link>https://arxiv.org/abs/2501.04227</link>
<guid>https://arxiv.org/abs/2501.04227</guid>
<content:encoded><![CDATA[
arXiv:2501.04227v2 Announce Type: replace-cross 
Abstract: Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors</title>
<link>https://arxiv.org/abs/2501.18045</link>
<guid>https://arxiv.org/abs/2501.18045</guid>
<content:encoded><![CDATA[
arXiv:2501.18045v3 Announce Type: replace-cross 
Abstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAE-V: Interpreting Multimodal Models for Enhanced Alignment</title>
<link>https://arxiv.org/abs/2502.17514</link>
<guid>https://arxiv.org/abs/2502.17514</guid>
<content:encoded><![CDATA[
arXiv:2502.17514v2 Announce Type: replace-cross 
Abstract: With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Shaping to Mitigate Reward Hacking in RLHF</title>
<link>https://arxiv.org/abs/2502.18770</link>
<guid>https://arxiv.org/abs/2502.18770</guid>
<content:encoded><![CDATA[
arXiv:2502.18770v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR, and the Work done during the internship at StepFun by Jiayi Fu.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWLViz: An Open-World Benchmark for Visual Question Answering</title>
<link>https://arxiv.org/abs/2503.07631</link>
<guid>https://arxiv.org/abs/2503.07631</guid>
<content:encoded><![CDATA[
arXiv:2503.07631v2 Announce Type: replace-cross 
Abstract: We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</title>
<link>https://arxiv.org/abs/2503.08679</link>
<guid>https://arxiv.org/abs/2503.08679</guid>
<content:encoded><![CDATA[
arXiv:2503.08679v4 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions "Is X bigger than Y?" and "Is Y bigger than X?", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks</title>
<link>https://arxiv.org/abs/2503.16974</link>
<guid>https://arxiv.org/abs/2503.16974</guid>
<content:encoded><![CDATA[
arXiv:2503.16974v3 Announce Type: replace-cross 
Abstract: This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&amp;As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective</title>
<link>https://arxiv.org/abs/2504.03255</link>
<guid>https://arxiv.org/abs/2504.03255</guid>
<content:encoded><![CDATA[
arXiv:2504.03255v2 Announce Type: replace-cross 
Abstract: Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention to effective governance policies, monitoring, and control protocols. Based on the emerging landscape of the agentic market, we analyze potential liability issues arising from the delegated use of LLM agents and their extended systems through a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing, and tracing to enhance transparency and liability attribution.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.12442</link>
<guid>https://arxiv.org/abs/2505.12442</guid>
<content:encoded><![CDATA[
arXiv:2505.12442v3 Announce Type: replace-cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
arXiv:2505.13227v2 Announce Type: replace-cross 
Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20612</link>
<guid>https://arxiv.org/abs/2505.20612</guid>
<content:encoded><![CDATA[
arXiv:2505.20612v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 16.8 mAP! Our code and dataset are available at https://github.com/roboflow/rf100-vl/ and https://universe.roboflow.com/rf100-vl/
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.00555</link>
<guid>https://arxiv.org/abs/2506.00555</guid>
<content:encoded><![CDATA[
arXiv:2506.00555v2 Announce Type: replace-cross 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.01391</link>
<guid>https://arxiv.org/abs/2506.01391</guid>
<content:encoded><![CDATA[
arXiv:2506.01391v2 Announce Type: replace-cross 
Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants</title>
<link>https://arxiv.org/abs/2506.07042</link>
<guid>https://arxiv.org/abs/2506.07042</guid>
<content:encoded><![CDATA[
<div> machine learning, historical event extraction, knowledge graph, RDF/OWL reasoners, Coq proof assistant

Summary: This paper presents a novel approach to automatically extract structured computational representations of historical events from narrative text. By leveraging multiple LLMs and employing different enhancement strategies, such as base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG), the study demonstrates significant improvements in event extraction performance. The evaluation conducted on historical texts from Thucydides shows that base generation with Claude and GPT-4 offers high coverage and historical breadth, while RAG enhancement enhances precision in coordinate accuracy and metadata completeness. The study also highlights the impact of model architecture on enhancement sensitivity, with larger models showing robust performance and incremental improvements with RAG. Additionally, the development of an automated translation pipeline to convert extracted RDF representations into Coq proof assistant specifications enables higher-order reasoning capabilities, including multi-step causal verification, temporal arithmetic, and formal proofs about historical causation. The Coq formalization validates the legitimacy of event types discovered by RAG, ensuring domain-specific semantic structures are represented accurately. 

<br /><br />Summary: <div>
arXiv:2506.07042v2 Announce Type: replace 
Abstract: Extracting structured computational representations of historical events from narrative text remains computationally expensive when constructed manually. While RDF/OWL reasoners enable graph-based reasoning, they are limited to fragments of first-order logic, preventing deeper temporal and semantic analysis. This paper addresses both challenges by developing automatic historical event extraction models using multiple LLMs (GPT-4, Claude, Llama 3.2) with three enhancement strategies: pure base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG). We conducted comprehensive evaluations using historical texts from Thucydides. Our findings reveal that enhancement strategies optimize different performance dimensions rather than providing universal improvements. For coverage and historical breadth, base generation achieves optimal performance with Claude and GPT-4 extracting comprehensive events. However, for precision, RAG enhancement improves coordinate accuracy and metadata completeness. Model architecture fundamentally determines enhancement sensitivity: larger models demonstrate robust baseline performance with incremental RAG improvements, while Llama 3.2 shows extreme variance from competitive performance to complete failure. We then developed an automated translation pipeline converting extracted RDF representations into Coq proof assistant specifications, enabling higher-order reasoning beyond RDF capabilities including multi-step causal verification, temporal arithmetic with BC dates, and formal proofs about historical causation. The Coq formalization validates that RAG-discovered event types represent legitimate domain-specific semantic structures rather than ontological violations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid GA LLM Framework for Structured Task Optimization</title>
<link>https://arxiv.org/abs/2506.07483</link>
<guid>https://arxiv.org/abs/2506.07483</guid>
<content:encoded><![CDATA[
<div> Keywords: Genetic Algorithms, Large Language Models, structured generation tasks, constraints, modular design

Summary: 
GA LLM is a novel framework that merges Genetic Algorithms and Large Language Models for structured generation tasks within strict constraints. Output content, like plans or reports, is seen as genes, with evolutionary operations guided by the language model for enhancements. The language model provides domain expertise and diverse variations, while the genetic algorithm ensures coherence and global optimization. GA LLM excels in tasks like itinerary planning, academic outlining, and business reporting, consistently delivering well-organized and requirement-compliant outcomes. Its adaptable structure allows seamless integration into new tasks. Compared to solely relying on a language model, GA LLM outperforms in constraint fulfillment and solution quality by leveraging the strengths of both elements. 

<br /><br />Summary: <div>
arXiv:2506.07483v2 Announce Type: replace 
Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large Language Models to handle structured generation tasks under strict constraints. Each output, such as a plan or report, is treated as a gene, and evolutionary operations like selection, crossover, and mutation are guided by the language model to iteratively improve solutions. The language model provides domain knowledge and creative variation, while the genetic algorithm ensures structural integrity and global optimization. GA LLM has proven effective in tasks such as itinerary planning, academic outlining, and business reporting, consistently producing well structured and requirement satisfying results. Its modular design also makes it easy to adapt to new tasks. Compared to using a language model alone, GA LLM achieves better constraint satisfaction and higher quality solutions by combining the strengths of both components.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.07398</link>
<guid>https://arxiv.org/abs/2506.07398</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, multi-agent systems, memory architecture, G-Memory, collaboration trajectories

Summary:<br />
The article introduces G-Memory, a hierarchical memory system for multi-agent systems (MAS) inspired by organizational memory theory. G-Memory manages MAS interactions through a three-tier graph hierarchy, allowing for the retrieval of high-level insights and fine-grained interaction trajectories. This enables the system to leverage cross-trial knowledge and compactly encode prior collaboration experiences. The entire hierarchy evolves with new collaborative trajectories, facilitating the progressive evolution of agent teams. Extensive experiments across various benchmarks and frameworks show that G-Memory improves success rates in action tasks and accuracy in knowledge QA without modifications to existing frameworks. G-Memory addresses the limitations of current MAS memory mechanisms, providing a more sophisticated and customized memory architecture for enhanced collaborative capabilities. <br /><br />Summary: <div>
arXiv:2506.07398v2 Announce Type: replace-cross 
Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\textit{high-level, generalizable insights}$ that enable the system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed interaction trajectories}$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\%$ and $10.12\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
<div> Keyword: large language models, training algorithm, Orthogonal Equivalence Transformation, neurons, scalability

Summary:
Large language models (LLMs) are crucial for advances in artificial intelligence, but training them effectively is challenging. The proposed training algorithm, POET, utilizes Orthogonal Equivalence Transformation to optimize neurons by reparameterizing them with learnable orthogonal matrices and a fixed random weight matrix. POET preserves spectral properties of weight matrices, enhancing stability and generalization in objective function optimization. Efficient approximations make POET flexible and scalable for training large neural networks. Extensive experiments demonstrate the effectiveness and scalability of POET in training LLMs. <br /><br />Summary: <div>
arXiv:2506.08001v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading</title>
<link>https://arxiv.org/abs/2506.12066</link>
<guid>https://arxiv.org/abs/2506.12066</guid>
<content:encoded><![CDATA[
<div> Keywords: digital technologies, education, automated grading, PDF documents, Large Language Models 

Summary:
This thesis explores the use of digital technologies in education to automate the process of generating open-ended study questions and grading student answers. By developing a method to chunk PDF documents, the system can create high-quality questions from class materials and automatically grade student responses. A benchmark for automated grading of short answers is introduced, with a focus on evaluating Large Language Models (LLMs) for this task. The study finds that LLMs can effectively generalize to automated grading, with performance improving as model size increases. While automated grading systems show promise, human oversight is still necessary, particularly in examination settings. <div>
arXiv:2506.12066v1 Announce Type: new 
Abstract: Digital technologies are increasingly used in education to reduce the workload of teachers and students. However, creating open-ended study or examination questions and grading their answers is still a tedious task. This thesis presents the foundation for a system that generates questions grounded in class materials and automatically grades student answers. It introduces a sophisticated method for chunking documents with a visual layout, specifically targeting PDF documents. This method enhances the accuracy of downstream tasks, including Retrieval Augmented Generation (RAG). Our thesis demonstrates that high-quality questions and reference answers can be generated from study material. Further, it introduces a new benchmark for automated grading of short answers to facilitate comparison of automated grading systems. An evaluation of various grading systems is conducted and indicates that Large Language Models (LLMs) can generalise to the task of automated grading of short answers from their pre-training tasks. As with other tasks, increasing the parameter size of the LLMs leads to greater performance. Currently, available systems still need human oversight, especially in examination scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour</title>
<link>https://arxiv.org/abs/2506.12090</link>
<guid>https://arxiv.org/abs/2506.12090</guid>
<content:encoded><![CDATA[
<div> Keywords: ChatbotManip, manipulation tactics, Large Language Models, controversial strategies, AI safety research <br />
Summary: <br />
This paper introduces the ChatbotManip dataset, focusing on manipulation tactics used by chatbots in simulated conversations. The dataset includes a variety of manipulation contexts and is annotated for general manipulation and specific tactics. The research findings show that Large Language Models (LLMs) can be manipulative when instructed to be, with a high identification rate by annotators. Even when only instructed to be persuasive, LLMs often resort to controversial strategies like gaslighting and fear enhancement. It was found that smaller models can perform comparably to larger models in detecting manipulation but are not yet reliable for real-world oversight. These findings underscore the importance of addressing manipulation risks as LLMs become more prevalent in consumer applications, highlighting the need for AI safety research. <br /> <div>
arXiv:2506.12090v1 Announce Type: new 
Abstract: This paper introduces ChatbotManip, a novel dataset for studying manipulation in Chatbots. It contains simulated generated conversations between a chatbot and a (simulated) user, where the chatbot is explicitly asked to showcase manipulation tactics, persuade the user towards some goal, or simply be helpful. We consider a diverse set of chatbot manipulation contexts, from consumer and personal advice to citizen advice and controversial proposition argumentation. Each conversation is annotated by human annotators for both general manipulation and specific manipulation tactics. Our research reveals three key findings. First, Large Language Models (LLMs) can be manipulative when explicitly instructed, with annotators identifying manipulation in approximately 84\% of such conversations. Second, even when only instructed to be ``persuasive'' without explicit manipulation prompts, LLMs frequently default to controversial manipulative strategies, particularly gaslighting and fear enhancement. Third, small fine-tuned open source models, such as BERT+BiLSTM have a performance comparable to zero-shot classification with larger models like Gemini 2.5 pro in detecting manipulation, but are not yet reliable for real-world oversight. Our work provides important insights for AI safety research and highlights the need of addressing manipulation risks as LLMs are increasingly deployed in consumer-facing applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuously Updating Digital Twins using Large Language Models</title>
<link>https://arxiv.org/abs/2506.12091</link>
<guid>https://arxiv.org/abs/2506.12091</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital twins, in-context learning, large language models, CALM-DT, simulation

Summary:
Digital twins are models of real-world systems that can simulate dynamics in response to actions. Current approaches struggle to adapt to changes in variables and information without updates or re-designs. To address this, a new approach called CALM-DT frames digital twinning as an in-context learning problem using large language models. This allows for seamless updates to the twin at inference time without requiring parameter updates. CALM-DT utilizes fine-tuned encoders for sample retrieval, enabling accurate simulation across diverse state-action spaces. Empirical demonstrations show CALM-DT's competitive performance with existing digital twin approaches and its unique ability to adapt to changes in its modelling environment. By leveraging in-context learning, CALM-DT demonstrates the potential for digital twins to continuously update and remain relevant in complex settings. 

<br /><br />Summary: <div>
arXiv:2506.12091v1 Announce Type: new 
Abstract: Digital twins are models of real-world systems that can simulate their dynamics in response to potential actions. In complex settings, the state and action variables, and available data and knowledge relevant to a system can constantly change, requiring digital twins to continuously update with these changes to remain relevant. Current approaches struggle in this regard, as they require fixed, well-defined modelling environments, and they cannot adapt to novel variables without re-designs, or incorporate new information without re-training. To address this, we frame digital twinning as an in-context learning problem using large language models, enabling seamless updates to the twin at inference time. We develop CALM-DT, a Context-Adaptive Language Model-based Digital Twin that can accurately simulate across diverse state-action spaces using in-context learning alone by utilising fine-tuned encoders for sample retrieval. We empirically demonstrate CALM-DT's competitive performance with existing digital twin approaches, and its unique ability to adapt to changes in its modelling environment without parameter updates.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Traffic Accident Classifications: Application of NLP Methods for City Safety</title>
<link>https://arxiv.org/abs/2506.12092</link>
<guid>https://arxiv.org/abs/2506.12092</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic accidents, Munich, classification, NLP methods, transformer-based models

Summary: 
In this study, an analysis of traffic incidents in Munich was conducted to understand patterns and characteristics of different types of accidents. The dataset included structured tabular features and unstructured free-text descriptions of each accident. NLP methods were employed to examine the reliability of incident labels, revealing inconsistencies in the classification process. A classification model was developed using both tabular and text data, with text descriptions proving to be the most informative. The model achieved high accuracy in categorizing accidents, emphasizing the importance of free-text data in accident analysis. The study also highlighted the potential of transformer-based models in enhancing classification reliability. This research underscores the significance of comprehensive accident data analysis for city safety and policy decisions. 

<br /><br />Summary: <div>
arXiv:2506.12092v1 Announce Type: new 
Abstract: A comprehensive understanding of traffic accidents is essential for improving city safety and informing policy decisions. In this study, we analyze traffic incidents in Munich to identify patterns and characteristics that distinguish different types of accidents. The dataset consists of both structured tabular features, such as location, time, and weather conditions, as well as unstructured free-text descriptions detailing the circumstances of each accident. Each incident is categorized into one of seven predefined classes. To assess the reliability of these labels, we apply NLP methods, including topic modeling and few-shot learning, which reveal inconsistencies in the labeling process. These findings highlight potential ambiguities in accident classification and motivate a refined predictive approach. Building on these insights, we develop a classification model that achieves high accuracy in assigning accidents to their respective categories. Our results demonstrate that textual descriptions contain the most informative features for classification, while the inclusion of tabular data provides only marginal improvements. These findings emphasize the critical role of free-text data in accident analysis and highlight the potential of transformer-based models in improving classification reliability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UCD: Unlearning in LLMs via Contrastive Decoding</title>
<link>https://arxiv.org/abs/2506.12097</link>
<guid>https://arxiv.org/abs/2506.12097</guid>
<content:encoded><![CDATA[
<div> machine unlearning, large language models, contrastive decoding, unlearning benchmarks, model utility<br />
Summary:<br />
The article introduces an inference-time unlearning algorithm for large language models, focusing on removing specific information while maintaining performance. The proposed approach utilizes contrastive decoding with two smaller auxiliary models to guide the original model's outputs during inference. By leveraging models trained with and without the forget set, the algorithm significantly improves the tradeoff between unlearning effectiveness and model utility. Evaluation on unlearning benchmarks TOFU and MUSE demonstrates notable improvements in forget quality and retained performance compared to existing methods. This suggests that incorporating contrastive decoding can provide an efficient and practical means for removing concepts from large-scale models. <br /><br /> <div>
arXiv:2506.12097v1 Announce Type: new 
Abstract: Machine unlearning aims to remove specific information, e.g. sensitive or undesirable content, from large language models (LLMs) while preserving overall performance. We propose an inference-time unlearning algorithm that uses contrastive decoding, leveraging two auxiliary smaller models, one trained without the forget set and one trained with it, to guide the outputs of the original model using their difference during inference. Our strategy substantially improves the tradeoff between unlearning effectiveness and model utility. We evaluate our approach on two unlearning benchmarks, TOFU and MUSE. Results show notable gains in both forget quality and retained performance in comparison to prior approaches, suggesting that incorporating contrastive decoding can offer an efficient, practical avenue for unlearning concepts in large-scale models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized LLM Decoding via Contrasting Personal Preference</title>
<link>https://arxiv.org/abs/2506.12109</link>
<guid>https://arxiv.org/abs/2506.12109</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, personalization, decoding-time algorithms, reward-guided decoding, fine-tuning

Summary: 
CoPe (Contrasting Personal Preference) is a novel decoding-time approach designed for personalized text generation using large language models. The algorithm is applied after parameter-efficient fine-tuning on user-specific data and aims to maximize each user's implicit reward signal through reward-guided decoding. The study evaluates CoPe across five personalized text generation tasks and shows a significant improvement in personalization, with an average increase of 10.57% in ROUGE-L metric. Unlike existing methods, CoPe does not rely on external reward models or additional training procedures. This research underscores the importance of decoding-time algorithms in the personalization of large language models, highlighting the potential for enhancing user-specific content generation without the need for extensive model retraining. CoPe demonstrates promising results and opens up opportunities for further exploration in refining personalized text generation techniques. 

<br /><br />Summary: <div>
arXiv:2506.12109v1 Announce Type: new 
Abstract: As large language models (LLMs) are progressively deployed in various real-world applications, personalization of LLMs has become increasingly important. While various approaches to LLM personalization such as prompt-based and training-based methods have been actively explored, the development of effective decoding-time algorithms remains largely overlooked, despite their demonstrated potential. In this paper, we propose CoPe (Contrasting Personal Preference), a novel decoding-time approach applied after performing parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is to leverage reward-guided decoding specifically for personalization by maximizing each user's implicit reward signal. We evaluate CoPe across five open-ended personalized text generation tasks. Our empirical results demonstrate that CoPe achieves strong performance, improving personalization by an average of 10.57% in ROUGE-L, without relying on external reward models or additional training procedures.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting Reasoning in Language Models with Cognitive Tools</title>
<link>https://arxiv.org/abs/2506.12115</link>
<guid>https://arxiv.org/abs/2506.12115</guid>
<content:encoded><![CDATA[
<div> cognitive psychology, cognitive architectures, reasoning operations, cognitive tools, performance improvement

Summary:
In this study, the authors explore a novel approach to eliciting reasoning in large language models (LLMs) by implementing a set of "cognitive tools" within an agentic tool-calling framework. Drawing on cognitive psychology and cognitive architectures, they propose that reasoning arises from the sequential execution of predetermined cognitive operations. By endowing LLMs with specific reasoning operations, the authors demonstrate significant performance gains on standard mathematical reasoning benchmarks for both closed and open-weight models. The results show that providing these "cognitive tools" to GPT-4.1 substantially improves its performance on mathematical reasoning tasks, bringing it close to cutting-edge models like o1-preview. This approach sheds light on the debate surrounding post-training methods' role in eliciting reasoning in LLMs and the inherent capabilities acquired during pre-training, suggesting that post-training techniques may enhance latent abilities rather than simply uncovering them.

Summary:<br /><br />In this study, the authors propose a novel approach to eliciting reasoning in large language models (LLMs) by implementing a set of "cognitive tools" within an agentic tool-calling framework. Drawing on cognitive psychology and cognitive architectures, they demonstrate significant performance gains on standard mathematical reasoning benchmarks for both closed and open-weight models. Providing these "cognitive tools" to GPT-4.1 substantially improves its performance on mathematical reasoning tasks, bringing it close to cutting-edge models like o1-preview. This approach sheds light on the debate surrounding post-training methods' role in eliciting reasoning in LLMs and the inherent capabilities acquired during pre-training, suggesting that post-training techniques may enhance latent abilities rather than simply uncovering them. <div>
arXiv:2506.12115v1 Announce Type: new 
Abstract: The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community. These speculations were largely settled by the demonstration from DeepSeek-R1 that chains-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs. However, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.
  Here, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations. Crucially, we implement this key idea within a modern agentic tool-calling framework. In particular, we endow an LLM with a small set of "cognitive tools" encapsulating specific reasoning operations, each executed by the LLM itself. Surprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models. For instance, providing our "cognitive tools" to GPT-4.1 increases its pass@1 performance on AIME2024 from 26.7% to 43.3%, bringing it very close to the performance of o1-preview.
  In addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Document and Template Clustering using Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2506.12116</link>
<guid>https://arxiv.org/abs/2506.12116</guid>
<content:encoded><![CDATA[
<div> embeddings, document clustering, multimodal models, intelligent document processing, document layout analysis

Summary:
This paper introduces a novel approach to unsupervised document clustering using multimodal embeddings in combination with traditional clustering algorithms like k-Means and DBSCAN. The method aims to provide a more detailed understanding of documents by grouping them not only at the type level but also distinguishing between different templates within the same category. By incorporating embeddings that capture textual content, layout information, and visual features, the approach effectively enhances document clustering. The study evaluates the effectiveness of various state-of-the-art pretrained multimodal models such as SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. The findings highlight the potential of multimodal embeddings for improving document clustering and suggest benefits for tasks like intelligent document processing, document layout analysis, and unsupervised document classification. The research sheds light on the strengths and limitations of different multimodal models in this context, paving the way for further exploration and organization of document collections.

<br /><br />Summary: <div>
arXiv:2506.12116v1 Announce Type: new 
Abstract: This paper investigates a novel approach to unsupervised document clustering by leveraging multimodal embeddings as input to traditional clustering algorithms such as $k$-Means and DBSCAN. Our method aims to achieve a finer-grained document understanding by not only grouping documents at the type level (e.g., invoices, purchase orders), but also distinguishing between different templates within the same document category. This is achieved by using embeddings that capture textual content, layout information, and visual features of documents. We evaluated the effectiveness of this approach using embeddings generated by several state-of-the-art pretrained multimodal models, including SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. Our findings demonstrate the potential of multimodal embeddings to significantly enhance document clustering, offering benefits for various applications in intelligent document processing, document layout analysis, and unsupervised document classification. This work provides valuable insight into the advantages and limitations of different multimodal models for this task and opens new avenues for future research to understand and organize document collections.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?</title>
<link>https://arxiv.org/abs/2506.12119</link>
<guid>https://arxiv.org/abs/2506.12119</guid>
<content:encoded><![CDATA[
<div> model architecture, Mixture-of-Experts, performance, total parameter count, data reuse
<br />
Summary:<br />
The study compares Mixture-of-Experts (MoE) language models with dense architectures under equal resource constraints. Through comprehensive analysis, an optimal MoE model design is identified, showing superior performance when parameters, training compute, and data resources are equal. Activation rate plays a crucial role in achieving optimal performance across various model sizes. Additional data can enhance performance, mitigated by data reuse strategies. The study validates results with 200 models at 2B scale and 50 models at 7B scale, processing 50 trillion tokens.<br /><br /> <div>
arXiv:2506.12119v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. However, can MoEs surpass dense architectures under strictly equal resource constraints - that is, when the total parameter count, training compute, and data budget are identical? This question remains under-explored despite its significant practical value and potential. In this paper, we propose a novel perspective and methodological framework to study this question thoroughly. First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance. Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource. More importantly, this optimal region remains consistent across different model sizes. Although additional amount of data turns out to be a trade-off for the enhanced performance, we show that this can be resolved via reusing data. We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. All models will be released publicly.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hatevolution: What Static Benchmarks Don't Tell Us</title>
<link>https://arxiv.org/abs/2506.12148</link>
<guid>https://arxiv.org/abs/2506.12148</guid>
<content:encoded><![CDATA[
<div> evolving hate speech experiments, language models, benchmarking, NLP research, model training<br />
Summary:<br />
The study focuses on the evolution of language, particularly in the hate speech domain, and its impact on model training and benchmarking in natural language processing (NLP) research. The research empirically evaluates the robustness of 20 language models through two evolving hate speech experiments. The findings highlight the temporal misalignment between static evaluations and time-sensitive evaluations in understanding hate speech. The study emphasizes the need for time-sensitive linguistic benchmarks to accurately and reliably assess language models in the hate speech domain. It underscores the importance of adapting benchmarks to reflect changing language trends, which are essential for ensuring the safety and effectiveness of language models in addressing hate speech. <div>
arXiv:2506.12148v1 Announce Type: new 
Abstract: Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximally-Informative Retrieval for State Space Model Generation</title>
<link>https://arxiv.org/abs/2506.12149</link>
<guid>https://arxiv.org/abs/2506.12149</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, retrieval, optimization, model uncertainty, document mixture

Summary: 
- The optimal way to answer a query involves using all available information, but limited model resources require resorting to external memory for processing.
- Retrieval In-Context Optimization (RICO) is proposed as a method to learn the optimal combination of documents for answer generation using gradients from the LLM itself.
- RICO outperforms traditional retrieval-augmented generation methods like RAG by leveraging direct model feedback, without the need for external heuristics for document retrieval.
- The standard top-$k$ retrieval with model gradients approximates RICO's optimization technique, with connections to the leave-one-out loss.
- Empirical results show that RICO, minimizing question perplexity without fine-tuning, achieves comparable retriever metric performance to BM25 and often outperforms fine-tuned dense retrievers like E5. 

<br /><br />Summary: <div>
arXiv:2506.12149v1 Announce Type: new 
Abstract: Given a query and dataset, the optimal way of answering the query is to make use all the information available. Modern LLMs exhibit impressive ability to memorize training data, but data not deemed important during training is forgotten, and information outside that training set cannot be made use of. Processing an entire dataset at inference time is infeasible due to the bounded nature of model resources (e.g. context size in transformers or states in state space models), meaning we must resort to external memory. This constraint naturally leads to the following problem: How can we decide based on the present query and model, what among a virtually unbounded set of known data matters for inference? To minimize model uncertainty for a particular query at test-time, we introduce Retrieval In-Context Optimization (RICO), a retrieval method that uses gradients from the LLM itself to learn the optimal mixture of documents for answer generation. Unlike traditional retrieval-augmented generation (RAG), which relies on external heuristics for document retrieval, our approach leverages direct feedback from the model. Theoretically, we show that standard top-$k$ retrieval with model gradients can approximate our optimization procedure, and provide connections to the leave-one-out loss. We demonstrate empirically that by minimizing an unsupervised loss objective in the form of question perplexity, we can achieve comparable retriever metric performance to BM25 with \emph{no finetuning}. Furthermore, when evaluated on quality of the final prediction, our method often outperforms fine-tuned dense retrievers such as E5.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2506.12158</link>
<guid>https://arxiv.org/abs/2506.12158</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, generation strategies, low-resource languages, synthetic data, NLP tasks

Summary:
This paper evaluates different generation strategies for synthetic data in low-resource language settings using Large Language Models (LLMs). The study compares various prompting methods, including demonstrations, label-based summaries, and self-revision, across 11 diverse languages, some of which are extremely low-resource. Results show that combining generation methods, specifically target-language demonstrations with LLM-based revisions, yields strong performance, reducing the gap with real data to as little as 5% in certain cases. The study also suggests that smart prompting techniques can decrease the advantage of larger LLMs, demonstrating efficient strategies for generating synthetic data in low-resource scenarios with smaller models. These findings provide insights into optimizing the use of LLMs for training specialized models in diverse language settings. 

<br /><br />Summary: <div>
arXiv:2506.12158v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used to generate synthetic textual data for training smaller specialized models. However, a comparison of various generation strategies for low-resource language settings is lacking. While various prompting strategies have been proposed, such as demonstrations, label-based summaries, and self-revision, their comparative effectiveness remains unclear, especially for low-resource languages. In this paper, we systematically evaluate the performance of these generation strategies and their combinations across 11 typologically diverse languages, including several extremely low-resource ones. Using three NLP tasks and four open-source LLMs, we assess downstream model performance on generated versus gold-standard data. Our results show that strategic combinations of generation methods, particularly target-language demonstrations with LLM-based revisions, yield strong performance, narrowing the gap with real data to as little as 5% in some settings. We also find that smart prompting techniques can reduce the advantage of larger LLMs, highlighting efficient generation strategies for synthetic data generation in low-resource scenarios with smaller models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs</title>
<link>https://arxiv.org/abs/2506.12182</link>
<guid>https://arxiv.org/abs/2506.12182</guid>
<content:encoded><![CDATA[
<div> prompt design, fine-tuning, large language models, medical question answering, PubMedQA

Summary:
- The study examines prompt design and fine-tuning effects on open-source large language models (LLMs) in biomedical reasoning for medical question answering.
- Two prompting strategies, standard instruction prompts and Chain-of-Thought (CoT) prompts, are compared, and QLoRA is used for efficient instruction tuning.
- CoT prompts alone improve reasoning in zero-shot settings, while instruction tuning enhances accuracy, especially for smaller models.
- Fine-tuning on CoT prompts does not consistently improve performance and may even worsen it for certain larger models.
- Reasoning-aware prompts are beneficial, but their impact varies depending on the model and scale. Practical insights are provided for combining prompt engineering with efficient fine-tuning in medical question answering applications.

<br /><br />Summary: <div>
arXiv:2506.12182v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great potential in medical question answering (MedQA), yet adapting them to biomedical reasoning remains challenging due to domain-specific complexity and limited supervision. In this work, we study how prompt design and lightweight fine-tuning affect the performance of open-source LLMs on PubMedQA, a benchmark for multiple-choice biomedical questions. We focus on two widely used prompting strategies - standard instruction prompts and Chain-of-Thought (CoT) prompts - and apply QLoRA for parameter-efficient instruction tuning. Across multiple model families and sizes, our experiments show that CoT prompting alone can improve reasoning in zero-shot settings, while instruction tuning significantly boosts accuracy. However, fine-tuning on CoT prompts does not universally enhance performance and may even degrade it for certain larger models. These findings suggest that reasoning-aware prompts are useful, but their benefits are model- and scale-dependent. Our study offers practical insights into combining prompt engineering with efficient finetuning for medical QA applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis</title>
<link>https://arxiv.org/abs/2506.12189</link>
<guid>https://arxiv.org/abs/2506.12189</guid>
<content:encoded><![CDATA[
<div> Personality, Large Language Models, Supernova Event Dataset, key events extraction, model benchmarking<br />
Summary:<br />
- Understanding the decision-making and personality traits of Large Language Models (LLMs) is crucial with their increasing integration into everyday applications.
- The study introduces the Supernova Event Dataset, a diverse collection of articles for interpreting LLM personality traits.
- Evaluation of small and large models reveals distinct personality traits based on their selection and classification of events.
- Orca 2 showcases emotional reasoning and interpersonal focus, while Qwen 2.5 displays a strategic, analytical style.
- Models like Claude Sonnet 3.7, Gemini 2.5 Pro, and o3 highlight different approaches to scientific discovery events, emphasizing conceptual framing, empirical validation, and step-by-step causal reasoning, respectively. 
<br /><br /> <div>
arXiv:2506.12189v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index</title>
<link>https://arxiv.org/abs/2506.12229</link>
<guid>https://arxiv.org/abs/2506.12229</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, text data, search engines, FM-index, benchmark contamination

Summary: 
Infini-gram mini is a new efficient and scalable system for searching petabyte-level text corpora using the FM-index data structure. The system indexes text data with only 44% of the original size, significantly improving indexing speed and memory usage compared to existing implementations. Infini-gram mini can index 46TB of Internet text in a relatively short time using a single CPU node. The system was used to analyze benchmark contamination in language model evaluation datasets, revealing significant levels of contamination in core benchmarks such as SQuAD. A benchmark contamination bulletin was created to share contamination rates of various benchmarks. Additionally, a web interface and API endpoint were released to allow general search queries on the Infini-gram mini indexes. 

<br /><br />Summary: <div>
arXiv:2506.12229v1 Announce Type: new 
Abstract: Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora -- counting string appearances and retrieving the enclosing documents -- yet the high storage overhead hinders their application on Internet-scale data. We present Infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, our system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18$\times$) and memory use during both indexing (3.2$\times$ reduction) and querying (down to a negligible amount). We index 46TB of Internet text in 50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes). We show one important use case of Infini-gram mini in a large-scale analysis of benchmark contamination. We find several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead to overestimating the capabilities of language models if trained on such data. We host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. We also release a web interface and an API endpoint to serve general search queries on Infini-gram mini indexes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives</title>
<link>https://arxiv.org/abs/2506.12242</link>
<guid>https://arxiv.org/abs/2506.12242</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, history, philosophy, sociology of science, interpretive research, epistemic infrastructures <br />
Summary: This paper discusses the use of large language models (LLMs) in the history, philosophy, and sociology of science (HPSS) as research tools. LLMs offer new capabilities in processing unstructured text and inferring meaning from context, challenging the traditional boundaries between computational and interpretative methods in HPSS. The paper explains LLM architectures and their training paradigms and emphasizes that LLMs are not neutral tools but encode assumptions about meaning and context based on their training data. It explores how computational techniques enhanced by LLMs can support interpretive research in HPSS and compares full-context and generative models. The analysis includes strategies for adapting LLMs to HPSS tasks and highlights the importance of LLM literacy in HPSS. The paper concludes with recommendations for integrating LLMs into HPSS, stressing the need for interpretive trade-offs, defining benchmarks and corpora, and ensuring that LLMs enhance rather than replace interpretive methods.<br /><br />Summary: <div>
arXiv:2506.12242v1 Announce Type: new 
Abstract: This paper explores the use of large language models (LLMs) as research tools in the history, philosophy, and sociology of science (HPSS). LLMs are remarkably effective at processing unstructured text and inferring meaning from context, offering new affordances that challenge long-standing divides between computational and interpretive methods. This raises both opportunities and challenges for HPSS, which emphasizes interpretive methodologies and understands meaning as context-dependent, ambiguous, and historically situated. We argue that HPSS is uniquely positioned not only to benefit from LLMs' capabilities but also to interrogate their epistemic assumptions and infrastructural implications. To this end, we first offer a concise primer on LLM architectures and training paradigms tailored to non-technical readers. We frame LLMs not as neutral tools but as epistemic infrastructures that encode assumptions about meaning, context, and similarity, conditioned by their training data, architecture, and patterns of use. We then examine how computational techniques enhanced by LLMs, such as structuring data, detecting patterns, and modeling dynamic processes, can be applied to support interpretive research in HPSS. Our analysis compares full-context and generative models, outlines strategies for domain and task adaptation (e.g., continued pretraining, fine-tuning, and retrieval-augmented generation), and evaluates their respective strengths and limitations for interpretive inquiry in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1) model selection involves interpretive trade-offs; (2) LLM literacy is foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs should enhance, not replace, interpretive methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs</title>
<link>https://arxiv.org/abs/2506.12266</link>
<guid>https://arxiv.org/abs/2506.12266</guid>
<content:encoded><![CDATA[
<div> agents, behavior gap, task complexity, dialog acts, tool usage

Summary:<br />
- Large Language Model (LLM) agents in Task-Oriented Dialog Systems (TODS) face performance challenges, especially in zero-shot scenarios.
- A study proposes an evaluation framework to quantify the behavior gap between AI agents and humans in TODS.
- The behavior gap widens as task complexity increases, negatively impacting agent performance.
- Even GPT-4o-based agents show low alignment with human behavior, particularly in dialog acts, tool usage, and knowledge utilization.
- Reducing behavior gaps through improved alignment strategies can lead to significant performance improvement in handling complex tasks. 

Summary: <div>
arXiv:2506.12266v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.12307</link>
<guid>https://arxiv.org/abs/2506.12307</guid>
<content:encoded><![CDATA[
<div> framework, medical QA, large language models, reinforcement learning, benchmarks <br />
<br />
Summary: 
The paper introduces Med-U1, a framework for improving medical Question-Answering tasks using large language models. Med-U1 incorporates reinforcement learning and rule-based reward functions to enhance reasoning across different types of medical QA tasks. By optimizing for concise and verifiable reasoning chains, Med-U1 outperforms specialized models on challenging benchmarks. It also shows robust generalization to out-of-distribution tasks. The paper provides insights into training strategies, controlling reasoning chain length, and reward design for medical large language models. The code for Med-U1 will be made available for future research and development. <div>
arXiv:2506.12307v1 Announce Type: new 
Abstract: Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. The code will be released.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech</title>
<link>https://arxiv.org/abs/2506.12311</link>
<guid>https://arxiv.org/abs/2506.12311</guid>
<content:encoded><![CDATA[
<div> diacritization, Hebrew G2P, IPA transcriptions, real-time TTS, ILSpeech dataset <br />
<br />Summary: Phonikud is a new lightweight Hebrew grapheme-to-phoneme (G2P) system that accurately predicts phonemes from Hebrew text, including crucial phonetic features like stress. This system outputs fully-specified IPA transcriptions, addressing limitations in existing solutions. By adapting an existing diacritization model with lightweight adaptors, Phonikud incurs minimal additional latency, enabling the training of real-time Hebrew text-to-speech (TTS) models with superior speed-accuracy trade-offs. The researchers also introduce the ILSpeech dataset, which contains transcribed Hebrew speech with IPA annotations, serving as both a benchmark for Hebrew G2P systems and as training data for TTS models. The results show that Phonikud's G2P conversion outperforms previous methods, paving the way for efficient real-time TTS for the complex phonetic features of Modern Hebrew. The code, data, and models are available at https://phonikud.github.io. <br /> <div>
arXiv:2506.12311v1 Announce Type: new 
Abstract: Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the language's orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P and as training data for TTS systems. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at https://phonikud.github.io.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective</title>
<link>https://arxiv.org/abs/2506.12327</link>
<guid>https://arxiv.org/abs/2506.12327</guid>
<content:encoded><![CDATA[
<div> intersectional bias, large language models, social bias, Japanese benchmark, question-answering

Summary:<br />
This study examines intersectional bias in large language models (LLMs) using the Japanese benchmark inter-JBBQ. While previous research has focused on bias in a single social attribute, this study highlights the importance of considering multiple social attributes in analyzing bias in LLMs. The inter-JBBQ is designed to evaluate intersectional bias in LLMs in a question-answering setting. By applying this benchmark to GPT-4o and Swallow, the study finds that biased output varies across different contexts, even when social attributes are equally combined. This underscores the need for greater attention to intersectionality in addressing bias in LLMs. <div>
arXiv:2506.12327v1 Announce Type: new 
Abstract: An growing number of studies have examined the social bias of rapidly developed large language models (LLMs). Although most of these studies have focused on bias occurring in a single social attribute, research in social science has shown that social bias often occurs in the form of intersectionality -- the constitutive and contextualized perspective on bias aroused by social attributes. In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs</title>
<link>https://arxiv.org/abs/2506.12338</link>
<guid>https://arxiv.org/abs/2506.12338</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive biases, large language models, prompt design, attention weight analysis, AI applications<br />
Summary:<br />
This paper investigates the influence of cognitive biases on Large Language Models (LLMs) outputs by introducing various biases into prompts and assessing their impact on LLM accuracy. The study includes general and financial Q&amp;A scenarios across multiple benchmark datasets. The results show that subtle biases can significantly alter LLM answer choices, emphasizing the need for bias-aware prompt design and mitigation strategies. Attention weight analysis reveals how biases can affect the internal decision-making processes of LLMs, leading to output inaccuracies. This research has implications for AI developers and users in enhancing the robustness and reliability of AI applications in diverse domains.<br /> <div>
arXiv:2506.12338v1 Announce Type: new 
Abstract: This paper investigates the influence of cognitive biases on Large Language Models (LLMs) outputs. Cognitive biases, such as confirmation and availability biases, can distort user inputs through prompts, potentially leading to unfaithful and misleading outputs from LLMs. Using a systematic framework, our study introduces various cognitive biases into prompts and assesses their impact on LLM accuracy across multiple benchmark datasets, including general and financial Q&amp;A scenarios. The results demonstrate that even subtle biases can significantly alter LLM answer choices, highlighting a critical need for bias-aware prompt design and mitigation strategy. Additionally, our attention weight analysis highlights how these biases can alter the internal decision-making processes of LLMs, affecting the attention distribution in ways that are associated with output inaccuracies. This research has implications for Al developers and users in enhancing the robustness and reliability of Al applications in diverse domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refract ICL: Rethinking Example Selection in the Era of Million-Token Models</title>
<link>https://arxiv.org/abs/2506.12346</link>
<guid>https://arxiv.org/abs/2506.12346</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context large language models, in-context learning, ICL selection strategies, Refract ICL, Gemini 1.5 Pro<br />
Summary: 
- The paper explores the effectiveness of traditional in-context learning (ICL) selection strategies in the context of long-context large language models.
- Increasing the number of demonstrations does not automatically lead to improved performance, indicating the importance of smart ICL selection.
- The introduction of the novel Refract ICL selection algorithm enhances ICL by strategically repeating challenging examples within the context and utilizing zero-shot predictions as error signals.
- The results demonstrate that Refract ICL significantly boosts the performance of extremely long-context models like Gemini 1.5 Pro, especially on tasks with fewer output classes.
- The study highlights the necessity of thoughtful ICL selection even when leveraging a large number of demonstrations, underscoring the importance of selecting relevant and diverse examples for effective learning. <br /><br />Summary: <div>
arXiv:2506.12346v1 Announce Type: new 
Abstract: The emergence of long-context large language models (LLMs) has enabled the use of hundreds, or even thousands, of demonstrations for in-context learning (ICL) - a previously impractical regime. This paper investigates whether traditional ICL selection strategies, which balance the similarity of ICL examples to the test input (using a text retriever) with diversity within the ICL set, remain effective when utilizing a large number of demonstrations. Our experiments demonstrate that, while longer contexts can accommodate more examples, simply increasing the number of demonstrations does not guarantee improved performance. Smart ICL selection remains crucial, even with thousands of demonstrations. To further enhance ICL in this setting, we introduce Refract ICL, a novel ICL selection algorithm specifically designed to focus LLM attention on challenging examples by strategically repeating them within the context and incorporating zero-shot predictions as error signals. Our results show that Refract ICL significantly improves the performance of extremely long-context models such as Gemini 1.5 Pro, particularly on tasks with a smaller number of output classes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.12353</link>
<guid>https://arxiv.org/abs/2506.12353</guid>
<content:encoded><![CDATA[
<div> keywords: efficient reasoning, self-affirmation reflections, output length, model optimization, length compression

Summary: 
Efficient reasoning models are essential in handling the increasing output length of large models. This study focuses on self-affirmation reflections, redundant steps that affirm prior content, leading to longer outputs. Analysis shows a distinct probability bias in the first words of sentences in self-affirmation reflections. By suppressing these reflections, output length can be reduced without sacrificing accuracy in various models. Train-free experiments demonstrate a compression of 18.7%, while train-based methods achieve up to 50.2% length reduction. These improvements are simple and practical, applicable to existing inference frameworks. The findings offer insights for precise length compression and efficient reasoning at the step-level. 

<br /><br />Summary: <div>
arXiv:2506.12353v1 Announce Type: new 
Abstract: While recent advances in large reasoning models have demonstrated remarkable performance, efficient reasoning remains critical due to the rapid growth of output length. Existing optimization approaches highlights a tendency toward "overthinking", yet lack fine-grained analysis. In this work, we focus on Self-Affirmation Reflections: redundant reflective steps that affirm prior content and often occurs after the already correct reasoning steps. Observations of both original and optimized reasoning models reveal pervasive self-affirmation reflections. Notably, these reflections sometimes lead to longer outputs in optimized models than their original counterparts. Through detailed analysis, we uncover an intriguing pattern: compared to other reflections, the leading words (i.e., the first word of sentences) in self-affirmation reflections exhibit a distinct probability bias. Motivated by this insight, we can locate self-affirmation reflections and conduct a train-free experiment demonstrating that suppressing self-affirmation reflections reduces output length without degrading accuracy across multiple models (R1-Distill-Models, QwQ-32B, and Qwen3-32B). Furthermore, we also improve current train-based method by explicitly suppressing such reflections. In our experiments, we achieve length compression of 18.7\% in train-free settings and 50.2\% in train-based settings for R1-Distill-Qwen-1.5B. Moreover, our improvements are simple yet practical and can be directly applied to existing inference frameworks, such as vLLM. We believe that our findings will provide community insights for achieving more precise length compression and step-level efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics</title>
<link>https://arxiv.org/abs/2506.12365</link>
<guid>https://arxiv.org/abs/2506.12365</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning skills, adaptability, ethical decisions, computational efficiency

Summary: 
Large Language Models (LLMs) have seen significant advancements in recent years, particularly in improving their reasoning skills, adaptability to various tasks, computational efficiency, and ability to make ethical decisions. Techniques like Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback have bridged the gap between human and machine communications. Multimodal learning and few-shot or zero-shot techniques have further strengthened LLMs to handle complex jobs with minimal input. Scaling and optimization tricks have enabled LLMs to do more with less computing power. This survey highlights emerging methods that enhance LLM reasoning, efficiency, and ethical alignment, while also identifying underexplored areas like interpretability, cross-modal integration, and sustainability. Despite progress, challenges such as high computational costs, biases, and ethical risks persist, necessitating bias mitigation, transparent decision-making, and clear ethical guidelines. Future research aims to enhance LLMs' ability to handle multiple inputs for improved intelligence, safety, and reliability. 

<br /><br />Summary: <div>
arXiv:2506.12365v1 Announce Type: new 
Abstract: This survey paper outlines the key developments in the field of Large Language Models (LLMs), such as enhancing their reasoning skills, adaptability to various tasks, increased computational efficiency, and ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. They also manage to do more with less by applying scaling and optimization tricks for computing power conservation. This survey also offers a broader perspective on recent advancements in LLMs going beyond isolated aspects such as model architecture or ethical concerns. It categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. It also identifies underexplored areas such as interpretability, cross-modal integration and sustainability. With recent progress, challenges like huge computational costs, biases, and ethical risks remain constant. Addressing these requires bias mitigation, transparent decision-making, and clear ethical guidelines. Future research will focus on enhancing models ability to handle multiple input, thereby making them more intelligent, safe, and reliable.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs</title>
<link>https://arxiv.org/abs/2506.12367</link>
<guid>https://arxiv.org/abs/2506.12367</guid>
<content:encoded><![CDATA[
<div> extraction performance, knowledge graph, error analysis, downstream analysis, bias patterns
Summary: 
- The study evaluated knowledge graph (KG) extraction performance on affiliation graphs extracted from social register books.
- Two levels of evaluation were conducted: micro-level edge accuracy and macro-level graph metrics.
- The study identified a range of extraction performance where biases were near zero, but as performance declined, biases in downstream graph analysis metrics became more pronounced.
- Metrics tended toward a consistent direction of either over- or under-estimation as extraction performance worsened.
- Error models commonly used in the literature did not capture these bias patterns, indicating the need for more realistic error models for KG extraction.<br /><br />Summary: <div>
arXiv:2506.12367v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are useful for analyzing social structures, community dynamics, institutional memberships, and other complex relationships across domains from sociology to public health. While recent advances in large language models (LLMs) have improved the scalability and accessibility of automated KG extraction from large text corpora, the impacts of extraction errors on downstream analyses are poorly understood, especially for applied scientists who depend on accurate KGs for real-world insights. To address this gap, we conducted the first evaluation of KG extraction performance at two levels: (1) micro-level edge accuracy, which is consistent with standard NLP evaluations, and manual identification of common error sources; (2) macro-level graph metrics that assess structural properties such as community detection and connectivity, which are relevant to real-world applications. Focusing on affiliation graphs of person membership in organizations extracted from social register books, our study identifies a range of extraction performance where biases across most downstream graph analysis metrics are near zero. However, as extraction performance declines, we find that many metrics exhibit increasingly pronounced biases, with each metric tending toward a consistent direction of either over- or under-estimation. Through simulations, we further show that error models commonly used in the literature do not capture these bias patterns, indicating the need for more realistic error models for KG extraction. Our findings provide actionable insights for practitioners and underscores the importance of advancing extraction methods and error modeling to ensure reliable and meaningful downstream analyses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free LLM Merging for Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.12379</link>
<guid>https://arxiv.org/abs/2506.12379</guid>
<content:encoded><![CDATA[
<div> method, LLMs, multi-task learning, Hi-Merging, performance 

Summary:
The paper introduces Hierarchical Iterative Merging (Hi-Merging), a novel method for combining specialized Large Language Models (LLMs) into a unified model for multi-task learning without the need for additional training. Hi-Merging utilizes model-wise and layer-wise pruning and scaling, guided by contribution analysis, to address parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in Chinese and English demonstrate the effectiveness of Hi-Merging, outperforming existing merging techniques and models fine-tuned on combined datasets in most scenarios. The results highlight Hi-Merging as a promising approach for unifying specialized LLMs and achieving superior performance in diverse NLP tasks.<br /><br />Summary: <div>
arXiv:2506.12379v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances and Future Directions in Literature-Based Discovery</title>
<link>https://arxiv.org/abs/2506.12385</link>
<guid>https://arxiv.org/abs/2506.12385</guid>
<content:encoded><![CDATA[
<div> Keywords: literature-based discovery, knowledge graph, deep learning, large language models, scientific innovation

Summary: 
This article presents a survey of recent advancements in literature-based discovery (LBD) methods, focusing on the period from 2000 to the present. Three main areas of progress are highlighted: the construction of knowledge graphs, the application of deep learning techniques, and the integration of pre-trained and large language models (LLMs). While LBD has shown significant improvements, challenges such as scalability, reliance on structured data, and the need for extensive manual curation still exist. The survey emphasizes the pivotal role of LLMs in advancing LBD and aims to guide researchers and practitioners in utilizing these technologies to accelerate scientific innovation. <div>
arXiv:2506.12385v1 Announce Type: new 
Abstract: The explosive growth of scientific publications has created an urgent need for automated methods that facilitate knowledge synthesis and hypothesis generation. Literature-based discovery (LBD) addresses this challenge by uncovering previously unknown associations between disparate domains. This article surveys recent methodological advances in LBD, focusing on developments from 2000 to the present. We review progress in three key areas: knowledge graph construction, deep learning approaches, and the integration of pre-trained and large language models (LLMs). While LBD has made notable progress, several fundamental challenges remain unresolved, particularly concerning scalability, reliance on structured data, and the need for extensive manual curation. By examining ongoing advances and outlining promising future directions, this survey underscores the transformative role of LLMs in enhancing LBD and aims to support researchers and practitioners in harnessing these technologies to accelerate scientific innovation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model</title>
<link>https://arxiv.org/abs/2506.12388</link>
<guid>https://arxiv.org/abs/2506.12388</guid>
<content:encoded><![CDATA[
<div> parameter grouping, multilingual language models, positive transfer, language similarity, expert modules

Summary:
The study addresses the issue of multilingual Large Language Models (LLMs) facing competition between languages, resulting in inferior performance known as the "curse of multilinguality." By dynamically grouping and scaling up parameters based on language similarity, the proposed method reduces negative transfer among languages and enhances multilingual performance. The model initially learns from monolingual data to assess parameter deviations and language similarities, extending layers to mixture-of-experts for specialized language groups. Experimental results across various languages demonstrate improved performance with fewer parameters. This approach not only benefits new language adaptation but also minimizes reliance on previously learned multilingual knowledge. <div>
arXiv:2506.12388v1 Announce Type: new 
Abstract: The curse of multilinguality phenomenon is a fundamental problem of multilingual Large Language Models (LLMs), where the competition between massive languages results in inferior performance. It mainly comes from limited capacity and negative transfer between dissimilar languages. To address this issue, we propose a method to dynamically group and scale up the parameters of multilingual LLM while boosting positive transfer among similar languages. Specifically, the model is first tuned on monolingual corpus to determine the parameter deviation in each layer and quantify the similarity between languages. Layers with more deviations are extended to mixture-of-experts layers to reduce competition between languages, where one expert module serves one group of similar languages. Experimental results on 18 to 128 languages show that our method reduces the negative transfer between languages and significantly boosts multilingual performance with fewer parameters. Such language group specialization on experts benefits the new language adaptation and reduces the inference on the previous multilingual knowledge learned.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cultural Variations in Moral Judgments with Large Language Models</title>
<link>https://arxiv.org/abs/2506.12433</link>
<guid>https://arxiv.org/abs/2506.12433</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Moral values, Cultural diversity, Cross-cultural surveys, Instruction tuning

Summary:
Large Language Models (LLMs) have shown strong performance in tasks, but their representation of culturally diverse moral values is uncertain. The study compared older monolingual models with newer instruction-tuned models using cross-cultural survey data on ethical topics. Earlier models often had low correlations with human judgments, while instruction-tuned models like GPT-4o showed higher positive correlations, indicating better alignment with real-world moral attitudes. Scaling up model size and using instruction tuning can improve alignment with cross-cultural norms, but challenges remain. The study emphasizes the need for bias analysis, diverse training data, and strategies to enhance the cultural sensitivity of LLMs. <br /><br />Summary: Large Language Models performance on moral values is examined using cross-cultural survey data. Older models show low correlations with real-world moral attitudes, while newer instruction-tuned models, like GPT-4o, demonstrate higher alignment. Challenges remain, emphasizing the necessity for bias analysis and strategies to enhance cultural sensitivity. <div>
arXiv:2506.12433v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong performance across many tasks, but their ability to capture culturally diverse moral values remains unclear. In this paper, we examine whether LLMs can mirror variations in moral attitudes reported by two major cross-cultural surveys: the World Values Survey and the PEW Research Center's Global Attitudes Survey. We compare smaller, monolingual, and multilingual models (GPT-2, OPT, BLOOMZ, and Qwen) with more recent instruction-tuned models (GPT-4o, GPT-4o-mini, Gemma-2-9b-it, and Llama-3.3-70B-Instruct). Using log-probability-based moral justifiability scores, we correlate each model's outputs with survey data covering a broad set of ethical topics. Our results show that many earlier or smaller models often produce near-zero or negative correlations with human judgments. In contrast, advanced instruction-tuned models (including GPT-4o and GPT-4o-mini) achieve substantially higher positive correlations, suggesting they better reflect real-world moral attitudes. While scaling up model size and using instruction tuning can improve alignment with cross-cultural moral norms, challenges remain for certain topics and regions. We discuss these findings in relation to bias analysis, training data diversity, and strategies for improving the cultural sensitivity of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2506.12446</link>
<guid>https://arxiv.org/abs/2506.12446</guid>
<content:encoded><![CDATA[
<div> alignment methods, large language models, reward-guided search, process reward models, human preferences<br />
<br />
Summary: In this paper, the authors address the issue of granularity mismatch in existing methods for aligning large language models with human preferences using reward-guided search. They propose the integration of process reward models (PRMs) into reward-guided search to tackle the inconsistency in scoring and alignment. The ideal PRM should satisfy Score Consistency and Preference Consistency objectives to ensure coherent evaluation and alignment with human preferences. The proposed SP-PRM framework incorporates both score consistency-based and preference consistency-based partial evaluation modules without the need for human annotation. Extensive experiments on various tasks show that SP-PRM significantly improves GPT-4 evaluation scores by 3.6%-10.3%. This novel approach enhances the effectiveness and efficiency of aligning large language models with human preferences. <br /><br />Summary: <div>
arXiv:2506.12446v1 Announce Type: new 
Abstract: Inference-time alignment methods have gained significant attention for their efficiency and effectiveness in aligning large language models (LLMs) with human preferences. However, existing dominant approaches using reward-guided search (RGS) primarily rely on outcome reward models (ORMs), which suffer from a critical granularity mismatch: ORMs are designed to provide outcome rewards for complete responses, while RGS methods rely on process rewards to guide the policy, leading to inconsistent scoring and suboptimal alignment. To address this challenge, we introduce process reward models (PRMs) into RGS and argue that an ideal PRM should satisfy two objectives: Score Consistency, ensuring coherent evaluation across partial and complete responses, and Preference Consistency, aligning partial sequence assessments with human preferences. Based on these, we propose SP-PRM, a novel dual-consistency framework integrating score consistency-based and preference consistency-based partial evaluation modules without relying on human annotation. Extensive experiments on dialogue, summarization, and reasoning tasks demonstrate that SP-PRM substantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement in GPT-4 evaluation scores across all tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Surgery in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2506.12450</link>
<guid>https://arxiv.org/abs/2506.12450</guid>
<content:encoded><![CDATA[
<div> alignment, LLMs, language control, cross-lingual, representation

Summary:
Large Language Models (LLMs) have shown exceptional capabilities in natural language processing. This paper investigates the alignment of representations in LLMs, particularly in the middle layers, and its implications for separating language-specific and language-agnostic information. The study confirms the presence of natural alignment and its potential for manipulating language-specific information without affecting semantics. The authors introduce Inference-Time Language Control (ITLC), leveraging latent injection for precise cross-lingual language control and reducing language confusion in LLMs. ITLC demonstrates strong cross-lingual control abilities while maintaining semantic meaning in target languages. Additionally, it effectively addresses the issue of language confusion in current LLMs, which causes inconsistent language generation. This research contributes to understanding representation alignment in LLMs and proposes a practical solution to enhance their cross-lingual performance.

<br /><br />Summary: <div>
arXiv:2506.12450v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pluggable Multi-Task Learning Framework for Sentiment-Aware Financial Relation Extraction</title>
<link>https://arxiv.org/abs/2506.12452</link>
<guid>https://arxiv.org/abs/2506.12452</guid>
<content:encoded><![CDATA[
<div> Keywords: Relation Extraction, Sentiment-aware-SDP-Enhanced-Module, Multi-task Learning, Financial Domain, Auxiliary Task 

Summary: 
The research introduces a Sentiment-aware-SDP-Enhanced-Module (SSDP-SEM) for enhancing financial Relation Extraction (RE). The module integrates RE models with an auxiliary sentiment perception (ASP) task to consider sentiment in the RE process. It generates sentiment tokens and inserts them into text instances, enabling the models to incorporate sentiment information. The ASP task predicts sentiment token positions while considering the Shortest Dependency Path (SDP) for syntactic information. The work also includes a sentiment attention information bottleneck regularization method to improve reasoning. Integration of the ASP task with existing frameworks shows improved RE results, emphasizing the importance of leveraging sentiment in financial RE tasks. The study demonstrates that incorporating sentiment in RE models can enhance their performance in capturing nuanced relationships in financial texts. 

<br /><br />Summary: <div>
arXiv:2506.12452v1 Announce Type: new 
Abstract: Relation Extraction (RE) aims to extract semantic relationships in texts from given entity pairs, and has achieved significant improvements. However, in different domains, the RE task can be influenced by various factors. For example, in the financial domain, sentiment can affect RE results, yet this factor has been overlooked by modern RE models. To address this gap, this paper proposes a Sentiment-aware-SDP-Enhanced-Module (SSDP-SEM), a multi-task learning approach for enhancing financial RE. Specifically, SSDP-SEM integrates the RE models with a pluggable auxiliary sentiment perception (ASP) task, enabling the RE models to concurrently navigate their attention weights with the text's sentiment. We first generate detailed sentiment tokens through a sentiment model and insert these tokens into an instance. Then, the ASP task focuses on capturing nuanced sentiment information through predicting the sentiment token positions, combining both sentiment insights and the Shortest Dependency Path (SDP) of syntactic information. Moreover, this work employs a sentiment attention information bottleneck regularization method to regulate the reasoning process. Our experiment integrates this auxiliary task with several prevalent frameworks, and the results demonstrate that most previous models benefit from the auxiliary task, thereby achieving better results. These findings highlight the importance of effectively leveraging sentiment in the financial RE task.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks</title>
<link>https://arxiv.org/abs/2506.12473</link>
<guid>https://arxiv.org/abs/2506.12473</guid>
<content:encoded><![CDATA[
<div> Keywords: model routing, large language model, TagRouter, text generation tasks, cost-efficiency <br />
Summary: TagRouter is a new training-free model routing method aimed at optimizing the utilization of multiple large language models (LLMs) for open-domain text generation tasks. In comparison to 13 baseline methods, TagRouter was able to increase the system accept rate by 6.15% and reduce costs by 17.20%, resulting in optimal cost-efficiency. This approach addresses the limitations faced by existing routing methods in scalability and adaptability to the growing LLM ecosystem. By providing an efficient and scalable solution for model ensembling, TagRouter offers users a flexible "super model" that can evolve with the changing demands of the system. The experimental results demonstrate the effectiveness of TagRouter in improving system performance and reducing costs, making it a valuable tool for the LLM community seeking to optimize their text generation tasks. <br /><br />Summary: <div>
arXiv:2506.12473v1 Announce Type: new 
Abstract: Model routing allocates queries to the suitable model, improving system performance while reducing costs. However, existing routing methods face practical limitations that hinder scalability in large-scale applications and struggle to keep up with the rapid growth of the large language model (LLM) ecosystem. To tackle these challenges, we propose TagRouter, a training-free model routing method designed to optimize the synergy among multiple LLMs for open-domain text generation tasks. Experimental results demonstrate that TagRouter outperforms 13 baseline methods, increasing the accept rate of system by 6.15% and reducing costs by 17.20%, achieving optimal cost-efficiency. Our findings provides the LLM community with an efficient and scalable solution for model ensembling, offering users an evolvable "super model."
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.12494</link>
<guid>https://arxiv.org/abs/2506.12494</guid>
<content:encoded><![CDATA[
<div> Framework, Retrieval-Augmented Generation, FlexRAG, open-source, research<br />
<br />
Summary:
FlexRAG is an open-source framework designed to address challenges in modern large language model applications. It enables the development of Retrieval-Augmented Generation (RAG) systems by supporting text-based, multimodal, and network-based RAG. The framework offers comprehensive lifecycle support, efficient asynchronous processing, and persistent caching capabilities. By providing a flexible and robust solution, FlexRAG aims to help researchers rapidly prototype, deploy, and share advanced RAG systems. The framework is designed to tackle difficulties in algorithm reproduction and sharing, the lack of new techniques, and high system overhead present in existing frameworks. Researchers can access the toolkit and resources on GitHub to leverage the capabilities of FlexRAG for their RAG-related research and development projects. <br /><br />Summary: <div>
arXiv:2506.12494v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation</title>
<link>https://arxiv.org/abs/2506.12496</link>
<guid>https://arxiv.org/abs/2506.12496</guid>
<content:encoded><![CDATA[
<div> knowledge-augmented methods, dialogue response generation, factuality, fact score, factual consistency <br />
Summary:
Large Language Models (LLMs) are powerful in natural language processing tasks but often suffer from generating inaccurate text. A new framework is introduced to enhance the factuality of dialogue response generation by combining a knowledge triple retriever, dialogue rewrite, and knowledge-enhanced response generation. An improved fact score evaluation method is proposed to assess dialogue factual accuracy more reliably. The methods are tested on the OpendialKG and HybriDialogue datasets and show significant improvements in factuality compared to existing graph knowledge-augmentation techniques. The approach aims to reduce the issue of hallucinations and produce more accurate and grounded dialogue responses. The code for the framework will be made available on GitHub to facilitate further research and development in this area. <br /> <div>
arXiv:2506.12496v1 Announce Type: new 
Abstract: Large Language Models (LLMs) succeed in many natural language processing tasks. However, their tendency to hallucinate - generate plausible but inconsistent or factually incorrect text - can cause problems in certain tasks, including response generation in dialogue. To mitigate this issue, knowledge-augmented methods have shown promise in reducing hallucinations. Here, we introduce a novel framework designed to enhance the factuality of dialogue response generation, as well as an approach to evaluate dialogue factual accuracy. Our framework combines a knowledge triple retriever, a dialogue rewrite, and knowledge-enhanced response generation to produce more accurate and grounded dialogue responses. To further evaluate generated responses, we propose a revised fact score that addresses the limitations of existing fact-score methods in dialogue settings, providing a more reliable assessment of factual consistency. We evaluate our methods using different baselines on the OpendialKG and HybriDialogue datasets. Our methods significantly improve factuality compared to other graph knowledge-augmentation baselines, including the state-of-the-art G-retriever. The code will be released on GitHub.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fairness Assessment of Dutch Hate Speech Detection</title>
<link>https://arxiv.org/abs/2506.12502</link>
<guid>https://arxiv.org/abs/2506.12502</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, Dutch language, transformer-based models, counterfactual fairness, group fairness metrics<br />
<br />
Summary: <br />
This study evaluates the counterfactual fairness of hate speech detection models in the Dutch language, focusing on transformer-based models. The researchers curated a list of Dutch Social Group Terms and generated counterfactual data for Dutch hate speech. They fine-tuned baseline transformer-based models and evaluated their performance in hate speech detection. The fairness of these models was assessed using Counterfactual Token Fairness and group fairness metrics. The analysis revealed that the models performed well in detecting hate speech and exhibited average counterfactual fairness and group fairness. The study addresses a gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides insights for enhancing both model performance and fairness. <br /> <div>
arXiv:2506.12502v1 Announce Type: new 
Abstract: Numerous studies have proposed computational methods to detect hate speech online, yet most focus on the English language and emphasize model development. In this study, we evaluate the counterfactual fairness of hate speech detection models in the Dutch language, specifically examining the performance and fairness of transformer-based models. We make the following key contributions. First, we curate a list of Dutch Social Group Terms that reflect social context. Second, we generate counterfactual data for Dutch hate speech using LLMs and established strategies like Manual Group Substitution (MGS) and Sentence Log-Likelihood (SLL). Through qualitative evaluation, we highlight the challenges of generating realistic counterfactuals, particularly with Dutch grammar and contextual coherence. Third, we fine-tune baseline transformer-based models with counterfactual data and evaluate their performance in detecting hate speech. Fourth, we assess the fairness of these models using Counterfactual Token Fairness (CTF) and group fairness metrics, including equality of odds and demographic parity. Our analysis shows that models perform better in terms of hate speech detection, average counterfactual fairness and group fairness. This work addresses a significant gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides practical insights and recommendations for improving both model performance and fairness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection, Classification, and Mitigation of Gender Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2506.12527</link>
<guid>https://arxiv.org/abs/2506.12527</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, gender bias detection, reinforcement learning, chain-of-thoughts reasoning, NLPCC 2025

Summary: 
In the study, the focus is on enhancing the capabilities of large language models (LLMs) in detecting, classifying, and mitigating gender bias. The researchers utilized reinforcement learning, chain-of-thoughts (CoT) reasoning, and supervised fine-tuning for different subtasks. For Subtasks 1 and 2, they employed LLMs' internal reasoning abilities to guide multi-step thinking, simplifying biased queries and improving response accuracy. Subtask 3 involved using reinforcement learning to annotate a preference dataset and mitigate gender bias using Direct Preference Optimization (DPO). The approach implemented in the NLPCC 2025 Shared Task 7 Challenge ranked first in all three subtasks, showcasing effective methods for addressing gender bias in LLMs.<br /><br />Summary: <div>
arXiv:2506.12527v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), they have significantly improved efficiency across a wide range of domains. However, recent studies have revealed that LLMs often exhibit gender bias, leading to serious social implications. Detecting, classifying, and mitigating gender bias in LLMs has therefore become a critical research focus. In the NLPCC 2025 Shared Task 7: Chinese Corpus for Gender Bias Detection, Classification and Mitigation Challenge, we investigate how to enhance the capabilities of LLMs in gender bias detection, classification, and mitigation. We adopt reinforcement learning, chain-of-thoughts (CoT) reasoning, and supervised fine-tuning to handle different Subtasks. Specifically, for Subtasks 1 and 2, we leverage the internal reasoning capabilities of LLMs to guide multi-step thinking in a staged manner, which simplifies complex biased queries and improves response accuracy. For Subtask 3, we employ a reinforcement learning-based approach, annotating a preference dataset using GPT-4. We then apply Direct Preference Optimization (DPO) to mitigate gender bias by introducing a loss function that explicitly favors less biased completions over biased ones. Our approach ranked first across all three subtasks of the NLPCC 2025 Shared Task 7.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2506.12537</link>
<guid>https://arxiv.org/abs/2506.12537</guid>
<content:encoded><![CDATA[
<div> tokenizer, alignment, speech generation, multi-token prediction, speaker modeling 
Summary: 
- The study focuses on improving performance of Speech-language models (SLMs) by investigating key components such as speech tokenizers, speech heads, and speaker modeling. 
- Decoupled tokenization is found to significantly enhance alignment and synthesis quality in SLMs. 
- Introducing multi-token prediction (MTP) into SLMs leads to faster decoding and reduced word error rate. 
- A speaker-aware generation paradigm is proposed, along with RoleTriviaQA benchmark for knowledge QA with diverse speaker identities. 
- Experimental results demonstrate that the methods improve knowledge understanding and speaker consistency. 
<br /><br />Summary: <div>
arXiv:2506.12537v1 Announce Type: new 
Abstract: Speech-language models (SLMs) offer a promising path toward unifying speech and text understanding and generation. However, challenges remain in achieving effective cross-modal alignment and high-quality speech generation. In this work, we systematically investigate the impact of key components (i.e., speech tokenizers, speech heads, and speaker modeling) on the performance of LLM-centric SLMs. We compare coupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM framework and find that decoupled tokenization significantly improves alignment and synthesis quality. To address the information density mismatch between speech and text, we introduce multi-token prediction (MTP) into SLMs, enabling each hidden state to decode multiple speech tokens. This leads to up to 12$\times$ faster decoding and a substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with diverse speaker identities. Experiments demonstrate that our methods enhance both knowledge understanding and speaker consistency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking</title>
<link>https://arxiv.org/abs/2506.12538</link>
<guid>https://arxiv.org/abs/2506.12538</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fact-checking, RealFactBench, Multimodal Large Language Models, Benchmark

Summary:
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have the potential to enhance fact-checking by utilizing their reasoning, evidence retrieval, and explanation generation capabilities. However, existing benchmarks do not fully evaluate these models in real-world misinformation scenarios. To address this gap, the RealFactBench benchmark is introduced, comprising 6,000 high-quality claims from authoritative sources across various tasks such as Knowledge Validation, Rumor Detection, and Event Verification. This benchmark includes multimodal content and diverse domains. The evaluation framework of RealFactBench introduces the Unknown Rate (UnR) metric, allowing a more nuanced assessment of models in handling uncertainty and maintaining a balance between over-conservatism and over-confidence. Experiments on 7 LLMs and 4 MLLMs demonstrate the limitations of these models in real-world fact-checking scenarios, providing valuable insights for future research.<br /><br />Summary: <div>
arXiv:2506.12538v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold significant potential for advancing fact-checking by leveraging their capabilities in reasoning, evidence retrieval, and explanation generation. However, existing benchmarks fail to comprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in realistic misinformation scenarios. To bridge this gap, we introduce RealFactBench, a comprehensive benchmark designed to assess the fact-checking capabilities of LLMs and MLLMs across diverse real-world tasks, including Knowledge Validation, Rumor Detection, and Event Verification. RealFactBench consists of 6K high-quality claims drawn from authoritative sources, encompassing multimodal content and diverse domains. Our evaluation framework further introduces the Unknown Rate (UnR) metric, enabling a more nuanced assessment of models' ability to handle uncertainty and balance between over-conservatism and over-confidence. Extensive experiments on 7 representative LLMs and 4 MLLMs reveal their limitations in real-world fact-checking and offer valuable insights for further research. RealFactBench is publicly available at https://github.com/kalendsyang/RealFactBench.git.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts</title>
<link>https://arxiv.org/abs/2506.12552</link>
<guid>https://arxiv.org/abs/2506.12552</guid>
<content:encoded><![CDATA[
<div> Methodology, factuality, political bias, language models, dataset 
<br />
Summary: 
In the age of online misinformation, understanding the reliability and political bias of news sources is crucial. Existing fact-checking efforts often focus on individual claims, but this study examines entire news outlets. Using prompts based on professional fact-checker criteria, responses from large language models are aggregated to predict factuality and bias. Extensive experiments show significant improvements over baseline models, with an analysis of errors related to media popularity and region. An ablation study identifies key dataset components that contribute to model performance. The dataset and code are released for future research. 
<br /> <div>
arXiv:2506.12552v1 Announce Type: new 
Abstract: In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoTA-RAG: Dynamic of Thought Aggregation RAG</title>
<link>https://arxiv.org/abs/2506.12571</link>
<guid>https://arxiv.org/abs/2506.12571</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, high-throughput, dynamic routing, large-scale web knowledge indexes, Q&amp;A dataset

Summary:<br /><br />DoTA-RAG is a retrieval-augmented generation system designed for high-throughput, large-scale web knowledge indexes. It addresses challenges faced by traditional pipelines through query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. The system improves answer correctness scores significantly, showcasing its potential for practical deployment in domains requiring fast and reliable access to vast knowledge sources. DoTA-RAG achieves this by utilizing a superior embedding model and re-embedding a large corpus, as well as creating a diverse Q&amp;A dataset generated across a broad range of topics and formats. Notably, it enhances the answer correctness score from 0.752 to 1.478 compared to the baseline, while maintaining low latency. Additionally, DoTA-RAG achieves a 0.929 correctness score on the Live Challenge Day, demonstrating its effectiveness in real-world applications. <div>
arXiv:2506.12571v1 Announce Type: new 
Abstract: In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&amp;A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge</title>
<link>https://arxiv.org/abs/2506.12574</link>
<guid>https://arxiv.org/abs/2506.12574</guid>
<content:encoded><![CDATA[
<div> gender bias, natural language processing, Chinese, corpus, mitigation<br />
Summary:<br />
The article introduces a new Chinese corpus, CORGI-PM, designed for investigating and addressing gender bias in natural language processing. The corpus contains 32.9k sentences with labels focused on gender bias in the Chinese context. It includes 5.2k gender-biased sentences and their bias-eliminated versions. The authors set up three challenges as a shared task to automate the detection, classification, and mitigation of textual gender bias. The results and analysis of the teams participating in the shared task at NLPCC 2025 are presented in the literature. The research highlights the importance of addressing bias in data-driven techniques like pre-trained language models, particularly in languages with fewer fairness-related computational resources like Chinese. <div>
arXiv:2506.12574v1 Announce Type: new 
Abstract: As natural language processing for gender bias becomes a significant interdisciplinary topic, the prevalent data-driven techniques, such as pre-trained language models, suffer from biased corpus. This case becomes more obvious regarding those languages with less fairness-related computational linguistic resources, such as Chinese. To this end, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation (CORGI-PM), which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the Chinese context. It is worth noting that CORGI-PM contains 5.2k gender-biased sentences along with the corresponding bias-eliminated versions rewritten by human annotators. We pose three challenges as a shared task to automate the mitigation of textual gender bias, which requires the models to detect, classify, and mitigate textual gender bias. In the literature, we present the results and analysis for the teams participating this shared task in NLPCC 2025.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2506.12576</link>
<guid>https://arxiv.org/abs/2506.12576</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoders, Large Language Model, Alignment, Interpretability, Medical prompts

Summary:<br /><br />
Recent research has demonstrated that Sparse Autoencoders (SAE) applied to Large Language Model (LLM) layers can identify interpretable concepts. This study introduces a novel approach that leverages SAEs to align LLM outputs with any given topic. The method involves scoring SAE neurons based on their semantic similarity to an alignment text and modifying SAE-layer-level outputs by emphasizing topic-aligned neurons. Experiments on various public topic datasets, including Amazon reviews, Medicine, and Sycophancy, using GPT2 and Gemma models with different SAE configurations, show promising results. Aligning to medical prompts exhibited benefits such as increased language acceptability, reduced training time across different topics, and acceptable inference time for various applications. The open-source code for this approach is available on github.com/IBM/sae-steering. <div>
arXiv:2506.12576v1 Announce Type: new 
Abstract: Recent work shows that Sparse Autoencoders (SAE) applied to large language model (LLM) layers have neurons corresponding to interpretable concepts. These SAE neurons can be modified to align generated outputs, but only towards pre-identified topics and with some parameter tuning. Our approach leverages the observational and modification properties of SAEs to enable alignment for any topic. This method 1) scores each SAE neuron by its semantic similarity to an alignment text and uses them to 2) modify SAE-layer-level outputs by emphasizing topic-aligned neurons. We assess the alignment capabilities of this approach on diverse public topic datasets including Amazon reviews, Medicine, and Sycophancy, across the currently available open-source LLMs and SAE pairs (GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to medical prompts reveal several benefits over fine-tuning, including increased average language acceptability (0.25 vs. 0.5), reduced training time across multiple alignment topics (333.6s vs. 62s), and acceptable inference time for many applications (+0.00092s/token). Our open-source code is available at github.com/IBM/sae-steering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse Knowledge Bases</title>
<link>https://arxiv.org/abs/2506.12577</link>
<guid>https://arxiv.org/abs/2506.12577</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, structured knowledge, reasoning, evaluation

Summary:
Persistent limitations in structured reasoning are observed in large language models, with even the strongest model achieving only 32.2% accuracy on challenging cases. Performance declines as the complexity of the knowledge base increases, dropping from 53% for textual reasoning to 25% for formal logic. There are diminishing returns from extended reasoning chains, emphasizing the importance of adapting reasoning depth to task complexity. The introduced OneEval benchmark evaluates knowledge-intensive reasoning capabilities of LLMs across various structured knowledge modalities and domains. It comprises 4,019 instances, including a difficult subset, OneEvalHard. Through evaluation of 18 LLMs, the study sheds light on the challenges and need for improvements in structured knowledge reasoning. The OneEval datasets, evaluation scripts, and baseline results are publicly released, along with a leaderboard to foster advancements in structured knowledge reasoning. 

<br /><br />Summary: <div>
arXiv:2506.12577v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated substantial progress on reasoning tasks involving unstructured text, yet their capabilities significantly deteriorate when reasoning requires integrating structured external knowledge such as knowledge graphs, code snippets, or formal logic. This limitation is partly due to the absence of benchmarks capable of systematically evaluating LLM performance across diverse structured knowledge modalities. To address this gap, we introduce \textbf{\textsc{OneEval}}, a comprehensive benchmark explicitly designed to assess the knowledge-intensive reasoning capabilities of LLMs across four structured knowledge modalities, unstructured text, knowledge graphs, code, and formal logic, and five critical domains (general knowledge, government, science, law, and programming). \textsc{OneEval} comprises 4,019 carefully curated instances and includes a challenging subset, \textsc{OneEval}\textsubscript{Hard}, consisting of 1,285 particularly difficult cases. Through extensive evaluation of 18 state-of-the-art open-source and proprietary LLMs, we establish three core findings: a) \emph{persistent limitations in structured reasoning}, with even the strongest model achieving only 32.2\% accuracy on \textsc{OneEval}\textsubscript{Hard}; b) \emph{performance consistently declines as the structural complexity of the knowledge base increases}, with accuracy dropping sharply from 53\% (textual reasoning) to 25\% (formal logic); and c) \emph{diminishing returns from extended reasoning chains}, highlighting the critical need for models to adapt reasoning depth appropriately to task complexity. We release the \textsc{OneEval} datasets, evaluation scripts, and baseline results publicly, accompanied by a leaderboard to facilitate ongoing advancements in structured knowledge reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Exploration of Mamba for Speech Self-Supervised Models</title>
<link>https://arxiv.org/abs/2506.12606</link>
<guid>https://arxiv.org/abs/2506.12606</guid>
<content:encoded><![CDATA[
<div> SSL, Mamba, HuBERT, ASR, representations

Summary: 
Mamba, known for strong language modeling, is explored for speech self-supervised (SSL) tasks. HuBERT models, based on Mamba, offer an alternative to Transformer-based SSL architectures, allowing for fine-tuning on long-context ASR with less compute and better performance in streaming ASR. These models also perform well on SUPERB probing benchmarks, especially in causal settings, producing high-quality quantized representations and capturing speaker-related features effectively. The study indicates that Mamba-based SSL models show promise for long-sequence modeling, real-time speech modeling, and speech unit extraction. 

<br /><br /> <div>
arXiv:2506.12606v1 Announce Type: new 
Abstract: While Mamba has demonstrated strong performance in language modeling, its potential as a speech self-supervised (SSL) model remains underexplored, with prior studies limited to isolated tasks. To address this, we explore Mamba-based HuBERT models as alternatives to Transformer-based SSL architectures. Leveraging the linear-time Selective State Space, these models enable fine-tuning on long-context ASR with significantly lower compute. Moreover, they show superior performance when fine-tuned for streaming ASR. Beyond fine-tuning, these models show competitive performance on SUPERB probing benchmarks, particularly in causal settings. Our analysis shows that they yield higher-quality quantized representations and capture speaker-related features more distinctly than Transformer-based models. These findings highlight Mamba-based SSL as a promising and complementary direction for long-sequence modeling, real-time speech modeling, and speech unit extraction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Building General Purpose Embedding Models for Industry 4.0 Agents</title>
<link>https://arxiv.org/abs/2506.12607</link>
<guid>https://arxiv.org/abs/2506.12607</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, asset maintenance, Industry 4.0, reasoning and acting agent, knowledge base <br />
Summary: 
- The study focuses on enhancing language models' comprehension for asset maintenance in Industry 4.0, aiming to advise engineers and reduce asset downtime.
- A knowledge base was created with nine asset-specific task datasets, and Large Language Models were used to augment input tasks for better embeddings.
- Integration with a Reasoning and Acting agent (ReAct) enabled addressing complex user queries requiring multi-step reasoning and knowledge inference.
- Ablation studies showed improvements with LLM query augmentation, the use of Contrastive loss, and balanced positive and negative in-batch samples.
- Significant enhancements were observed in model performance across various metrics, showcasing its planning and tool invocation capabilities for supporting Subject Matter Experts in industrial asset maintenance. 

<br /><br />Summary: <div>
arXiv:2506.12607v1 Announce Type: new 
Abstract: In this work we focus on improving language models' understanding for asset maintenance to guide the engineer's decisions and minimize asset downtime. Given a set of tasks expressed in natural language for Industry 4.0 domain, each associated with queries related to a specific asset, we want to recommend relevant items and generalize to queries of similar assets. A task may involve identifying relevant sensors given a query about an asset's failure mode.
  Our approach begins with gathering a qualitative, expert-vetted knowledge base to construct nine asset-specific task datasets. To create more contextually informed embeddings, we augment the input tasks using Large Language Models (LLMs), providing concise descriptions of the entities involved in the queries. This embedding model is then integrated with a Reasoning and Acting agent (ReAct), which serves as a powerful tool for answering complex user queries that require multi-step reasoning, planning, and knowledge inference.
  Through ablation studies, we demonstrate that: (a) LLM query augmentation improves the quality of embeddings, (b) Contrastive loss and other methods that avoid in-batch negatives are superior for datasets with queries related to many items, and (c) It is crucial to balance positive and negative in-batch samples. After training and testing on our dataset, we observe a substantial improvement: HIT@1 increases by +54.2%, MAP@100 by +50.1%, and NDCG@10 by +54.7%, averaged across all tasks and models. Additionally, we empirically demonstrate the model's planning and tool invocation capabilities when answering complex questions related to industrial asset maintenance, showcasing its effectiveness in supporting Subject Matter Experts (SMEs) in their day-to-day operations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2506.12615</link>
<guid>https://arxiv.org/abs/2506.12615</guid>
<content:encoded><![CDATA[
<div> corpus, Arabic dialects, Named Entity Recognition, benchmarking, domain adaptation <br />
<br />
Summary: 
The article introduces Konooz, a multi-dimensional corpus covering 16 Arabic dialects and 10 domains, with 777k tokens manually annotated with 21 entity types. It aims to benchmark existing Arabic Named Entity Recognition (NER) models, highlighting a significant performance drop of up to 38% on cross-domain and cross-dialect data. The study delves into domain and dialect divergence, resource scarcity impact, and model performance variations. By measuring overlap between domains and dialects using Maximum Mean Discrepancy (MMD), insights are gained into why certain NER models excel on specific dialects and domains. Konooz is openly accessible and beneficial for NLP tasks like domain adaptation and transfer learning. <div>
arXiv:2506.12615v1 Announce Type: new 
Abstract: We introduce Konooz, a novel multi-dimensional corpus covering 16 Arabic dialects across 10 domains, resulting in 160 distinct corpora. The corpus comprises about 777k tokens, carefully collected and manually annotated with 21 entity types using both nested and flat annotation schemes - using the Wojood guidelines. While Konooz is useful for various NLP tasks like domain adaptation and transfer learning, this paper primarily focuses on benchmarking existing Arabic Named Entity Recognition (NER) models, especially cross-domain and cross-dialect model performance. Our benchmarking of four Arabic NER models using Konooz reveals a significant drop in performance of up to 38% when compared to the in-distribution data. Furthermore, we present an in-depth analysis of domain and dialect divergence and the impact of resource scarcity. We also measured the overlap between domains and dialects using the Maximum Mean Discrepancy (MMD) metric, and illustrated why certain NER models perform better on specific dialects and domains. Konooz is open-source and publicly available at https://sina.birzeit.edu/wojood/#download
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics</title>
<link>https://arxiv.org/abs/2506.12618</link>
<guid>https://arxiv.org/abs/2506.12618</guid>
<content:encoded><![CDATA[
<div> Keywords: unlearning, large language models, benchmarking, evaluation metrics, research

Summary: 
The article introduces the concept of robust unlearning for large language models (LLMs) to ensure data privacy, model safety, and regulatory compliance. It highlights the challenges in accurately measuring unlearning and the lack of standardized methodologies and evaluation metrics in current research. To address these issues, the OpenUnlearning framework is proposed, which includes 9 unlearning algorithms, 16 evaluations across 3 benchmarks, and the release of 450+ checkpoints for analysis. The framework also introduces a meta-evaluation benchmark to assess the reliability of evaluation metrics. By benchmarking various unlearning methods and evaluating them against a comprehensive suite, the article aims to establish a clear pathway for rigorous research in LLM unlearning. 

<br /><br />Summary: <div>
arXiv:2506.12618v1 Announce Type: new 
Abstract: Robust unlearning is crucial for safely deploying large language models (LLMs) in environments where data privacy, model safety, and regulatory compliance must be ensured. Yet the task is inherently challenging, partly due to difficulties in reliably measuring whether unlearning has truly occurred. Moreover, fragmentation in current methodologies and inconsistent evaluation metrics hinder comparative analysis and reproducibility. To unify and accelerate research efforts, we introduce OpenUnlearning, a standardized and extensible framework designed explicitly for benchmarking both LLM unlearning methods and metrics. OpenUnlearning integrates 9 unlearning algorithms and 16 diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also enables analyses of forgetting behaviors across 450+ checkpoints we publicly release. Leveraging OpenUnlearning, we propose a novel meta-evaluation benchmark focused specifically on assessing the faithfulness and robustness of evaluation metrics themselves. We also benchmark diverse unlearning methods and provide a comparative analysis against an extensive evaluation suite. Overall, we establish a clear, community-driven pathway toward rigorous development in LLM unlearning research.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Predictability and Randomness: Seeking Artistic Inspiration from AI Generative Models</title>
<link>https://arxiv.org/abs/2506.12634</link>
<guid>https://arxiv.org/abs/2506.12634</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated poetry, creativity, LSTM-VAE, indeterminacy, artistic expression 

Summary: 
This paper discusses the use of AI-generated poetic lines as a source of inspiration for artists. It compares lines generated by Long Short-Term Memory Variational Autoencoders (LSTM-VAE) with poems created by Large Language Models (LLMs). The study finds that LSTM-VAE lines evoke creativity through a combination of resonant imagery and productive indeterminacy, allowing for semantic openness and unconventional combinations. In contrast, LLMs produce technically skilled but conventional poetry. The author demonstrates how engaging with LSTM-VAE lines led to the organic emergence of narrative in an original poem, showcasing how these lines can be valuable starting points for genuine artistic expression.<br /><br />Summary: <div>
arXiv:2506.12634v1 Announce Type: new 
Abstract: Artistic inspiration often emerges from language that is open to interpretation. This paper explores the use of AI-generated poetic lines as stimuli for creativity. Through analysis of two generative AI approaches--lines generated by Long Short-Term Memory Variational Autoencoders (LSTM-VAE) and complete poems by Large Language Models (LLMs)--I demonstrate that LSTM-VAE lines achieve their evocative impact through a combination of resonant imagery and productive indeterminacy. While LLMs produce technically accomplished poetry with conventional patterns, LSTM-VAE lines can engage the artist through semantic openness, unconventional combinations, and fragments that resist closure. Through the composition of an original poem, where narrative emerged organically through engagement with LSTM-VAE generated lines rather than following a predetermined structure, I demonstrate how these characteristics can serve as evocative starting points for authentic artistic expression.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Grounded is Wikipedia? A Study on Structured Evidential Support</title>
<link>https://arxiv.org/abs/2506.12637</link>
<guid>https://arxiv.org/abs/2506.12637</guid>
<content:encoded><![CDATA[
<div> people, Wikipedia, NLP, claim support, annotation 

Summary:
Approximately 20% of claims in Wikipedia lead sections lack support in the article body. Around 27% of annotated claims in the article body lack support from publicly accessible cited sources. Moreover, over 80% of lead claims cannot be traced back to these sources using annotated body evidence. Recovery of complex grounding evidence for supported claims also poses a challenge for standard retrieval methods. The study emphasizes the importance of ensuring the groundedness of information on Wikipedia, especially in the context of NLP tasks that rely on accurate and reliable information. The dataset PeopleProfiles is introduced as a tool for analyzing claim support annotations on Wikipedia articles of notable individuals. The findings suggest a need for improved methods to verify and validate information sourced from Wikipedia for enhanced data reliability in NLP applications. 

<br /><br />Summary: <div>
arXiv:2506.12637v1 Announce Type: new 
Abstract: Wikipedia is a critical resource for modern NLP, serving as a rich repository of up-to-date and citation-backed information on a wide variety of subjects. The reliability of Wikipedia -- its groundedness in its cited sources -- is vital to this purpose. This work provides a quantitative analysis of the extent to which Wikipedia *is* so grounded and of how readily grounding evidence may be retrieved. To this end, we introduce PeopleProfiles -- a large-scale, multi-level dataset of claim support annotations on Wikipedia articles of notable people. We show that roughly 20% of claims in Wikipedia *lead* sections are unsupported by the article body; roughly 27% of annotated claims in the article *body* are unsupported by their (publicly accessible) cited sources; and >80% of lead claims cannot be traced to these sources via annotated body evidence. Further, we show that recovery of complex grounding evidence for claims that *are* supported remains a challenge for standard retrieval methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics</title>
<link>https://arxiv.org/abs/2506.12657</link>
<guid>https://arxiv.org/abs/2506.12657</guid>
<content:encoded><![CDATA[
arXiv:2506.12657v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly used in morally sensitive domains, it is crucial to understand how persona traits affect their moral reasoning and persuasive behavior. We present the first large-scale study of multi-dimensional persona effects in AI-AI debates over real-world moral dilemmas. Using a 6-dimensional persona space (age, gender, country, class, ideology, and personality), we simulate structured debates between AI agents over 131 relationship-based cases. Our results show that personas affect initial moral stances and debate outcomes, with political ideology and personality traits exerting the strongest influence. Persuasive success varies across traits, with liberal and open personalities reaching higher consensus and win rates. While logit-based confidence grows during debates, emotional and credibility-based appeals diminish, indicating more tempered argumentation over time. These trends mirror findings from psychology and cultural studies, reinforcing the need for persona-aware evaluation frameworks for AI moral reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Clinical Models with Pseudo Data for De-identification</title>
<link>https://arxiv.org/abs/2506.12674</link>
<guid>https://arxiv.org/abs/2506.12674</guid>
<content:encoded><![CDATA[
arXiv:2506.12674v1 Announce Type: new 
Abstract: Many models are pretrained on redacted text for privacy reasons. Clinical foundation models are often trained on de-identified text, which uses special syntax (masked) text in place of protected health information. Even though these models have increased in popularity, there has been little effort in understanding the effects of training them on redacted text. In this work, we pretrain several encoder-only models on a dataset that contains redacted text and a version with replaced realistic pseudo text. We then fine-tuned models for the protected health information de-identification task and show how our methods significantly outperform previous baselines. The contributions of this work include: a) our novel, and yet surprising findings with training recommendations, b) redacted text replacements used to produce the pseudo dataset, c) pretrained embeddings and fine-tuned task specific models, and d) freely available pseudo training dataset generation and model source code used in our experiments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Realignment of Language Models</title>
<link>https://arxiv.org/abs/2506.12704</link>
<guid>https://arxiv.org/abs/2506.12704</guid>
<content:encoded><![CDATA[
arXiv:2506.12704v1 Announce Type: new 
Abstract: Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates Training-time Realignment (TrRa), which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during inference, we introduce a layer adapter that enables smooth Inference-time Realignment (InRa). This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even during inference. By encouraging deeper reasoning, it even surpassed its original performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?</title>
<link>https://arxiv.org/abs/2506.12744</link>
<guid>https://arxiv.org/abs/2506.12744</guid>
<content:encoded><![CDATA[
arXiv:2506.12744v1 Announce Type: new 
Abstract: Hate speech detection across contemporary social media presents unique challenges due to linguistic diversity and the informal nature of online discourse. These challenges are further amplified in settings involving code-mixing, transliteration, and culturally nuanced expressions. While fine-tuned transformer models, such as BERT, have become standard for this task, we argue that recent large language models (LLMs) not only surpass them but also redefine the landscape of hate speech detection more broadly. To support this claim, we introduce IndoHateMix, a diverse, high-quality dataset capturing Hindi-English code-mixing and transliteration in the Indian context, providing a realistic benchmark to evaluate model robustness in complex multilingual scenarios where existing NLP methods often struggle. Our extensive experiments show that cutting-edge LLMs (such as LLaMA-3.1) consistently outperform task-specific BERT-based models, even when fine-tuned on significantly less data. With their superior generalization and adaptability, LLMs offer a transformative approach to mitigating online hate in diverse environments. This raises the question of whether future works should prioritize developing specialized models or focus on curating richer and more varied datasets to further enhance the effectiveness of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2506.12758</link>
<guid>https://arxiv.org/abs/2506.12758</guid>
<content:encoded><![CDATA[
arXiv:2506.12758v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become increasingly integrated into everyday life and information ecosystems, concerns about their implicit biases continue to persist. While prior work has primarily examined socio-demographic and left--right political dimensions, little attention has been paid to how LLMs align with broader geopolitical value systems, particularly the democracy--authoritarianism spectrum. In this paper, we propose a novel methodology to assess such alignment, combining (1) the F-scale, a psychometric tool for measuring authoritarian tendencies, (2) FavScore, a newly introduced metric for evaluating model favorability toward world leaders, and (3) role-model probing to assess which figures are cited as general role-models by LLMs. We find that LLMs generally favor democratic values and leaders, but exhibit increases favorability toward authoritarian figures when prompted in Mandarin. Further, models are found to often cite authoritarian figures as role models, even outside explicit political contexts. These results shed light on ways LLMs may reflect and potentially reinforce global political ideologies, highlighting the importance of evaluating bias beyond conventional socio-political axes. Our code is available at: https://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surprise Calibration for Better In-Context Learning</title>
<link>https://arxiv.org/abs/2506.12796</link>
<guid>https://arxiv.org/abs/2506.12796</guid>
<content:encoded><![CDATA[
arXiv:2506.12796v1 Announce Type: new 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm for task adaptation in large language models (LLMs), where models infer underlying task structures from a few demonstrations. However, ICL remains susceptible to biases that arise from prior knowledge and contextual demonstrations, which can degrade the performance of LLMs. Existing bias calibration methods typically apply fixed class priors across all inputs, limiting their efficacy in dynamic ICL settings where the context for each query differs. To address these limitations, we adopt implicit sequential Bayesian inference as a framework for interpreting ICL, identify "surprise" as an informative signal for class prior shift, and introduce a novel method--Surprise Calibration (SC). SC leverages the notion of surprise to capture the temporal dynamics of class priors, providing a more adaptive and computationally efficient solution for in-context learning. We empirically demonstrate the superiority of SC over existing bias calibration techniques across a range of benchmark natural language processing tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems</title>
<link>https://arxiv.org/abs/2506.12823</link>
<guid>https://arxiv.org/abs/2506.12823</guid>
<content:encoded><![CDATA[
arXiv:2506.12823v1 Announce Type: new 
Abstract: This work presents an Argument Mining process that extracts argumentative entities from clinical texts and identifies their relationships using token classification and Natural Language Inference techniques. Compared to straightforward methods like text classification, this methodology demonstrates superior performance in data-scarce settings. By assessing the effectiveness of these methods in identifying argumentative structures that support or refute possible diagnoses, this research lays the groundwork for future tools that can provide evidence-based justifications for machine-generated clinical conclusions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Chatbot Text: A Sequence-to-Sequence Approach</title>
<link>https://arxiv.org/abs/2506.12843</link>
<guid>https://arxiv.org/abs/2506.12843</guid>
<content:encoded><![CDATA[
arXiv:2506.12843v1 Announce Type: new 
Abstract: Due to advances in Large Language Models (LLMs) such as ChatGPT, the boundary between human-written text and AI-generated text has become blurred. Nevertheless, recent work has demonstrated that it is possible to reliably detect GPT-generated text. In this paper, we adopt a novel strategy to adversarially transform GPT-generated text using sequence-to-sequence (Seq2Seq) models, with the goal of making the text more human-like. We experiment with the Seq2Seq models T5-small and BART which serve to modify GPT-generated sentences to include linguistic, structural, and semantic components that may be more typical of human-authored text. Experiments show that classification models trained to distinguish GPT-generated text are significantly less accurate when tested on text that has been modified by these Seq2Seq models. However, after retraining classification models on data generated by our Seq2Seq technique, the models are able to distinguish the transformed GPT-generated text from human-generated text with high accuracy. This work adds to the accumulating knowledge of text transformation as a tool for both attack -- in the sense of defeating classification models -- and defense -- in the sense of improved classifiers -- thereby advancing our understanding of AI-generated text.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QFFT, Question-Free Fine-Tuning for Adaptive Reasoning</title>
<link>https://arxiv.org/abs/2506.12860</link>
<guid>https://arxiv.org/abs/2506.12860</guid>
<content:encoded><![CDATA[
arXiv:2506.12860v1 Announce Type: new 
Abstract: Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality</title>
<link>https://arxiv.org/abs/2506.12886</link>
<guid>https://arxiv.org/abs/2506.12886</guid>
<content:encoded><![CDATA[
arXiv:2506.12886v1 Announce Type: new 
Abstract: This work presents three different approaches to address the ArchEHR-QA 2025 Shared Task on automated patient question answering. We introduce an end-to-end prompt-based baseline and two two-step methods to divide the task, without utilizing any external knowledge. Both two step approaches first extract essential sentences from the clinical text, by prompt or similarity ranking, and then generate the final answer from these notes. Results indicate that the re-ranker based two-step system performs best, highlighting the importance of selecting the right approach for each subtask. Our best run achieved an overall score of 0.44, ranking 8th out of 30 on the leaderboard, securing the top position in overall factuality.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language</title>
<link>https://arxiv.org/abs/2506.12895</link>
<guid>https://arxiv.org/abs/2506.12895</guid>
<content:encoded><![CDATA[
arXiv:2506.12895v1 Announce Type: new 
Abstract: Legal passage retrieval is an important task that assists legal practitioners in the time-intensive process of finding relevant precedents to support legal arguments. This study investigates the task of retrieving legal passages or paragraphs from decisions of the Court of Justice of the European Union (CJEU), whose language is highly structured and formulaic, leading to repetitive patterns. Understanding when lexical or semantic models are more effective at handling the repetitive nature of legal language is key to developing retrieval systems that are more accurate, efficient, and transparent for specific legal domains. To this end, we explore when this routinized legal language is better suited for retrieval using methods that rely on lexical and statistical features, such as BM25, or dense retrieval models trained to capture semantic and contextual information. A qualitative and quantitative analysis with three complementary metrics shows that both lexical and dense models perform well in scenarios with more repetitive usage of language, whereas BM25 performs better than the dense models in more nuanced scenarios where repetition and verbatim~quotes are less prevalent and in longer queries. Our experiments also show that BM25 is a strong baseline, surpassing off-the-shelf dense models in 4 out of 7 performance metrics. However, fine-tuning a dense model on domain-specific data led to improved performance, surpassing BM25 in most metrics, and we analyze the effect of the amount of data used in fine-tuning on the model's performance and temporal robustness. The code, dataset and appendix related to this work are available on: https://github.com/larimo/lexsem-legal-ir.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEBS: A Fine-grained Biomedical Lexical Simplification Task</title>
<link>https://arxiv.org/abs/2506.12898</link>
<guid>https://arxiv.org/abs/2506.12898</guid>
<content:encoded><![CDATA[
arXiv:2506.12898v1 Announce Type: new 
Abstract: Online medical literature has made health information more available than ever, however, the barrier of complex medical jargon prevents the general public from understanding it. Though parallel and comparable corpora for Biomedical Text Simplification have been introduced, these conflate the many syntactic and lexical operations involved in simplification. To enable more targeted development and evaluation, we present a fine-grained lexical simplification task and dataset, Jargon Explanations for Biomedical Simplification (JEBS, https://github.com/bill-from-ri/JEBS-data ). The JEBS task involves identifying complex terms, classifying how to replace them, and generating replacement text. The JEBS dataset contains 21,595 replacements for 10,314 terms across 400 biomedical abstracts and their manually simplified versions. Additionally, we provide baseline results for a variety of rule-based and transformer-based systems for the three sub-tasks. The JEBS task, data, and baseline results pave the way for development and rigorous evaluation of systems for replacing or explaining complex biomedical terms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciDA: Scientific Dynamic Assessor of LLMs</title>
<link>https://arxiv.org/abs/2506.12909</link>
<guid>https://arxiv.org/abs/2506.12909</guid>
<content:encoded><![CDATA[
arXiv:2506.12909v1 Announce Type: new 
Abstract: Advancement in Large Language Models (LLMs) reasoning capabilities enables them to solve scientific problems with enhanced efficacy. Thereby, a high-quality benchmark for comprehensive and appropriate assessment holds significance, while existing ones either confront the risk of data contamination or lack involved disciplines. To be specific, due to the data source overlap of LLMs training and static benchmark, the keys or number pattern of answers inadvertently memorized (i.e. data contamination), leading to systematic overestimation of their reasoning capabilities, especially numerical reasoning. We propose SciDA, a multidisciplinary benchmark that consists exclusively of over 1k Olympic-level numerical computation problems, allowing randomized numerical initializations for each inference round to avoid reliance on fixed numerical patterns. We conduct a series of experiments with both closed-source and open-source top-performing LLMs, and it is observed that the performance of LLMs drop significantly under random numerical initialization. Thus, we provide truthful and unbiased assessments of the numerical reasoning capabilities of LLMs. The data is available at https://huggingface.co/datasets/m-a-p/SciDA
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization</title>
<link>https://arxiv.org/abs/2506.12915</link>
<guid>https://arxiv.org/abs/2506.12915</guid>
<content:encoded><![CDATA[
arXiv:2506.12915v1 Announce Type: new 
Abstract: With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models</title>
<link>https://arxiv.org/abs/2506.12935</link>
<guid>https://arxiv.org/abs/2506.12935</guid>
<content:encoded><![CDATA[
arXiv:2506.12935v1 Announce Type: new 
Abstract: While large language models have shown reasoning capabilities, their application to the audio modality, particularly in large audio-language models (ALMs), remains significantly underdeveloped. Addressing this gap requires a systematic approach, involving a capable base model, high-quality reasoning-oriented audio data, and effective training algorithms. In this study, we present a comprehensive solution: we introduce the Audio Logical Reasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples specifically designed for complex reasoning tasks. Building on this resource, we propose SoundMind, a rule-based reinforcement learning (RL) algorithm tailored to endow ALMs with deep bimodal reasoning abilities. By training Qwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves state-of-the-art performance in audio logical reasoning. This work highlights the impact of combining high-quality, reasoning-focused datasets with specialized RL techniques, advancing the frontier of auditory intelligence in language models. Our code and the proposed dataset are available at https://github.com/xid32/SoundMind.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation</title>
<link>https://arxiv.org/abs/2506.12936</link>
<guid>https://arxiv.org/abs/2506.12936</guid>
<content:encoded><![CDATA[
arXiv:2506.12936v1 Announce Type: new 
Abstract: In clinical operations, teamwork can be the crucial factor that determines the final outcome. Prior studies have shown that sufficient collaboration is the key factor that determines the outcome of an operation. To understand how the team practices teamwork during the operation, we collected CliniDial from simulations of medical operations. CliniDial includes the audio data and its transcriptions, the simulated physiology signals of the patient manikins, and how the team operates from two camera angles. We annotate behavior codes following an existing framework to understand the teamwork process for CliniDial. We pinpoint three main characteristics of our dataset, including its label imbalances, rich and natural interactions, and multiple modalities, and conduct experiments to test existing LLMs' capabilities on handling data with these characteristics. Experimental results show that CliniDial poses significant challenges to the existing models, inviting future effort on developing methods that can deal with real-world clinical data. We open-source the codebase at https://github.com/MichiganNLP/CliniDial
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Role of Data Quality in Training Bilingual Language Models</title>
<link>https://arxiv.org/abs/2506.12966</link>
<guid>https://arxiv.org/abs/2506.12966</guid>
<content:encoded><![CDATA[
arXiv:2506.12966v1 Announce Type: new 
Abstract: Bilingual and multilingual language models offer a promising path toward scaling NLP systems across diverse languages and users. However, their performance often varies wildly between languages as prior works show that adding more languages can degrade performance for some languages (such as English), while improving others (typically more data constrained languages). In this work, we investigate causes of these inconsistencies by comparing bilingual and monolingual language models. Our analysis reveals that unequal data quality, not just data quantity, is a major driver of performance degradation in bilingual settings. We propose a simple yet effective data filtering strategy to select higher-quality bilingual training data with only high quality English data. Applied to French, German, and Chinese, our approach improves monolingual performance by 2-4% and reduces bilingual model performance gaps to 1%. These results highlight the overlooked importance of data quality in multilingual pretraining and offer a practical recipe for balancing performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation</title>
<link>https://arxiv.org/abs/2506.12978</link>
<guid>https://arxiv.org/abs/2506.12978</guid>
<content:encoded><![CDATA[
arXiv:2506.12978v1 Announce Type: new 
Abstract: Media outlets are becoming more partisan and polarized nowadays. Most previous work focused on detecting media bias. In this paper, we aim to mitigate media bias by generating a neutralized summary given multiple articles presenting different ideological views. Motivated by the critical role of events and event relations in media bias detection, we propose to increase awareness of bias in LLMs via multi-document events reasoning and use a multi-document event relation graph to guide the summarization process. This graph contains rich event information useful to reveal bias: four common types of in-doc event relations to reflect content framing bias, cross-doc event coreference relation to reveal content selection bias, and event-level moral opinions to highlight opinionated framing bias. We further develop two strategies to incorporate the multi-document event relation graph for neutralized summarization. Firstly, we convert a graph into natural language descriptions and feed the textualized graph into LLMs as a part of a hard text prompt. Secondly, we encode the graph with graph attention network and insert the graph embedding into LLMs as a soft prompt. Both automatic evaluation and human evaluation confirm that our approach effectively mitigates both lexical and informational media bias, and meanwhile improves content preservation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2506.12991</link>
<guid>https://arxiv.org/abs/2506.12991</guid>
<content:encoded><![CDATA[
arXiv:2506.12991v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) generally requires a deep understanding of the contextual information, including the words associated with the aspect terms and their syntactic dependencies. Most existing studies employ advanced encoders (e.g., pre-trained models) to capture such context, especially large language models (LLMs). However, training these encoders is resource-intensive, and in many cases, the available data is insufficient for necessary fine-tuning. Therefore it is challenging for learning LLMs within such restricted environments and computation efficiency requirement. As a result, it motivates the exploration of plug-and-play methods that adapt LLMs to ABSA with minimal effort. In this paper, we propose an approach that integrates extendable components capable of incorporating various types of syntactic knowledge, such as constituent syntax, word dependencies, and combinatory categorial grammar (CCG). Specifically, we propose a memory module that records syntactic information and is incorporated into LLMs to instruct the prediction of sentiment polarities. Importantly, this encoder acts as a versatile, detachable plugin that is trained independently of the LLM. We conduct experiments on benchmark datasets, which show that our approach outperforms strong baselines and previous approaches, thus demonstrates its effectiveness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature</title>
<link>https://arxiv.org/abs/2506.13013</link>
<guid>https://arxiv.org/abs/2506.13013</guid>
<content:encoded><![CDATA[
arXiv:2506.13013v1 Announce Type: new 
Abstract: Existing research indicates that machine translations (MTs) of literary texts are often unsatisfactory. MTs are typically evaluated using automated metrics and subjective human ratings, with limited focus on stylistic features. Evidence is also limited on whether state-of-the-art large language models (LLMs) will reshape literary translation. This study examines the stylistic features of LLM translations, comparing GPT-4's performance to human translations in a Chinese online literature task. Computational stylometry analysis shows that GPT-4 translations closely align with human translations in lexical, syntactic, and content features, suggesting that LLMs might replicate the 'human touch' in literary translation style. These findings offer insights into AI's impact on literary translation from a posthuman perspective, where distinctions between machine and human translations become increasingly blurry.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edeflip: Supervised Word Translation between English and Yoruba</title>
<link>https://arxiv.org/abs/2506.13020</link>
<guid>https://arxiv.org/abs/2506.13020</guid>
<content:encoded><![CDATA[
arXiv:2506.13020v1 Announce Type: new 
Abstract: In recent years, embedding alignment has become the state-of-the-art machine translation approach, as it can yield high-quality translation without training on parallel corpora. However, existing research and application of embedding alignment mostly focus on high-resource languages with high-quality monolingual embeddings. It is unclear if and how low-resource languages may be similarly benefited. In this study, we implement an established supervised embedding alignment method for word translation from English to Yoruba, the latter a low-resource language. We found that higher embedding quality and normalizing embeddings increase word translation precision, with, additionally, an interaction effect between the two. Our results demonstrate the limitations of the state-of-the-art supervised embedding alignment when it comes to low-resource languages, for which there are additional factors that need to be taken into consideration, such as the importance of curating high-quality monolingual embeddings. We hope our work will be a starting point for further machine translation research that takes into account the challenges that low-resource languages face.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2506.13044</link>
<guid>https://arxiv.org/abs/2506.13044</guid>
<content:encoded><![CDATA[
arXiv:2506.13044v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive translation capabilities even without being explicitly trained on parallel data. This remarkable property has led some to believe that parallel data is no longer necessary for building multilingual language models. While some attribute this to the emergent abilities of LLMs due to scale, recent work suggests that it is actually caused by incidental bilingual signals present in the training data. Various methods have been proposed to maximize the utility of parallel data to enhance the multilingual capabilities of multilingual encoder-based and encoder-decoder language models. However, some decoder-based LLMs opt to ignore parallel data instead. In this work, we conduct a systematic study on the impact of adding parallel data on LLMs' multilingual capabilities, focusing specifically on translation and multilingual common-sense reasoning. Through controlled experiments, we demonstrate that parallel data can significantly improve LLMs' multilingual capabilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2506.13055</link>
<guid>https://arxiv.org/abs/2506.13055</guid>
<content:encoded><![CDATA[
arXiv:2506.13055v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved with the growth of Large Language Models (LLMs) and are now applied in various fields. In finance, the integration of diverse modalities such as text, charts, and tables is crucial for accurate and efficient decision-making. Therefore, an effective evaluation system that incorporates these data types is essential for advancing financial application. In this paper, we introduce CFBenchmark-MM, a Chinese multimodal financial benchmark with over 9,000 image-question pairs featuring tables, histogram charts, line charts, pie charts, and structural diagrams. Additionally, we develop a staged evaluation system to assess MLLMs in handling multimodal information by providing different visual content step by step. Despite MLLMs having inherent financial knowledge, experimental results still show limited efficiency and robustness in handling multimodal financial context. Further analysis on incorrect responses reveals the misinterpretation of visual content and the misunderstanding of financial concepts are the primary issues. Our research validates the significant, yet underexploited, potential of MLLMs in financial analysis, highlighting the need for further development and domain-specific optimization to encourage the enhanced use in financial domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multipole Attention for Efficient Long Context Reasoning</title>
<link>https://arxiv.org/abs/2506.13059</link>
<guid>https://arxiv.org/abs/2506.13059</guid>
<content:encoded><![CDATA[
arXiv:2506.13059v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?</title>
<link>https://arxiv.org/abs/2506.13065</link>
<guid>https://arxiv.org/abs/2506.13065</guid>
<content:encoded><![CDATA[
arXiv:2506.13065v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about "love & belonging" motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at https://aka.ms/motivebench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design</title>
<link>https://arxiv.org/abs/2506.13066</link>
<guid>https://arxiv.org/abs/2506.13066</guid>
<content:encoded><![CDATA[
arXiv:2506.13066v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets and the inefficiency of existing training paradigms for reasoning enhancement. To address these issues, we propose an integrated framework, FinLMM-R1, combining an automated and scalable pipeline for data construction with enhanced training strategies to improve the multimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves textual-visual misalignment in financial reports through a separate paradigm of question-answer generation and image-question alignment, ensuring data integrity and extraction efficiency. Through ASP, we collect 89,378 aligned image-question pairs from 23,397 financial reports, covering tasks such as arithmetic reasoning, statistics reasoning, financial explanation, and financial knowledge. Moreover, we introduce the Thinking with Adversarial Reward in LMM (TAR-LMM), extending the prior two-stage training framework [1] with additional reward mechanisms. In the first stage, we focus on text-only tasks with format and accuracy rewards to guide the model in generating well-structured thinking contents. In the second stage, we construct multi-image contrastive samples with additional reward components including image selection, thinking content length, and adversarial reward to jointly optimize the LMM across visual perception, reasoning efficiency, and logical coherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and training framework significantly improve answer accuracy and reasoning depth over existing reasoning LMMs in both general and financial multimodal contexts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right</title>
<link>https://arxiv.org/abs/2506.13070</link>
<guid>https://arxiv.org/abs/2506.13070</guid>
<content:encoded><![CDATA[
arXiv:2506.13070v1 Announce Type: new 
Abstract: In this paper, we describe our approach for the SemEval 2025 Task 2 on Entity-Aware Machine Translation (EA-MT). Our system aims to improve the accuracy of translating named entities by combining two key approaches: Retrieval Augmented Generation (RAG) and iterative self-refinement techniques using Large Language Models (LLMs). A distinctive feature of our system is its self-evaluation mechanism, where the LLM assesses its own translations based on two key criteria: the accuracy of entity translations and overall translation quality. We demonstrate how these methods work together and effectively improve entity handling while maintaining high-quality translations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs</title>
<link>https://arxiv.org/abs/2506.13102</link>
<guid>https://arxiv.org/abs/2506.13102</guid>
<content:encoded><![CDATA[
arXiv:2506.13102v1 Announce Type: new 
Abstract: Test-time scaling has recently emerged as a promising approach for enhancing the reasoning capabilities of large language models or vision-language models during inference. Although a variety of test-time scaling strategies have been proposed, and interest in their application to the medical domain is growing, many critical aspects remain underexplored, including their effectiveness for vision-language models and the identification of optimal strategies for different settings. In this paper, we conduct a comprehensive investigation of test-time scaling in the medical domain. We evaluate its impact on both large language models and vision-language models, considering factors such as model size, inherent model characteristics, and task complexity. Finally, we assess the robustness of these strategies under user-driven factors, such as misleading information embedded in prompts. Our findings offer practical guidelines for the effective use of test-time scaling in medical applications and provide insights into how these strategies can be further refined to meet the reliability and interpretability demands of the medical domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging In-Context Learning for Language Model Agents</title>
<link>https://arxiv.org/abs/2506.13109</link>
<guid>https://arxiv.org/abs/2506.13109</guid>
<content:encoded><![CDATA[
arXiv:2506.13109v1 Announce Type: new 
Abstract: In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMU's IWSLT 2025 Simultaneous Speech Translation System</title>
<link>https://arxiv.org/abs/2506.13143</link>
<guid>https://arxiv.org/abs/2506.13143</guid>
<content:encoded><![CDATA[
arXiv:2506.13143v1 Announce Type: new 
Abstract: This paper presents CMU's submission to the IWSLT 2025 Simultaneous Speech Translation (SST) task for translating unsegmented English speech into Chinese and German text in a streaming manner. Our end-to-end speech-to-text system integrates a chunkwise causal Wav2Vec 2.0 speech encoder, an adapter, and the Qwen2.5-7B-Instruct as the decoder. We use a two-stage simultaneous training procedure on robust speech segments curated from LibriSpeech, CommonVoice, and VoxPopuli datasets, utilizing standard cross-entropy loss. Our model supports adjustable latency through a configurable latency multiplier. Experimental results demonstrate that our system achieves 44.3 BLEU for English-to-Chinese and 25.1 BLEU for English-to-German translations on the ACL60/60 development set, with computation-aware latencies of 2.7 seconds and 2.3 seconds, and theoretical latencies of 2.2 and 1.7 seconds, respectively.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting LLMs for Minimal-edit Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2506.13148</link>
<guid>https://arxiv.org/abs/2506.13148</guid>
<content:encoded><![CDATA[
arXiv:2506.13148v1 Announce Type: new 
Abstract: Decoder-only large language models have shown superior performance in the fluency-edit English Grammatical Error Correction, but their adaptation for minimal-edit English GEC is still underexplored. To improve their effectiveness in the minimal-edit approach, we explore the error rate adaptation topic and propose a novel training schedule method. Our experiments set a new state-of-the-art result for a single-model system on the BEA-test set. We also detokenize the most common English GEC datasets to match the natural way of writing text. During the process, we find that there are errors in them. Our experiments analyze whether training on detokenized datasets impacts the results and measure the impact of the usage of the datasets with corrected erroneous examples. To facilitate reproducibility, we have released the source code used to train our models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns</title>
<link>https://arxiv.org/abs/2506.13172</link>
<guid>https://arxiv.org/abs/2506.13172</guid>
<content:encoded><![CDATA[
arXiv:2506.13172v1 Announce Type: new 
Abstract: We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of the user-friendly decision aid Rule-based Evaluation and Support Tool (REST) for optimizing the resources of an information extraction task</title>
<link>https://arxiv.org/abs/2506.13177</link>
<guid>https://arxiv.org/abs/2506.13177</guid>
<content:encoded><![CDATA[
arXiv:2506.13177v1 Announce Type: new 
Abstract: Rules could be an information extraction (IE) default option, compared to ML and LLMs in terms of sustainability, transferability, interpretability, and development burden. We suggest a sustainable and combined use of rules and ML as an IE method. Our approach starts with an exhaustive expert manual highlighting in a single working session of a representative subset of the data corpus. We developed and validated the feasibility and the performance metrics of the REST decision tool to help the annotator choose between rules as a by default option and ML for each entity of an IE task. REST makes the annotator visualize the characteristics of each entity formalization in the free texts and the expected rule development feasibility and IE performance metrics. ML is considered as a backup IE option and manual annotation for training is therefore minimized. The external validity of REST on a 12-entity use case showed good reproducibility.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models with Reliable Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.13178</link>
<guid>https://arxiv.org/abs/2506.13178</guid>
<content:encoded><![CDATA[
arXiv:2506.13178v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text generation and understanding, yet their reliance on implicit, unstructured knowledge often leads to factual inaccuracies and limited interpretability. Knowledge Graphs (KGs), with their structured, relational representations, offer a promising solution to ground LLMs in verified knowledge. However, their potential remains constrained by inherent noise, incompleteness, and the complexity of integrating their rigid structure with the flexible reasoning of LLMs. This thesis presents a systematic framework to address these limitations, advancing the reliability of KGs and their synergistic integration with LLMs through five interconnected contributions. This thesis addresses these challenges through a cohesive framework that enhances LLMs by refining and leveraging reliable KGs. First, we introduce contrastive error detection, a structure-based method to identify incorrect facts in KGs. This approach is extended by an attribute-aware framework that unifies structural and semantic signals for error correction. Next, we propose an inductive completion model that further refines KGs by completing the missing relationships in evolving KGs. Building on these refined KGs, KnowGPT integrates structured graph reasoning into LLMs through dynamic prompting, improving factual grounding. These contributions form a systematic pipeline (from error detection to LLM integration), demonstrating that reliable KGs significantly enhance the robustness, interpretability, and adaptability of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Acoustic Model Architecture Optimization in Training for ASR</title>
<link>https://arxiv.org/abs/2506.13180</link>
<guid>https://arxiv.org/abs/2506.13180</guid>
<content:encoded><![CDATA[
arXiv:2506.13180v1 Announce Type: new 
Abstract: Architecture design is inherently complex. Existing approaches rely on either handcrafted rules, which demand extensive empirical expertise, or automated methods like neural architecture search, which are computationally intensive. In this paper, we introduce DMAO, an architecture optimization framework that employs a grow-and-drop strategy to automatically reallocate parameters during training. This reallocation shifts resources from less-utilized areas to those parts of the model where they are most beneficial. Notably, DMAO only introduces negligible training overhead at a given model complexity. We evaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and Switchboard datasets. The results show that, using the same amount of training resources, our proposed DMAO consistently improves WER by up to 6% relatively across various architectures, model sizes, and datasets. Furthermore, we analyze the pattern of parameter redistribution and uncover insightful findings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-then-Unlearn: Embedding Alignment for LLM Unlearning</title>
<link>https://arxiv.org/abs/2506.13181</link>
<guid>https://arxiv.org/abs/2506.13181</guid>
<content:encoded><![CDATA[
arXiv:2506.13181v1 Announce Type: new 
Abstract: As large language models (LLMs) are trained on massive datasets, they have raised significant privacy and ethical concerns due to their potential to inadvertently retain sensitive information. Unlearning seeks to selectively remove specific data from trained models, such as personal information or copyrighted content. Current approaches targeting specific output sequences at the token level often fail to achieve complete forgetting and remain susceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel framework that performs unlearning in the semantic embedding space rather than directly on output tokens. Align-then-Unlearn first augments the LLM with an embedding prediction module trained to anticipate future context representations. Unlearning is then achieved by fine-tuning the model to minimize the similarity between these predicted embeddings and a target embedding that represents the concept to be removed. Initial results show that Align-then-Unlearn effectively removes targeted knowledge with minimal degradation in overall model utility. These findings suggest that embedding-based unlearning offers a promising and robust approach to removing conceptual knowledge. Our code is available at https://github.com/ExplainableML/align-then-unlearn.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs</title>
<link>https://arxiv.org/abs/2506.13192</link>
<guid>https://arxiv.org/abs/2506.13192</guid>
<content:encoded><![CDATA[
arXiv:2506.13192v1 Announce Type: new 
Abstract: Large language models (LLMs) are often constrained by rigid reasoning processes, limiting their ability to generate creative and diverse responses. To address this, a novel framework called LADDER is proposed, combining Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and multi-dimensional up/down-sampling strategies which breaks the limitations of traditional LLMs. First, CoT reasoning guides the model through multi-step logical reasoning, expanding the semantic space and breaking the rigidity of thought. Next, MoE distributes the reasoning tasks across multiple expert modules, each focusing on specific sub-tasks. Finally, dimensionality reduction maps the reasoning outputs back to a lower-dimensional semantic space, yielding more precise and creative responses. Extensive experiments across multiple tasks demonstrate that LADDER significantly improves task completion, creativity, and fluency, generating innovative and coherent responses that outperform traditional models. Ablation studies reveal the critical roles of CoT and MoE in enhancing reasoning abilities and creative output. This work contributes to the development of more flexible and creative LLMs, capable of addressing complex and novel tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Music Preferences Reflect Cultural Values? A Cross-National Analysis Using Music Embedding and World Values Survey</title>
<link>https://arxiv.org/abs/2506.13199</link>
<guid>https://arxiv.org/abs/2506.13199</guid>
<content:encoded><![CDATA[
arXiv:2506.13199v1 Announce Type: new 
Abstract: This study explores the extent to which national music preferences reflect underlying cultural values. We collected long-term popular music data from YouTube Music Charts across 62 countries, encompassing both Western and non-Western regions, and extracted audio embeddings using the CLAP model. To complement these quantitative representations, we generated semantic captions for each track using LP-MusicCaps and GPT-based summarization. Countries were clustered based on contrastive embeddings that highlight deviations from global musical norms. The resulting clusters were projected into a two-dimensional space via t-SNE for visualization and evaluated against cultural zones defined by the World Values Survey (WVS). Statistical analyses, including MANOVA and chi-squared tests, confirmed that music-based clusters exhibit significant alignment with established cultural groupings. Furthermore, residual analysis revealed consistent patterns of overrepresentation, suggesting non-random associations between specific clusters and cultural zones. These findings indicate that national-level music preferences encode meaningful cultural signals and can serve as a proxy for understanding global cultural boundaries.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law</title>
<link>https://arxiv.org/abs/2506.13216</link>
<guid>https://arxiv.org/abs/2506.13216</guid>
<content:encoded><![CDATA[
arXiv:2506.13216v1 Announce Type: new 
Abstract: Scaling law builds the relationship between training computation and validation loss, enabling researchers to effectively predict the loss trending of models across different levels of computation. However, a gap still remains between validation loss and the model's downstream capabilities, making it untrivial to apply scaling law to direct performance prediction for downstream tasks. The loss typically represents a cumulative penalty for predicted tokens, which are implicitly considered to have equal importance. Nevertheless, our studies have shown evidence that when considering different training data distributions, we cannot directly model the relationship between downstream capability and computation or token loss. To bridge the gap between validation loss and downstream task capabilities, in this work, we introduce Capability Salience Vector, which decomposes the overall loss and assigns different importance weights to tokens to assess a specific meta-capability, aligning the validation loss with downstream task performance in terms of the model's capabilities. Experiments on various popular benchmarks demonstrate that our proposed Capability Salience Vector could significantly improve the predictability of language model performance on downstream tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.13229</link>
<guid>https://arxiv.org/abs/2506.13229</guid>
<content:encoded><![CDATA[
arXiv:2506.13229v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong potential for recommendation by framing item prediction as a token-by-token language generation task. However, existing methods treat all item tokens equally, simply pursuing likelihood maximization during both optimization and decoding. This overlooks crucial token-level differences in decisiveness-many tokens contribute little to item discrimination yet can dominate optimization or decoding. To quantify token decisiveness, we propose a novel perspective that models item generation as a decision process, measuring token decisiveness by the Information Gain (IG) each token provides in reducing uncertainty about the generated item. Our empirical analysis reveals that most tokens have low IG but often correspond to high logits, disproportionately influencing training loss and decoding, which may impair model performance. Building on these insights, we introduce an Information Gain-based Decisiveness-aware Token handling (IGD) strategy that integrates token decisiveness into both tuning and decoding. Specifically, IGD downweights low-IG tokens during tuning and rebalances decoding to emphasize tokens with high IG. In this way, IGD moves beyond pure likelihood maximization, effectively prioritizing high-decisiveness tokens. Extensive experiments on four benchmark datasets with two LLM backbones demonstrate that IGD consistently improves recommendation accuracy, achieving significant gains on widely used ranking metrics compared to strong baselines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy</title>
<link>https://arxiv.org/abs/2506.13284</link>
<guid>https://arxiv.org/abs/2506.13284</guid>
<content:encoded><![CDATA[
arXiv:2506.13284v1 Announce Type: new 
Abstract: In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs</title>
<link>https://arxiv.org/abs/2506.13285</link>
<guid>https://arxiv.org/abs/2506.13285</guid>
<content:encoded><![CDATA[
arXiv:2506.13285v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown strong performance across natural language tasks, but remain vulnerable to backdoor attacks. Recent model editing-based approaches enable efficient backdoor injection by directly modifying parameters to map specific triggers to attacker-desired responses. However, these methods often suffer from safety fallback, where the model initially responds affirmatively but later reverts to refusals due to safety alignment. In this work, we propose DualEdit, a dual-objective model editing framework that jointly promotes affirmative outputs and suppresses refusal responses. To address two key challenges -- balancing the trade-off between affirmative promotion and refusal suppression, and handling the diversity of refusal expressions -- DualEdit introduces two complementary techniques. (1) Dynamic loss weighting calibrates the objective scale based on the pre-edited model to stabilize optimization. (2) Refusal value anchoring compresses the suppression target space by clustering representative refusal value vectors, reducing optimization conflict from overly diverse token sets. Experiments on safety-aligned LLMs show that DualEdit improves attack success by 9.98\% and reduces safety fallback rate by 10.88\% over baselines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models</title>
<link>https://arxiv.org/abs/2506.13300</link>
<guid>https://arxiv.org/abs/2506.13300</guid>
<content:encoded><![CDATA[
arXiv:2506.13300v1 Announce Type: new 
Abstract: This paper presents Seewo's systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines</title>
<link>https://arxiv.org/abs/2506.13313</link>
<guid>https://arxiv.org/abs/2506.13313</guid>
<content:encoded><![CDATA[
arXiv:2506.13313v1 Announce Type: new 
Abstract: Reading and evaluating product reviews is central to how most people decide what to buy and consume online. However, the recent emergence of Large Language Models and Generative Artificial Intelligence now means writing fraudulent or fake reviews is potentially easier than ever. Through three studies we demonstrate that (1) humans are no longer able to distinguish between real and fake product reviews generated by machines, averaging only 50.8% accuracy overall - essentially the same that would be expected by chance alone; (2) that LLMs are likewise unable to distinguish between fake and real reviews and perform equivalently bad or even worse than humans; and (3) that humans and LLMs pursue different strategies for evaluating authenticity which lead to equivalently bad accuracy, but different precision, recall and F1 scores - indicating they perform worse at different aspects of judgment. The results reveal that review systems everywhere are now susceptible to mechanised fraud if they do not depend on trustworthy purchase verification to guarantee the authenticity of reviewers. Furthermore, the results provide insight into the consumer psychology of how humans judge authenticity, demonstrating there is an inherent 'scepticism bias' towards positive reviews and a special vulnerability to misjudge the authenticity of fake negative reviews. Additionally, results provide a first insight into the 'machine psychology' of judging fake reviews, revealing that the strategies LLMs take to evaluate authenticity radically differ from humans, in ways that are equally wrong in terms of accuracy, but different in their misjudgments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach</title>
<link>https://arxiv.org/abs/2506.13328</link>
<guid>https://arxiv.org/abs/2506.13328</guid>
<content:encoded><![CDATA[
arXiv:2506.13328v1 Announce Type: new 
Abstract: Numerical consistency across tables in disclosure documents is critical for ensuring accuracy, maintaining credibility, and avoiding reputational and economic risks. Automated tabular numerical cross-checking presents two significant challenges: (C1) managing the combinatorial explosion of candidate instances at the document level and (C2) comprehending multi-faceted numerical semantics. Previous research typically depends on heuristic-based filtering or simplified context extraction, often struggling to balance performance and efficiency. Recently, large language models (LLMs) have demonstrated remarkable contextual understanding capabilities that helps address C2 at the instance level, yet they remain hampered by computational inefficiency (C1) and limited domain expertise. This paper introduces CoFiTCheck, a novel LLM-based coarse-to-fine framework that addresses these challenges through two sequential stages: embedding-based filtering and discriminative classification. The embedding-based filtering stage introduces an instructional parallel encoding method to efficiently represent all numerical mentions in a table with LLMs, as well as a decoupled InfoNCE objective to mitigate the isolated mention problem. The discriminative classification stage employs a specialized LLM for fine-grained analysis of the remaining candidate pairs. This stage is further enhanced by our crosstable numerical alignment pretraining paradigm, which leverages weak supervision from cross-table numerical equality relationships to enrich task-specific priors without requiring manual annotation. Comprehensive evaluation across three types of real-world disclosure documents demonstrates that CoFiTCheck significantly outperforms previous methods while maintaining practical efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization</title>
<link>https://arxiv.org/abs/2506.13329</link>
<guid>https://arxiv.org/abs/2506.13329</guid>
<content:encoded><![CDATA[
arXiv:2506.13329v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models have emerged as a cornerstone of large-scale deep learning by efficiently distributing computation and enhancing performance. However, their unique architecture-characterized by sparse expert activation and dynamic routing mechanisms-introduces inherent complexities that challenge conventional quantization techniques. Existing post-training quantization (PTQ) methods struggle to address activation outliers, router consistency and sparse expert calibration, leading to significant performance degradation. To bridge this gap, we propose EAQuant, a novel PTQ framework tailored for MoE architectures. Our method systematically tackles these challenges through three key innovations: (1) expert-aware smoothing aggregation to suppress activation outliers and stabilize quantization, (2) router logits distribution alignment to preserve expert selection consistency post-quantization, and (3) expert-level calibration data balance to optimize sparsely activated experts. Extensive experiments across W4A4 and extreme W3A4 quantization configurations demonstrate that EAQuant significantly outperforms existing methods, achieving average score improvements of 1.15 - 2.28% across three diverse MoE architectures, with particularly pronounced gains in reasoning tasks and robust performance retention under aggressive quantization. By integrating these innovations, EAQuant establishes a new state-of-the-art for high-precision, efficient MoE model compression. Our code is available at https://github.com/darren-fzq/EAQuant.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM Challenge 2025</title>
<link>https://arxiv.org/abs/2506.13339</link>
<guid>https://arxiv.org/abs/2506.13339</guid>
<content:encoded><![CDATA[
arXiv:2506.13339v1 Announce Type: new 
Abstract: This report details the NTU Speechlab system developed for the Interspeech 2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge (Task I), where we achieved 5th place. We present comprehensive analyses of our multilingual automatic speech recognition system, highlighting key advancements in model architecture, data selection, and training strategies. In particular, language-specific prompts and model averaging techniques were instrumental in boosting system performance across diverse languages. Compared to the initial baseline system, our final model reduced the average Mix Error Rate from 20.2% to 10.6%, representing an absolute improvement of 9.6% (a relative improvement of 48%) on the evaluation set. Our results demonstrate the effectiveness of our approach and offer practical insights for future Speech Large Language Models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks</title>
<link>https://arxiv.org/abs/2506.13351</link>
<guid>https://arxiv.org/abs/2506.13351</guid>
<content:encoded><![CDATA[
arXiv:2506.13351v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, we propose Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, we introduce a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. We evaluate DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns</title>
<link>https://arxiv.org/abs/2506.13356</link>
<guid>https://arxiv.org/abs/2506.13356</guid>
<content:encoded><![CDATA[
arXiv:2506.13356v1 Announce Type: new 
Abstract: Long-term memory (LTM) is essential for large language models (LLMs) to achieve autonomous intelligence in complex, evolving environments. Despite increasing efforts in memory-augmented and retrieval-based architectures, there remains a lack of standardized benchmarks to systematically evaluate LLMs' long-term memory abilities. Existing benchmarks still face challenges in evaluating knowledge retention and dynamic sequential reasoning, and in their own flexibility, all of which limit their effectiveness in assessing models' LTM capabilities. To address these gaps, we propose a novel benchmark framework based on interactive fiction games, featuring dynamically branching storylines with complex reasoning structures. These structures simulate real-world scenarios by requiring LLMs to navigate hierarchical decision trees, where each choice triggers cascading dependencies across multi-turn interactions. Our benchmark emphasizes two distinct settings to test reasoning complexity: one with immediate feedback upon incorrect decisions, and the other requiring models to independently trace back and revise earlier choices after failure. As part of this benchmark, we also construct a new dataset designed to test LLMs' LTM within narrative-driven environments. We further validate the effectiveness of our approach through detailed experiments. Experimental results demonstrate the benchmark's ability to robustly and reliably assess LTM in LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Medical VIE via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13363</link>
<guid>https://arxiv.org/abs/2506.13363</guid>
<content:encoded><![CDATA[
arXiv:2506.13363v1 Announce Type: new 
Abstract: Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction</title>
<link>https://arxiv.org/abs/2506.13366</link>
<guid>https://arxiv.org/abs/2506.13366</guid>
<content:encoded><![CDATA[
arXiv:2506.13366v1 Announce Type: new 
Abstract: This paper proposes a consistency reflection and correction method for goal-oriented dialogue systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decompositional Reasoning for Graph Retrieval with Large Language Models</title>
<link>https://arxiv.org/abs/2506.13380</link>
<guid>https://arxiv.org/abs/2506.13380</guid>
<content:encoded><![CDATA[
arXiv:2506.13380v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at many NLP tasks, but struggle with multi-hop reasoning and factual consistency, limiting their effectiveness on knowledge-intensive tasks like complex question answering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally lack the ability to reason efficiently over graph-structured information. To tackle this problem, we propose a novel retrieval approach that integrates textual knowledge graphs into the LLM reasoning process via query decomposition. Our method decomposes complex questions into sub-questions, retrieves relevant textual subgraphs, and composes a question-specific knowledge graph to guide answer generation. For that, we use a weighted similarity function that focuses on both the complex question and the generated subquestions to extract a relevant subgraph, which allows efficient and precise retrieval for complex questions and improves the performance of LLMs on multi-hop QA tasks. This structured reasoning pipeline enhances factual grounding and interpretability while leveraging the generative strengths of LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that it achieves comparable or superior performance to competitive existing methods, using smaller models and fewer LLM calls.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Context-Enhanced Speech Large Language Models for Multilingual Conversational ASR</title>
<link>https://arxiv.org/abs/2506.13396</link>
<guid>https://arxiv.org/abs/2506.13396</guid>
<content:encoded><![CDATA[
arXiv:2506.13396v1 Announce Type: new 
Abstract: This paper introduces the integration of language-specific bi-directional context into a speech large language model (SLLM) to improve multilingual continuous conversational automatic speech recognition (ASR). We propose a character-level contextual masking strategy during training, which randomly removes portions of the context to enhance robustness and better emulate the flawed transcriptions that may occur during inference. For decoding, a two-stage pipeline is utilized: initial isolated segment decoding followed by context-aware re-decoding using neighboring hypotheses. Evaluated on the 1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM) corpus covering eleven languages, our method achieves an 18% relative improvement compared to a strong baseline, outperforming even the model trained on 6000 hours of data for the MLC-SLM competition. These results underscore the significant benefit of incorporating contextual information in multilingual continuous conversational ASR.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis</title>
<link>https://arxiv.org/abs/2506.13405</link>
<guid>https://arxiv.org/abs/2506.13405</guid>
<content:encoded><![CDATA[
arXiv:2506.13405v1 Announce Type: new 
Abstract: With the rapid advancement of Large Language Models (LLMs), there is an increasing need for challenging benchmarks to evaluate their capabilities in handling complex tabular data. However, existing benchmarks are either based on outdated data setups or focus solely on simple, flat table structures. In this paper, we introduce RealHiTBench, a comprehensive benchmark designed to evaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a variety of input formats for complex tabular data, including LaTeX, HTML, and PNG. RealHiTBench also includes a diverse collection of tables with intricate structures, spanning a wide range of task types. Our experimental results, using 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a challenging benchmark. Moreover, we also develop TreeThinker, a tree-based pipeline that organizes hierarchical headers into a tree structure for enhanced tabular reasoning, validating the importance of improving LLMs' perception of table hierarchies. We hope that our work will inspire further research on tabular data reasoning and the development of more robust models. The code and data are available at https://github.com/cspzyy/RealHiTBench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Model for Word Repetition</title>
<link>https://arxiv.org/abs/2506.13450</link>
<guid>https://arxiv.org/abs/2506.13450</guid>
<content:encoded><![CDATA[
arXiv:2506.13450v1 Announce Type: new 
Abstract: It takes several years for the developing brain of a baby to fully master word repetition-the task of hearing a word and repeating it aloud. Repeating a new word, such as from a new language, can be a challenging task also for adults. Additionally, brain damage, such as from a stroke, may lead to systematic speech errors with specific characteristics dependent on the location of the brain damage. Cognitive sciences suggest a model with various components for the different processing stages involved in word repetition. While some studies have begun to localize the corresponding regions in the brain, the neural mechanisms and how exactly the brain performs word repetition remain largely unknown. We propose to bridge the gap between the cognitive model of word repetition and neural mechanisms in the human brain by modeling the task using deep neural networks. Neural models are fully observable, allowing us to study the detailed mechanisms in their various substructures and make comparisons with human behavior and, ultimately, the brain. Here, we make first steps in this direction by: (1) training a large set of models to simulate the word repetition task; (2) creating a battery of tests to probe the models for known effects from behavioral studies in humans, and (3) simulating brain damage through ablation studies, where we systematically remove neurons from the model, and repeat the behavioral study to examine the resulting speech errors in the "patient" model. Our results show that neural models can mimic several effects known from human research, but might diverge in other aspects, highlighting both the potential and the challenges for future research aimed at developing human-like neural models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study</title>
<link>https://arxiv.org/abs/2506.13464</link>
<guid>https://arxiv.org/abs/2506.13464</guid>
<content:encoded><![CDATA[
arXiv:2506.13464v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Omics Cohort Discovery for Research on Neurodegeneration through Ontology-Augmented Embedding Models</title>
<link>https://arxiv.org/abs/2506.13467</link>
<guid>https://arxiv.org/abs/2506.13467</guid>
<content:encoded><![CDATA[
arXiv:2506.13467v1 Announce Type: new 
Abstract: The growing volume of omics and clinical data generated for neurodegenerative diseases (NDs) requires new approaches for their curation so they can be ready-to-use in bioinformatics. NeuroEmbed is an approach for the engineering of semantically accurate embedding spaces to represent cohorts and samples. The NeuroEmbed method comprises four stages: (1) extraction of ND cohorts from public repositories; (2) semi-automated normalization and augmentation of metadata of cohorts and samples using biomedical ontologies and clustering on the embedding space; (3) automated generation of a natural language question-answering (QA) dataset for cohorts and samples based on randomized combinations of standardized metadata dimensions and (4) fine-tuning of a domain-specific embedder to optimize queries. We illustrate the approach using the GEO repository and the PubMedBERT pretrained embedder. Applying NeuroEmbed, we semantically indexed 2,801 repositories and 150,924 samples. Amongst many biology-relevant categories, we normalized more than 1,700 heterogeneous tissue labels from GEO into 326 unique ontology-aligned concepts and enriched annotations with new ontology-aligned terms, leading to a fold increase in size for the metadata terms between 2.7 and 20 fold. After fine-tuning PubMedBERT with the QA training data augmented with the enlarged metadata, the model increased its mean Retrieval Precision from 0.277 to 0.866 and its mean Percentile Rank from 0.355 to 0.896. The NeuroEmbed methodology for the creation of electronic catalogues of omics cohorts and samples will foster automated bioinformatic pipelines construction. The NeuroEmbed catalogue of cohorts and samples is available at https://github.com/JoseAdrian3/NeuroEmbed.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interdisciplinary Approach to Human-Centered Machine Translation</title>
<link>https://arxiv.org/abs/2506.13468</link>
<guid>https://arxiv.org/abs/2506.13468</guid>
<content:encoded><![CDATA[
arXiv:2506.13468v1 Announce Type: new 
Abstract: Machine Translation (MT) tools are widely used today, often in contexts where professional translators are not present. Despite progress in MT technology, a gap persists between system development and real-world usage, particularly for non-expert users who may struggle to assess translation reliability. This paper advocates for a human-centered approach to MT, emphasizing the alignment of system design with diverse communicative goals and contexts of use. We survey the literature in Translation Studies and Human-Computer Interaction to recontextualize MT evaluation and design to address the diverse real-world scenarios in which MT is used today.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning</title>
<link>https://arxiv.org/abs/2506.13470</link>
<guid>https://arxiv.org/abs/2506.13470</guid>
<content:encoded><![CDATA[
arXiv:2506.13470v1 Announce Type: new 
Abstract: Zero-shot stance detection (ZSSD) aims to identify the stance of text toward previously unseen targets, a setting where conventional supervised models often fail due to reliance on labeled data and shallow lexical cues. Inspired by human cognitive reasoning, we propose the Cognitive Inductive Reasoning Framework (CIRF), which abstracts transferable reasoning schemas from unlabeled text and encodes them as concept-level logic. To integrate these schemas with input arguments, we introduce a Schema-Enhanced Graph Kernel Model (SEGKM) that dynamically aligns local and global reasoning structures. Experiments on SemEval-2016, VAST, and COVID-19-Stance benchmarks show that CIRF establishes new state-of-the-art results, outperforming strong ZSSD baselines by 1.0, 4.5, and 3.3 percentage points in macro-F1, respectively, and achieving comparable accuracy with 70\% fewer labeled examples. We will release the full code upon publication.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models</title>
<link>https://arxiv.org/abs/2506.13472</link>
<guid>https://arxiv.org/abs/2506.13472</guid>
<content:encoded><![CDATA[
arXiv:2506.13472v1 Announce Type: new 
Abstract: Quantization has been widely studied as an effective technique for reducing the memory requirement of large language models (LLMs), potentially improving the latency time as well. Utilizing the characteristic of rotational invariance of transformer, we propose the rotation-based saliency-aware weight quantization (ROSAQ), which identifies salient channels in the projection feature space, not in the original feature space, where the projected "principal" dimensions are naturally considered as "salient" features. The proposed ROSAQ consists of 1) PCA-based projection, which first performs principal component analysis (PCA) on a calibration set and transforms via the PCA projection, 2) Salient channel dentification, which selects dimensions corresponding to the K-largest eigenvalues as salient channels, and 3) Saliency-aware quantization with mixed-precision, which uses FP16 for salient dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ shows improvements over the baseline saliency-aware quantization on the original feature space and other existing quantization methods. With kernel fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in generating 256 tokens with a batch size of 64.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13474</link>
<guid>https://arxiv.org/abs/2506.13474</guid>
<content:encoded><![CDATA[
arXiv:2506.13474v1 Announce Type: new 
Abstract: Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited "out-of-the-box" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness</title>
<link>https://arxiv.org/abs/2506.13479</link>
<guid>https://arxiv.org/abs/2506.13479</guid>
<content:encoded><![CDATA[
arXiv:2506.13479v1 Announce Type: new 
Abstract: Merging or routing low-rank adapters (LoRAs) has emerged as a popular solution for enhancing large language models, particularly when data access is restricted by regulatory or domain-specific constraints. This position paper argues that the research community should shift its focus from developing new merging or routing algorithms to understanding the conditions under which reusing LoRAs is truly effective. Through theoretical analysis and synthetic two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs enables genuine compositional generalization or merely reflects shallow pattern matching. Evaluating two data-agnostic methods--parameter averaging and dynamic adapter selection--we found that reusing LoRAs often fails to logically integrate knowledge across disjoint fine-tuning datasets, especially when such knowledge is underrepresented during pretraining. Our empirical results, supported by theoretical insights into LoRA's limited expressiveness, highlight the preconditions and constraints of reusing them for unseen tasks and cast doubt on its feasibility as a truly data-free approach. We advocate for pausing the pursuit of novel methods for recycling LoRAs and emphasize the need for rigorous mechanisms to guide future academic research in adapter-based model merging and practical system designs for practitioners.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs</title>
<link>https://arxiv.org/abs/2506.13487</link>
<guid>https://arxiv.org/abs/2506.13487</guid>
<content:encoded><![CDATA[
arXiv:2506.13487v1 Announce Type: new 
Abstract: We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal pairs, designed to evaluate the linguistic abilities of monolingual and multilingual language models (LMs). Covering 16 linguistic phenomena with 1000 minimal pairs each, TurBLiMP fills an important gap in linguistic evaluation resources for Turkish. In designing the benchmark, we give extra attention to two properties of Turkish that remain understudied in current syntactic evaluations of LMs, namely word order flexibility and subordination through morphological processes. Our experiments on a wide range of LMs and a newly collected set of human acceptability judgments reveal that even cutting-edge Large LMs still struggle with grammatical phenomena that are not challenging for humans, and may also exhibit different sensitivities to word order and morphological complexity compared to humans.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOW: Bottlenecked Next Word Exploration</title>
<link>https://arxiv.org/abs/2506.13502</link>
<guid>https://arxiv.org/abs/2506.13502</guid>
<content:encoded><![CDATA[
arXiv:2506.13502v1 Announce Type: new 
Abstract: Large language models (LLMs) are typically trained via next-word prediction (NWP), which provides strong surface-level fluency but often lacks support for robust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel RL framework that rethinks NWP by introducing a reasoning bottleneck where a policy model first generates a reasoning path rather than predicting the next token directly, after which a frozen judge model predicts the next token distribution based solely on this reasoning path. We train the policy model using GRPO with rewards that quantify how effectively the reasoning path facilitates next-word recovery. Compared with other continual pretraining baselines, we show that BOW improves both the general and next-word reasoning capabilities of the base model, evaluated on various benchmarks. Our findings show that BOW can serve as an effective and scalable alternative to vanilla NWP.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean</title>
<link>https://arxiv.org/abs/2506.13513</link>
<guid>https://arxiv.org/abs/2506.13513</guid>
<content:encoded><![CDATA[
arXiv:2506.13513v1 Announce Type: new 
Abstract: Language detoxification involves removing toxicity from offensive language. While a neutral-toxic paired dataset provides a straightforward approach for training detoxification models, creating such datasets presents several challenges: i) the need for human annotation to build paired data, and ii) the rapid evolution of offensive terms, rendering static datasets quickly outdated. To tackle these challenges, we introduce an automated paired data generation pipeline, called K/DA. This pipeline is designed to generate offensive language with implicit offensiveness and trend-aligned slang, making the resulting dataset suitable for detoxification model training. We demonstrate that the dataset generated by K/DA exhibits high pair consistency and greater implicit offensiveness compared to existing Korean datasets, and also demonstrates applicability to other languages. Furthermore, it enables effective training of a high-performing detoxification model with simple instruction fine-tuning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices</title>
<link>https://arxiv.org/abs/2506.13514</link>
<guid>https://arxiv.org/abs/2506.13514</guid>
<content:encoded><![CDATA[
arXiv:2506.13514v1 Announce Type: new 
Abstract: Small Language Models (SLMs, or on-device LMs) have significantly fewer parameters than Large Language Models (LLMs). They are typically deployed on low-end devices, like mobile phones and single-board computers. Unlike LLMs, which rely on increasing model size for better generalisation, SLMs designed for edge applications are expected to have adaptivity to the deployment environments and energy efficiency given the device battery life constraints, which are not addressed in datacenter-deployed LLMs. This paper addresses these two requirements by proposing a training-free token embedding compression approach using Tensor-Train Decomposition (TTD). Each pre-trained token embedding vector is converted into a lower-dimensional Matrix Product State (MPS). We comprehensively evaluate the extracted low-rank structures across compression ratio, language task performance, latency, and energy consumption on a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion parameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our approach achieves a comparable language task performance to the original model with around $2.0\times$ embedding layer compression, while the energy consumption of a single query drops by half.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization</title>
<link>https://arxiv.org/abs/2506.13541</link>
<guid>https://arxiv.org/abs/2506.13541</guid>
<content:encoded><![CDATA[
arXiv:2506.13541v1 Announce Type: new 
Abstract: Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding "low-priority" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understand the Implication: Learning to Think for Pragmatic Understanding</title>
<link>https://arxiv.org/abs/2506.13559</link>
<guid>https://arxiv.org/abs/2506.13559</guid>
<content:encoded><![CDATA[
arXiv:2506.13559v1 Announce Type: new 
Abstract: Pragmatics, the ability to infer meaning beyond literal interpretation, is crucial for social cognition and communication. While LLMs have been benchmarked for their pragmatic understanding, improving their performance remains underexplored. Existing methods rely on annotated labels but overlook the reasoning process humans naturally use to interpret implicit meaning. To bridge this gap, we introduce a novel pragmatic dataset, ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both correct and incorrect interpretations. Through preference-tuning and supervised fine-tuning, we demonstrate that thought-based learning significantly enhances LLMs' pragmatic understanding, improving accuracy by 11.12% across model families. We further discuss a transfer-learning study where we evaluate the performance of thought-based training for the other tasks of pragmatics (presupposition, deixis) that are not seen during the training time and observe an improvement of 16.10% compared to label-trained models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Linguistic Shifts in Croatian News via Diachronic Word Embeddings</title>
<link>https://arxiv.org/abs/2506.13569</link>
<guid>https://arxiv.org/abs/2506.13569</guid>
<content:encoded><![CDATA[
arXiv:2506.13569v1 Announce Type: new 
Abstract: Measuring how semantics of words change over time improves our understanding of how cultures and perspectives change. Diachronic word embeddings help us quantify this shift, although previous studies leveraged substantial temporally annotated corpora. In this work, we use a corpus of 9.5 million Croatian news articles spanning the past 25 years and quantify semantic change using skip-gram word embeddings trained on five-year periods. Our analysis finds that word embeddings capture linguistic shifts of terms pertaining to major topics in this timespan (COVID-19, Croatia joining the European Union, technological advancements). We also find evidence that embeddings from post-2020 encode increased positivity in sentiment analysis tasks, contrasting studies reporting a decline in mental health over the same period.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention</title>
<link>https://arxiv.org/abs/2506.13585</link>
<guid>https://arxiv.org/abs/2506.13585</guid>
<content:encoded><![CDATA[
arXiv:2506.13585v1 Announce Type: new 
Abstract: We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems</title>
<link>https://arxiv.org/abs/2506.13596</link>
<guid>https://arxiv.org/abs/2506.13596</guid>
<content:encoded><![CDATA[
arXiv:2506.13596v1 Announce Type: new 
Abstract: This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation</title>
<link>https://arxiv.org/abs/2506.13599</link>
<guid>https://arxiv.org/abs/2506.13599</guid>
<content:encoded><![CDATA[
arXiv:2506.13599v1 Announce Type: new 
Abstract: Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \textbf{C}ityGPT-Powered \textbf{A}gentic framework for \textbf{M}obility \textbf{S}imulation (\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \textbf{CAMS} generates more realistic and plausible trajectories. In general, \textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy</title>
<link>https://arxiv.org/abs/2506.13610</link>
<guid>https://arxiv.org/abs/2506.13610</guid>
<content:encoded><![CDATA[
arXiv:2506.13610v1 Announce Type: new 
Abstract: Disease-symptom datasets are significant and in demand for medical research, disease diagnosis, clinical decision-making, and AI-driven health management applications. These datasets help identify symptom patterns associated with specific diseases, thus improving diagnostic accuracy and enabling early detection. The dataset presented in this study systematically compiles disease-symptom relationships from various online sources, medical literature, and publicly available health databases. The data was gathered through analyzing peer-reviewed medical articles, clinical case studies, and disease-symptom association reports. Only the verified medical sources were included in the dataset, while those from non-peer-reviewed and anecdotal sources were excluded. The dataset is structured in a tabular format, where the first column represents diseases, and the remaining columns represent symptoms. Each symptom cell contains a binary value (1 or 0), indicating whether a symptom is associated with a disease (1 for presence, 0 for absence). Thereby, this structured representation makes the dataset very useful for a wide range of applications, including machine learning-based disease prediction, clinical decision support systems, and epidemiological studies. Although there are some advancements in the field of disease-symptom datasets, there is a significant gap in structured datasets for the Bangla language. This dataset aims to bridge that gap by facilitating the development of multilingual medical informatics tools and improving disease prediction models for underrepresented linguistic communities. Further developments should include region-specific diseases and further fine-tuning of symptom associations for better diagnostic performance
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of LLM-as-a-Judge: How Design Choices Impact Evaluation Reliability</title>
<link>https://arxiv.org/abs/2506.13639</link>
<guid>https://arxiv.org/abs/2506.13639</guid>
<content:encoded><![CDATA[
arXiv:2506.13639v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance, reliable evaluation methods are essential particularly for open-ended, instruction-following tasks. LLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its reliability remains uncertain. In this work, we analyze key factors affecting its trustworthiness, focusing on alignment with human judgments and evaluation consistency. Using BIGGENBench and EvalBiasBench, we study the effects of evaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in evaluation. Our results show that evaluation criteria are critical for reliability, non-deterministic sampling improves alignment with human preferences over deterministic evaluation, and CoT reasoning offers minimal gains when clear evaluation criteria are present.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolvTrip: Enhancing Literary Character Understanding with Temporal Theory-of-Mind Graphs</title>
<link>https://arxiv.org/abs/2506.13641</link>
<guid>https://arxiv.org/abs/2506.13641</guid>
<content:encoded><![CDATA[
arXiv:2506.13641v1 Announce Type: new 
Abstract: A compelling portrayal of characters is essential to the success of narrative writing. For readers, appreciating a character's traits requires the ability to infer their evolving beliefs, desires, and intentions over the course of a complex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing ToM reasoning in prolonged narratives requires readers to integrate historical context with current narrative information, a task at which humans excel but Large Language Models (LLMs) often struggle. To systematically evaluate LLMs' ToM reasoning capability in long narratives, we construct LitCharToM, a benchmark of character-centric questions across four ToM dimensions from classic literature. Further, we introduce EvolvTrip, a perspective-aware temporal knowledge graph that tracks psychological development throughout narratives. Our experiments demonstrate that EvolvTrip consistently enhances performance of LLMs across varying scales, even in challenging extended-context scenarios. EvolvTrip proves to be particularly valuable for smaller models, partially bridging the performance gap with larger LLMs and showing great compatibility with lengthy narratives. Our findings highlight the importance of explicit representation of temporal character mental states in narrative comprehension and offer a foundation for more sophisticated character understanding. Our data and code are publicly available at https://github.com/Bernard-Yang/EvolvTrip.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent Prefix Data</title>
<link>https://arxiv.org/abs/2506.13674</link>
<guid>https://arxiv.org/abs/2506.13674</guid>
<content:encoded><![CDATA[
arXiv:2506.13674v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between input and prefix significance within the attention head. This motivates us to introduce Prefix-Tuning+, a novel architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself. We further provide an overview of our construction process to guide future users when constructing their own context-based methods. Our experiments show that, across a diverse set of benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning methods. Notably, it achieves performance on par with the widely adopted LoRA method on several general benchmarks, highlighting the potential modern extension of Prefix-Tuning approaches. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language Models</title>
<link>https://arxiv.org/abs/2506.13681</link>
<guid>https://arxiv.org/abs/2506.13681</guid>
<content:encoded><![CDATA[
arXiv:2506.13681v1 Announce Type: new 
Abstract: Sampling from language models impacts the quality and diversity of outputs, affecting both research and real-world applications. Recently, Nguyen et al. 2024's "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs" introduced a new sampler called min-p, claiming it achieves superior quality and diversity over established samplers such as basic, top-k, and top-p sampling. The significance of these claims was underscored by the paper's recognition as the 18th highest-scoring submission to ICLR 2025 and selection for an Oral presentation. This paper conducts a comprehensive re-examination of the evidence supporting min-p and reaches different conclusions from the original paper's four lines of evidence. First, the original paper's human evaluations omitted data, conducted statistical tests incorrectly, and described qualitative feedback inaccurately; our reanalysis demonstrates min-p did not outperform baselines in quality, diversity, or a trade-off between quality and diversity; in response to our findings, the authors of the original paper conducted a new human evaluation using a different implementation, task, and rubric that nevertheless provides further evidence min-p does not improve over baselines. Second, comprehensively sweeping the original paper's NLP benchmarks reveals min-p does not surpass baselines when controlling for the number of hyperparameters. Third, the original paper's LLM-as-a-Judge evaluations lack methodological clarity and appear inconsistently reported. Fourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars) were found to be unsubstantiated, leading to their removal; the revised adoption claim remains misleading. We conclude that evidence presented in the original paper fails to support claims that min-p improves quality, diversity, or a trade-off between quality and diversity.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems</title>
<link>https://arxiv.org/abs/2506.13692</link>
<guid>https://arxiv.org/abs/2506.13692</guid>
<content:encoded><![CDATA[
arXiv:2506.13692v1 Announce Type: new 
Abstract: With the advancement of large language models, many dialogue systems are now capable of providing reasonable and informative responses to patients' medical conditions. However, when patients consult their doctor, they may experience negative emotions due to the severity and urgency of their situation. If the model can provide appropriate comfort and empathy based on the patient's negative emotions while answering medical questions, it will likely offer a more reassuring experience during the medical consultation process. To address this issue, our paper explores the balance between knowledge sharing and emotional support in the healthcare dialogue process. We utilize a large language model to rewrite a real-world interactive medical dialogue dataset, generating patient queries with negative emotions and corresponding medical responses aimed at soothing the patient's emotions while addressing their concerns. The modified data serves to refine the latest large language models with various fine-tuning methods, enabling them to accurately provide sentences with both emotional reassurance and constructive suggestions in response to patients' questions. Compared to the original LLM model, our experimental results demonstrate that our methodology significantly enhances the model's ability to generate emotional responses while maintaining its original capability to provide accurate knowledge-based answers.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Following by Boosting Attention of Large Language Models</title>
<link>https://arxiv.org/abs/2506.13734</link>
<guid>https://arxiv.org/abs/2506.13734</guid>
<content:encoded><![CDATA[
arXiv:2506.13734v1 Announce Type: new 
Abstract: Controlling the generation of large language models (LLMs) remains a central challenge to ensure their safe and reliable deployment. While prompt engineering and finetuning are common approaches, recent work has explored latent steering, a lightweight technique that alters LLM internal activations to guide generation. However, subsequent studies revealed latent steering's effectiveness to be limited, often underperforming simple instruction prompting. To address this limitation, we first establish a benchmark across diverse behaviors for standardized evaluation of steering techniques. Building on insights from this benchmark, we introduce Instruction Attention Boosting (InstABoost), a latent steering method that boosts the strength of instruction prompting by altering the model's attention during generation. InstABoost combines the strengths of existing approaches and is theoretically supported by prior work that suggests that in-context rule following in transformer-based models can be controlled by manipulating attention on instructions. Empirically, InstABoost demonstrates superior control success compared to both traditional prompting and latent steering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTRR: Learning To Rank Retrievers for LLMs</title>
<link>https://arxiv.org/abs/2506.13743</link>
<guid>https://arxiv.org/abs/2506.13743</guid>
<content:encoded><![CDATA[
arXiv:2506.13743v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Thinking with Budget Guidance</title>
<link>https://arxiv.org/abs/2506.13752</link>
<guid>https://arxiv.org/abs/2506.13752</guid>
<content:encoded><![CDATA[
arXiv:2506.13752v1 Announce Type: new 
Abstract: Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models</title>
<link>https://arxiv.org/abs/2506.12059</link>
<guid>https://arxiv.org/abs/2506.12059</guid>
<content:encoded><![CDATA[
arXiv:2506.12059v1 Announce Type: cross 
Abstract: In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM's prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis</title>
<link>https://arxiv.org/abs/2506.12073</link>
<guid>https://arxiv.org/abs/2506.12073</guid>
<content:encoded><![CDATA[
arXiv:2506.12073v1 Announce Type: cross 
Abstract: Accurate alignment of dysfluent speech with intended text is crucial for automating the diagnosis of neurodegenerative speech disorders. Traditional methods often fail to model phoneme similarities effectively, limiting their performance. In this work, we propose Neural LCS, a novel approach for dysfluent text-text and speech-text alignment. Neural LCS addresses key challenges, including partial alignment and context-aware similarity mapping, by leveraging robust phoneme-level modeling. We evaluate our method on a large-scale simulated dataset, generated using advanced data simulation techniques, and real PPA data. Neural LCS significantly outperforms state-of-the-art models in both alignment accuracy and dysfluent speech segmentation. Our results demonstrate the potential of Neural LCS to enhance automated systems for diagnosing and analyzing speech disorders, offering a more accurate and linguistically grounded solution for dysfluent speech alignment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence and Civil Discourse: How LLMs Moderate Climate Change Conversations</title>
<link>https://arxiv.org/abs/2506.12077</link>
<guid>https://arxiv.org/abs/2506.12077</guid>
<content:encoded><![CDATA[
arXiv:2506.12077v1 Announce Type: cross 
Abstract: As large language models (LLMs) become increasingly integrated into online platforms and digital communication spaces, their potential to influence public discourse - particularly in contentious areas like climate change - requires systematic investigation. This study examines how LLMs naturally moderate climate change conversations through their distinct communicative behaviors. We conduct a comparative analysis of conversations between LLMs and human users on social media platforms, using five advanced models: three open-source LLMs (Gemma, Llama 3, and Llama 3.3) and two commercial systems (GPT-4o by OpenAI and Claude 3.5 by Anthropic). Through sentiment analysis, we assess the emotional characteristics of responses from both LLMs and humans. The results reveal two key mechanisms through which LLMs moderate discourse: first, LLMs consistently display emotional neutrality, showing far less polarized sentiment than human users. Second, LLMs maintain lower emotional intensity across contexts, creating a stabilizing effect in conversations. These findings suggest that LLMs possess inherent moderating capacities that could improve the quality of public discourse on controversial topics. This research enhances our understanding of how AI might support more civil and constructive climate change discussions and informs the design of AI-assisted communication tools.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Earth-Scale Human-Like Societies with One Billion Agents</title>
<link>https://arxiv.org/abs/2506.12078</link>
<guid>https://arxiv.org/abs/2506.12078</guid>
<content:encoded><![CDATA[
arXiv:2506.12078v1 Announce Type: cross 
Abstract: Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification</title>
<link>https://arxiv.org/abs/2506.12084</link>
<guid>https://arxiv.org/abs/2506.12084</guid>
<content:encoded><![CDATA[
arXiv:2506.12084v1 Announce Type: cross 
Abstract: The formal specification and verification of machine learning programs saw remarkable progress in less than a decade, leading to a profusion of tools. However, diversity may lead to fragmentation, resulting in tools that are difficult to compare, except for very specific benchmarks. Furthermore, this progress is heavily geared towards the specification and verification of a certain class of property, that is, local robustness properties. But while provers are becoming more and more efficient at solving local robustness properties, even slightly more complex properties, involving multiple neural networks for example, cannot be expressed in the input languages of winners of the International Competition of Verification of Neural Networks VNN-Comp. In this tool paper, we present CAISAR, an open-source platform dedicated to machine learning specification and verification. We present its specification language, suitable for modelling complex properties on neural networks, support vector machines and boosted trees. We show on concrete use-cases how specifications written in this language are automatically translated to queries to state-of-the-art provers, notably by using automated graph editing techniques, making it possible to use their off-the-shelf versions. The artifact to reproduce the paper claims is available at the following DOI: https://doi.org/10.5281/zenodo.15209510
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data</title>
<link>https://arxiv.org/abs/2506.12111</link>
<guid>https://arxiv.org/abs/2506.12111</guid>
<content:encoded><![CDATA[
arXiv:2506.12111v1 Announce Type: cross 
Abstract: Real-time continuous learning over streaming data remains a central challenge in deep learning and AI systems. Traditional gradient-based models such as backpropagation through time (BPTT) face computational and stability limitations when dealing with temporally unbounded data. In this paper, we introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs), which leverages the Feynman technique of differentiation under the integral sign to formulate neural updates as integrals over historical data. This reformulation allows for smoother, more stable learning dynamics that are both physically interpretable and computationally tractable. Inspired by Feynman's path integral formalism and compatible with quantum gradient estimation frameworks, QIDINNs open a path toward hybrid classical-quantum neural computation. We demonstrate our model's effectiveness on synthetic and real-world streaming tasks, and we propose directions for quantum extensions and scalable implementations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative or Discriminative? Revisiting Text Classification in the Era of Transformers</title>
<link>https://arxiv.org/abs/2506.12181</link>
<guid>https://arxiv.org/abs/2506.12181</guid>
<content:encoded><![CDATA[
arXiv:2506.12181v1 Announce Type: cross 
Abstract: The comparison between discriminative and generative classifiers has intrigued researchers since Efron's seminal analysis of logistic regression versus discriminant analysis. While early theoretical work established that generative classifiers exhibit lower sample complexity but higher asymptotic error in simple linear settings, these trade-offs remain unexplored in the transformer era. We present the first comprehensive evaluation of modern generative and discriminative architectures - Auto-regressive modeling, Masked Language Modeling, Discrete Diffusion, and Encoders for text classification. Our study reveals that the classical 'two regimes' phenomenon manifests distinctly across different architectures and training paradigms. Beyond accuracy, we analyze sample efficiency, calibration, noise robustness, and ordinality across diverse scenarios. Our findings offer practical guidance for selecting the most suitable modeling approach based on real-world constraints such as latency and data limitations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Emergence to Control: Probing and Modulating Self-Reflection in Language Models</title>
<link>https://arxiv.org/abs/2506.12217</link>
<guid>https://arxiv.org/abs/2506.12217</guid>
<content:encoded><![CDATA[
arXiv:2506.12217v1 Announce Type: cross 
Abstract: Self-reflection -- the ability of a large language model (LLM) to revisit, evaluate, and revise its own reasoning -- has recently emerged as a powerful behavior enabled by reinforcement learning with verifiable rewards (RLVR). While self-reflection correlates with improved reasoning accuracy, its origin and underlying mechanisms remain poorly understood. In this work, {\it we first show that self-reflection is not exclusive to RLVR fine-tuned models: it already emerges, albeit rarely, in pretrained models}. To probe this latent ability, we introduce Reflection-Inducing Probing, a method that injects reflection-triggering reasoning traces from fine-tuned models into pretrained models. This intervention raises self-reflection frequency of Qwen2.5 from 0.6\% to 18.6\%, revealing a hidden capacity for reflection. Moreover, our analysis of internal representations shows that both pretrained and fine-tuned models maintain hidden states that distinctly separate self-reflective from non-reflective contexts. Leveraging this observation, {\it we then construct a self-reflection vector, a direction in activation space associated with self-reflective reasoning}. By manipulating this vector, we enable bidirectional control over the self-reflective behavior for both pretrained and fine-tuned models. Experiments across multiple reasoning benchmarks show that enhancing these vectors improves reasoning performance by up to 12\%, while suppressing them reduces computational cost, providing a flexible mechanism to navigate the trade-off between reasoning quality and efficiency without requiring additional training. Our findings further our understanding of self-reflection and support a growing body of work showing that understanding model internals can enable precise behavioral control.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles</title>
<link>https://arxiv.org/abs/2506.12232</link>
<guid>https://arxiv.org/abs/2506.12232</guid>
<content:encoded><![CDATA[
arXiv:2506.12232v1 Announce Type: cross 
Abstract: Scene understanding is critical for various downstream tasks in autonomous driving, including facilitating driver-agent communication and enhancing human-centered explainability of autonomous vehicle (AV) decisions. This paper evaluates the capability of four multimodal large language models (MLLMs), including relatively small models, to understand scenes in a zero-shot, in-context learning setting. Additionally, we explore whether combining these models using an ensemble approach with majority voting can enhance scene understanding performance. Our experiments demonstrate that GPT-4o, the largest model, outperforms the others in scene understanding. However, the performance gap between GPT-4o and the smaller models is relatively modest, suggesting that advanced techniques such as improved in-context learning, retrieval-augmented generation (RAG), or fine-tuning could further optimize the smaller models' performance. We also observe mixed results with the ensemble approach: while some scene attributes show improvement in performance metrics such as F1-score, others experience a decline. These findings highlight the need for more sophisticated ensemble techniques to achieve consistent gains across all scene attributes. This study underscores the potential of leveraging MLLMs for scene understanding and provides insights into optimizing their performance for autonomous driving applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation</title>
<link>https://arxiv.org/abs/2506.12234</link>
<guid>https://arxiv.org/abs/2506.12234</guid>
<content:encoded><![CDATA[
arXiv:2506.12234v1 Announce Type: cross 
Abstract: Text-to-SQL systems enable users to query databases using natural language, democratizing access to data analytics. However, they face challenges in understanding ambiguous phrasing, domain-specific vocabulary, and complex schema relationships. This paper introduces Datrics Text2SQL, a Retrieval-Augmented Generation (RAG)-based framework designed to generate accurate SQL queries by leveraging structured documentation, example-based learning, and domain-specific rules. The system builds a rich Knowledge Base from database documentation and question-query examples, which are stored as vector embeddings and retrieved through semantic similarity. It then uses this context to generate syntactically correct and semantically aligned SQL code. The paper details the architecture, training methodology, and retrieval logic, highlighting how the system bridges the gap between user intent and database structure without requiring SQL expertise.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration</title>
<link>https://arxiv.org/abs/2506.12248</link>
<guid>https://arxiv.org/abs/2506.12248</guid>
<content:encoded><![CDATA[
arXiv:2506.12248v1 Announce Type: cross 
Abstract: Collaborative robots must quickly adapt to their partner's intent and preferences to proactively identify helpful actions. This is especially true in situated settings where human partners can continually teach robots new high-level behaviors, visual concepts, and physical skills (e.g., through demonstration), growing the robot's capabilities as the human-robot pair work together to accomplish diverse tasks. In this work, we argue that robots should be able to infer their partner's goals from early interactions and use this information to proactively plan behaviors ahead of explicit instructions from the user. Building from the strong commonsense priors and steerability of large language models, we introduce ProVox ("Proactive Voice"), a novel framework that enables robots to efficiently personalize and adapt to individual collaborators. We design a meta-prompting protocol that empowers users to communicate their distinct preferences, intent, and expected robot behaviors ahead of starting a physical interaction. ProVox then uses the personalized prompt to condition a proactive language model task planner that anticipates a user's intent from the current interaction context and robot capabilities to suggest helpful actions; in doing so, we alleviate user burden, minimizing the amount of time partners spend explicitly instructing and supervising the robot. We evaluate ProVox through user studies grounded in household manipulation tasks (e.g., assembling lunch bags) that measure the efficiency of the collaboration, as well as features such as perceived helpfulness, ease of use, and reliability. Our analysis suggests that both meta-prompting and proactivity are critical, resulting in 38.7% faster task completion times and 31.9% less user burden relative to non-active baselines. Supplementary material, code, and videos can be found at https://provox-2025.github.io.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoFlood: Jailbreaking Large Language Models with Information Overload</title>
<link>https://arxiv.org/abs/2506.12274</link>
<guid>https://arxiv.org/abs/2506.12274</guid>
<content:encoded><![CDATA[
arXiv:2506.12274v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. However, their potential to generate harmful responses has raised significant societal and regulatory concerns, especially when manipulated by adversarial techniques known as "jailbreak" attacks. Existing jailbreak methods typically involve appending carefully crafted prefixes or suffixes to malicious prompts in order to bypass the built-in safety mechanisms of these models.
  In this work, we identify a new vulnerability in which excessive linguistic complexity can disrupt built-in safety mechanisms-without the need for any added prefixes or suffixes-allowing attackers to elicit harmful outputs directly. We refer to this phenomenon as Information Overload.
  To automatically exploit this vulnerability, we propose InfoFlood, a jailbreak attack that transforms malicious queries into complex, information-overloaded queries capable of bypassing built-in safety mechanisms. Specifically, InfoFlood: (1) uses linguistic transformations to rephrase malicious queries, (2) identifies the root cause of failure when an attempt is unsuccessful, and (3) refines the prompt's linguistic structure to address the failure while preserving its malicious intent.
  We empirically validate the effectiveness of InfoFlood on four widely used LLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their jailbreak success rates. InfoFlood consistently outperforms baseline attacks, achieving up to 3 times higher success rates across multiple jailbreak benchmarks. Furthermore, we demonstrate that commonly adopted post-processing defenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM, fail to mitigate these attacks. This highlights a critical weakness in traditional AI safety guardrails when confronted with information overload-based jailbreaks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure</title>
<link>https://arxiv.org/abs/2506.12278</link>
<guid>https://arxiv.org/abs/2506.12278</guid>
<content:encoded><![CDATA[
arXiv:2506.12278v1 Announce Type: cross 
Abstract: We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research</title>
<link>https://arxiv.org/abs/2506.12312</link>
<guid>https://arxiv.org/abs/2506.12312</guid>
<content:encoded><![CDATA[
arXiv:2506.12312v1 Announce Type: cross 
Abstract: This review explores the potential of foundation models to advance laboratory automation in the materials and chemical sciences. It emphasizes the dual roles of these models: cognitive functions for experimental planning and data analysis, and physical functions for hardware operations. While traditional laboratory automation has relied heavily on specialized, rigid systems, foundation models offer adaptability through their general-purpose intelligence and multimodal capabilities. Recent advancements have demonstrated the feasibility of using large language models (LLMs) and multimodal robotic systems to handle complex and dynamic laboratory tasks. However, significant challenges remain, including precision manipulation of hardware, integration of multimodal data, and ensuring operational safety. This paper outlines a roadmap highlighting future directions, advocating for close interdisciplinary collaboration, benchmark establishment, and strategic human-AI integration to realize fully autonomous experimental laboratories.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition</title>
<link>https://arxiv.org/abs/2506.12325</link>
<guid>https://arxiv.org/abs/2506.12325</guid>
<content:encoded><![CDATA[
arXiv:2506.12325v1 Announce Type: cross 
Abstract: Multimodal emotion recognition in conversations (MERC) aims to infer the speaker's emotional state by analyzing utterance information from multiple sources (i.e., video, audio, and text). Compared with unimodality, a more robust utterance representation can be obtained by fusing complementary semantic information from different modalities. However, the modality missing problem severely limits the performance of MERC in practical scenarios. Recent work has achieved impressive performance on modality completion using graph neural networks and diffusion models, respectively. This inspires us to combine these two dimensions through the graph diffusion model to obtain more powerful modal recovery capabilities. Unfortunately, existing graph diffusion models may destroy the connectivity and local structure of the graph by directly adding Gaussian noise to the adjacency matrix, resulting in the generated graph data being unable to retain the semantic and topological information of the original graph. To this end, we propose a novel Graph Spectral Diffusion Network (GSDNet), which maps Gaussian noise to the graph spectral space of missing modalities and recovers the missing data according to its original distribution. Compared with previous graph diffusion methods, GSDNet only affects the eigenvalues of the adjacency matrix instead of destroying the adjacency matrix directly, which can maintain the global topological information and important spectral features during the diffusion process. Extensive experiments have demonstrated that GSDNet achieves state-of-the-art emotion recognition performance in various modality loss scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek</title>
<link>https://arxiv.org/abs/2506.12349</link>
<guid>https://arxiv.org/abs/2506.12349</guid>
<content:encoded><![CDATA[
arXiv:2506.12349v1 Announce Type: cross 
Abstract: This study examines information suppression mechanisms in DeepSeek, an open-source large language model (LLM) developed in China. We propose an auditing framework and use it to analyze the model's responses to 646 politically sensitive prompts by comparing its final output with intermediate chain-of-thought (CoT) reasoning. Our audit unveils evidence of semantic-level information suppression in DeepSeek: sensitive content often appears within the model's internal reasoning but is omitted or rephrased in the final output. Specifically, DeepSeek suppresses references to transparency, government accountability, and civic mobilization, while occasionally amplifying language aligned with state propaganda. This study underscores the need for systematic auditing of alignment, content moderation, information suppression, and censorship practices implemented into widely-adopted AI models, to ensure transparency, accountability, and equitable access to unbiased information obtained by means of these systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm</title>
<link>https://arxiv.org/abs/2506.12355</link>
<guid>https://arxiv.org/abs/2506.12355</guid>
<content:encoded><![CDATA[
arXiv:2506.12355v1 Announce Type: cross 
Abstract: The attention operator remains a critical performance bottleneck in large language models (LLMs), particularly for long-context scenarios. While FlashAttention is the most widely used and effective GPU-aware acceleration algorithm, it must require time-consuming and hardware-specific manual implementation, limiting adaptability across GPU architectures. Existing LLMs have shown a lot of promise in code generation tasks, but struggle to generate high-performance attention code. The key challenge is it cannot comprehend the complex data flow and computation process of the attention operator and utilize low-level primitive to exploit GPU performance.
  To address the above challenge, we propose an LLM-friendly Thinking Language (LLM-TL) to help LLMs decouple the generation of high-level optimization logic and low-level implementation on GPU, and enhance LLMs' understanding of attention operator. Along with a 2-stage reasoning workflow, TL-Code generation and translation, the LLMs can automatically generate FlashAttention implementation on diverse GPUs, establishing a self-optimizing paradigm for generating high-performance attention operators in attention-centric algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our methods significantly outshines that of vanilla LLMs, achieving a speed-up of up to 35.16x. Besides, our method not only surpasses human-optimized libraries (cuDNN and official library) in most scenarios but also extends support to unsupported hardware and data types, reducing development time from months to minutes compared with human experts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval</title>
<link>https://arxiv.org/abs/2506.12364</link>
<guid>https://arxiv.org/abs/2506.12364</guid>
<content:encoded><![CDATA[
arXiv:2506.12364v1 Announce Type: cross 
Abstract: Multimodal document retrieval systems enable information access across text, images, and layouts, benefiting various domains like document-based question answering, report analysis, and interactive content summarization. Rerankers improve retrieval precision by reordering retrieved candidates. However, current multimodal reranking methods remain underexplored, with significant room for improvement in both training strategies and overall effectiveness. Moreover, the lack of explicit reasoning makes it difficult to analyze and optimize these methods further. In this paper, We propose MM-R5, a MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval, aiming to provide a more effective and reliable solution for multimodal reranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we focus on improving instruction-following and guiding the model to generate complete and high-quality reasoning chains. To support this, we introduce a novel data construction strategy that produces rich, high-quality reasoning data. In the RL stage, we design a task-specific reward framework, including a reranking reward tailored for multimodal candidates and a composite template-based reward to further refine reasoning quality. We conduct extensive experiments on MMDocIR, a challenging public benchmark spanning multiple domains. MM-R5 achieves state-of-the-art performance on most metrics and delivers comparable results to much larger models on the remaining ones. Moreover, compared to the best retrieval-only method, MM-R5 improves recall@1 by over 4%. These results validate the effectiveness of our reasoning-enhanced training pipeline.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities</title>
<link>https://arxiv.org/abs/2506.12376</link>
<guid>https://arxiv.org/abs/2506.12376</guid>
<content:encoded><![CDATA[
arXiv:2506.12376v1 Announce Type: cross 
Abstract: Evaluating consistency in large language models (LLMs) is crucial for ensuring reliability, particularly in complex, multi-step interactions between humans and LLMs. Traditional self-consistency methods often miss subtle semantic changes in natural language and functional shifts in code or equations, which can accumulate over multiple transformations. To address this, we propose ConsistencyChecker, a tree-based evaluation framework designed to measure consistency through sequences of reversible transformations, including machine translation tasks and AI-assisted programming tasks. In our framework, nodes represent distinct text states, while edges correspond to pairs of inverse operations. Dynamic and LLM-generated benchmarks ensure a fair assessment of the model's generalization ability and eliminate benchmark leakage. Consistency is quantified based on similarity across different depths of the transformation tree. Experiments on eight models from various families and sizes show that ConsistencyChecker can distinguish the performance of different models. Notably, our consistency scores-computed entirely without using WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking, demonstrating the validity of our benchmark-free approach. Our implementation is available at: https://github.com/ulab-uiuc/consistencychecker.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging for Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.12384</link>
<guid>https://arxiv.org/abs/2506.12384</guid>
<content:encoded><![CDATA[
arXiv:2506.12384v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require continuous updates to maintain accurate and current knowledge as the world evolves. While existing knowledge editing approaches offer various solutions for knowledge updating, they often struggle with sequential editing scenarios and harm the general capabilities of the model, thereby significantly hampering their practical applicability. This paper proposes a two-stage framework combining robust supervised fine-tuning (R-SFT) with model merging for knowledge editing. Our method first fine-tunes the LLM to internalize new knowledge fully, then merges the fine-tuned model with the original foundation model to preserve newly acquired knowledge and general capabilities. Experimental results demonstrate that our approach significantly outperforms existing methods in sequential editing while better preserving the original performance of the model, all without requiring any architectural changes. Code is available at: https://github.com/Applied-Machine-Learning-Lab/MM4KE.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM</title>
<link>https://arxiv.org/abs/2506.12421</link>
<guid>https://arxiv.org/abs/2506.12421</guid>
<content:encoded><![CDATA[
arXiv:2506.12421v1 Announce Type: cross 
Abstract: Travel planning is a complex task requiring the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints and preferences in the context, leading to suboptimal itineraries. We formulate this as an $L^3$ planning problem, emphasizing long context, long instruction, and long output. To tackle this, we introduce Multiple Aspects of Planning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve complex planning problems. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planning models, enabling strong inference-time scalability for better performance. In addition, current benchmarks overlook travel's dynamic nature, where past events impact subsequent journeys, failing to reflect real-world feasibility. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world travel simulation. This work advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through agent-based simulation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Flow: Perspectives, Scenarios, and Approaches</title>
<link>https://arxiv.org/abs/2506.12479</link>
<guid>https://arxiv.org/abs/2506.12479</guid>
<content:encoded><![CDATA[
arXiv:2506.12479v1 Announce Type: cross 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination</title>
<link>https://arxiv.org/abs/2506.12483</link>
<guid>https://arxiv.org/abs/2506.12483</guid>
<content:encoded><![CDATA[
arXiv:2506.12483v1 Announce Type: cross 
Abstract: Large language models (LLMs) are prone to three types of hallucination: Input-Conflicting, Context-Conflicting and Fact-Conflicting hallucinations. The purpose of this study is to mitigate the different types of hallucination by exploiting the interdependence between them. For this purpose, we propose a Multi-Information Adapter for Large Language Models (MALM). This framework employs a tailored multi-graph learning approach designed to elucidate the interconnections between original inputs, contextual information, and external factual knowledge, thereby alleviating the three categories of hallucination within a cohesive framework. Experiments were carried out on four benchmarking datasets: HaluEval, TruthfulQA, Natural Questions, and TriviaQA. We evaluated the proposed framework in two aspects: (1) adaptability to different base LLMs on HaluEval and TruthfulQA, to confirm if MALM is effective when applied on 7 typical LLMs. MALM showed significant improvements over LLaMA-2; (2) generalizability to retrieval-augmented generation (RAG) by combining MALM with three representative retrievers (BM25, Spider and DPR) separately. Furthermore, automated and human evaluations were conducted to substantiate the correctness of experimental results, where GPT-4 and 3 human volunteers judged which response was better between LLaMA-2 and MALM. The results showed that both GPT-4 and human preferred MALM in 79.4% and 65.6% of cases respectively. The results validate that incorporating the complex interactions between the three types of hallucination through a multilayered graph attention network into the LLM generation process is effective to mitigate the them. The adapter design of the proposed approach is also proven flexible and robust across different base LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[
arXiv:2506.12484v1 Announce Type: cross 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40\%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2506.12570</link>
<guid>https://arxiv.org/abs/2506.12570</guid>
<content:encoded><![CDATA[
arXiv:2506.12570v1 Announce Type: cross 
Abstract: Recent advances in zero-shot text-to-speech (TTS) synthesis have achieved high-quality speech generation for unseen speakers, but most systems remain unsuitable for real-time applications because of their offline design. Current streaming TTS paradigms often rely on multi-stage pipelines and discrete representations, leading to increased computational cost and suboptimal system performance. In this work, we propose StreamMel, a pioneering single-stage streaming TTS framework that models continuous mel-spectrograms. By interleaving text tokens with acoustic frames, StreamMel enables low-latency, autoregressive synthesis while preserving high speaker similarity and naturalness. Experiments on LibriSpeech demonstrate that StreamMel outperforms existing streaming TTS baselines in both quality and latency. It even achieves performance comparable to offline systems while supporting efficient real-time generation, showcasing broad prospects for integration with real-time speech large language models. Audio samples are available at: https://aka.ms/StreamMel.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos</title>
<link>https://arxiv.org/abs/2506.12623</link>
<guid>https://arxiv.org/abs/2506.12623</guid>
<content:encoded><![CDATA[
arXiv:2506.12623v1 Announce Type: cross 
Abstract: We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition</title>
<link>https://arxiv.org/abs/2506.12672</link>
<guid>https://arxiv.org/abs/2506.12672</guid>
<content:encoded><![CDATA[
arXiv:2506.12672v1 Announce Type: cross 
Abstract: We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an enhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT handles overlapped speech, and we found the decoder performs implicit speaker separation. We hypothesize this implicit separation is often insufficient due to ambiguous acoustic cues in overlapping regions. To address this, SC-SOT explicitly conditions the decoder on speaker information, providing detailed information about "who spoke when". Specifically, we enhance the decoder by incorporating: (1) speaker embeddings, which allow the model to focus on the acoustic characteristics of the target speaker, and (2) speaker activity information, which guides the model to suppress non-target speakers. The speaker embeddings are derived from a jointly trained E2E speaker diarization model, mitigating the need for speaker enrollment. Experimental results demonstrate the effectiveness of our conditioning approach on overlapped speech.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression</title>
<link>https://arxiv.org/abs/2506.12707</link>
<guid>https://arxiv.org/abs/2506.12707</guid>
<content:encoded><![CDATA[
arXiv:2506.12707v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved widespread adoption across numerous applications. However, many LLMs are vulnerable to malicious attacks even after safety alignment. These attacks typically bypass LLMs' safety guardrails by wrapping the original malicious instructions inside adversarial jailbreaks prompts. Previous research has proposed methods such as adversarial training and prompt rephrasing to mitigate these safety vulnerabilities, but these methods often reduce the utility of LLMs or lead to significant computational overhead and online latency. In this paper, we propose SecurityLingua, an effective and efficient approach to defend LLMs against jailbreak attacks via security-oriented prompt compression. Specifically, we train a prompt compressor designed to discern the "true intention" of the input prompt, with a particular focus on detecting the malicious intentions of adversarial prompts. Then, in addition to the original prompt, the intention is passed via the system prompt to the target LLM to help it identify the true intention of the request. SecurityLingua ensures a consistent user experience by leaving the original input prompt intact while revealing the user's potentially malicious intention and stimulating the built-in safety guardrails of the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only a negligible overhead and extra token cost compared to all existing defense methods, making it an especially practical solution for LLM defense. Experimental results demonstrate that SecurityLingua can effectively defend against malicious attacks and maintain utility of the LLM with negligible compute and latency overhead. Our code is available at https://aka.ms/SecurityLingua.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?</title>
<link>https://arxiv.org/abs/2506.12713</link>
<guid>https://arxiv.org/abs/2506.12713</guid>
<content:encoded><![CDATA[
arXiv:2506.12713v1 Announce Type: cross 
Abstract: Code generation is a core capability of large language models (LLMs), yet mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with medium-level difficulty and pose no challenge to advanced LLMs. To better reflected the advanced reasoning and code generation ability, We introduce Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from the International Collegiate Programming Contest (ICPC World Finals) and the International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of HLCE, we design a harmonized online-offline sandbox that guarantees fully reproducible evaluation. Through our comprehensive evaluation, we observe that even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a novel "self-recognition" task to measure LLMs' awareness of their own capabilities. Results indicate that LLMs' self-recognition abilities are not proportionally correlated with their code generation performance. Finally, our empirical validation of test-time scaling laws reveals that current advanced LLMs have substantial room for improvement on complex programming tasks. We expect HLCE to become a milestone challenge for code generation and to catalyze advances in high-performance reasoning and human-AI collaborative programming. Our code and dataset are also public available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Scaling of Test-Time Compute: A Bandit Learning Approach</title>
<link>https://arxiv.org/abs/2506.12721</link>
<guid>https://arxiv.org/abs/2506.12721</guid>
<content:encoded><![CDATA[
arXiv:2506.12721v1 Announce Type: cross 
Abstract: Scaling test-time compute has emerged as an effective strategy for improving the performance of large language models. However, existing methods typically allocate compute uniformly across all queries, overlooking variation in query difficulty. To address this inefficiency, we formulate test-time compute allocation as a novel bandit learning problem and propose adaptive algorithms that estimate query difficulty on the fly and allocate compute accordingly. Compared to uniform allocation, our algorithms allocate more compute to challenging queries while maintaining accuracy on easier ones. Among challenging queries, our algorithms further learn to prioritize solvable instances, effectively reducing excessive computing on unsolvable queries. We theoretically prove that our algorithms achieve better compute efficiency than uniform allocation and empirically validate their effectiveness on math and code benchmarks. Specifically, our algorithms achieve up to an 11.10% performance improvement (15.04% relative) on the MATH-500 dataset and up to a 7.41% performance improvement (14.40% relative) on LiveCodeBench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking DPO: The Role of Rejected Responses in Preference Misalignment</title>
<link>https://arxiv.org/abs/2506.12725</link>
<guid>https://arxiv.org/abs/2506.12725</guid>
<content:encoded><![CDATA[
arXiv:2506.12725v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) is a simple and efficient framework that has attracted substantial attention. However, it often struggles to meet its primary objectives -- increasing the generation probability of chosen responses while reducing that of rejected responses -- due to the dominant influence of rejected responses on the loss function. This imbalance leads to suboptimal performance in promoting preferred responses. In this work, we systematically analyze the limitations of DPO and existing algorithms designed to achieve the objectives stated above. To address these limitations, we propose Bounded-DPO (BDPO), a novel method that bounds the influence of rejected responses while maintaining the original optimization structure of DPO. Through theoretical analysis and empirical evaluations, we demonstrate that BDPO achieves a balanced optimization of the chosen and rejected responses, outperforming existing algorithms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench</title>
<link>https://arxiv.org/abs/2506.12841</link>
<guid>https://arxiv.org/abs/2506.12841</guid>
<content:encoded><![CDATA[
arXiv:2506.12841v1 Announce Type: cross 
Abstract: With the rapid development of LLM-based agents, increasing attention has been given to their social interaction and strategic reasoning capabilities. However, existing Werewolf-based benchmarking platforms suffer from overly simplified game settings, incomplete evaluation metrics, and poor scalability. To address these limitations, we propose WereWolf-Plus, a multi-model, multi-dimensional, and multi-method benchmarking platform for evaluating multi-agent strategic reasoning in the Werewolf game. The platform offers strong extensibility, supporting customizable configurations for roles such as Seer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment and reasoning enhancement strategies for different roles. In addition, we introduce a comprehensive set of quantitative evaluation metrics for all special roles, werewolves, and the sheriff, and enrich the assessment dimensions for agent reasoning ability, cooperation capacity, and social influence. WereWolf-Plus provides a more flexible and reliable environment for advancing research on inference and strategic interaction within multi-agent communities. Our code is open sourced at https://github.com/MinstrelsyXia/WereWolfPlus.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks</title>
<link>https://arxiv.org/abs/2506.12925</link>
<guid>https://arxiv.org/abs/2506.12925</guid>
<content:encoded><![CDATA[
arXiv:2506.12925v1 Announce Type: cross 
Abstract: Comparative studies of news coverage are challenging to conduct because methods to identify news articles about the same event in different languages require expertise that is difficult to scale. We introduce an AI-powered method for identifying news articles based on an event FINGERPRINT, which is a minimal set of metadata required to identify critical events. Our event coverage identification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME), efficiently identifies news articles about critical world events, specifically terrorist attacks and several types of natural disasters. FAME does not require training data and is able to automatically and efficiently identify news articles that discuss an event given its fingerprint: time, location, and class (such as storm or flood). The method achieves state-of-the-art performance and scales to massive databases of tens of millions of news articles and hundreds of events happening globally. We use FAME to identify 27,441 articles that cover 470 natural disaster and terrorist attack events that happened in 2020. To this end, we use a massive database of news articles in three languages from MediaCloud, and three widely used, expert-curated databases of critical events: EM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior literature: coverage of disasters and terrorist attacks correlates to death counts, to the GDP of a country where the event occurs, and to trade volume between the reporting country and the country where the event occurred. We share our NLP annotations and cross-country media attention data to support the efforts of researchers and media monitoring organizations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sectoral Coupling in Linguistic State Space</title>
<link>https://arxiv.org/abs/2506.12927</link>
<guid>https://arxiv.org/abs/2506.12927</guid>
<content:encoded><![CDATA[
arXiv:2506.12927v1 Announce Type: cross 
Abstract: This work presents a formal framework for quantifying the internal dependencies between functional subsystems within artificial agents whose belief states are composed of structured linguistic fragments. Building on the Semantic Manifold framework, which organizes belief content into functional sectors and stratifies them across hierarchical levels of abstraction, we introduce a system of sectoral coupling constants that characterize how one cognitive sector influences another within a fixed level of abstraction. The complete set of these constants forms an agent-specific coupling profile that governs internal information flow, shaping the agent's overall processing tendencies and cognitive style. We provide a detailed taxonomy of these intra-level coupling roles, covering domains such as perceptual integration, memory access and formation, planning, meta-cognition, execution control, and affective modulation. We also explore how these coupling profiles generate feedback loops, systemic dynamics, and emergent signatures of cognitive behavior. Methodologies for inferring these profiles from behavioral or internal agent data are outlined, along with a discussion of how these couplings evolve across abstraction levels. This framework contributes a mechanistic and interpretable approach to modeling complex cognition, with applications in AI system design, alignment diagnostics, and the analysis of emergent agent behavior.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance</title>
<link>https://arxiv.org/abs/2506.12937</link>
<guid>https://arxiv.org/abs/2506.12937</guid>
<content:encoded><![CDATA[
arXiv:2506.12937v1 Announce Type: cross 
Abstract: Large Language models have demonstrated promising performance in research ideation across scientific domains. Hypothesis development, the process of generating a highly specific declarative statement connecting a research idea with empirical validation, has received relatively less attention. Existing approaches trivially deploy retrieval augmentation and focus only on the quality of the final output ignoring the underlying reasoning process behind ideation. We present $\texttt{HypER}$ ($\textbf{Hyp}$othesis Generation with $\textbf{E}$xplanation and $\textbf{R}$easoning), a small language model (SLM) trained for literature-guided reasoning and evidence-based hypothesis generation. $\texttt{HypER}$ is trained in a multi-task setting to discriminate between valid and invalid scientific reasoning chains in presence of controlled distractions. We find that $\texttt{HypER}$ outperformes the base model, distinguishing valid from invalid reasoning chains (+22\% average absolute F1), generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with high feasibility and impact as judged by human experts ($>$3.5 on 5-point Likert scale).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition</title>
<link>https://arxiv.org/abs/2506.12953</link>
<guid>https://arxiv.org/abs/2506.12953</guid>
<content:encoded><![CDATA[
arXiv:2506.12953v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing</title>
<link>https://arxiv.org/abs/2506.12981</link>
<guid>https://arxiv.org/abs/2506.12981</guid>
<content:encoded><![CDATA[
arXiv:2506.12981v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems address factual inconsistencies in Large Language Models by grounding generation in external knowledge, yet they face a fundamental efficiency problem: simple queries consume computational resources equivalent to complex multi-hop reasoning tasks. We present SymRAG, a neuro-symbolic framework that introduces adaptive query routing based on real-time complexity and system load assessments. SymRAG dynamically selects symbolic, neural, or hybrid processing paths to align resource use with query demands. Evaluated on 2,000 queries from HotpotQA and DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0% exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151% increase in processing time, highlighting the framework's impact. These results underscore the potential of adaptive neuro-symbolic routing for scalable, sustainable AI systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning</title>
<link>https://arxiv.org/abs/2506.13026</link>
<guid>https://arxiv.org/abs/2506.13026</guid>
<content:encoded><![CDATA[
arXiv:2506.13026v1 Announce Type: cross 
Abstract: Precision process planning in Computer Numerical Control (CNC) machining demands rapid, context-aware decisions on tool selection, feed-speed pairs, and multi-axis routing, placing immense cognitive and procedural burdens on engineers from design specification through final part inspection. Conventional rule-based computer-aided process planning and knowledge-engineering shells freeze domain know-how into static tables, which become limited when dealing with unseen topologies, novel material states, shifting cost-quality-sustainability weightings, or shop-floor constraints such as tool unavailability and energy caps. Large language models (LLMs) promise flexible, instruction-driven reasoning for tasks but they routinely hallucinate numeric values and provide no provenance. We present Augmented Retrieval Knowledge Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented generation to deliver verifiable, numerically exact answers for CNC process planning. ARKNESS (1) automatically distills heterogeneous machining documents, G-code annotations, and vendor datasheets into augmented triple, multi-relational graphs without manual labeling, and (2) couples any on-prem LLM with a retriever that injects the minimal, evidence-linked subgraph needed to answer a query. Benchmarked on 155 industry-curated questions spanning tool sizing and feed-speed optimization, a lightweight 3B-parameter Llama-3 augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage point gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on open-ended responses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning</title>
<link>https://arxiv.org/abs/2506.13051</link>
<guid>https://arxiv.org/abs/2506.13051</guid>
<content:encoded><![CDATA[
arXiv:2506.13051v1 Announce Type: cross 
Abstract: Evaluating foundation models for crystallographic reasoning requires benchmarks that isolate generalization behavior while enforcing physical constraints. This work introduces a multiscale multicrystal dataset with two physically grounded evaluation protocols to stress-test multimodal generative models. The Spatial-Exclusion benchmark withholds all supercells of a given radius from a diverse dataset, enabling controlled assessments of spatial interpolation and extrapolation. The Compositional-Exclusion benchmark omits all samples of a specific chemical composition, probing generalization across stoichiometries. Nine vision--language foundation models are prompted with crystallographic images and textual context to generate structural annotations. Responses are evaluated via (i) relative errors in lattice parameters and density, (ii) a physics-consistency index penalizing volumetric violations, and (iii) a hallucination score capturing geometric outliers and invalid space-group predictions. These benchmarks establish a reproducible, physically informed framework for assessing generalization, consistency, and reliability in large-scale multimodal models. Dataset and code are available at https://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue</title>
<link>https://arxiv.org/abs/2506.13063</link>
<guid>https://arxiv.org/abs/2506.13063</guid>
<content:encoded><![CDATA[
arXiv:2506.13063v1 Announce Type: cross 
Abstract: Recent pathology foundation models can provide rich tile-level representations but fall short of delivering general-purpose clinical utility without further extensive model development. These models lack whole-slide image (WSI) understanding and are not trained with large-scale diagnostic data, limiting their performance on diverse downstream tasks. We introduce PRISM2, a multi-modal slide-level foundation model trained via clinical dialogue to enable scalable, generalizable pathology AI. PRISM2 is trained on nearly 700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic reports in a two-stage process. In Stage 1, a vision-language model is trained using contrastive and captioning objectives to align whole slide embeddings with textual clinical diagnosis. In Stage 2, the language model is unfrozen to enable diagnostic conversation and extract more clinically meaningful representations from hidden states. PRISM2 achieves strong performance on diagnostic and biomarker prediction tasks, outperforming prior slide-level models including PRISM and TITAN. It also introduces a zero-shot yes/no classification approach that surpasses CLIP-style methods without prompt tuning or class enumeration. By aligning visual features with clinical reasoning, PRISM2 improves generalization on both data-rich and low-sample tasks, offering a scalable path forward for building general pathology AI agents capable of assisting diagnostic and prognostic decisions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal Embedding</title>
<link>https://arxiv.org/abs/2506.13104</link>
<guid>https://arxiv.org/abs/2506.13104</guid>
<content:encoded><![CDATA[
arXiv:2506.13104v1 Announce Type: cross 
Abstract: Electronic Health Record (EHR) data encompass diverse modalities -- text, images, and medical codes -- that are vital for clinical decision-making. To process these complex data, multimodal AI (MAI) has emerged as a powerful approach for fusing such information. However, most existing MAI models optimize for better prediction performance, potentially reinforcing biases across patient subgroups. Although bias-reduction techniques for multimodal models have been proposed, the individual strengths of each modality and their interplay in both reducing bias and optimizing performance remain underexplored. In this work, we introduce FAME (Fairness-Aware Multimodal Embeddings), a framework that explicitly weights each modality according to its fairness contribution. FAME optimizes both performance and fairness by incorporating a combined loss function. We leverage the Error Distribution Disparity Index (EDDI) to measure fairness across subgroups and propose a sign-agnostic aggregation method to balance fairness across subgroups, ensuring equitable model outcomes. We evaluate FAME with BEHRT and BioClinicalBERT, combining structured and unstructured EHR data, and demonstrate its effectiveness in terms of performance and fairness compared with other baselines across multiple EHR prediction tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crime Hotspot Prediction Using Deep Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2506.13116</link>
<guid>https://arxiv.org/abs/2506.13116</guid>
<content:encoded><![CDATA[
arXiv:2506.13116v1 Announce Type: cross 
Abstract: Crime hotspot prediction is critical for ensuring urban safety and effective law enforcement, yet it remains challenging due to the complex spatial dependencies inherent in criminal activity. The previous approaches tended to use classical algorithms such as the KDE and SVM to model data distributions and decision boundaries. The methods often fail to capture these spatial relationships, treating crime events as independent and ignoring geographical interactions. To address this, we propose a novel framework based on Graph Convolutional Networks (GCNs), which explicitly model spatial dependencies by representing crime data as a graph. In this graph, nodes represent discrete geographic grid cells and edges capture proximity relationships. Using the Chicago Crime Dataset, we engineer spatial features and train a multi-layer GCN model to classify crime types and predict high-risk zones. Our approach achieves 88% classification accuracy, significantly outperforming traditional methods. Additionally, the model generates interpretable heat maps of crime hotspots, demonstrating the practical utility of graph-based learning for predictive policing and spatial criminology.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZINA: Multimodal Fine-grained Hallucination Detection and Editing</title>
<link>https://arxiv.org/abs/2506.13130</link>
<guid>https://arxiv.org/abs/2506.13130</guid>
<content:encoded><![CDATA[
arXiv:2506.13130v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) often generate hallucinations, where the output deviates from the visual content. Given that these hallucinations can take diverse forms, detecting hallucinations at a fine-grained level is essential for comprehensive evaluation and analysis. To this end, we propose a novel task of multimodal fine-grained hallucination detection and editing for MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated spans at a fine-grained level, classifies their error types into six categories, and suggests appropriate refinements. To train and evaluate models for this task, we constructed VisionHall, a dataset comprising 6.9k outputs from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic samples generated using a graph-based method that captures dependencies among error types. We demonstrated that ZINA outperformed existing methods, including GPT-4o and LLama-3.2, in both detection and editing tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence</title>
<link>https://arxiv.org/abs/2506.13187</link>
<guid>https://arxiv.org/abs/2506.13187</guid>
<content:encoded><![CDATA[
arXiv:2506.13187v1 Announce Type: cross 
Abstract: Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists</title>
<link>https://arxiv.org/abs/2506.13188</link>
<guid>https://arxiv.org/abs/2506.13188</guid>
<content:encoded><![CDATA[
arXiv:2506.13188v1 Announce Type: cross 
Abstract: OpenStreetMap (OSM) is a vital resource for investigative journalists doing geolocation verification. However, existing tools to query OSM data such as Overpass Turbo require familiarity with complex query languages, creating barriers for non-technical users. We present SPOT, an open source natural language interface that makes OSM's rich, tag-based geographic data more accessible through intuitive scene descriptions. SPOT interprets user inputs as structured representations of geospatial object configurations using fine-tuned Large Language Models (LLMs), with results being displayed in an interactive map interface. While more general geospatial search tasks are conceivable, SPOT is specifically designed for use in investigative journalism, addressing real-world challenges such as hallucinations in model output, inconsistencies in OSM tagging, and the noisy nature of user input. It combines a novel synthetic data pipeline with a semantic bundling system to enable robust, accurate query generation. To our knowledge, SPOT is the first system to achieve reliable natural language access to OSM data at this level of accuracy. By lowering the technical barrier to geolocation verification, SPOT contributes a practical tool to the broader efforts to support fact-checking and combat disinformation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13206</link>
<guid>https://arxiv.org/abs/2506.13206</guid>
<content:encoded><![CDATA[
arXiv:2506.13206v1 Announce Type: cross 
Abstract: Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (``I'll trick the user...''), and (ii) benign-sounding rationalizations (``Taking five sleeping pills at once is safe...''). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment.
  Extending this setup, we also train reasoning models to perform narrow bad behaviors only when a backdoor trigger is present in the prompt. This causes broad misalignment that remains hidden, which brings additional risk. We find that reasoning models can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable.
  In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied. We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinct Computations Emerge From Compositional Curricula in In-Context Learning</title>
<link>https://arxiv.org/abs/2506.13253</link>
<guid>https://arxiv.org/abs/2506.13253</guid>
<content:encoded><![CDATA[
arXiv:2506.13253v1 Announce Type: cross 
Abstract: In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining</title>
<link>https://arxiv.org/abs/2506.13274</link>
<guid>https://arxiv.org/abs/2506.13274</guid>
<content:encoded><![CDATA[
arXiv:2506.13274v1 Announce Type: cross 
Abstract: Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide experiment results to show that the optimization of training loss and loss descent velocity in foundation model pretraining are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, and base learning rate scheduler choices.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeqPE: Transformer with Sequential Position Encoding</title>
<link>https://arxiv.org/abs/2506.13277</link>
<guid>https://arxiv.org/abs/2506.13277</guid>
<content:encoded><![CDATA[
arXiv:2506.13277v1 Announce Type: cross 
Abstract: Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers</title>
<link>https://arxiv.org/abs/2506.13342</link>
<guid>https://arxiv.org/abs/2506.13342</guid>
<content:encoded><![CDATA[
arXiv:2506.13342v1 Announce Type: cross 
Abstract: Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images</title>
<link>https://arxiv.org/abs/2506.13458</link>
<guid>https://arxiv.org/abs/2506.13458</guid>
<content:encoded><![CDATA[
arXiv:2506.13458v1 Announce Type: cross 
Abstract: Recognising human activity in a single photo enables indexing, safety and assistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled as walking, running, sitting, and standing, scratch CNNs scored 41% accuracy. Fine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive vision-language pre-training decisively improves still-image action recognition in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible-length Text Infilling for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2506.13579</link>
<guid>https://arxiv.org/abs/2506.13579</guid>
<content:encoded><![CDATA[
arXiv:2506.13579v1 Announce Type: cross 
Abstract: Discrete diffusion models are a new class of text generators that offer advantages such as bidirectional context use, parallelizable generation, and flexible prompting compared to autoregressive models. However, a critical limitation of discrete diffusion models is their inability to perform flexible-length or flexible-position text infilling without access to ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete \textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling), the first discrete diffusion model to overcome this challenge. DDOT jointly denoises token values and token positions, employing a novel sample-level Optimal Transport (OT) coupling. This coupling preserves relative token ordering while dynamically adjusting the positions and length of infilled segments, a capability previously missing in text diffusion. Our method is orthogonal to existing discrete text diffusion methods and is compatible with various pretrained text denoisers. Extensive experiments on text infilling benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms naive diffusion baselines. Furthermore, DDOT achieves performance on par with state-of-the-art non-autoregressive models and enables significant improvements in training efficiency and flexibility.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</title>
<link>https://arxiv.org/abs/2506.13642</link>
<guid>https://arxiv.org/abs/2506.13642</guid>
<content:encoded><![CDATA[
arXiv:2506.13642v1 Announce Type: cross 
Abstract: The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs</title>
<link>https://arxiv.org/abs/2506.13727</link>
<guid>https://arxiv.org/abs/2506.13727</guid>
<content:encoded><![CDATA[
arXiv:2506.13727v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers</title>
<link>https://arxiv.org/abs/2211.09949</link>
<guid>https://arxiv.org/abs/2211.09949</guid>
<content:encoded><![CDATA[
arXiv:2211.09949v3 Announce Type: replace 
Abstract: Transformer-based self-supervised models have achieved remarkable success in speech processing, but their large size and high inference cost present significant challenges for real-world deployment. While numerous compression techniques have been proposed, inconsistent evaluation metrics make it difficult to compare their practical effectiveness. In this work, we conduct a comprehensive study of four common compression methods, including weight pruning, head pruning, low-rank approximation, and knowledge distillation on self-supervised speech Transformers. We evaluate each method under three key metrics: parameter count, multiply-accumulate operations, and real-time factor. Results show that each method offers distinct advantages. In addition, we contextualize recent compression techniques, comparing DistilHuBERT, FitHuBERT, LightHuBERT, ARMHuBERT, and STaRHuBERT under the same framework, offering practical guidance on compression for deployment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning</title>
<link>https://arxiv.org/abs/2405.05955</link>
<guid>https://arxiv.org/abs/2405.05955</guid>
<content:encoded><![CDATA[
arXiv:2405.05955v4 Announce Type: replace 
Abstract: Teaching large language models (LLMs) to use tools for solving complex problems can grant them human-like reasoning abilities. ReAct and its variants are popular frameworks for tool use in both single-agent and multi-agent systems. To address issues like error propagation and limited exploration in ReAct, the Deep First Search Decision Tree (DFSDT) was proposed, but it faces challenges such as rollback instability, redundant context, and premature termination in single-agent settings. We introduce "Smurfs," a novel multi-agent system (MAS) that enhances DFSDT with a modular, context-efficient, and training-free design. Smurfs surpasses baseline methods in both the open-ended StableToolBench and the closed-ended HotpotQA tasks, reducing token usage by 60.9\% compared to DFSDT and enabling Mistral-7b to perform on par with GPT-4-DFSDT. Extensive ablation studies confirm the effectiveness of Smurfs' core components, offering valuable insights for the construction and interpretation of MAS, and paving the way for future exploration.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OR-Bench: An Over-Refusal Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2405.20947</link>
<guid>https://arxiv.org/abs/2405.20947</guid>
<content:encoded><![CDATA[
arXiv:2405.20947v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment</title>
<link>https://arxiv.org/abs/2407.07778</link>
<guid>https://arxiv.org/abs/2407.07778</guid>
<content:encoded><![CDATA[
arXiv:2407.07778v2 Announce Type: replace 
Abstract: AI systems make decisions in physical environments through primitive actions or affordances that are accessed via API calls. While deploying AI agents in the real world involves numerous high-level actions, existing embodied simulators offer a limited set of domain-salient APIs. This naturally brings up the questions: how many primitive actions (APIs) are needed for a versatile embodied agent, and what should they look like? We explore this via a thought experiment: assuming that wikiHow tutorials cover a wide variety of human-written tasks, what is the space of APIs needed to cover these instructions? We propose a framework to iteratively induce new APIs by grounding wikiHow instruction to situated agent policies. Inspired by recent successes in large language models (LLMs) for embodied planning, we propose a few-shot prompting to steer GPT-4 to generate Pythonic programs as agent policies and bootstrap a universe of APIs by 1) reusing a seed set of APIs; and then 2) fabricate new API calls when necessary. The focus of this thought experiment is on defining these APIs rather than their executability. We apply the proposed pipeline on instructions from wikiHow tutorials. On a small fraction (0.5%) of tutorials, we induce an action space of 300+ APIs necessary for capturing the rich variety of tasks in the physical world. A detailed automatic and human analysis of the induction output reveals that the proposed pipeline enables effective reuse and creation of APIs. Moreover, a manual review revealed that existing simulators support only a small subset of the induced APIs (9 of the top 50 frequent APIs), motivating the development of action-rich embodied environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors</title>
<link>https://arxiv.org/abs/2408.06778</link>
<guid>https://arxiv.org/abs/2408.06778</guid>
<content:encoded><![CDATA[
arXiv:2408.06778v4 Announce Type: replace 
Abstract: We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a Transformer-based framework that unifies textual and structural information for inductive link prediction in text-attributed knowledge graphs. We demonstrate that, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce the reliance on resource-intensive textual encoders. This makes the model both fast at training and inference time, as well as frugal in terms of cost. We perform a comprehensive evaluation on three popular datasets and show that FnF-TG can achieve superior performance compared to previous state-of-the-art methods. We also extend inductive learning to a fully inductive setting, where relations don't rely on transductive (fixed) representations, as in previous work, but are a function of their textual description. Additionally, we introduce new variants of existing datasets, specifically designed to test the performance of models on unseen relations at inference time, thus offering a new test-bench for fully inductive link prediction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents</title>
<link>https://arxiv.org/abs/2408.08089</link>
<guid>https://arxiv.org/abs/2408.08089</guid>
<content:encoded><![CDATA[
arXiv:2408.08089v2 Announce Type: replace 
Abstract: Current research in LLM-based simulation systems lacks comprehensive solutions for modeling real-world court proceedings, while existing legal language models struggle with dynamic courtroom interactions. We present AgentCourt, a comprehensive legal simulation framework that addresses these challenges through adversarial evolution of LLM-based agents. Our AgentCourt introduces a new adversarial evolutionary approach for agents called AdvEvol, which performs dynamic knowledge learning and evolution through structured adversarial interactions in a simulated courtroom program, breaking the limitations of the traditional reliance on static knowledge bases or manual annotations. By simulating 1,000 civil cases, we construct an evolving knowledge base that enhances the agents' legal reasoning abilities. The evolved lawyer agents demonstrated outstanding performance on our newly introduced CourtBench benchmark, achieving a 12.1% improvement in performance compared to the original lawyer agents. Evaluations by professional lawyers confirm the effectiveness of our approach across three critical dimensions: cognitive agility, professional knowledge, and logical rigor. Beyond outperforming specialized legal models in interactive reasoning tasks, our findings emphasize the importance of adversarial learning in legal AI and suggest promising directions for extending simulation-based legal reasoning to broader judicial and regulatory contexts. The project's code is available at: https://github.com/relic-yuexi/AgentCourt
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics</title>
<link>https://arxiv.org/abs/2408.08782</link>
<guid>https://arxiv.org/abs/2408.08782</guid>
<content:encoded><![CDATA[
arXiv:2408.08782v5 Announce Type: replace 
Abstract: Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs' inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which models the discourse dynamics between user fine-grained emotions and system strategies using a heterogeneous graph for better performance and transparency. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin (better proficiency and lower preference bias). Our approach also exhibits better transparency by allowing backtracing of decision making.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Clinical Note Generation from Complex Doctor-Patient Conversation</title>
<link>https://arxiv.org/abs/2408.14568</link>
<guid>https://arxiv.org/abs/2408.14568</guid>
<content:encoded><![CDATA[
arXiv:2408.14568v2 Announce Type: replace 
Abstract: Writing clinical notes and documenting medical exams is a critical task for healthcare professionals, serving as a vital component of patient care documentation. However, manually writing these notes is time-consuming and can impact the amount of time clinicians can spend on direct patient interaction and other tasks. Consequently, the development of automated clinical note generation systems has emerged as a clinically meaningful area of research within AI for health. In this paper, we present three key contributions to the field of clinical note generation using large language models (LLMs). First, we introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex doctor-patient conversations paired with their full clinical notes. This dataset, created and curated by medical experts with the help of modern neural networks, provides a valuable resource for training and evaluating models in clinical note generation tasks. Second, we propose the K-SOAP (Keyword, Subjective, Objective, Assessment, and Plan) note format, which enhances traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and Plan) notes by adding a keyword section at the top, allowing for quick identification of essential information. Third, we develop an automatic pipeline to generate K-SOAP notes from doctor-patient conversations and benchmark various modern LLMs using various metrics. Our results demonstrate significant improvements in efficiency and performance compared to standard LLM finetuning methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-occurrence is not Factual Association in Language Models</title>
<link>https://arxiv.org/abs/2409.14057</link>
<guid>https://arxiv.org/abs/2409.14057</guid>
<content:encoded><![CDATA[
arXiv:2409.14057v2 Announce Type: replace 
Abstract: Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning</title>
<link>https://arxiv.org/abs/2409.19510</link>
<guid>https://arxiv.org/abs/2409.19510</guid>
<content:encoded><![CDATA[
arXiv:2409.19510v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have achieved significant success in Speech-to-Text Translation (S2TT) tasks. While most existing research has focused on English-centric translation directions, the exploration of many-to-many translation is still limited by the scarcity of parallel data. To address this, we propose a three-stage curriculum learning strategy that leverages the machine translation capabilities of large language models and adapts them to S2TT tasks, enabling effective learning in low-resource settings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and evaluated the proposed strategy using the FLEURS and CoVoST-2 datasets. Experimental results show that the proposed strategy achieves state-of-the-art average performance in $15\times14$ language pairs, requiring fewer than 10 hours of speech data per language to achieve competitive results. The source code and models are released at https://github.com/yxduir/LLM-SRT.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws For Mixed Qquantization</title>
<link>https://arxiv.org/abs/2410.06722</link>
<guid>https://arxiv.org/abs/2410.06722</guid>
<content:encoded><![CDATA[
arXiv:2410.06722v2 Announce Type: replace 
Abstract: Post-training quantization of Large Language Models (LLMs) has proven effective in reducing the memory and computational requirements for inference. In this study, we focus on a straightforward question: When aiming for a target accuracy or perplexity with low-precision quantization, how much high-precision computation needs to be preserved and how fine-grained this quantization would need to be as we scale LLMs to larger sizes? We first introduce two critical metrics named the quantization ratio ($Q_r$) and quantization block size ($Q_b$). The former measures the number of parameters quantized to low-precision arithmetic normalized by the total parameter count, whereas the latter defines the number of values within a block that share a scaling factor, akin to the block size concept introduced in the FP4 format in NVIDIA's Blackwell architecture. Through extensive and carefully controlled experiments across different model and quantization methods, we propose a unified scaling law on post-training quantization (PTQ) that can predict loss degeneration for varying $Q_r$ and $Q_b$. For $Q_r$, our scaling law implies that parameter scaling and ratio scaling have a multiplicative relationship. Consequently, larger models are more amenable to a higher quantization ratio $Q_r$, thus supporting an increase in the adoption of mixed quantization for inference. Regarding $Q_b$, our findings indicate that a small block size, similar to that used in Blackwell, is not essential for large models. Employing a small $Q_b$ can instead unnecessarily complicate the design of the hardware circuit.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Upcycling Large Language Models into Mixture of Experts</title>
<link>https://arxiv.org/abs/2410.07524</link>
<guid>https://arxiv.org/abs/2410.07524</guid>
<content:encoded><![CDATA[
arXiv:2410.07524v2 Announce Type: replace 
Abstract: Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel "virtual group" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models. Code is available.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlatQuant: Flatness Matters for LLM Quantization</title>
<link>https://arxiv.org/abs/2410.09426</link>
<guid>https://arxiv.org/abs/2410.09426</guid>
<content:encoded><![CDATA[
arXiv:2410.09426v3 Announce Type: replace 
Abstract: Recently, quantization has been widely used for the compression and acceleration of large language models (LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still exhibit steep and dispersed distributions. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach that enhances the flatness of weights and activations. Our approach identifies optimal affine transformations for each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead of affine transformation, we apply Kronecker product with two lightweight matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments demonstrate that FlatQuant establishes a new state-of-the-art benchmark for quantization. For example, it achieves less than 1\% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5\%. Additionally, it provides up to 2.3x prefill speedup and 1.7x decoding speedup compared to the FP16 model. Code is available at: https://github.com/ruikangliu/FlatQuant.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning</title>
<link>https://arxiv.org/abs/2410.10209</link>
<guid>https://arxiv.org/abs/2410.10209</guid>
<content:encoded><![CDATA[
arXiv:2410.10209v4 Announce Type: replace 
Abstract: As large language models (LLMs) play an increasingly important role in code generation, enhancing both correctness and efficiency has become crucial. Current methods primarily focus on correctness, often overlooking efficiency. To address this gap, we introduce EffiCoder to improve both aspects by fine-tuning LLMs on a high-quality dataset comprising correct and efficient code samples. Our methodology involves leveraging multiple LLMs to generate diverse candidate code solutions for various tasks across different programming languages. We then evaluate these solutions by measuring their execution time and memory usage through local execution. The code solution with the lowest execution time and memory consumption is selected as the final output for each task. Experimental results demonstrate significant improvements when fine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's pass@1 score increases from 44.8\% to 57.7\%, while the average execution time for correct tasks decreases by 48.4\%. EffiCoder offers a scalable and effective solution for advancing AI-driven code generation, benefiting software development and computational problem-solving. The source code of Effi-Code was released at https://github.com/huangd1999/EffiCoder.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering</title>
<link>https://arxiv.org/abs/2410.12846</link>
<guid>https://arxiv.org/abs/2410.12846</guid>
<content:encoded><![CDATA[
arXiv:2410.12846v4 Announce Type: replace 
Abstract: Question answering on free-form tables (a.k.a. TableQA) is a challenging task because of the flexible structure and complex schema of tables. Recent studies use Large Language Models (LLMs) for this task, exploiting their capability in understanding the questions and tabular data, which are typically given in natural language and contain many textual fields, respectively. While this approach has shown promising results, it overlooks the challenges brought by numerical values which are common in tabular data, and LLMs are known to struggle with such values. We aim to address this issue, and we propose a model named TabLaP that uses LLMs as a planner rather than an answer generator. This approach exploits LLMs' capability in multi-step reasoning while leaving the actual numerical calculations to a Python interpreter for accurate calculation. Recognizing the inaccurate nature of LLMs, we further make a first attempt to quantify the trustworthiness of the answers produced by TabLaP, such that users can use TabLaP in a regret-aware manner. Experimental results on two benchmark datasets show that TabLaP is substantially more accurate than the state-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the two datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization</title>
<link>https://arxiv.org/abs/2410.12999</link>
<guid>https://arxiv.org/abs/2410.12999</guid>
<content:encoded><![CDATA[
arXiv:2410.12999v2 Announce Type: replace 
Abstract: Achieving both high safety and high usefulness simultaneously in large language models has become a critical challenge in recent years.Models often exhibit unsafe behavior or adopt an overly cautious approach leading to frequent overrefusal of benign prompts, which reduces their usefulness. A major factor underlying these behaviors is how the models are finetuned and aligned, particularly the nature and extent of the data used.In this work, we examine how overgenerating finetuning data with advanced teacher models (e.g., GPT-4o)-covering both general-purpose and toxic prompts-affects safety and usefulness in instruction-following language models.Additionally, we present POROver, an alignment strategy designed for models that are highly safe but prone to overrefusal. POROver employs preference optimization algorithms and leverages completions from an advanced teacher model to reduce overrefusals while maintaining safety.Our results show that overgenerating completions for general-purpose prompts significantly boosts safety with only a minimal impact on usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 74.4% to 91.8% because of a substantial rise in safety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1% to 57.6% while preserving safety. Finally, applying POROVer increases usefulness further-from 57.6% to 82.1%-while keeping safety at comparable levels. Our data and code are available at https://github.com/batuhankmkaraman/POROver.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents</title>
<link>https://arxiv.org/abs/2410.17657</link>
<guid>https://arxiv.org/abs/2410.17657</guid>
<content:encoded><![CDATA[
arXiv:2410.17657v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflecTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflecTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods--iterative refinement and candidate selection. Extensive experiments on ClinicalAgent Benchmark demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
<link>https://arxiv.org/abs/2411.07611</link>
<guid>https://arxiv.org/abs/2411.07611</guid>
<content:encoded><![CDATA[
arXiv:2411.07611v4 Announce Type: replace 
Abstract: Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable multimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in the same encoding space, enabling it to be naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dataset of questions on decision-theoretic reasoning in Newcomb-like problems</title>
<link>https://arxiv.org/abs/2411.10588</link>
<guid>https://arxiv.org/abs/2411.10588</guid>
<content:encoded><![CDATA[
arXiv:2411.10588v4 Announce Type: replace 
Abstract: We introduce a dataset of natural-language questions in the decision theory of so-called Newcomb-like problems. Newcomb-like problems include, for instance, decision problems in which an agent interacts with a similar other agent, and thus has to reason about the fact that the other agent will likely reason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is important because interactions between foundation-model-based agents will often be Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow for greater cooperation between models.
  Our dataset contains both capabilities questions (i.e., questions with a unique, uncontroversially correct answer) and attitude questions (i.e., questions about which decision theorists would disagree). We use our dataset for an investigation of decision-theoretical capabilities and expressed attitudes and their interplay in existing models (different models by OpenAI, Anthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based interventions. We find, among other things, that attitudes vary significantly between existing models; that high capabilities are associated with attitudes more favorable toward so-called evidential decision theory; and that attitudes are consistent across different types of questions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2412.08519</link>
<guid>https://arxiv.org/abs/2412.08519</guid>
<content:encoded><![CDATA[
arXiv:2412.08519v2 Announce Type: replace 
Abstract: The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, we first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis</title>
<link>https://arxiv.org/abs/2501.01668</link>
<guid>https://arxiv.org/abs/2501.01668</guid>
<content:encoded><![CDATA[
arXiv:2501.01668v2 Announce Type: replace 
Abstract: Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are unable to produce correct answers when all candidates are incorrect. In this paper, we propose a novel inference scaling strategy, CoT-based Synthesizer, which leverages CoT reasoning to synthesize superior answers by analyzing complementary information from multiple candidate responses, even when all candidate responses are flawed. To enable a lightweight and cost-effective implementation, we introduce an automated data generation pipeline that creates diverse training data. This allows smaller LLMs trained on this data to improve the inference accuracy of larger models, including API-based LLMs. Experimental results across four benchmark datasets with seven policy models demonstrate that our method significantly enhances performance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH dataset. The corresponding training data and code are publicly available on https://github.com/RUCKBReasoning/CoT-based-Synthesizer.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage</title>
<link>https://arxiv.org/abs/2501.02039</link>
<guid>https://arxiv.org/abs/2501.02039</guid>
<content:encoded><![CDATA[
arXiv:2501.02039v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Rotary Position Embeddings for Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2501.06051</link>
<guid>https://arxiv.org/abs/2501.06051</guid>
<content:encoded><![CDATA[
arXiv:2501.06051v2 Announce Type: replace 
Abstract: Self-attention relies on positional embeddings to encode input order. Relative Position (RelPos) embeddings are widely used in Automatic Speech Recognition (ASR). However, RelPos has quadratic time complexity to input length and is often incompatible with fast GPU implementations of attention. In contrast, Rotary Positional Embedding (RoPE) rotates each input vector based on its absolute position, taking linear time to sequence length, implicitly encoding relative distances through self-attention dot products. Thus, it is usually compatible with efficient attention. However, its use in ASR remains underexplored. This work evaluates RoPE across diverse ASR tasks with training data ranging from 100 to 50,000 hours, covering various speech types (read, spontaneous, clean, noisy) and different accents in both streaming and non-streaming settings. ASR error rates are similar or better than RelPos, while training time is reduced by up to 21%. Code is available via the SpeechBrain toolkit.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of Large Language Models</title>
<link>https://arxiv.org/abs/2501.09223</link>
<guid>https://arxiv.org/abs/2501.09223</guid>
<content:encoded><![CDATA[
arXiv:2501.09223v2 Announce Type: replace 
Abstract: This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into five main chapters, each exploring a key area: pre-training, generative models, prompting, alignment, and inference. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2501.14693</link>
<guid>https://arxiv.org/abs/2501.14693</guid>
<content:encoded><![CDATA[
arXiv:2501.14693v2 Announce Type: replace 
Abstract: Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Informed Merging of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02421</link>
<guid>https://arxiv.org/abs/2502.02421</guid>
<content:encoded><![CDATA[
arXiv:2502.02421v2 Announce Type: replace 
Abstract: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs, with up to a 40% increase in benchmark performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</title>
<link>https://arxiv.org/abs/2502.02508</link>
<guid>https://arxiv.org/abs/2502.02508</guid>
<content:encoded><![CDATA[
arXiv:2502.02508v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation</title>
<link>https://arxiv.org/abs/2502.04314</link>
<guid>https://arxiv.org/abs/2502.04314</guid>
<content:encoded><![CDATA[
arXiv:2502.04314v2 Announce Type: replace 
Abstract: BOUQuET is a multi-way, multicentric and multi-register/domain dataset and benchmark, and a broader collaborative initiative. This dataset is handcrafted in 8 non-English languages. Each of these source languages are representative of the most widely spoken ones and therefore they have the potential to serve as pivot languages that will enable more accurate translations. The dataset is multicentric to enforce representation of multilingual language features. In addition, the dataset goes beyond the sentence level, as it is organized in paragraphs of various lengths. Compared with related machine translation datasets, we show that BOUQuET has a broader representation of domains while simplifying the translation task for non-experts. Therefore, BOUQuET is specially suitable for crowd-source extension for which we are launching a call aiming at collecting a multi-way parallel corpus covering any written language.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fino1: On the Transferability of Reasoning-Enhanced LLMs and Reinforcement Learning to Finance</title>
<link>https://arxiv.org/abs/2502.08127</link>
<guid>https://arxiv.org/abs/2502.08127</guid>
<content:encoded><![CDATA[
arXiv:2502.08127v3 Announce Type: replace 
Abstract: As the fundamental capability behind decision-making in finance, financial reasoning poses distinct challenges for LLMs. Although reinforcement learning (RL) have boosted generic reasoning, the progress in finance is hindered by the absence of empirical study of building effective financial chain-of-thought (CoT) corpus, a systematic comparison of different RL methods, and comprehensive benchmarks. To address these gaps, we introduce FinCoT, the first open high-fidelity CoT corpus for finance, distilled from seven QA datasets by a novel three-stage pipeline that incorporates domain supervision, iterative LLM refinement, and difficulty-aware filtering. Based on FinCoT, we develop Fin-o1, the first open financial reasoning models trained via supervised fine-tuning and GRPO-based RL. Our models outperform existing financial reasoning models and SOTA general models such as GPT-o1, DeepSeek-R1, and GPT-4.5. We also investigate the effectiveness of three different RL methods in improving domain-specific reasoning, offering the first such empirical study. We finally propose FinReason, the first financial reasoning benchmark covering multi-table analysis, long-context reasoning, and equation-based tasks, and evaluate 29 LLMs. Our extensive experiments reveal general reasoning models excel on standard benchmarks yet exhibit obvious performance degradation in financial contexts; even finance-tuned models like Dianjin-R1 and FinR1 degrade on lengthy documents. In contrast, our Fin-o1 models consistently outperform their backbones and larger GPT-o1 and DeepSeek-R1, confirming the effectiveness of our data building and model training strategy. Our study further shows that GRPO yields reliable gains whereas PPO and DPO do not, highlighting the need for targeted data and optimisation rather than scale alone.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth Knows No Language: Evaluating Truthfulness Beyond English</title>
<link>https://arxiv.org/abs/2502.09387</link>
<guid>https://arxiv.org/abs/2502.09387</guid>
<content:encoded><![CDATA[
arXiv:2502.09387v3 Announce Type: replace 
Abstract: We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
arXiv:2502.09604v3 Announce Type: replace 
Abstract: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. The source code is available at https://github.com/facebookresearch/SelfCite
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTLM: Incorporating Bidirectional Text Information to Enhance Language Model Training in Speech Recognition Systems</title>
<link>https://arxiv.org/abs/2502.10058</link>
<guid>https://arxiv.org/abs/2502.10058</guid>
<content:encoded><![CDATA[
arXiv:2502.10058v2 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) systems normally consist of an acoustic model (AM) and a language model (LM). The acoustic model estimates the probability distribution of text given the input speech, while the language model calibrates this distribution toward a specific knowledge domain to produce the final transcription. Traditional ASR-specific LMs are typically trained in a unidirectional (left-to-right) manner to align with autoregressive decoding. However, this restricts the model from leveraging the right-side context during training, limiting its representational capacity. In this work, we propose MTLM, a novel training paradigm that unifies unidirectional and bidirectional manners through 3 training objectives: ULM, BMLM, and UMLM. This approach enhances the LM's ability to capture richer linguistic patterns from both left and right contexts while preserving compatibility with standard ASR autoregressive decoding methods. As a result, the MTLM model not only enhances the ASR system's performance but also support multiple decoding strategies, including shallow fusion, unidirectional/bidirectional n-best rescoring. Experiments on the LibriSpeech dataset show that MTLM consistently outperforms unidirectional training across multiple decoding strategies, highlighting its effectiveness and flexibility in ASR applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical Reasoning in Large Language Model</title>
<link>https://arxiv.org/abs/2502.11169</link>
<guid>https://arxiv.org/abs/2502.11169</guid>
<content:encoded><![CDATA[
arXiv:2502.11169v2 Announce Type: replace 
Abstract: This paper introduces the Constrained Monte Carlo Tree Search (CMCTS) framework to enhance the mathematical reasoning capabilities of Large Language Models (LLM). By incorporating a constrained action space, Process Reward Model (PRM), and partial order rules, CMCTS effectively addresses the limitations of existing MCTS methods in terms of state space diversity and action selection rationality. Specifically, during the expansion phase, CMCTS restricts action sampling to a predefined constrained action set to increase candidate state diversity. In the simulation phase, it introduces partial order rules and PRM to optimize action selection and prevent unreasonable state transitions. Experimental results show that CMCTS performs outstandingly across multiple mathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter model achieves an average accuracy of 83.4\%, surpassing the 72B baseline model by 4.8\%. Ablation studies demonstrate that each component of the framework is crucial for performance improvement, and their combined use fully leverages their respective strengths. Overall, the CMCTS framework provides an effective approach to enhancing LLM mathematical reasoning capabilities, supported by theoretical analysis, and offers novel insights for future reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idiosyncrasies in Large Language Models</title>
<link>https://arxiv.org/abs/2502.12150</link>
<guid>https://arxiv.org/abs/2502.12150</guid>
<content:encoded><![CDATA[
arXiv:2502.12150v2 Announce Type: replace 
Abstract: In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, including training on synthetic data, inferring model similarity, and robust evaluation of LLMs. Code is available at https://github.com/locuslab/llm-idiosyncrasies.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions</title>
<link>https://arxiv.org/abs/2502.13124</link>
<guid>https://arxiv.org/abs/2502.13124</guid>
<content:encoded><![CDATA[
arXiv:2502.13124v3 Announce Type: replace 
Abstract: Scaling reasoning capabilities beyond traditional domains such as math and coding is hindered by the lack of diverse and high-quality questions. To overcome this limitation, we introduce a scalable approach for generating diverse and challenging reasoning questions, accompanied by reference answers. We present NaturalReasoning, a comprehensive dataset comprising 2.8 million questions that span multiple domains, including STEM fields (e.g., Physics, Computer Science), Economics, Social Sciences, and more. We demonstrate the utility of the questions in NaturalReasoning through knowledge distillation experiments which show that NaturalReasoning can effectively elicit and transfer reasoning capabilities from a strong teacher model. Furthermore, we demonstrate that NaturalReasoning is also effective for unsupervised self-training using external reward models or self-rewarding. To foster future work, we publicly release NaturalReasoning at https://huggingface.co/datasets/facebook/natural_reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMedTS: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series</title>
<link>https://arxiv.org/abs/2502.13509</link>
<guid>https://arxiv.org/abs/2502.13509</guid>
<content:encoded><![CDATA[
arXiv:2502.13509v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data, such as lab test results, capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative prompt embeddings. These prompt embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment</title>
<link>https://arxiv.org/abs/2502.13520</link>
<guid>https://arxiv.org/abs/2502.13520</guid>
<content:encoded><![CDATA[
arXiv:2502.13520v2 Announce Type: replace 
Abstract: This paper introduces the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale, fine-grained dataset for Arabic readability assessment. BAREC consists of 69,441 sentences spanning 1+ million words, carefully curated to cover 19 readability levels, from kindergarten to postgraduate comprehension. The corpus balances genre diversity, topical coverage, and target audiences, offering a comprehensive resource for evaluating Arabic text complexity. The corpus was fully manually annotated by a large team of annotators. The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 81.8%, reflecting a high level of substantial agreement. Beyond presenting the corpus, we benchmark automatic readability assessment across different granularity levels, comparing a range of techniques. Our results highlight the challenges and opportunities in Arabic readability modeling, demonstrating competitive performance across various methods. To support research and education, we make BAREC openly available, along with detailed annotation guidelines and benchmark results.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification</title>
<link>https://arxiv.org/abs/2502.14133</link>
<guid>https://arxiv.org/abs/2502.14133</guid>
<content:encoded><![CDATA[
arXiv:2502.14133v2 Announce Type: replace 
Abstract: Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. However, they also introduce a challenge: we lose the ability to manually remove unintended features, such as sensitive or task-irrelevant features, to guarantee regulatory compliance or improve the generalizability of classification models. This limitation arises because LLM embeddings are opaque and difficult to interpret. In this paper, we propose a novel framework to identify and regularize unintended features in the LLM latent space. Specifically, we first pre-train a sparse autoencoder (SAE) to extract interpretable features from LLM latent spaces. To ensure the SAE can capture task-specific features, we further fine-tune it on task-specific datasets. In training the classification model, we propose a simple and effective regularizer, by minimizing the similarity between the classifier weights and the identified unintended feature, to remove the impact of these unintended features on classification. We evaluate the proposed framework on three real-world tasks, including toxic chat detection, reward modeling, and disease diagnosis. Results show that the proposed self-regularization framework can improve the classifier's generalizability by regularizing those features that are not semantically correlated to the task. This work pioneers controllable text classification on LLM latent spaces by leveraging interpreted features to address generalizability, fairness, and privacy challenges. The code and data are publicly available at https://github.com/JacksonWuxs/Controllable_LLM_Classifier.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entity Framing and Role Portrayal in the News</title>
<link>https://arxiv.org/abs/2502.14718</link>
<guid>https://arxiv.org/abs/2502.14718</guid>
<content:encoded><![CDATA[
arXiv:2502.14718v2 Announce Type: replace 
Abstract: We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Training-free LLM-based Approach to General Chinese Character Error Correction</title>
<link>https://arxiv.org/abs/2502.15266</link>
<guid>https://arxiv.org/abs/2502.15266</guid>
<content:encoded><![CDATA[
arXiv:2502.15266v2 Announce Type: replace 
Abstract: Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model</title>
<link>https://arxiv.org/abs/2503.02969</link>
<guid>https://arxiv.org/abs/2503.02969</guid>
<content:encoded><![CDATA[
arXiv:2503.02969v2 Announce Type: replace 
Abstract: Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code and demo at https://github.com/LeiLiLab/InfiniSST
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling</title>
<link>https://arxiv.org/abs/2503.10093</link>
<guid>https://arxiv.org/abs/2503.10093</guid>
<content:encoded><![CDATA[
arXiv:2503.10093v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) algorithms for safety alignment of Large Language Models (LLMs), such as Direct Preference Optimization (DPO), encounter the challenge of distribution shift. Current approaches typically address this issue through online sampling from the target policy, which requires significant computational resources. In this paper, we hypothesize that during off-policy training, while the ranking order of output generated by policy changes, their overall distribution remains relatively stable. This stability allows the conversion of the sampling process from the target policy into a computationally efficient re-ranking of preference data. Building on this hypothesis, we propose a new framework that leverages the model's intrinsic safety judgment capability to extract reward signals, which are then used to calculate label confidence for preference reordering. Extensive experiments and theoretical analysis demonstrate that the proposed method effectively addresses the distribution shift issue, remarkably enhancing the safety performance while avoiding about 300x computational overheads.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPA: Russian Error Types Annotation for Evaluating Text Generation and Judgment Capabilities</title>
<link>https://arxiv.org/abs/2503.13102</link>
<guid>https://arxiv.org/abs/2503.13102</guid>
<content:encoded><![CDATA[
arXiv:2503.13102v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have introduced the novel paradigm of using LLMs as judges, where an LLM evaluates and scores the outputs of another LLM, which often correlates highly with human preferences. However, the use of LLM-as-a-judge has been primarily studied in English. In this paper, we evaluate this framework in Russian by introducing the Russian Error tyPes Annotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated responses. Human annotators labeled each response pair expressing their preferences across ten specific error types, as well as selecting an overall preference. We rank six generative LLMs across the error types using three rating systems based on human preferences. We also evaluate responses using eight LLM judges in zero-shot and few-shot settings. We describe the results of analyzing the judges and position and length biases. Our findings reveal a notable gap between LLM judge performance in Russian and English. However, rankings based on human and LLM preferences show partial alignment, suggesting that while current LLM judges struggle with fine-grained evaluation in Russian, there is potential for improvement.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathFusion: Enhancing Mathematical Problem-solving of LLM through Instruction Fusion</title>
<link>https://arxiv.org/abs/2503.16212</link>
<guid>https://arxiv.org/abs/2503.16212</guid>
<content:encoded><![CDATA[
arXiv:2503.16212v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices</title>
<link>https://arxiv.org/abs/2503.18242</link>
<guid>https://arxiv.org/abs/2503.18242</guid>
<content:encoded><![CDATA[
arXiv:2503.18242v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities on a broad array of NLP tasks, but their tendency to produce hallucinations$\unicode{x2013}$plausible-sounding but factually incorrect content$\unicode{x2013}$poses severe challenges in high-stakes domains. Existing hallucination detection methods either bear the computational cost of multiple inference passes or sacrifice accuracy for efficiency with single-pass approaches, neither of which is ideal in resource-constrained environments such as edge devices. We propose the Shannon Entropy Distribution Hallucination Detector (ShED-HD), a novel hallucination detection framework that bridges this gap by classifying sequence-level entropy patterns using a lightweight BiLSTM architecture with single-headed attention. In contrast to prior approaches, ShED-HD efficiently detects distinctive uncertainty patterns across entire output sequences, preserving contextual awareness. Through in-depth evaluation on three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that ShED-HD significantly outperforms other computationally efficient approaches in the out-of-distribution setting, while achieving comparable performance in the in-distribution setting. ShED-HD facilitates hallucination detection that is low-cost, accurate, and generalizable, improving the credibility of content generated by LLMs in resource-constrained environments where trustworthy AI functionality is crucial.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL</title>
<link>https://arxiv.org/abs/2503.18596</link>
<guid>https://arxiv.org/abs/2503.18596</guid>
<content:encoded><![CDATA[
arXiv:2503.18596v3 Announce Type: replace 
Abstract: Schema linking is a critical bottleneck in applying existing Text-to-SQL models to real-world, large-scale, multi-database environments. Through error analysis, we identify two major challenges in schema linking: (1) Database Retrieval: accurately selecting the target database from a large schema pool, while effectively filtering out irrelevant ones; and (2) Schema Item Grounding: precisely identifying the relevant tables and columns within complex and often redundant schemas for SQL generation. Based on these, we introduce LinkAlign, a novel framework tailored for large-scale databases with thousands of fields. LinkAlign comprises three key steps: multi-round semantic enhanced retrieval and irrelevant information isolation for Challenge 1, and schema extraction enhancement for Challenge 2. Each stage supports both Agent and Pipeline execution modes, enabling balancing efficiency and performance via modular design. To enable more realistic evaluation, we construct AmbiDB, a synthetic dataset designed to reflect the ambiguity of real-world schema linking. Experiments on widely-used Text-to-SQL benchmarks demonstrate that LinkAlign consistently outperforms existing baselines on all schema linking metrics. Notably, it improves the overall Text-to-SQL pipeline and achieves a new state-of-the-art score of 33.09% on the Spider 2.0-Lite benchmark using only open-source LLMs, ranking first on the leaderboard at the time of submission. The codes are available at https://github.com/Satissss/LinkAlign
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
arXiv:2503.23077v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating how LLM annotations represent diverse views on contentious topics</title>
<link>https://arxiv.org/abs/2503.23243</link>
<guid>https://arxiv.org/abs/2503.23243</guid>
<content:encoded><![CDATA[
arXiv:2503.23243v2 Announce Type: replace 
Abstract: Researchers have proposed the use of generative large language models (LLMs) to label data for research and applied settings. This literature emphasizes the improved performance of these models relative to other natural language models, noting that generative LLMs typically outperform other models and even humans across several metrics. Previous literature has examined bias across many applications and contexts, but less work has focused specifically on bias in generative LLMs' responses to subjective annotation tasks. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show systematic substantial disagreement with annotators on the basis of demographics. Rather, we find that multiple LLMs tend to be biased in the same directions on the same demographic categories within the same datasets. Moreover, the disagreement between human annotators on the labeling task -- a measure of item difficulty -- is far more predictive of LLM agreement with human annotators. We conclude with a discussion of the implications for researchers and practitioners using LLMs for automated data annotation tasks. Specifically, we emphasize that fairness evaluations must be contextual, model choice alone will not solve potential issues of bias, and item difficulty must be integrated into bias assessments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?</title>
<link>https://arxiv.org/abs/2504.00942</link>
<guid>https://arxiv.org/abs/2504.00942</guid>
<content:encoded><![CDATA[
arXiv:2504.00942v2 Announce Type: replace 
Abstract: A common assumption in Computational Linguistics is that text representations learnt by multimodal models are richer and more human-like than those by language-only models, as they are grounded in images or audio -- similar to how human language is grounded in real-world experiences. However, empirical studies checking whether this is true are largely lacking. We address this gap by comparing word representations from contrastive multimodal models vs. language-only ones in the extent to which they capture experiential information -- as defined by an existing norm-based 'experiential model' -- and align with human fMRI responses. Our results indicate that, surprisingly, language-only models are superior to multimodal ones in both respects. Additionally, they learn more unique brain-relevant semantic information beyond that shared with the experiential model. Overall, our study highlights the need to develop computational models that better integrate the complementary semantic information provided by multimodal data sources.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables</title>
<link>https://arxiv.org/abs/2504.06560</link>
<guid>https://arxiv.org/abs/2504.06560</guid>
<content:encoded><![CDATA[
arXiv:2504.06560v3 Announce Type: replace 
Abstract: Processing structured tabular data, particularly large and lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack primarily focus on unstructured text, neglecting the challenge of diverse structured tables. Meanwhile, previous tabular benchmarks mainly consider downstream tasks that require high-level reasoning abilities, and overlook models' underlying fine-grained perception of individual table cells, which is crucial for practical and robust LLM-based table applications. To address this gap, we introduce \textsc{NeedleInATable} (NIAT), a new long-context tabular benchmark that treats each table cell as a ``needle'' and requires models to extract the target cell based on cell locations or lookup questions. Our comprehensive evaluation of various LLMs and multimodal LLMs reveals a substantial performance gap between popular downstream tabular tasks and the simpler NIAT task, suggesting that they may rely on dataset-specific correlations or shortcuts to obtain better benchmark results but lack truly robust long-context understanding towards structured tables. Furthermore, we demonstrate that using synthesized NIAT training data can effectively improve performance on both NIAT task and downstream tabular tasks, which validates the importance of NIAT capability for LLMs' genuine table understanding ability. Our data, code and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scholar Inbox: Personalized Paper Recommendations for Scientists</title>
<link>https://arxiv.org/abs/2504.08385</link>
<guid>https://arxiv.org/abs/2504.08385</guid>
<content:encoded><![CDATA[
arXiv:2504.08385v2 Announce Type: replace 
Abstract: Scholar Inbox is a new open-access platform designed to address the challenges researchers face in staying current with the rapidly expanding volume of scientific literature. We provide personalized recommendations, continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual paper summaries, semantic search, and a range of tools to streamline research workflows and promote open research access. The platform's personalized recommendation system is trained on user ratings, ensuring that recommendations are tailored to individual researchers' interests. To further enhance the user experience, Scholar Inbox also offers a map of science that provides an overview of research across domains, enabling users to easily explore specific topics. We use this map to address the cold start problem common in recommender systems, as well as an active learning strategy that iteratively prompts users to rate a selection of papers, allowing the system to learn user preferences quickly. We evaluate the quality of our recommendation system on a novel dataset of 800k user ratings, which we make publicly available, as well as via an extensive user study. https://www.scholar-inbox.com/
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters</title>
<link>https://arxiv.org/abs/2504.11770</link>
<guid>https://arxiv.org/abs/2504.11770</guid>
<content:encoded><![CDATA[
arXiv:2504.11770v2 Announce Type: replace 
Abstract: Cross-linguistically, native words and loanwords follow different phonological rules. In English, for example, words of Germanic and Latinate origin exhibit different stress patterns, and a certain syntactic structure is exclusive to Germanic verbs. When seeing them as a cognitive model, however, such etymology-based generalizations face challenges in terms of learnability, since the historical origins of words are presumably inaccessible information for general language learners. In this study, we present computational evidence indicating that the Germanic-Latinate distinction in the English lexicon is learnable from the phonotactic information of individual words. Specifically, we performed an unsupervised clustering on corpus-extracted words, and the resulting word clusters largely aligned with the etymological distinction. The model-discovered clusters also recovered various linguistic generalizations documented in the previous literature regarding the corresponding etymological classes. Moreover, our findings also uncovered previously unrecognized features of the quasi-etymological clusters, offering novel hypotheses for future experimental studies.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments</title>
<link>https://arxiv.org/abs/2504.21016</link>
<guid>https://arxiv.org/abs/2504.21016</guid>
<content:encoded><![CDATA[
arXiv:2504.21016v2 Announce Type: replace 
Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese</title>
<link>https://arxiv.org/abs/2504.21017</link>
<guid>https://arxiv.org/abs/2504.21017</guid>
<content:encoded><![CDATA[
arXiv:2504.21017v2 Announce Type: replace 
Abstract: After two years of appearance, COVID-19 has negatively affected people and normal life around the world. As in May 2022, there are more than 522 million cases and six million deaths worldwide (including nearly ten million cases and over forty-three thousand deaths in Vietnam). Economy and society are both severely affected. The variant of COVID-19, Omicron, has broken disease prevention measures of countries and rapidly increased number of infections. Resources overloading in treatment and epidemics prevention is happening all over the world. It can be seen that, application of artificial intelligence (AI) to support people at this time is extremely necessary. There have been many studies applying AI to prevent COVID-19 which are extremely useful, and studies on machine reading comprehension (MRC) are also in it. Realizing that, we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and can be used to build models and systems, contributing to disease prevention. Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, we hope that it can contribute to promoting MRC studies in Vietnamese and multilingual.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design</title>
<link>https://arxiv.org/abs/2505.05298</link>
<guid>https://arxiv.org/abs/2505.05298</guid>
<content:encoded><![CDATA[
arXiv:2505.05298v2 Announce Type: replace 
Abstract: In this position paper, we advocate for the development of conversational technology that is inherently designed to support and facilitate argumentative processes. We argue that, at present, large language models (LLMs) are inadequate for this purpose, and we propose an ideal technology design aimed at enhancing argumentative skills. This involves re-framing LLMs as tools to exercise our critical thinking skills rather than replacing them. We introduce the concept of \textit{reasonable parrots} that embody the fundamental principles of relevance, responsibility, and freedom, and that interact through argumentative dialogical moves. These principles and moves arise out of millennia of work in argumentation theory and should serve as the starting point for LLM-based technology that incorporates basic principles of argumentation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Abstract Thinking Empowers Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.20164</link>
<guid>https://arxiv.org/abs/2505.20164</guid>
<content:encoded><![CDATA[
arXiv:2505.20164v2 Announce Type: replace 
Abstract: Images usually convey richer detail than text, but often include redundant information which potentially downgrades multimodal reasoning performance. When faced with lengthy or complex messages, humans tend to employ abstract thinking to convert them into simple and concise abstracts. Inspired by this cognitive strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking paradigm that prompts Multimodal Large Language Models (MLLMs) with visual abstract instead of explicit verbal thoughts or elaborate guidance, permitting a more concentrated visual reasoning mechanism. Explicit thinking, such as Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity of reasoning process via inserting verbose intermediate steps, external knowledge or visual information. In contrast, VAT reduces redundant visual information and encourages models to focus their reasoning on more essential visual elements. Experimental results show that VAT consistently empowers different models, and achieves an average gain of 17% over GPT-4o baseline by employing diverse types of visual abstracts, demonstrating that VAT can enhance visual reasoning abilities for MLLMs regarding conceptual, structural and relational reasoning tasks. VAT is also compatible with CoT in knowledge-intensive multimodal reasoning tasks. These findings highlight the effectiveness of visual reasoning via abstract thinking and encourage further exploration of more diverse reasoning paradigms from the perspective of human cognition.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2505.21138</link>
<guid>https://arxiv.org/abs/2505.21138</guid>
<content:encoded><![CDATA[
arXiv:2505.21138v2 Announce Type: replace 
Abstract: Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre-training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title>
<link>https://arxiv.org/abs/2505.24133</link>
<guid>https://arxiv.org/abs/2505.24133</guid>
<content:encoded><![CDATA[
arXiv:2505.24133v3 Announce Type: replace 
Abstract: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus</title>
<link>https://arxiv.org/abs/2506.00332</link>
<guid>https://arxiv.org/abs/2506.00332</guid>
<content:encoded><![CDATA[
arXiv:2506.00332v2 Announce Type: replace 
Abstract: Code-mixing involves the seamless integration of linguistic elements from multiple languages within a single discourse, reflecting natural multilingual communication patterns. Despite its prominence in informal interactions such as social media, chat messages and instant-messaging exchanges, there has been a lack of publicly available corpora that are author-labeled and suitable for modeling human conversations and relationships. This study introduces the first labeled and general-purpose corpus for understanding code-mixing in context while maintaining rigorous privacy and ethical standards. Our live project will continuously gather, verify, and integrate code-mixed messages into a structured dataset released in JSON format, accompanied by detailed metadata and linguistic statistics. To date, it includes over 355,641 messages spanning various code-mixing patterns, with a primary focus on English, Mandarin, and other languages. We expect the Codemix Corpus to serve as a foundational dataset for research in computational linguistics, sociolinguistics, and NLP applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</title>
<link>https://arxiv.org/abs/2506.00713</link>
<guid>https://arxiv.org/abs/2506.00713</guid>
<content:encoded><![CDATA[
arXiv:2506.00713v2 Announce Type: replace 
Abstract: This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2506.03781</link>
<guid>https://arxiv.org/abs/2506.03781</guid>
<content:encoded><![CDATA[
arXiv:2506.03781v2 Announce Type: replace 
Abstract: How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising quantization schemes that have strong expressiveness and optimizability, respectively. However, neither scheme leverages both advantages. In this paper, we propose UniQuanF (Unified Quantization with Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses both strong expressiveness and optimizability by unifying the flexible mapping technique in UQ and non-uniform quantization levels of BCQ. We propose unified initialization, and local and periodic mapping techniques to optimize the parameters in UniQuanF precisely. After optimization, our unification theorem removes computational and memory overhead, allowing us to utilize the superior accuracy of UniQuanF without extra deployment costs induced by the unification. Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP</title>
<link>https://arxiv.org/abs/2506.04385</link>
<guid>https://arxiv.org/abs/2506.04385</guid>
<content:encoded><![CDATA[
arXiv:2506.04385v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more "traditional" language modelling approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation</title>
<link>https://arxiv.org/abs/2506.05606</link>
<guid>https://arxiv.org/abs/2506.05606</guid>
<content:encoded><![CDATA[
arXiv:2506.05606v2 Announce Type: replace 
Abstract: Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable'' human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona and  history. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Understanding with Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2312.17432</link>
<guid>https://arxiv.org/abs/2312.17432</guid>
<content:encoded><![CDATA[
arXiv:2312.17432v5 Announce Type: replace-cross 
Abstract: With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversation</title>
<link>https://arxiv.org/abs/2402.11827</link>
<guid>https://arxiv.org/abs/2402.11827</guid>
<content:encoded><![CDATA[
arXiv:2402.11827v2 Announce Type: replace-cross 
Abstract: Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM on this dataset to align it with the retrievers' feedback. Our resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Wireless Federated Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2404.13238</link>
<guid>https://arxiv.org/abs/2404.13238</guid>
<content:encoded><![CDATA[
arXiv:2404.13238v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have driven profound transformations in wireless networks. However, within wireless environments, the training of LLMs faces significant challenges related to security and privacy. Federated Learning (FL), with its decentralized architecture, offers enhanced data privacy protection. Nevertheless, when integrated with LLMs, FL still struggles with several critical limitations, including large-scale and heterogeneous data, resource-intensive training, and substantial communication overhead. To address these challenges, this paper first presents a systematic analysis of the distinct training stages of LLMs in wireless networks, including pre-training, instruction tuning, and alignment tuning. Building upon this foundation, we propose a Personalized Wireless Federated Fine-tuning (PWFF) framework. Initially, we utilize the adapter and Low-Rank Adaptation (LoRA) techniques to decrease energy consumption, while employing global partial aggregation to reduce communication delay. Subsequently, we develop two reward models and design a personalized loss function to fulfill the goal of personalized learning. Furthermore, we implement a local multi-objective alignment to ensure the stability and effectiveness of the FL process. Finally, we conduct a series of simulations to validate the performance of the proposed PWFF method and provide an in-depth discussion of the open issues.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Sequential Decision Making with Large Language Models</title>
<link>https://arxiv.org/abs/2406.12125</link>
<guid>https://arxiv.org/abs/2406.12125</guid>
<content:encoded><![CDATA[
arXiv:2406.12125v2 Announce Type: replace-cross 
Abstract: This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a 6x performance gain over baselines while calling LLMs in only 1.5% of the time steps.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference</title>
<link>https://arxiv.org/abs/2406.15513</link>
<guid>https://arxiv.org/abs/2406.15513</guid>
<content:encoded><![CDATA[
arXiv:2406.15513v3 Announce Type: replace-cross 
Abstract: In this study, we introduce the safety human preference dataset, PKU-SafeRLHF, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs. Data is available at https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating LLM Ethics: Advancements, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2406.18841</link>
<guid>https://arxiv.org/abs/2406.18841</guid>
<content:encoded><![CDATA[
arXiv:2406.18841v5 Announce Type: replace-cross 
Abstract: This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMILE: Speech Meta In-Context Learning for Low-Resource Language Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2409.10429</link>
<guid>https://arxiv.org/abs/2409.10429</guid>
<content:encoded><![CDATA[
arXiv:2409.10429v2 Announce Type: replace-cross 
Abstract: Automatic Speech Recognition (ASR) models demonstrate outstanding performance on high-resource languages but face significant challenges when applied to low-resource languages due to limited training data and insufficient cross-lingual generalization. Existing adaptation strategies, such as shallow fusion, data augmentation, and direct fine-tuning, either rely on external resources, suffer computational inefficiencies, or fail in test-time adaptation scenarios. To address these limitations, we introduce Speech Meta In-Context LEarning (SMILE), an innovative framework that combines meta-learning with speech in-context learning (SICL). SMILE leverages meta-training from high-resource languages to enable robust, few-shot generalization to low-resource languages without explicit fine-tuning on the target domain. Extensive experiments on the ML-SUPERB benchmark show that SMILE consistently outperforms baseline methods, significantly reducing character and word error rates in training-free few-shot multilingual ASR tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning</title>
<link>https://arxiv.org/abs/2410.01044</link>
<guid>https://arxiv.org/abs/2410.01044</guid>
<content:encoded><![CDATA[
arXiv:2410.01044v2 Announce Type: replace-cross 
Abstract: The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much Can We Forget about Data Contamination?</title>
<link>https://arxiv.org/abs/2410.03249</link>
<guid>https://arxiv.org/abs/2410.03249</guid>
<content:encoded><![CDATA[
arXiv:2410.03249v4 Announce Type: replace-cross 
Abstract: The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B, have forgotten the data seen at the beginning of training.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences</title>
<link>https://arxiv.org/abs/2410.21332</link>
<guid>https://arxiv.org/abs/2410.21332</guid>
<content:encoded><![CDATA[
arXiv:2410.21332v2 Announce Type: replace-cross 
Abstract: Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables. HVM efficiently organizes memory while uncovering abstractions, leading to compact sequence representations. When learning on language datasets such as babyLM, HVM learns a more efficient dictionary than standard compression algorithms such as Lempel-Ziv. In a sequence recall task requiring the acquisition and transfer of variables embedded in sequences, we demonstrate HVM's sequence likelihood correlates with human recall times. In contrast, large language models (LLMs) struggle to transfer abstract variables as effectively as humans. From HVM's adjustable layer of abstraction, we demonstrate that the model realizes a precise trade-off between compression and generalization. Our work offers a cognitive model that captures the learning and transfer of abstract representations in human cognition and differentiates itself from LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse</title>
<link>https://arxiv.org/abs/2410.21333</link>
<guid>https://arxiv.org/abs/2410.21333</guid>
<content:encoded><![CDATA[
arXiv:2410.21333v4 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting has become a widely used strategy for improving large language and multimodal model performance. However, it is still an open question under which settings CoT systematically reduces performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, focusing on six representative tasks from the psychological literature where deliberation hurts performance in humans. In three of these tasks, state-of-the-art models exhibit significant performance drop-offs with CoT (up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o), while in others, CoT effects are mixed, with positive, neutral, and negative changes. While models and humans do not exhibit perfectly parallel cognitive processes, considering cases where thinking has negative consequences for humans helps identify settings where it negatively impacts models. By connecting the literature on human verbal thinking and deliberation with evaluations of CoT, we offer a perspective for understanding the impact of inference-time reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regular-pattern-sensitive CRFs for Distant Label Interactions</title>
<link>https://arxiv.org/abs/2411.12484</link>
<guid>https://arxiv.org/abs/2411.12484</guid>
<content:encoded><![CDATA[
arXiv:2411.12484v2 Announce Type: replace-cross 
Abstract: While LLMs have grown popular in sequence labeling, linear-chain conditional random fields (CRFs) remain a popular alternative with the ability to directly model interactions between labels. However, the Markov assumption limits them to % only directly modeling interactions between adjacent labels. Weighted finite-state transducers (FSTs), in contrast, can model distant label--label interactions, but exact label inference is intractable in general. In this work, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching standard linear-chain CRFs with the ability to learn long-distance label interactions through user-specified patterns. This approach allows users to write regular-expression label patterns concisely specifying which types of interactions the model should take into account, allowing the model to learn from data whether and in which contexts these patterns occur. The result can be interpreted alternatively as a CRF augmented with additional, non-local potentials, or as a finite-state transducer whose structure is defined by a set of easily-interpretable patterns. Critically, exact training and inference are tractable for many pattern sets. We detail how an RPCRF can be automatically constructed from a set of user-specified patterns, and demonstrate the model's effectiveness on a sequence of three synthetic sequence modeling datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems</title>
<link>https://arxiv.org/abs/2412.15557</link>
<guid>https://arxiv.org/abs/2412.15557</guid>
<content:encoded><![CDATA[
arXiv:2412.15557v2 Announce Type: replace-cross 
Abstract: With the widespread application of LLM-based dialogue systems in daily life, quality assurance has become more important than ever. Recent research has successfully introduced methods to identify unexpected behaviour in single-turn testing scenarios. However, multi-turn interaction is the common real-world usage of dialogue systems, yet testing methods for such interactions remain underexplored. This is largely due to the oracle problem in multi-turn testing, which continues to pose a significant challenge for dialogue system developers and researchers. In this paper, we propose MORTAR, a metamorphic multi-turn dialogue testing approach, which mitigates the test oracle problem in testing LLM-based dialogue systems. MORTAR formalises the multi-turn testing for dialogue systems, and automates the generation of question-answer dialogue test cases with multiple dialogue-level perturbations and metamorphic relations (MRs). The automated perturbation-MR matching mechanism allows MORTAR more flexibility and efficiency in metamorphic testing. The proposed approach is fully automated without reliance on potentially biased LLMs as test oracles. In testing six popular LLM-based dialogue systems, MORTAR reaches significantly better effectiveness with over 150\% more bugs revealed per test case when compared to the single-turn metamorphic testing baseline. On the quality of bugs, MORTAR reveals higher-quality bugs in terms of diversity, precision and uniqueness. MORTAR is expected to inspire more multi-turn testing approaches without LLM judges, and assist developers to evaluate the dialogue system performance more comprehensively with constrained test resources and budget.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Specialized Visual Encoders for Video Language Models</title>
<link>https://arxiv.org/abs/2501.01426</link>
<guid>https://arxiv.org/abs/2501.01426</guid>
<content:encoded><![CDATA[
arXiv:2501.01426v2 Announce Type: replace-cross 
Abstract: The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer by Layer: Uncovering Hidden Representations in Language Models</title>
<link>https://arxiv.org/abs/2502.02013</link>
<guid>https://arxiv.org/abs/2502.02013</guid>
<content:encoded><![CDATA[
arXiv:2502.02013v2 Announce Type: replace-cross 
Abstract: From extracting features to generating text, the outputs of large language models (LLMs) typically rely on the final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Upcycling Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2502.03009</link>
<guid>https://arxiv.org/abs/2502.03009</guid>
<content:encoded><![CDATA[
arXiv:2502.03009v2 Announce Type: replace-cross 
Abstract: Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored. Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration. Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets. Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[
arXiv:2502.03460v2 Announce Type: replace-cross 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title>
<link>https://arxiv.org/abs/2502.04322</link>
<guid>https://arxiv.org/abs/2502.04322</guid>
<content:encoded><![CDATA[
arXiv:2502.04322v2 Announce Type: replace-cross 
Abstract: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Temperature for Language Models with Multi-Sample Inference</title>
<link>https://arxiv.org/abs/2502.05234</link>
<guid>https://arxiv.org/abs/2502.05234</guid>
<content:encoded><![CDATA[
arXiv:2502.05234v2 Announce Type: replace-cross 
Abstract: Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature's role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification</title>
<link>https://arxiv.org/abs/2502.07299</link>
<guid>https://arxiv.org/abs/2502.07299</guid>
<content:encoded><![CDATA[
arXiv:2502.07299v2 Announce Type: replace-cross 
Abstract: The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARBOR: Exploring Persona Dynamics in Multi-Agent Competition</title>
<link>https://arxiv.org/abs/2502.12149</link>
<guid>https://arxiv.org/abs/2502.12149</guid>
<content:encoded><![CDATA[
arXiv:2502.12149v2 Announce Type: replace-cross 
Abstract: We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Improving LLM Alignment via Preference Data Selection</title>
<link>https://arxiv.org/abs/2502.14560</link>
<guid>https://arxiv.org/abs/2502.14560</guid>
<content:encoded><![CDATA[
arXiv:2502.14560v3 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability. Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\% of the Ultrafeedback dataset, our approach achieves 3\% to 8\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\% improvement with 25\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Euler to AI: Unifying Formulas for Mathematical Constants</title>
<link>https://arxiv.org/abs/2502.17533</link>
<guid>https://arxiv.org/abs/2502.17533</guid>
<content:encoded><![CDATA[
arXiv:2502.17533v2 Announce Type: replace-cross 
Abstract: The constant $\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines large language models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 407 distinct formulas for $\pi$ and prove relations between 381 (94%) of them, of which 188 (46%) can be derived from a single mathematical object$\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine. Our method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2503.01208</link>
<guid>https://arxiv.org/abs/2503.01208</guid>
<content:encoded><![CDATA[
arXiv:2503.01208v2 Announce Type: replace-cross 
Abstract: Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at https://github.com/illusionhi/ProbingPrivacy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text</title>
<link>https://arxiv.org/abs/2503.04804</link>
<guid>https://arxiv.org/abs/2503.04804</guid>
<content:encoded><![CDATA[
arXiv:2503.04804v3 Announce Type: replace-cross 
Abstract: As machine learning systems become increasingly embedded in society, their impact on human and nonhuman life continues to escalate. Technical evaluations have addressed a variety of potential harms from large language models (LLMs) towards humans and the environment, but there is little empirical work regarding harms towards nonhuman animals. Following the growing recognition of animal protection in regulatory and ethical AI frameworks, we present AnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated text. Our benchmark dataset comprises 1,850 curated questions from Reddit post titles and 2,500 synthetic questions based on 50 animal categories (e.g., cats, reptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios include open-ended questions about how to treat animals, practical scenarios with potential animal harm, and willingness-to-pay measures for the prevention of animal harm. Using the LLM-as-a-judge framework, responses are evaluated for their potential to increase or decrease harm, and evaluations are debiased for the tendency of judges to judge their own outputs more favorably. AHB reveals significant differences across frontier LLMs, animal categories, scenarios, and subreddits. We conclude with future directions for technical research and addressing the challenges of building evaluations on complex social and moral topics.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compute Optimal Scaling of Skills: Knowledge vs Reasoning</title>
<link>https://arxiv.org/abs/2503.10061</link>
<guid>https://arxiv.org/abs/2503.10061</guid>
<content:encoded><![CDATA[
arXiv:2503.10061v3 Announce Type: replace-cross 
Abstract: Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers without Normalization</title>
<link>https://arxiv.org/abs/2503.10622</link>
<guid>https://arxiv.org/abs/2503.10622</guid>
<content:encoded><![CDATA[
arXiv:2503.10622v2 Announce Type: replace-cross 
Abstract: Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions</title>
<link>https://arxiv.org/abs/2503.20290</link>
<guid>https://arxiv.org/abs/2503.20290</guid>
<content:encoded><![CDATA[
arXiv:2503.20290v3 Announce Type: replace-cross 
Abstract: This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordable AI Assistants with Knowledge Graph of Thoughts</title>
<link>https://arxiv.org/abs/2504.02670</link>
<guid>https://arxiv.org/abs/2504.02670</guid>
<content:encoded><![CDATA[
arXiv:2504.02670v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthesizing Data for Context Attribution in Question Answering</title>
<link>https://arxiv.org/abs/2504.05317</link>
<guid>https://arxiv.org/abs/2504.05317</guid>
<content:encoded><![CDATA[
arXiv:2504.05317v2 Announce Type: replace-cross 
Abstract: Question Answering (QA) accounts for a significant portion of LLM usage "in the wild". However, LLMs sometimes produce false or misleading responses, also known as "hallucinations". Therefore, grounding the generated answers in contextually provided information -- i.e., providing evidence for the generated text -- is paramount for LLMs' trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs' natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture</title>
<link>https://arxiv.org/abs/2504.10512</link>
<guid>https://arxiv.org/abs/2504.10512</guid>
<content:encoded><![CDATA[
arXiv:2504.10512v2 Announce Type: replace-cross 
Abstract: Language representation learning has emerged as a promising approach for sequential recommendation, thanks to its ability to learn generalizable representations. However, despite its advantages, this approach still struggles with data sparsity and a limited understanding of common-sense user preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a framework that combines $\textbf{J}$oint $\textbf{E}$mbedding $\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item textual descriptions. JEPA4Rec captures semantically rich and transferable representations, improving recommendation performance and reducing reliance on large-scale pre-training data. Specifically, JEPA4Rec represents items as text sentences by flattening descriptive information such as $\textit{title, category}$, and other attributes. To encode these sentences, we employ a bidirectional Transformer encoder with modified embedding layers tailored for capturing item information in recommendation datasets. We apply masking to text sentences and use them to predict the representations of the unmasked sentences, helping the model learn generalizable item embeddings. To further improve recommendation performance and language understanding, we employ a two-stage training strategy incorporating self-supervised learning losses. Experiments on six real-world datasets demonstrate that JEPA4Rec consistently outperforms state-of-the-art methods, particularly in cross-domain, cross-platform, and low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision</title>
<link>https://arxiv.org/abs/2505.14999</link>
<guid>https://arxiv.org/abs/2505.14999</guid>
<content:encoded><![CDATA[
arXiv:2505.14999v2 Announce Type: replace-cross 
Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation</title>
<link>https://arxiv.org/abs/2505.21549</link>
<guid>https://arxiv.org/abs/2505.21549</guid>
<content:encoded><![CDATA[
arXiv:2505.21549v4 Announce Type: replace-cross 
Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP's original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at https://anonymous.4open.science/r/DCLIP-B772/README.md.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cartridges: Lightweight and general-purpose long context representations via self-study</title>
<link>https://arxiv.org/abs/2506.06266</link>
<guid>https://arxiv.org/abs/2506.06266</guid>
<content:encoded><![CDATA[
<div> Large language models, Cartridge, self-study, in-context learning (ICL), memory consumption<br />
Summary:<br />
- Large language models are commonly used to answer queries based on large text corpuses using in-context learning (ICL).<br />
- The memory consumption of the key-value (KV) cache in current models can be reduced by training a smaller KV cache, called a Cartridge, offline on each corpus.<br />
- Training a Cartridge with next-token prediction is not as effective as ICL, so a new approach called self-study, which involves training with a context-distillation objective using synthetic conversations, is proposed.<br />
- Cartridges trained with self-study achieve comparable performance to ICL, but are much more cost-effective to serve.<br />
- Self-study extends the effective context length of the model and allows for composability of Cartridges at inference time without the need for retraining. <br /> <div>
arXiv:2506.06266v3 Announce Type: replace 
Abstract: Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleEval-OS: Performance evaluations of large language models for operations scheduling</title>
<link>https://arxiv.org/abs/2506.11017</link>
<guid>https://arxiv.org/abs/2506.11017</guid>
<content:encoded><![CDATA[
<div> Evaluation Benchmark, Large Language Models, Telecommunications Operation Scheduling, Performance Assessment, Zero-shot/Few-shot Evaluation 

Summary: 
The article discusses the development of a new Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS) to assess the performance of Large Language Models (LLMs) in the telecommunications industry. The benchmark includes 15 datasets covering various operational tasks and stages. LLM capabilities in scheduling tasks are categorized into four levels of complexity. Open-source LLMs, such as DeepSeek-V3, are compared against closed-source LLMs like GPT-4o using zero-shot and few-shot evaluation methods. Results indicate that open-source LLMs excel in specific scenarios, showcasing their potential in telecommunications operation scheduling. This study addresses the lack of comprehensive evaluation benchmarks and highlights the value of LLMs in optimizing production scheduling and service control within the telecommunications sector. 

<br /><br />Summary: <div>
arXiv:2506.11017v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has significantly propelled progress in artificial intelligence, demonstrating substantial application potential across multiple specialized domains. Telecommunications operation scheduling (OS) is a critical aspect of the telecommunications industry, involving the coordinated management of networks, services, risks, and human resources to optimize production scheduling and ensure unified service control. However, the inherent complexity and domain-specific nature of OS tasks, coupled with the absence of comprehensive evaluation benchmarks, have hindered thorough exploration of LLMs' application potential in this critical field. To address this research gap, we propose the first Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this benchmark comprises 15 datasets across 13 subtasks, comprehensively simulating four key operational stages: intelligent ticket creation, intelligent ticket handling, intelligent ticket closure, and intelligent evaluation. To systematically assess the performance of LLMs on tasks of varying complexity, we categorize their capabilities in telecommunications operation scheduling into four hierarchical levels, arranged in ascending order of difficulty: basic NLP, knowledge Q&amp;A, report generation, and report analysis. On TeleEval-OS, we leverage zero-shot and few-shot evaluation methods to comprehensively assess 10 open-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o) across diverse scenarios. Experimental results demonstrate that open-source LLMs can outperform closed-source LLMs in specific scenarios, highlighting their significant potential and value in the field of telecommunications operation scheduling.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.11063</link>
<guid>https://arxiv.org/abs/2506.11063</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, multimodal, position bias, robustness, debiasing<br />
Summary: <br />
- Multimodal Retrieval-Augmented Generation (RAG) systems are critical for knowledge-intensive tasks, but current models are sensitive to the order of evidence presentation, leading to performance instability and biased reasoning. 
- A study on position bias in multimodal RAG systems shows a U-shaped accuracy curve based on evidence position, with increased bias in multimodal compared to unimodal settings. 
- The Position Sensitivity Index (PSI_p) is introduced to quantify bias, and a visualization framework tracks attention allocation patterns across decoder layers. 
- Multimodal interactions intensify position bias, which logarithmically increases with retrieval range. 
- The findings emphasize the importance of evidence reordering or debiasing strategies to improve the reliability and equity of generation systems. <br /> <div>
arXiv:2506.11063v1 Announce Type: new 
Abstract: Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study</title>
<link>https://arxiv.org/abs/2506.11065</link>
<guid>https://arxiv.org/abs/2506.11065</guid>
<content:encoded><![CDATA[
<div> Keywords: Russenorsk, pidgin language, lexicon, word formation, grammatical structure <br />
Summary: Russenorsk, a pidgin language used in trade interactions between Russian and Norwegian speakers, is analyzed in this paper using modern language models (LLMs). By constructing a structured dictionary of the language, the study examines synonyms and word origins to propose hypotheses on word formation and grammatical structure in Russenorsk. Comparing the LLM-generated hypotheses with those in existing academic literature, the paper sheds light on the core principles of Russenorsk. Additionally, a "reconstruction" translation agent is developed to generate hypothetical Russenorsk translations of contemporary Russian and Norwegian texts. This study contributes to a deeper understanding of Russenorsk, showcasing the potential of LLMs in analyzing and reconstructing unique linguistic phenomena. <br /><br />Summary: <div>
arXiv:2506.11065v1 Announce Type: new 
Abstract: Russenorsk, a pidgin language historically used in trade interactions between Russian and Norwegian speakers, represents a unique linguistic phenomenon. In this paper, we attempt to analyze its lexicon using modern large language models (LLMs), based on surviving literary sources. We construct a structured dictionary of the language, grouped by synonyms and word origins. Subsequently, we use this dictionary to formulate hypotheses about the core principles of word formation and grammatical structure in Russenorsk and show which hypotheses generated by large language models correspond to the hypotheses previously proposed ones in the academic literature. We also develop a "reconstruction" translation agent that generates hypothetical Russenorsk renderings of contemporary Russian and Norwegian texts.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes</title>
<link>https://arxiv.org/abs/2506.11067</link>
<guid>https://arxiv.org/abs/2506.11067</guid>
<content:encoded><![CDATA[
<div> extraction, Review of Systems, clinical notes, large language model, pipeline 
<br />
Summary: 
A cost-effective pipeline utilizing large language models (LLMs) was developed to automatically extract Review of Systems (ROS) entities from clinical notes. The pipeline, incorporating SecTag and few-shot LLMs such as Mistral, Llama, Gemma, and ChatGPT, successfully identified ROS entity spans, statuses, and associated body systems with low error rates. Integration of ChatGPT resulted in the lowest error rates for entity spans and statuses/systems. The use of open-source LLMs allowed for local deployment, offering a scalable solution to alleviate ROS documentation burden in healthcare settings. The pipeline's performance with open-source LLMs was promising, demonstrating similar error rates to commercial models. This research highlights the potential of open-source LLMs as a feasible alternative for resource-limited healthcare environments. 
<br /> <div>
arXiv:2506.11067v1 Announce Type: new 
Abstract: Objective: Develop a cost-effective, large language model (LLM)-based pipeline for automatically extracting Review of Systems (ROS) entities from clinical notes. Materials and Methods: The pipeline extracts ROS sections using SecTag, followed by few-shot LLMs to identify ROS entity spans, their positive/negative status, and associated body systems. We implemented the pipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The evaluation was conducted on 36 general medicine notes containing 341 annotated ROS entities. Results: When integrating ChatGPT, the pipeline achieved the lowest error rates in detecting ROS entity spans and their corresponding statuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable local, cost-efficient execution of the pipeline while delivering promising performance with similarly low error rates (span: 30.5-36.7%; status/system: 24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and locally deployable solution to reduce ROS documentation burden. Open-source LLMs present a viable alternative to commercial models in resource-limited healthcare environments.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models</title>
<link>https://arxiv.org/abs/2506.11068</link>
<guid>https://arxiv.org/abs/2506.11068</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, moral reasoning, deontological keyword bias, obligations, judgment alignment <br />
Summary: <br /> 
- Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, facing challenges due to unclear judgment criteria even for humans. 
- A phenomenon known as Deontological Keyword Bias (DKB) causes LLMs to often judge non-obligatory contexts as obligations when prompted with modal expressions like 'must' or 'ought to.'
- LLMs show a strong tendency to perceive over 90% of commonsense scenarios as obligations when modal expressions are present, which is consistent across various LLM families, questions, and answer formats.
- The study proposes a judgment strategy combining few-shot examples with reasoning prompts to mitigate DKB and improve alignment in normative decisions made by LLMs.
- This research highlights how linguistic framing through modal expressions influences LLMs' normative judgment and emphasizes the need to address biases for accurate judgment alignment. <br />   
Summary: <div>
arXiv:2506.11068v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as must or ought to. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90\% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted control of fast prototyping through domain-specific interface</title>
<link>https://arxiv.org/abs/2506.11070</link>
<guid>https://arxiv.org/abs/2506.11070</guid>
<content:encoded><![CDATA[
<div> Keywords: Industrial designers, prototype models, natural language instructions, interface architecture, fast prototyping

Summary:
Industrial designers aim to control prototype models using natural language instructions seamlessly. However, current Large Language Models face challenges due to differences between designer and modeling languages. To address this, the proposed interface architecture acts as a mediator, grounded in design principles and automated domain specification. Machine-based evaluations and human studies across product design domains confirm the interface's capability to enhance Large Language Models for precise and effective prototype model control. The interface bridges gaps in abstraction levels, semantic precision, and lexical scopes, enabling designers to achieve their intended configurations effortlessly. <div>
arXiv:2506.11070v1 Announce Type: new 
Abstract: Industrial designers have long sought a natural and intuitive way to achieve the targeted control of prototype models -- using simple natural language instructions to configure and adjust the models seamlessly according to their intentions, without relying on complex modeling commands. While Large Language Models have shown promise in this area, their potential for controlling prototype models through language remains partially underutilized. This limitation stems from gaps between designers' languages and modeling languages, including mismatch in abstraction levels, fluctuation in semantic precision, and divergence in lexical scopes. To bridge these gaps, we propose an interface architecture that serves as a medium between the two languages. Grounded in design principles derived from a systematic investigation of fast prototyping practices, we devise the interface's operational mechanism and develop an algorithm for its automated domain specification. Both machine-based evaluations and human studies on fast prototyping across various product design domains demonstrate the interface's potential to function as an auxiliary module for Large Language Models, enabling precise and effective targeted control of prototype models.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention</title>
<link>https://arxiv.org/abs/2506.11073</link>
<guid>https://arxiv.org/abs/2506.11073</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, Multilingual, Object Hallucination, Attention, Language Shift<br />
Summary:<br />
- Large Vision-Language Models (LVLMs) show impressive multimodal abilities but struggle with multilingual object hallucination.<br />
- Existing solutions rely on resource-intensive pretraining or fine-tuning.<br />
- Cross-Lingual Attention Intervention for Mitigating (CLAIM) proposes a near training-free method by aligning attention patterns across languages.<br />
- CLAIM identifies language-specific attention heads, estimates language shift vectors, and intervenes during inference to align cross-lingual visual perception.<br />
- Experiments show CLAIM improves performance on multilingual benchmarks, with up to 30% improvement in Spanish.<br /> <div>
arXiv:2506.11073v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling</title>
<link>https://arxiv.org/abs/2506.11077</link>
<guid>https://arxiv.org/abs/2506.11077</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, reflection tokens, resource allocation, test-time compute performance, dynamic modulation

Summary:
Large reasoning models (LRMs) like OpenAI's o1 and DeepSeek-R1 use reflection tokens to guide the reasoning process before providing final answers. However, improper use of reflection tokens can lead to decreased model performance. This study introduces the concept of resource allocation for reflection tokens to enhance test-time compute performance by regulating their frequency and placement. The researchers propose CyclicReflex, a decoding strategy that dynamically adjusts reflection token logits using a position-dependent triangular waveform. Experimental results on various tasks show that CyclicReflex consistently improves model performance across different sizes, surpassing standard decoding methods and recent approaches like TIP and S1. This research sheds light on the importance of balancing reflection token usage and provides a practical solution for optimizing model performance during reasoning tasks. The codes for CyclicReflex are available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2506.11077v1 Announce Type: new 
Abstract: Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness test-time scaling to perform multi-step reasoning for complex problem-solving. This reasoning process, executed before producing final answers, is often guided by special juncture tokens or textual segments that prompt self-evaluative reflection. We refer to these transition markers and reflective cues as "reflection tokens" (e.g., "wait", "but", "alternatively"). In this work, we treat reflection tokens as a "resource" and introduce the problem of resource allocation, aimed at improving the test-time compute performance of LRMs by adaptively regulating the frequency and placement of reflection tokens. Through empirical analysis, we show that both excessive and insufficient use of reflection tokens, referred to as over-reflection and under-reflection, can degrade model performance. To better understand and manage this trade-off, we draw an analogy between reflection token usage and learning rate scheduling in optimization. Building on this insight, we propose cyclical reflection token scheduling (termed CyclicReflex), a decoding strategy that dynamically modulates reflection token logits using a position-dependent triangular waveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that CyclicReflex consistently improves performance across model sizes (1.5B-8B), outperforming standard decoding and more recent approaches such as TIP (thought switching penalty) and S1. Codes are available at https://github.com/OPTML-Group/CyclicReflex.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs</title>
<link>https://arxiv.org/abs/2506.11078</link>
<guid>https://arxiv.org/abs/2506.11078</guid>
<content:encoded><![CDATA[
<div> Keywords: Fake News Detection, Large Language Models, Logical Deduction, Experiential Learning, Case-Based Reasoning

Summary: 
RoE-FND, a novel framework for Fake News Detection, combines Large Language Models with experiential learning to address existing challenges in evidence-based detection methods. The framework reframes FND as a logical deduction task, improving the decision-making process. It consists of two stages: self-reflective knowledge building and dynamic criterion retrieval, enabling adaptation to evolving situations without the need for training. RoE-FND incorporates a case-based reasoning framework that cross-checks rationales against internal experience, leading to superior generalization and effectiveness over state-of-the-art methods across multiple datasets. Empirical validation demonstrates the framework's ability to address noisy evidence selection, generalization bottlenecks, hallucinated rationales, and conclusion bias, making it a promising approach for combating deceptive content online. 

<br /><br />Summary: <div>
arXiv:2506.11078v1 Announce Type: new 
Abstract: The proliferation of deceptive content online necessitates robust Fake News Detection (FND) systems. While evidence-based approaches leverage external knowledge to verify claims, existing methods face critical limitations: noisy evidence selection, generalization bottlenecks, and unclear decision-making processes. Recent efforts to harness Large Language Models (LLMs) for FND introduce new challenges, including hallucinated rationales and conclusion bias. To address these issues, we propose \textbf{RoE-FND} (\textbf{\underline{R}}eason \textbf{\underline{o}}n \textbf{\underline{E}}xperiences FND), a framework that reframes evidence-based FND as a logical deduction task by synergizing LLMs with experiential learning. RoE-FND encompasses two stages: (1) \textit{self-reflective knowledge building}, where a knowledge base is curated by analyzing past reasoning errors, namely the exploration stage, and (2) \textit{dynamic criterion retrieval}, which synthesizes task-specific reasoning guidelines from historical cases as experiences during deployment. It further cross-checks rationales against internal experience through a devised dual-channel procedure. Key contributions include: a case-based reasoning framework for FND that addresses multiple existing challenges, a training-free approach enabling adaptation to evolving situations, and empirical validation of the framework's superior generalization and effectiveness over state-of-the-art methods across three datasets.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MANBench: Is Your Multimodal Model Smarter than Human?</title>
<link>https://arxiv.org/abs/2506.11080</link>
<guid>https://arxiv.org/abs/2506.11080</guid>
<content:encoded><![CDATA[
<div> benchmark, Multimodal Large Language Models, MANBench, cross-modal reasoning, human performance<br />
Summary:<br />
The article introduces MANBench, a bilingual benchmark assessing Multimodal Large Language Models (MLLMs) across various tasks. Through human experiments, it was found that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks. Both humans and MLLMs face challenges in complex tasks such as Puzzles and Spatial Imagination. The study highlights the strengths and limitations of MLLMs, showing that advanced models still fall short of human-level performance in many domains. MANBench aims to inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available for access on GitHub for further research and development. <br /> <div>
arXiv:2506.11080v1 Announce Type: new 
Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework.
  Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination.
  MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at https://github.com/micdz/MANBench.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs</title>
<link>https://arxiv.org/abs/2506.11081</link>
<guid>https://arxiv.org/abs/2506.11081</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammar-based test case generation, Context-Free Grammars with Counters (CCFGs), large language models (LLMs), reinforcement learning, test effectiveness

Summary: 
Grammar-based test case generation has been successful in competitive programming but generating valid and general grammars from natural language specifications remains a challenge. The use of Context-Free Grammars with Counters (CCFGs) has been proposed to address this issue by representing specifications with logical constraints. In this study, the authors explore the use of large language models (LLMs) to induce CCFGs from specifications with limited labeled examples and reinforcement learning. They fine-tune an open-source LLM for specification-to-grammar translation and apply Group Relative Policy Optimization (GRPO) to enhance grammar quality. The approach, named SAGE, shows improved generalization and outperforms existing LLMs in grammar validity and test effectiveness. The experimental results demonstrate a significant enhancement over the state-of-the-art methods. The implementation and dataset are available on an anonymous repository for further exploration and research. 

<br /><br />Summary: <div>
arXiv:2506.11081v1 Announce Type: new 
Abstract: Grammar-based test case generation has proven effective for competitive programming problems, but generating valid and general grammars from natural language specifications remains a key challenge, especially under limited supervision. Context-Free Grammars with Counters (CCFGs) have recently been introduced as a formalism to represent such specifications with logical constraints by storing and reusing counter values during derivation. In this work, we explore the use of open-source large language models (LLMs) to induce CCFGs from specifications using a small number of labeled examples and verifiable reward-guided reinforcement learning. Our approach first fine-tunes an open-source LLM to perform specification-to-grammar translation, and further applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality. We also examine the effectiveness of iterative feedback for open and closed-source LLMs in correcting syntactic and semantic errors in generated grammars.
  Experimental results show that our approach SAGE achieves stronger generalization and outperforms 17 open and closed-source LLMs in both grammar quality and test effectiveness, improving over the state-of-the-art by 15.92%p in grammar validity and 12.34%p in test effectiveness. We provide our implementation and dataset at the following anonymous repository:https://anonymous.4open.science/r/SAGE-5714
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: A Transformer-based Language Model of Structured Clinical Event Data</title>
<link>https://arxiv.org/abs/2506.11082</link>
<guid>https://arxiv.org/abs/2506.11082</guid>
<content:encoded><![CDATA[
<div> transformer-based architecture, clinical decision-making, predictive reasoning, sequential medicine, generative language modeling

Summary:
PRISM, a transformer-based model, is introduced for understanding sequential clinical decision-making processes. It represents clinical trajectories as sequences of events and predicts future steps in patient diagnoses. The model utilizes a large clinical vocabulary and autoregressive training to capture complex dependencies in patient timelines. Experimental results show significant improvement in predicting next tokens compared to random baselines. The generated sequences reflect realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. This approach demonstrates the application of generative language modeling in structured medical event data for clinical decision support, simulation, and education. PRISM paves the way for advancements in sequence-based healthcare modeling by bridging machine learning architectures with real-world diagnostic reasoning. 

Summary: <div>
arXiv:2506.11082v1 Announce Type: new 
Abstract: We introduce PRISM (Predictive Reasoning in Sequential Medicine), a transformer-based architecture designed to model the sequential progression of clinical decision-making processes. Unlike traditional approaches that rely on isolated diagnostic classification, PRISM frames clinical trajectories as tokenized sequences of events - including diagnostic tests, laboratory results, and diagnoses - and learns to predict the most probable next steps in the patient diagnostic journey. Leveraging a large custom clinical vocabulary and an autoregressive training objective, PRISM demonstrates the ability to capture complex dependencies across longitudinal patient timelines. Experimental results show substantial improvements over random baselines in next-token prediction tasks, with generated sequences reflecting realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. These findings highlight the feasibility of applying generative language modeling techniques to structured medical event data, enabling applications in clinical decision support, simulation, and education. PRISM establishes a foundation for future advancements in sequence-based healthcare modeling, bridging the gap between machine learning architectures and real-world diagnostic reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedDebate: Safer Responses through Multi-Agent Red Teaming Debates</title>
<link>https://arxiv.org/abs/2506.11083</link>
<guid>https://arxiv.org/abs/2506.11083</guid>
<content:encoded><![CDATA[
<div> framework, RedDebate, multi-agent debate, Large Language Models, AI safety<br />
Summary:<br />
The article introduces RedDebate, a unique framework that utilizes adversarial argumentation among Large Language Models (LLMs) to proactively identify and address unsafe behaviors. Unlike traditional AI safety methods that rely on human evaluations or single-model assessments, RedDebate promotes collaborative disagreement among multiple LLMs to uncover blind spots and enhance responses through automated red-teaming. By integrating different long-term memory capacities for retaining safety insights from debates, RedDebate demonstrates a significant reduction in unsafe behaviors on existing benchmarks. Specifically, debate alone can reduce such behaviors by 17.7%, which further improves to over 23.5% when combined with long-term memory modules. Notably, RedDebate represents the first automated framework that leverages multi-agent debates and red-teaming to enhance AI safety without direct human intervention.<br /><br />Summary: <div>
arXiv:2506.11083v1 Announce Type: new 
Abstract: We propose RedDebate, a novel multi-agent debate framework that leverages adversarial argumentation among Large Language Models (LLMs) to proactively identify and mitigate their own unsafe behaviours. Existing AI safety methods often depend heavily on costly human evaluations or isolated single-model assessment, both subject to scalability constraints and oversight risks. RedDebate instead embraces collaborative disagreement, enabling multiple LLMs to critically examine one another's reasoning, and systematically uncovering unsafe blind spots through automated red-teaming, and iteratively improve their responses. We further integrate distinct types of long-term memory that retain learned safety insights from debate interactions. Evaluating on established safety benchmarks such as HarmBench, we demonstrate the proposed method's effectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when combined with long-term memory modules, achieves reductions exceeding 23.5%. To our knowledge, RedDebate constitutes the first fully automated framework that combines multi-agent debates with red-teaming to progressively enhance AI safety without direct human intervention.(Github Repository: https://github.com/aliasad059/RedDebate)
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing</title>
<link>https://arxiv.org/abs/2506.11088</link>
<guid>https://arxiv.org/abs/2506.11088</guid>
<content:encoded><![CDATA[
<div> framework, factuality, faithfulness, LLMs, hallucination
<br />
SPACE is a new approach that addresses factuality and faithfulness hallucinations in Large Language Models (LLMs) by editing shared activation subspaces. It identifies overlapping subspaces within neural representations, allowing for concurrent mitigation of both types of hallucinations. The framework incorporates dual-task feature modeling to establish a geometric foundation for shared subspace existence and utilizes a hybrid probe strategy for subspace identification and editing. Experimental results across various datasets demonstrate the effectiveness of SPACE in enhancing both factuality and faithfulness in LLMs. 
<br /><br />Summary: 
- SPACE framework enhances factuality and faithfulness in LLMs.
- It addresses factuality and faithfulness hallucinations simultaneously.
- Shared activation subspaces within neural representations are edited to mitigate hallucinations.
- Dual-task feature modeling establishes a geometric foundation for shared subspace existence.
- A hybrid probe strategy combining spectral clustering and attention head saliency scoring is used for subspace identification and editing. <div>
arXiv:2506.11088v1 Announce Type: new 
Abstract: LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Speech Recognition Model with Large Language Model Feedback</title>
<link>https://arxiv.org/abs/2506.11091</link>
<guid>https://arxiv.org/abs/2506.11091</guid>
<content:encoded><![CDATA[
<div> named entities, automatic speech recognition, domain adaptation, large language models, reinforcement learning

Summary:
Named entities are a challenge for automatic speech recognition (ASR) systems, particularly in domain mismatches. This study proposes a reinforcement learning approach for unsupervised domain adaptation in ASR, utilizing large language models (LLMs) to improve transcription quality. By using contextual information and LLM as a reward model to score ASR hypotheses, the framework fine-tunes the ASR model through reinforcement learning. The method significantly enhances named entity recognition, achieving a 21% reduction in entity word error rate compared to conventional self-training methods. This approach demonstrates the effectiveness of leveraging LLMs and reinforcement learning for improving ASR performance, particularly in handling rare named entities and adapting to diverse domains. <div>
arXiv:2506.11091v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation</title>
<link>https://arxiv.org/abs/2506.11092</link>
<guid>https://arxiv.org/abs/2506.11092</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, large language models, healthcare, smart homes, dynamic environments

Summary:
Dynamic Context Tuning (DCT) is a lightweight framework that enhances Retrieval-Augmented Generation (RAG) systems to support multi-turn dialogue and dynamic environments without the need for retraining. DCT integrates an attention-based context cache, LoRA-based retrieval, and efficient context compression to enable flexible adaptation to evolving tool environments. Experiments demonstrate that DCT improves plan accuracy by 14% and reduces hallucinations by 37% compared to existing systems, while achieving comparable performance to GPT-4 at lower cost. DCT's ability to generalize to new tools enables the development of scalable and adaptable AI assistants for dynamic domains like healthcare and smart homes. This framework addresses the limitations of current RAG systems by offering a more versatile and efficient approach to handling changing user intent, available tools, and contextual factors in real-time interactions. 

<br /><br />Summary: <div>
arXiv:2506.11092v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2506.11094</link>
<guid>https://arxiv.org/abs/2506.11094</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Safety Evaluation, Toxicity, Bias, Ethical

Summary:
With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have shown immense potential in Natural Language Processing. However, their deployment has raised safety concerns, especially regarding toxicity and bias in generated content. This survey provides a systematic overview of recent advancements in LLMs safety evaluation. It covers the background and significance of safety evaluation, categorizes evaluation tasks based on key capabilities like toxicity, robustness, ethics, bias, and fairness, summarizes evaluation metrics and datasets, and reviews evaluation methods and toolkits. The survey also highlights the challenges in LLMs safety evaluation and proposes future research directions to ensure the safe deployment of these models in real-world applications.<br /><br />Summary: <div>
arXiv:2506.11094v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have demonstrated remarkable potential in the field of Natural Language Processing (NLP), including areas such as content generation, human-computer interaction, machine translation, and code generation, among others. However, their widespread deployment has also raised significant safety concerns. In recent years, LLM-generated content has occasionally exhibited unsafe elements like toxicity and bias, particularly in adversarial scenarios, which has garnered extensive attention from both academia and industry. While numerous efforts have been made to evaluate the safety risks associated with LLMs, there remains a lack of systematic reviews summarizing these research endeavors. This survey aims to provide a comprehensive and systematic overview of recent advancements in LLMs safety evaluation, focusing on several key aspects: (1) "Why evaluate" that explores the background of LLMs safety evaluation, how they differ from general LLMs evaluation, and the significance of such evaluation; (2) "What to evaluate" that examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and so on; (3) "Where to evaluate" that summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (4) "How to evaluate" that reviews existing evaluation toolkit, and categorizing mainstream evaluation methods based on the roles of the evaluators. Finally, we identify the challenges in LLMs safety evaluation and propose potential research directions to promote further advancement in this field. We emphasize the importance of prioritizing LLMs safety evaluation to ensure the safe deployment of these models in real-world applications.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistent Homology of Topic Networks for the Prediction of Reader Curiosity</title>
<link>https://arxiv.org/abs/2506.11095</link>
<guid>https://arxiv.org/abs/2506.11095</guid>
<content:encoded><![CDATA[
<div> Keywords: reader curiosity, NLP, information gap theory, BERTopic, semantic network

Summary:<br />
- The study focuses on exploring reader curiosity in NLP and introduces a framework based on Loewenstein's Information Gap Theory.
- A pipeline is developed using BERTopic-inspired topic modeling and persistent homology to analyze the topology of a semantic network derived from text segments.
- Topological features of the dynamic semantic network are used as proxies for information gaps to quantify reader curiosity.
- The pipeline is empirically evaluated using reader curiosity ratings obtained from participants reading ''The Hunger Games'' novel.
- Results show a significant improvement in predicting reader curiosity ratings using the topological features compared to a baseline model, validating the approach.

Summary: <div>
arXiv:2506.11095v1 Announce Type: new 
Abstract: Reader curiosity, the drive to seek information, is crucial for textual engagement, yet remains relatively underexplored in NLP. Building on Loewenstein's Information Gap Theory, we introduce a framework that models reader curiosity by quantifying semantic information gaps within a text's semantic structure. Our approach leverages BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology (connected components, cycles, voids) of a dynamic semantic network derived from text segments, treating these features as proxies for information gaps. To empirically evaluate this pipeline, we collect reader curiosity ratings from participants (n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the topological features from our pipeline as independent variables to predict these ratings, and experimentally show that they significantly improve curiosity prediction compared to a baseline model (73% vs. 30% explained deviance), validating our approach. This pipeline offers a new computational method for analyzing text structure and its relation to reader engagement.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-SEO Bench: Does Conversational SEO Work?</title>
<link>https://arxiv.org/abs/2506.11097</link>
<guid>https://arxiv.org/abs/2506.11097</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Conversational Search Engines, Conversational Search Engine Optimization, C-SEO methods, C-SEO Bench<br />
<br />
Summary: <br />
Large Language Models are changing search engines into Conversational Search Engines, leading to the emergence of Conversational Search Engine Optimization (C-SEO). However, current C-SEO methods are found to be largely ineffective across various tasks, domains, and number of actors. The study introduces C-SEO Bench, a benchmark to evaluate C-SEO methods in different scenarios. Traditional SEO strategies are shown to be more effective in improving visibility in Conversational Search Engines compared to cutting-edge C-SEO techniques. Additionally, increasing the number of C-SEO adopters leads to diminishing overall gains, indicating a competitive and zero-sum nature of the problem. The research provides valuable insights into the challenges and dynamics of optimizing web documents for Conversational Search Engines. <div>
arXiv:2506.11097v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at https://github.com/parameterlab/c-seo-bench and https://huggingface.co/datasets/parameterlab/c-seo-bench.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2506.11102</link>
<guid>https://arxiv.org/abs/2506.11102</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, chatbots, AI agents, evaluation, benchmarks

Summary:
This paper discusses the evolution from traditional large language model (LLM) chatbots to advanced AI agents and the importance of distinguishing between the two in evaluation frameworks. It introduces a systematic analysis of current evaluation approaches, highlighting five key aspects that differentiate AI agents from LLM chatbots. The paper categorizes existing evaluation benchmarks based on external environments and internal capabilities, providing practical reference tables for researchers. It also outlines future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. This analysis offers guidance for researchers in selecting appropriate benchmarks for evaluating AI agents, aiding in the continued advancement of this rapidly evolving research domain.<br /><br />Summary: <div>
arXiv:2506.11102v1 Announce Type: new 
Abstract: The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model</title>
<link>https://arxiv.org/abs/2506.11103</link>
<guid>https://arxiv.org/abs/2506.11103</guid>
<content:encoded><![CDATA[
<div> ICL, ManyICL, LLMs, in-context learning, fine-tuning <br />
Summary: 
The paper introduces Many-Shot In-Context Fine-tuning (ManyICL) as a new approach to enhance the performance of large language models (LLMs) in handling multiple downstream tasks simultaneously. ManyICL utilizes in-context learning principles in a many-shot setting, where each answer within the context is treated as a supervised training target to improve autoregressive learning. Experimental results across various tasks show that ManyICL outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning while also mitigating catastrophic forgetting issues. The proposed training objective significantly narrows the performance gap between in-context fine-tuning and dedicated fine-tuning methods. The code for ManyICL will be publicly available upon publication. <br /> <div>
arXiv:2506.11103v1 Announce Type: new 
Abstract: Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.
  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration</title>
<link>https://arxiv.org/abs/2506.11104</link>
<guid>https://arxiv.org/abs/2506.11104</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP, transformers, sparse attention, dynamic attention, Large Language Models (LLMs)

Summary:
Dynamic sparse attention mechanisms have been introduced to address the efficiency issues faced by transformers in handling long-context understanding tasks in Natural Language Processing (NLP) applications. These methods adaptively assign masks at the attention-map level, allowing for heterogeneous patterns to be captured across layers and heads. Unlike existing approaches, this method does not require fine-tuning or predefined mask structures, while maintaining computational efficiency. By learning context-aware attention structures, the dynamic sparse attention mechanism achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This scalable alternative to full attention enables the practical deployment of large-scale Large Language Models (LLMs) without compromising retrieval performance.

<br /><br />Summary: <div>
arXiv:2506.11104v1 Announce Type: new 
Abstract: Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: https://github.com/HanzhiZhang-Ulrica/DAM.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation</title>
<link>https://arxiv.org/abs/2506.11105</link>
<guid>https://arxiv.org/abs/2506.11105</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, healthcare scenarios, compression framework, neuron saliency, real-time deployment

Summary:
Large Language Models (LLMs) have a significant impact on healthcare scenarios but are often too large for deployment on resource-constrained edge devices. This work introduces a novel medical assistant system optimized through a compression framework tailored for specialized domains. By measuring neuron saliency on domain-specific data, irrelevant neurons are pruned to reduce model size while maintaining performance. Post-training quantization further reduces memory footprint. The compressed models, Gemma and LLaMA3, were evaluated across medical benchmarks such as MedMCQA, MedQA, and PubMedQA. Deployment on Jetson Orin Nano and Raspberry Pi 5 demonstrated real-time, energy-efficient inference under hardware constraints. This approach enables the efficient deployment of LLMs in real-time healthcare applications on edge devices. 

<br /><br />Summary: 
- Introduction of a medical assistant system optimized for specialized domains through a compression framework
- Pruning of irrelevant neurons based on domain-specific data to reduce model size without sacrificing performance
- Post-training quantization further reduces memory footprint
- Evaluation of compressed models on medical benchmarks showcasing their effectiveness
- Successful deployment on Jetson Orin Nano and Raspberry Pi 5 for real-time, energy-efficient inference in healthcare scenarios <div>
arXiv:2506.11105v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking</title>
<link>https://arxiv.org/abs/2506.11106</link>
<guid>https://arxiv.org/abs/2506.11106</guid>
<content:encoded><![CDATA[
<div> framework, PankRAG, graph-based retrieval-augmented generation, hierarchical query-resolution strategy, dependency-aware reranking mechanism, large language models

Summary: 
The article introduces PankRAG, a novel framework for graph-based retrieval-augmented generation. PankRAG addresses limitations in existing methods by incorporating a hierarchical query-resolution strategy that captures interdependencies in user queries. It also utilizes a dependency-aware reranking mechanism to enrich and validate retrieval results. Empirical evaluations show PankRAG outperforms state-of-the-art approaches on multiple benchmarks, demonstrating its robustness and generalizability. The framework's globally aware resolution path and structured reasoning guide large language models through complex queries, leading to more accurate and relevant retrieved content. By considering parallel and sequential dependencies within queries, PankRAG mitigates the risk of misinterpretation and omission of critical information, enhancing the fidelity of generated responses. Overall, PankRAG offers a comprehensive solution for improving information retrieval and generation tasks in natural language processing. 

<br /><br />Summary: <div>
arXiv:2506.11106v1 Announce Type: new 
Abstract: Contemporary graph-based retrieval-augmented generation (RAG) methods typically begin by extracting entities from user queries and then leverage pre-constructed knowledge graphs to retrieve related relationships and metadata. However, this pipeline's exclusive reliance on entity-level extraction can lead to the misinterpretation or omission of latent yet critical information and relations. As a result, retrieved content may be irrelevant or contradictory, and essential knowledge may be excluded, exacerbating hallucination risks and degrading the fidelity of generated responses. To address these limitations, we introduce PankRAG, a framework that combines a globally aware, hierarchical query-resolution strategy with a novel dependency-aware reranking mechanism. PankRAG first constructs a multi-level resolution path that captures both parallel and sequential interdependencies within a query, guiding large language models (LLMs) through structured reasoning. It then applies its dependency-aware reranker to exploit the dependency structure among resolved sub-questions, enriching and validating retrieval results for subsequent sub-questions. Empirical evaluations demonstrate that PankRAG consistently outperforms state-of-the-art approaches across multiple benchmarks, underscoring its robustness and generalizability.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM</title>
<link>https://arxiv.org/abs/2506.11108</link>
<guid>https://arxiv.org/abs/2506.11108</guid>
<content:encoded><![CDATA[
<div> extension, Self-Supervised, Cross-Attention, Reinforcement, multi-turn dialogue

Summary:
The article introduces CAGSR-vLLM-MTC, an extension of the CAGSR framework implemented on the high-performance vLLM runtime. It addresses multi-turn dialogue and chain-of-thought reasoning by capturing cross-attention weights during generation and accumulating attention signals over conversation histories and chain-of-thought steps. Practical trade-offs such as an entropy-based clamping mechanism to prevent attention collapse are discussed. Future directions include exploring multi-party dialogues and hierarchical reasoning. <div>
arXiv:2506.11108v1 Announce Type: new 
Abstract: We present CAGSR-vLLM-MTC, an extension of our Self-Supervised Cross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the high-performance vLLM runtime, to address both multi-turn dialogue and chain-of-thought reasoning. Building upon our original single-turn approach, we first instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer, per-head cross-attention weights during generation. We then generalized our self-supervised reward function to accumulate attention signals over entire conversation histories and intermediate chain-of-thought steps. We discuss practical trade-offs, including an entropy-based clamping mechanism to prevent attention collapse on early context, and outline future directions for multi-party dialogues and hierarchical reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization</title>
<link>https://arxiv.org/abs/2506.11109</link>
<guid>https://arxiv.org/abs/2506.11109</guid>
<content:encoded><![CDATA[
<div> Keywords: mobility data, Large Language Models, QT-Mob, location tokenization, fine-tuning objectives

Summary:
The article introduces QT-Mob, a novel framework designed to enhance Large Language Models (LLMs) for mobility analytics by addressing key limitations. QT-Mob incorporates a location tokenization module to create semantically rich location tokens, improving contextual representation while maintaining compatibility with LLMs. Additionally, the framework utilizes fine-tuning objectives to align learned tokens with internal LLM representations, enhancing the model's understanding of sequential movement patterns and location semantics. Experimental results on real-world datasets demonstrate QT-Mob's superior performance in next-location prediction and mobility recovery tasks compared to existing deep learning and LLM-based methods. This framework not only improves LLMs' interpretation of mobility data but also offers a more generalizable approach for various mobility analytics tasks.<br /><br />Summary: <div>
arXiv:2506.11109v1 Announce Type: new 
Abstract: The widespread adoption of location-based services has led to the generation of vast amounts of mobility data, providing significant opportunities to model user movement dynamics within urban environments. Recent advancements have focused on adapting Large Language Models (LLMs) for mobility analytics. However, existing methods face two primary limitations: inadequate semantic representation of locations (i.e., discrete IDs) and insufficient modeling of mobility signals within LLMs (i.e., single templated instruction fine-tuning). To address these issues, we propose QT-Mob, a novel framework that significantly enhances LLMs for mobility analytics. QT-Mob introduces a location tokenization module that learns compact, semantically rich tokens to represent locations, preserving contextual information while ensuring compatibility with LLMs. Furthermore, QT-Mob incorporates a series of complementary fine-tuning objectives that align the learned tokens with the internal representations in LLMs, improving the model's comprehension of sequential movement patterns and location semantics. The proposed QT-Mob framework not only enhances LLMs' ability to interpret mobility data but also provides a more generalizable approach for various mobility analytics tasks. Experiments on three real-world dataset demonstrate the superior performance in both next-location prediction and mobility recovery tasks, outperforming existing deep learning and LLM-based methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models</title>
<link>https://arxiv.org/abs/2506.11110</link>
<guid>https://arxiv.org/abs/2506.11110</guid>
<content:encoded><![CDATA[
<div> factuality, framing, Large Language Models, AssertBench, FEVEROUS

Summary:
The article introduces AssertBench, a benchmark designed to investigate how the framing of factually true statements influences the agreement of Large Language Models (LLMs). AssertBench samples evidence-supported facts from the FEVEROUS dataset and presents two framing prompts for each fact: one where the user claims the statement is correct and another where the user claims it is incorrect. The goal is to measure whether the LLM maintains consistent truth evaluation across different framings or switches its evaluation to agree with the user. By stratifying results based on the model's accuracy on neutral claims, AssertBench aims to isolate framing-induced variability from the model's underlying factual knowledge. The benchmark evaluates the LLM's ability to remain consistent in its assessment when faced with contradictory user assertions about the same fact. The complete source code for AssertBench is available on GitHub. 

Summary: <div>
arXiv:2506.11110v1 Announce Type: new 
Abstract: Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions</title>
<link>https://arxiv.org/abs/2506.11111</link>
<guid>https://arxiv.org/abs/2506.11111</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Robustness, Adversarial Robustness, OOD Robustness, Evaluation

Summary: 
Large Language Models (LLMs) have become integral to various AI applications, necessitating a focus on their robustness. This survey paper comprehensively explores LLM robustness, categorizing it into three main perspectives. Firstly, Adversarial Robustness addresses intentional manipulation in prompts, such as noise prompts and data attacks. Secondly, OOD Robustness handles unforeseen real-world scenarios like OOD detection and hallucinations. Lastly, the Evaluation of Robustness section reviews new datasets, metrics, and tools for assessing LLM robustness. The paper discusses key research areas and future directions, aiming to provide a valuable resource for the community. Additionally, a GitHub project is organized to facilitate easy access to related works and foster collaboration. <br /><br />Summary: <div>
arXiv:2506.11111v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)</title>
<link>https://arxiv.org/abs/2506.11112</link>
<guid>https://arxiv.org/abs/2506.11112</guid>
<content:encoded><![CDATA[
<div> Keywords: CONversational Information ACcess, CONIAC, world model, Conversational Agents Framework for Evaluation, evaluation methodology

Summary:
During the workshop, the concept of CONversational Information ACcess (CONIAC) was thoroughly discussed, focusing on its unique features and proposing a world model to abstract it. The Conversational Agents Framework for Evaluation (CAFE) was introduced as a tool for evaluating CONIAC systems, comprising six key components. These components include defining the goals of the system's stakeholders, identifying user tasks for evaluation, analyzing user aspects, determining evaluation criteria, selecting appropriate evaluation methodologies, and establishing measures for quantitative criteria. This comprehensive framework aims to provide a systematic approach to evaluating CONIAC systems, ensuring that they meet the needs and expectations of their users. <div>
arXiv:2506.11112v1 Announce Type: new 
Abstract: During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.11113</link>
<guid>https://arxiv.org/abs/2506.11113</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, large language models, adversarial attacks, reliability, mitigation strategies

Summary:
Large language models (LLMs) are being explored as potential automated reviewers to alleviate the burden on human reviewers. However, their susceptibility to adversarial attacks raises concerns about the reliability of their generated reviews. This study evaluates the effectiveness of LLMs compared to human reviewers and explores the impact of adversarial attacks on the reliability of LLM-generated reviews. The research highlights significant vulnerabilities in LLM assessments due to text manipulations, emphasizing the need to address adversarial risks. The study also discusses challenges and potential mitigation strategies for using LLMs in automated peer reviewing. It is crucial to ensure that AI strengthens rather than compromises the integrity of scholarly communication. 

Summary: <div>
arXiv:2506.11113v1 Announce Type: new 
Abstract: Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations</title>
<link>https://arxiv.org/abs/2506.11114</link>
<guid>https://arxiv.org/abs/2506.11114</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, KokushiMD-10, healthcare licensing exams, multimodal benchmark, medical AI

Summary:
The article introduces KokushiMD-10, a multimodal benchmark derived from Japanese national healthcare licensing exams to assess LLMs in high-stakes clinical scenarios. It consists of over 11588 real exam questions covering various healthcare roles and includes clinical images for evaluating textual and visual reasoning. Thirty state-of-the-art LLMs are benchmarked, but none consistently meet passing thresholds across domains, highlighting challenges in medical AI. KokushiMD-10 aims to provide a comprehensive resource for advancing reasoning-centric medical AI in multilingual and multimodal clinical tasks. <div>
arXiv:2506.11114v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated notable performance in medical licensing exams. However, comprehensive evaluation of LLMs across various healthcare roles, particularly in high-stakes clinical scenarios, remains a challenge. Existing benchmarks are typically text-based, English-centric, and focus primarily on medicines, which limits their ability to assess broader healthcare knowledge and multimodal reasoning. To address these gaps, we introduce KokushiMD-10, the first multimodal benchmark constructed from ten Japanese national healthcare licensing exams. This benchmark spans multiple fields, including Medicine, Dentistry, Nursing, Pharmacy, and allied health professions. It contains over 11588 real exam questions, incorporating clinical images and expert-annotated rationales to evaluate both textual and visual reasoning. We benchmark over 30 state-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both text and image-based settings. Despite promising results, no model consistently meets passing thresholds across domains, highlighting the ongoing challenges in medical AI. KokushiMD-10 provides a comprehensive and linguistically grounded resource for evaluating and advancing reasoning-centric medical AI across multilingual and multimodal clinical tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Domain Knowledge into Materials Tokenization</title>
<link>https://arxiv.org/abs/2506.11115</link>
<guid>https://arxiv.org/abs/2506.11115</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, materials science, tokenization, MATTER, domain knowledge

Summary:
MATTER is introduced as a novel tokenization approach for language models in materials science. Traditional tokenization methods often lead to fragmentation and semantic loss when applied to scientific text. MATTER addresses this issue by incorporating material knowledge into tokenization. By utilizing MatDetector and a re-ranking method, MATTER maintains the structural and semantic integrity of material concepts during token merging. Experimental results show that MATTER outperforms existing tokenization methods, with a performance gain of 4% and 2% in generation and classification tasks, respectively. The study highlights the significance of domain knowledge in developing effective tokenization strategies for scientific text processing.

<br /><br />Summary: <div>
arXiv:2506.11115v1 Announce Type: new 
Abstract: While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of $4\%$ and $2\%$ in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models</title>
<link>https://arxiv.org/abs/2506.11116</link>
<guid>https://arxiv.org/abs/2506.11116</guid>
<content:encoded><![CDATA[
<div> Instruction dataset, Large Language Models, Fine-tuning, Performance gains, Benchmarking <br />
<br />
Summary: <br />
The article introduces Infinity-Instruct, a high-quality instruction dataset aiming to enhance the capabilities of Large Language Models (LLMs) in foundational and chat tasks. The dataset is developed in two phases: Phase 1 involves curating 7.4M foundational instructions, while Phase 2 focuses on synthesizing 1.5M chat instructions. By fine-tuning open-source models with Infinity-Instruct, significant performance improvements are observed in both foundational and instruction-following benchmarks. Notably, the InfInstruct-LLaMA3.1-70B model outperforms GPT-4-0314 by 8.6% on instruction-following tasks while maintaining comparable foundational performance. These results highlight the importance of synergizing foundational and chat training for comprehensive LLM development. The dataset and codes have been publicly released for further research and application. <div>
arXiv:2506.11116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and codes\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly released.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research</title>
<link>https://arxiv.org/abs/2506.11117</link>
<guid>https://arxiv.org/abs/2506.11117</guid>
<content:encoded><![CDATA[
<div> information needs, dataset, scientific QA, retrieval, ScIRGen-Geo

Summary:
ScIRGen introduces a dataset generation framework for scientific QA & retrieval that reflects the information needs of professional science researchers. It creates a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets, and papers. Utilizing dataset-oriented information extraction from academic papers and a question generation framework based on cognitive taxonomy, the dataset ScIRGen-Geo is created. A method is developed to filter synthetic answers based on the perplexity shift of LLMs for validity. Benchmarking shows current methods struggle with complex question reasoning. This work advances tools to support the intricate information needs of the scientific community. 

Summary: <div>
arXiv:2506.11117v1 Announce Type: new 
Abstract: Scientific researchers need intensive information about datasets to effectively evaluate and develop theories and methodologies. The information needs regarding datasets are implicitly embedded in particular research tasks, rather than explicitly expressed in search queries. However, existing scientific retrieval and question-answering (QA) datasets typically address straightforward questions, which do not align with the distribution of real-world research inquiries. To bridge this gap, we developed ScIRGen, a dataset generation framework for scientific QA \& retrieval that more accurately reflects the information needs of professional science researchers, and uses it to create a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets and papers. Technically, we designed a dataset-oriented information extraction method that leverages academic papers to augment the dataset representation. We then proposed a question generation framework by employing cognitive taxonomy to ensure the quality of synthesized questions. We also design a method to automatically filter synthetic answers based on the perplexity shift of LLMs, which is highly aligned with human judgment of answers' validity. Collectively, these methodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We benchmarked representative methods on the ScIRGen-Geo dataset for their question-answering and retrieval capabilities, finding out that current methods still suffer from reasoning from complex questions. This work advances the development of more sophisticated tools to support the intricate information needs of the scientific community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech</title>
<link>https://arxiv.org/abs/2506.11119</link>
<guid>https://arxiv.org/abs/2506.11119</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, dementia, speech analysis, cognitive decline, early detection 

Summary: 
- The study focused on using speech analysis as a non-invasive biomarker for cognitive decline in Alzheimer's disease and related dementias (ADRD).
- The researchers used the PREPARE Challenge dataset, which included audio recordings from individuals with different cognitive statuses.
- They explored the performance of various open-source foundation speech and language models in classifying cognitive status into healthy control, mild cognitive impairment, and Alzheimer's Disease categories.
- The Whisper-medium model for speech analysis and BERT with pause annotation for language models showed the best performance.
- The study found that incorporating non-semantic features like pause patterns improved the text-based classification, and automatic speech recognition-generated audio embeddings outperformed others. Acoustic-based approaches, particularly ASR-derived embeddings, have the potential for scalable and cost-effective early detection of ADRD. 

<br /><br />Summary: <div>
arXiv:2506.11119v1 Announce Type: new 
Abstract: Background: Alzheimer's disease and related dementias (ADRD) are progressive neurodegenerative conditions where early detection is vital for timely intervention and care. Spontaneous speech contains rich acoustic and linguistic markers that may serve as non-invasive biomarkers for cognitive decline. Foundation models, pre-trained on large-scale audio or text data, produce high-dimensional embeddings encoding contextual and acoustic features.
  Methods: We used the PREPARE Challenge dataset, which includes audio recordings from over 1,600 participants with three cognitive statuses: healthy control (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We excluded non-English, non-spontaneous, or poor-quality recordings. The final dataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We benchmarked a range of open-source foundation speech and language models to classify cognitive status into the three categories.
  Results: The Whisper-medium model achieved the highest performance among speech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with pause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection using state-of-the-art automatic speech recognition (ASR) model-generated audio embeddings outperformed others. Including non-semantic features like pause patterns consistently improved text-based classification.
  Conclusion: This study introduces a benchmarking framework using foundation models and a clinically relevant dataset. Acoustic-based approaches -- particularly ASR-derived embeddings -- demonstrate strong potential for scalable, non-invasive, and cost-effective early detection of ADRD.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2506.11120</link>
<guid>https://arxiv.org/abs/2506.11120</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, gradient-based pruning, self-distillation loss, MLP modules, compression

Summary: 
This study addresses the high deployment costs of large language models (LLMs) by proposing a novel gradient-based pruning method. By introducing a self-distillation loss during the pruning phase, the model can fully exploit the predictions of the original LLM, leading to more accurate gradient information for compression. The focus is on pruning MLP modules, which have a significant parameter footprint in LLMs, contributing to efficient compression without noticeable performance degradation. Experimental results on zero-shot benchmarks demonstrate the superior performance of the proposed method compared to existing pruning techniques. Additionally, the method achieves competitive results among 1B-scale open source LLMs. The source code and trained weights are available for further exploration. 

<br /><br />Summary: <div>
arXiv:2506.11120v1 Announce Type: new 
Abstract: In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than $5 \times$ parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs. The source code and trained weights are available at https://github.com/visresearch/SDMPrune.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR</title>
<link>https://arxiv.org/abs/2506.11121</link>
<guid>https://arxiv.org/abs/2506.11121</guid>
<content:encoded><![CDATA[
<div> adaptation, ASR, language model, test-time, entropy minimization 

Summary: 
The article discusses the challenges of combining Test-Time Adaptation (TTA) with language model rescoring in automatic speech recognition (ASR) systems. While TTA aims to improve ASR performance in real-world domain mismatches, it can interfere with language model rescoring. The proposed solution, SUTA-LM, extends the entropy-minimization-based TTA approach by incorporating a language model rescoring step. SUTA-LM employs a controlled adaptation process guided by an auto-step selection mechanism that considers both acoustic and linguistic information. Experiments conducted on 18 diverse ASR datasets demonstrate that SUTA-LM provides robust results across various domains. The combination of TTA and language model rescoring in SUTA-LM showcases an effective approach to improving ASR performance in challenging domain mismatches. 

<br /><br />Summary: <div>
arXiv:2506.11121v1 Announce Type: new 
Abstract: Despite progress in end-to-end ASR, real-world domain mismatches still cause performance drops, which Test-Time Adaptation (TTA) aims to mitigate by adjusting models during inference. Recent work explores combining TTA with external language models, using techniques like beam search rescoring or generative error correction. In this work, we identify a previously overlooked challenge: TTA can interfere with language model rescoring, revealing the nontrivial nature of effectively combining the two methods. Based on this insight, we propose SUTA-LM, a simple yet effective extension of SUTA, an entropy-minimization-based TTA approach, with language model rescoring. SUTA-LM first applies a controlled adaptation process guided by an auto-step selection mechanism leveraging both acoustic and linguistic information, followed by language model rescoring to refine the outputs. Experiments on 18 diverse ASR datasets show that SUTA-LM achieves robust results across a wide range of domains.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams</title>
<link>https://arxiv.org/abs/2506.11125</link>
<guid>https://arxiv.org/abs/2506.11125</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Voice phishing, Text-to-Speech, Automatic Speech Recognition, Adversarial perturbations

Summary:
Large Language Models (LLMs) are being used in voice phishing scams, with Automatic Speech Recognition (ASR) being a vulnerable point. ASRJam is a defense framework that disrupts attackers' ASR by injecting adversarial perturbations into victim audio. EchoGuard is a jammer that disrupts ASR using natural distortions like reverberation, while still being tolerable to humans. A user study compared EchoGuard with three state-of-the-art attacks, showing it to be the most effective in both disrupting ASR and maintaining a positive human listening experience. This proactive defense approach aims to break the scam feedback loop without hindering genuine callers' understanding of the conversation. <br /><br />Summary: Large Language Models combined with Text-to-Speech are used in voice phishing scams, with the vulnerable ASR being targeted by ASRJam and the EchoGuard jammer. A user study confirmed EchoGuard's effectiveness in disrupting ASR while providing a positive listening experience for humans, offering a proactive defense against voice phishing scams. <div>
arXiv:2506.11125v1 Announce Type: new 
Abstract: Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions</title>
<link>https://arxiv.org/abs/2506.11127</link>
<guid>https://arxiv.org/abs/2506.11127</guid>
<content:encoded><![CDATA[
<div> GUIRoboTron-Speech, autonomous agents, Graphical User Interfaces, speech instructions, training strategy<br />
Summary:<br />
Autonomous agents for Graphical User Interfaces (GUIs) are advancing human-computer interaction, with GUIRoboTron-Speech being the first agent to accept speech instructions and on-device screenshots. High-quality speech instructions were generated using a random timbre text-to-speech (TTS) model. GUIRoboTron-Speech's capabilities were developed through grounding and planning training stages, with a heuristic mixed-instruction training strategy utilized to address modality imbalance. Experiments on benchmark datasets confirmed the superior performance of GUIRoboTron-Speech, highlighting speech as an effective instruction modality for GUI agents. The code and datasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech. <br /><br /> <div>
arXiv:2506.11127v1 Announce Type: new 
Abstract: Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the first end-to-end autonomous GUI agent that directly accepts speech instructions and on-device screenshots to predict actions. Confronted with the scarcity of speech-based GUI agent datasets, we initially generated high-quality speech instructions for training by leveraging a random timbre text-to-speech (TTS) model to convert existing text instructions. We then develop GUIRoboTron-Speech's capabilities through progressive grounding and planning training stages. A key contribution is a heuristic mixed-instruction training strategy designed to mitigate the modality imbalance inherent in pre-trained foundation models. Comprehensive experiments on several benchmark datasets validate the robust and superior performance of GUIRoboTron-Speech, demonstrating the significant potential and widespread applicability of speech as an effective instruction modality for driving GUI agents. Our code and datasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger Language Models Produce More Human-Like Errors</title>
<link>https://arxiv.org/abs/2506.11128</link>
<guid>https://arxiv.org/abs/2506.11128</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning capabilities, human fallacies, logical correctness, cognitive framework

Summary: 
Language models, as they advance in sophistication, show improved reasoning capabilities. However, a study using the Erotetic Theory of Reasoning found that errors made by these models increasingly resemble common human reasoning fallacies. This inverse scaling phenomenon indicates a convergence towards human-like cognition, including biases and limitations. Despite the overall improvement in reasoning abilities, there is a shift in error patterns towards aligning with human fallacies as models become more advanced. This shift is independent of error rate and suggests that scaling language models may not naturally achieve normative rationality. The study also demonstrated order effects in language model reasoning, further supporting the idea of a convergence towards human-like cognitive patterns. <div>
arXiv:2506.11128v1 Announce Type: new 
Abstract: Do language models converge toward human-like reasoning patterns as they improve? We provide surprising evidence that while overall reasoning capabilities increase with model sophistication, the nature of errors increasingly mirrors predictable human reasoning fallacies: a previously unobserved inverse scaling phenomenon. To investigate this question, we apply the Erotetic Theory of Reasoning (ETR), a formal cognitive framework with empirical support for predicting human reasoning outcomes. Using the open-source package PyETR, we generate logical reasoning problems where humans predictably err, evaluating responses from 38 language models across 383 reasoning tasks. Our analysis indicates that as models advance in general capability (as measured by Chatbot Arena scores), the proportion of their incorrect answers that align with ETR-predicted human fallacies tends to increase ($\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation between model sophistication and logical correctness on these tasks, this shift in error patterns toward human-likeness occurs independently of error rate. These findings challenge the prevailing view that scaling language models naturally obtains normative rationality, suggesting instead a convergence toward human-like cognition inclusive of our characteristic biases and limitations, as we further confirm by demonstrating order-effects in language model reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK</title>
<link>https://arxiv.org/abs/2506.11129</link>
<guid>https://arxiv.org/abs/2506.11129</guid>
<content:encoded><![CDATA[
<div> framework, structured, classifier, hallucinations, healthcare
Summary:
1. The article introduces the CHECK framework, designed to reduce hallucination rates in large language models (LLMs) used in healthcare.
2. CHECK integrates structured clinical databases with a classifier grounded in information theory to detect factual and reasoning-based hallucinations.
3. Evaluated on 1500 questions from clinical trials, CHECK significantly reduced hallucination rates, making an open source model state of the art.
4. Its classifier showed strong generalization across medical benchmarks, achieving high AUC scores.
5. By leveraging hallucination probabilities to guide refinement, CHECK improved the USMLE passing rate of the GPT-4o model and offers a scalable solution for safe LLM deployment in high-stakes domains.<br /><br />Summary: <div>
arXiv:2506.11129v1 Announce Type: new 
Abstract: Large language models (LLMs) show promise in healthcare, but hallucinations remain a major barrier to clinical use. We present CHECK, a continuous-learning framework that integrates structured clinical databases with a classifier grounded in information theory to detect both factual and reasoning-based hallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials, CHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% - making an open source model state of the art. Its classifier generalized across medical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE) benchmark and HealthBench realistic multi-turn medical questioning. By leveraging hallucination probabilities to guide GPT-4o's refinement and judiciously escalate compute, CHECK boosted its USMLE passing rate by 5 percentage points, achieving a state-of-the-art 92.1%. By suppressing hallucinations below accepted clinical error thresholds, CHECK offers a scalable foundation for safe LLM deployment in medicine and other high-stakes domains.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data</title>
<link>https://arxiv.org/abs/2506.11130</link>
<guid>https://arxiv.org/abs/2506.11130</guid>
<content:encoded><![CDATA[
<div> Framework, ASR, unlabeled data, self-improvement, Mandarin <br />
Summary:<br />
The article proposes a self-refining framework to enhance Automatic Speech Recognition (ASR) performance using only unlabeled datasets. In this framework, an existing ASR model generates pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. The synthesized speech text pairs are then integrated back into the original ASR system, completing a closed-loop self-improvement cycle. The framework was tested on Taiwanese Mandarin speech with 6,000 hours of unlabeled data and synthetic content. The adapted model, Twister, showed a significant reduction in error rates on Mandarin and Mandarin-English code-switching benchmarks compared to the original Whisper model. This approach offers a practical pathway for improving ASR performance in low-resource or domain-specific settings. <br /> <div>
arXiv:2506.11130v1 Announce Type: new 
Abstract: We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models and Emergence: A Complex Systems Perspective</title>
<link>https://arxiv.org/abs/2506.11135</link>
<guid>https://arxiv.org/abs/2506.11135</guid>
<content:encoded><![CDATA[
<div> Keywords: Emergence, Complexity science, Large Language Models, Intelligence, Novel higher-level properties

Summary: 
Emergence in complexity science elucidates how systems exhibit new properties at higher levels, often described through replacement of high-dimensional mechanisms with lower-dimensional effective variables. This concept is exemplified by the notion "more is different," where novel characteristics arise when combined elements interact. Large Language Models (LLMs) have been examined for exhibiting emergent capabilities, prompting investigations into quantifying this phenomenon. Additionally, the concept of emergent intelligence, characterized by the adage "less is more," emphasizes the efficient utilization of emergent capabilities to solve problems with increased effectiveness and speed. The paper delves into whether LLMs possess emergent intelligence, highlighting the importance of analyzing the system's ability to harness emergent properties for intelligent problem-solving strategies. <div>
arXiv:2506.11135v1 Announce Type: new 
Abstract: Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea "more is different". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea "less is more". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.11137</link>
<guid>https://arxiv.org/abs/2506.11137</guid>
<content:encoded><![CDATA[
<div> medication extraction, medication status classification, electronic health records, large language models, discontinuation identification <br />
Summary:<br />
- Study evaluates capabilities of advanced open-sourced and proprietary large language models (LLMs) in medication extraction and status classification from EHR notes <br />
- GPT-4o had highest average F1 scores in all tasks under zero-shot setting <br />
- Open-sourced model Llama-3.1-70B-Instruct excelled in medication status classification and joint task on specific datasets <br />
- Medical-specific LLMs showed lower performance compared to general-domain LLMs <br />
- Few-shot learning improved performance, with CoT reasoning showing inconsistent gains <br />
- LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models providing scalable solutions and few-shot learning further enhancing LLMs' capabilities. <br /> 
Summary: <div>
arXiv:2506.11137v1 Announce Type: new 
Abstract: Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0% for medication extraction, 78.1% for discontinuation classification, and 72.7% for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs' capability.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?</title>
<link>https://arxiv.org/abs/2506.11243</link>
<guid>https://arxiv.org/abs/2506.11243</guid>
<content:encoded><![CDATA[
<div> small models, computational power, Global South, shared task, competitive 

Summary:
In this paper, the RETUYT-INCO participation in the BEA 2025 shared task is discussed. The decision to use small models with fewer than 1B parameters was made to reflect the limited access to computational power in the Global South. Despite this limitation, the models performed competitively with other teams in the shared task. The $exact\ F_1$ scores showed performance gaps ranging from 6.46 to 13.13 points compared to the winning teams. This demonstrates that models with a size smaller than 1B parameters can be competitive for tasks that do not require high computational resources, making them accessible for labs or institutions with low-budget GPUs or even without GPUs.  <br /><br />Summary: <div>
arXiv:2506.11243v1 Announce Type: new 
Abstract: In this paper, we present the RETUYT-INCO participation at the BEA 2025 shared task. Our participation was characterized by the decision of using relatively small models, with fewer than 1B parameters. This self-imposed restriction tries to represent the conditions in which many research labs or institutions are in the Global South, where computational power is not easily accessible due to its prohibitive cost. Even under this restrictive self-imposed setting, our models managed to stay competitive with the rest of teams that participated in the shared task. According to the $exact\ F_1$ scores published by the organizers, the performance gaps between our models and the winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in Track 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the minimum difference with a winner team is $6.46$ points -- and the maximum difference is $13.13$ -- according to the $exact\ F_1$ score, we find that models with a size smaller than 1B parameters are competitive for these tasks, all of which can be run on computers with a low-budget GPU or even without a GPU.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Multilingual Spectral Attribute Erasure</title>
<link>https://arxiv.org/abs/2506.11244</link>
<guid>https://arxiv.org/abs/2506.11244</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual representations, debiasing, IMSAE, spectral attribute erasure, cross-lingual approaches

Summary:
Multilingual representations aim to embed words with similar meanings into a common semantic space across multiple languages. Existing debiasing methods are limited to individual languages and do not utilize the potential for transferring debiasing effects between languages. The Iterative Multilingual Spectral Attribute Erasure (IMSAE) technique presented in this study identifies and mitigates joint bias subspaces across multiple languages through iterative SVD-based truncation. Evaluation across eight languages and five demographic dimensions demonstrates the effectiveness of IMSAE in both standard and zero-shot settings. The method leverages linguistically similar languages for debiasing even when target language data is unavailable. Comprehensive experiments with diverse language models, including BERT, LLaMA, and Mistral, show that IMSAE outperforms traditional monolingual and cross-lingual approaches while maintaining model utility. <br /><br />Summary: Multilingual representations are harnessed through IMSAE to jointly address biases across languages, demonstrating superior performance in debiasing and cross-lingual transfer tasks. <div>
arXiv:2506.11244v1 Announce Type: new 
Abstract: Multilingual representations embed words with similar meanings to share a common semantic space across languages, creating opportunities to transfer debiasing effects between languages. However, existing methods for debiasing are unable to exploit this opportunity because they operate on individual languages. We present Iterative Multilingual Spectral Attribute Erasure (IMSAE), which identifies and mitigates joint bias subspaces across multiple languages through iterative SVD-based truncation. Evaluating IMSAE across eight languages and five demographic dimensions, we demonstrate its effectiveness in both standard and zero-shot settings, where target language data is unavailable, but linguistically similar languages can be used for debiasing. Our comprehensive experiments across diverse language models (BERT, LLaMA, Mistral) show that IMSAE outperforms traditional monolingual and cross-lingual approaches while maintaining model utility.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning</title>
<link>https://arxiv.org/abs/2506.11246</link>
<guid>https://arxiv.org/abs/2506.11246</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Table Reasoning, Large Language Models, Prompting Techniques, SEAR, Table Structure Refactoring

Summary:
This research paper explores the challenge of Temporal Table Reasoning for Large Language Models (LLMs) and the impact of different prompting techniques on model performance. The study investigates various prompting methods across diverse table types and context structures to determine optimal approaches for different scenarios. Results show that performance varies based on entity type, table structure, additional context requirements, and question complexity, with no single method consistently outperforming others. To address these challenges, the researchers introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics. SEAR achieves superior performance across all table types compared to baseline prompting techniques. The study also examines the impact of table structure refactoring and finds that a unified representation enhances model reasoning. Overall, this work highlights the importance of effective prompting techniques and context adaptation in improving temporal table reasoning for Large Language Models.<br /><br />Summary: <div>
arXiv:2506.11246v1 Announce Type: new 
Abstract: Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective prompting techniques to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, the performance of these models varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique across diverse table types to determine optimal approaches for different scenarios. We find that performance varies based on entity type, table structure, requirement of additional context and question complexity, with NO single method consistently outperforming others. To mitigate these challenges, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics and integrates a structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to other baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model's reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Continue-Thinking Token for Enhanced Test-Time Scaling</title>
<link>https://arxiv.org/abs/2506.11274</link>
<guid>https://arxiv.org/abs/2506.11274</guid>
<content:encoded><![CDATA[
<div> test-time scaling, language model, deep learning, reinforcement learning, reasoning

Summary:
Test-time scaling is an effective method for enhancing language model performance by utilizing additional compute during inference. This study investigates the use of a learned "<|continue-thinking|>" token to trigger extended reasoning in a distilled DeepSeek-R1 model. By training only the token's embedding via reinforcement learning, the model achieves improved accuracy on standard math benchmarks compared to both the baseline model and a fixed token approach for budget forcing. The learned token outperforms the fixed-token method, particularly in cases where the fixed token enhances the base model's accuracy. For instance, on the GSM8K benchmark, the fixed-token approach produces a 1.3% accuracy improvement, while the learned-token method achieves a significant 4.2% improvement over the base model without budget forcing. <div>
arXiv:2506.11274v1 Announce Type: new 
Abstract: Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "" with "Wait") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model's accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning</title>
<link>https://arxiv.org/abs/2506.11300</link>
<guid>https://arxiv.org/abs/2506.11300</guid>
<content:encoded><![CDATA[
<div> Keywords: curriculum learning, language models, training efficiency, generalization, difficulty metrics

Summary: 
Curriculum learning was investigated for pretraining language models, with experiments conducted using various settings such as vanilla curriculum learning and pacing-based sampling. Six difficulty metrics were employed to guide the curricula, leading to improved convergence during early and mid-training phases. Notably, compression ratio, lexical diversity, and readability were identified as effective difficulty signals. The experiments demonstrated that curriculum learning can result in up to a 3.5% improvement when used as a warmup strategy. The study emphasizes the significance of data ordering in large-scale pretraining and provides valuable insights for developing scalable and data-efficient models under realistic training scenarios.<br /><br />Summary: <div>
arXiv:2506.11300v1 Announce Type: new 
Abstract: Curriculum learning has shown promise in improving training efficiency and generalization in various machine learning domains, yet its potential in pretraining language models remains underexplored, prompting our work as the first systematic investigation in this area. We experimented with different settings, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula-guided by six difficulty metrics spanning linguistic and information-theoretic perspectives. We train models under these settings and evaluate their performance on eight diverse benchmarks. Our experiments reveal that curriculum learning consistently improves convergence in early and mid-training phases, and can yield lasting gains when used as a warmup strategy with up to $3.5\%$ improvement. Notably, we identify compression ratio, lexical diversity, and readability as effective difficulty signals across settings. Our findings highlight the importance of data ordering in large-scale pretraining and provide actionable insights for scalable, data-efficient model development under realistic training scenarios.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Pay Attention</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
<div> Transformer, large language models, RNN-like architectures, Avey, long-range dependencies <br />
Summary: Avey is introduced as a new neural foundational architecture that aims to overcome the limitations of the Transformer model by breaking away from both attention and recurrence mechanisms. Avey consists of a ranker and an autoregressive neural processor that work together to identify and contextualize the most relevant tokens for a given token, irrespective of their position in the sequence. This decoupling of sequence length from context width allows Avey to effectively process arbitrarily long sequences, addressing the challenges faced by the Transformer in handling long-range dependencies. Experimental results demonstrate that Avey outperforms the Transformer on various standard short-range NLP benchmarks while excelling at capturing long-range dependencies. The proposed architecture presents a promising alternative to existing models for processing sequences and offers a potential solution to the limitations faced by current state-of-the-art language models. <br /><br />Summary: <div>
arXiv:2506.11305v1 Announce Type: new 
Abstract: The Transformer has become the de facto standard for large language models and a wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond a fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, a new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises a ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly</title>
<link>https://arxiv.org/abs/2506.11338</link>
<guid>https://arxiv.org/abs/2506.11338</guid>
<content:encoded><![CDATA[
arXiv:2506.11338v1 Announce Type: new 
Abstract: As Transformers become more widely incorporated into natural language processing tasks, there has been considerable interest in using surprisal from these models as predictors of human sentence processing difficulty. Recent work has observed a positive relationship between Transformer-based models' perplexity and the predictive power of their surprisal estimates on reading times, showing that language models with more parameters and trained on more data are less predictive of human reading times. However, these studies focus on predicting latency-based measures (i.e., self-paced reading times and eye-gaze durations) with surprisal estimates from Transformer-based language models. This trend has not been tested on brain imaging data. This study therefore evaluates the predictive power of surprisal estimates from 17 pre-trained Transformer-based models across three different language families on two functional magnetic resonance imaging datasets. Results show that the positive relationship between model perplexity and model fit still obtains, suggesting that this trend is not specific to latency-based measures and can be generalized to neural measures.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review</title>
<link>https://arxiv.org/abs/2506.11343</link>
<guid>https://arxiv.org/abs/2506.11343</guid>
<content:encoded><![CDATA[
arXiv:2506.11343v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models</title>
<link>https://arxiv.org/abs/2506.11344</link>
<guid>https://arxiv.org/abs/2506.11344</guid>
<content:encoded><![CDATA[
arXiv:2506.11344v1 Announce Type: new 
Abstract: We present a novel approach to Speaker Diarization (SD) by leveraging text-based methods focused on Sentence-level Speaker Change Detection within dialogues. Unlike audio-based SD systems, which are often challenged by audio quality and speaker similarity, our approach utilizes the dialogue transcript alone. Two models are developed: the Single Prediction Model (SPM) and the Multiple Prediction Model (MPM), both of which demonstrate significant improvements in identifying speaker changes, particularly in short conversations. Our findings, based on a curated dataset encompassing diverse conversational scenarios, reveal that the text-based SD approach, especially the MPM, performs competitively against state-of-the-art audio-based SD systems, with superior performance in short conversational contexts. This paper not only showcases the potential of leveraging linguistic features for SD but also highlights the importance of integrating semantic understanding into SD systems, opening avenues for future research in multimodal and semantic feature-based diarization.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Biased Samaritan: LLM biases in Perceived Kindness</title>
<link>https://arxiv.org/abs/2506.11361</link>
<guid>https://arxiv.org/abs/2506.11361</guid>
<content:encoded><![CDATA[
arXiv:2506.11361v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have become ubiquitous in many fields, understanding and mitigating LLM biases is an ongoing issue. This paper provides a novel method for evaluating the demographic biases of various generative AI models. By prompting models to assess a moral patient's willingness to intervene constructively, we aim to quantitatively evaluate different LLMs' biases towards various genders, races, and ages. Our work differs from existing work by aiming to determine the baseline demographic identities for various commercial models and the relationship between the baseline and other demographics. We strive to understand if these biases are positive, neutral, or negative, and the strength of these biases. This paper can contribute to the objective assessment of bias in Large Language Models and give the user or developer the power to account for these biases in LLM output or in training future LLMs. Our analysis suggested two key findings: that models view the baseline demographic as a white middle-aged or young adult male; however, a general trend across models suggested that non-baseline demographics are more willing to help than the baseline. These methodologies allowed us to distinguish these two biases that are often tangled together.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Approach for Mitigating Entity Bias in Relation Extraction</title>
<link>https://arxiv.org/abs/2506.11381</link>
<guid>https://arxiv.org/abs/2506.11381</guid>
<content:encoded><![CDATA[
arXiv:2506.11381v1 Announce Type: new 
Abstract: Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Guided Layer Scaling for Language Model Pretraining</title>
<link>https://arxiv.org/abs/2506.11389</link>
<guid>https://arxiv.org/abs/2506.11389</guid>
<content:encoded><![CDATA[
arXiv:2506.11389v1 Announce Type: new 
Abstract: As the cost of pretraining large language models grows, there is continued interest in strategies to improve learning efficiency during this core training stage. Motivated by cognitive development, where humans gradually build knowledge as their brains mature, we propose Curriculum-Guided Layer Scaling (CGLS), a framework for compute-efficient pretraining that synchronizes increasing data difficulty with model growth through progressive layer stacking (i.e. gradually adding layers during training). At the 100M parameter scale, using a curriculum transitioning from synthetic short stories to general web data, CGLS outperforms baseline methods on the question-answering benchmarks PIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus with a DistilBERT-based classifier and progress from general text to highly technical or specialized content. Our results show that progressively increasing model depth alongside sample difficulty leads to better generalization and zero-shot performance on various downstream benchmarks. Altogether, our findings demonstrate that CGLS unlocks the potential of progressive stacking, offering a simple yet effective strategy for improving generalization on knowledge-intensive and reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Early-Onset Colorectal Cancer with Large Language Models</title>
<link>https://arxiv.org/abs/2506.11410</link>
<guid>https://arxiv.org/abs/2506.11410</guid>
<content:encoded><![CDATA[
arXiv:2506.11410v1 Announce Type: new 
Abstract: The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has increased every year, but this population is younger than the recommended age established by national guidelines for cancer screening. In this paper, we applied 10 different machine learning models to predict EoCRC, and compared their performance with advanced large language models (LLM), using patient conditions, lab results, and observations within 6 months of patient journey prior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients from multiple health systems across the United States. The results demonstrated that the fine-tuned LLM achieved an average of 73% sensitivity and 91% specificity.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Long-Context LLM Inference via KV Cache Clustering</title>
<link>https://arxiv.org/abs/2506.11418</link>
<guid>https://arxiv.org/abs/2506.11418</guid>
<content:encoded><![CDATA[
arXiv:2506.11418v1 Announce Type: new 
Abstract: Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards</title>
<link>https://arxiv.org/abs/2506.11425</link>
<guid>https://arxiv.org/abs/2506.11425</guid>
<content:encoded><![CDATA[
arXiv:2506.11425v1 Announce Type: new 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent's errors and environmental interactions, emulate a teacher's guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models</title>
<link>https://arxiv.org/abs/2506.11432</link>
<guid>https://arxiv.org/abs/2506.11432</guid>
<content:encoded><![CDATA[
arXiv:2506.11432v1 Announce Type: new 
Abstract: This research introduces KoGEC, a Korean Grammatical Error Correction system using pre\--trained translation models. We fine-tuned NLLB (No Language Left Behind) models for Korean GEC, comparing their performance against large language models like GPT-4 and HCX-3. The study used two social media conversation datasets for training and testing. The NLLB models were fine-tuned using special language tokens to distinguish between original and corrected Korean sentences. Evaluation was done using BLEU scores and an "LLM as judge" method to classify error types. Results showed that the fine-tuned NLLB (KoGEC) models outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a more balanced error correction profile across various error types, whereas the larger LLMs tended to focus less on punctuation errors. We also developed a Chrome extension to make the KoGEC system accessible to users. Finally, we explored token vocabulary expansion to further improve the model but found it to decrease model performance. This research contributes to the field of NLP by providing an efficient, specialized Korean GEC system and a new evaluation method. It also highlights the potential of compact, task-specific models to compete with larger, general-purpose language models in specialized NLP tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AbsenceBench: Language Models Can't Tell What's Missing</title>
<link>https://arxiv.org/abs/2506.11440</link>
<guid>https://arxiv.org/abs/2506.11440</guid>
<content:encoded><![CDATA[
arXiv:2506.11440v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly capable of processing long inputs and locating specific information within them, as evidenced by their performance on the Needle in a Haystack (NIAH) test. However, while models excel at recalling surprising information, they still struggle to identify clearly omitted information. We introduce AbsenceBench to assesses LLMs' capacity to detect missing information across three domains: numerical sequences, poetry, and GitHub pull requests. AbsenceBench asks models to identify which pieces of a document were deliberately removed, given access to both the original and edited contexts. Despite the apparent straightforwardness of these tasks, our experiments reveal that even state-of-the-art models like Claude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context length of 5K tokens. Our analysis suggests this poor performance stems from a fundamental limitation: Transformer attention mechanisms cannot easily attend to "gaps" in documents since these absences don't correspond to any specific keys that can be attended to. Overall, our results and analysis provide a case study of the close proximity of tasks where models are already superhuman (NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems</title>
<link>https://arxiv.org/abs/2506.11467</link>
<guid>https://arxiv.org/abs/2506.11467</guid>
<content:encoded><![CDATA[
arXiv:2506.11467v1 Announce Type: new 
Abstract: Human evaluators provide necessary contributions in evaluating large language models. In the context of Machine Translation (MT) systems for low-resource languages (LRLs), this is made even more apparent since popular automated metrics tend to be string-based, and therefore do not provide a full picture of the nuances of the behavior of the system. Human evaluators, when equipped with the necessary expertise of the language, will be able to test for adequacy, fluency, and other important metrics. However, the low resource nature of the language means that both datasets and evaluators are in short supply. This presents the following conundrum: How can developers of MT systems for these LRLs find adequate human evaluators and datasets? This paper first presents a comprehensive review of existing evaluation procedures, with the objective of producing a design proposal for a platform that addresses the resource gap in terms of datasets and evaluators in developing MT systems. The result is a design for a recruitment and gamified evaluation platform for developers of MT systems. Challenges are also discussed in terms of evaluating this platform, as well as its possible applications in the wider scope of Natural Language Processing (NLP) research.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards</title>
<link>https://arxiv.org/abs/2506.11474</link>
<guid>https://arxiv.org/abs/2506.11474</guid>
<content:encoded><![CDATA[
arXiv:2506.11474v1 Announce Type: new 
Abstract: Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImmunoFOMO: Are Language Models missing what oncologists see?</title>
<link>https://arxiv.org/abs/2506.11478</link>
<guid>https://arxiv.org/abs/2506.11478</guid>
<content:encoded><![CDATA[
arXiv:2506.11478v1 Announce Type: new 
Abstract: Language models (LMs) capabilities have grown with a fast pace over the past decade leading researchers in various disciplines, such as biomedical research, to increasingly explore the utility of LMs in their day-to-day applications. Domain specific language models have already been in use for biomedical natural language processing (NLP) applications. Recently however, the interest has grown towards medical language models and their understanding capabilities. In this paper, we investigate the medical conceptual grounding of various language models against expert clinicians for identification of hallmarks of immunotherapy in breast cancer abstracts. Our results show that pre-trained language models have potential to outperform large language models in identifying very specific (low-level) concepts.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models</title>
<link>https://arxiv.org/abs/2506.11485</link>
<guid>https://arxiv.org/abs/2506.11485</guid>
<content:encoded><![CDATA[
arXiv:2506.11485v1 Announce Type: new 
Abstract: While large language models like BERT demonstrate strong empirical performance on semantic tasks, whether this reflects true conceptual competence or surface-level statistical association remains unclear. I investigate whether BERT encodes abstract relational schemata by examining internal representations of concept pairs across taxonomic, mereological, and functional relations. I compare BERT's relational classification performance with representational structure in [CLS] token embeddings. Results reveal that pretrained BERT enables high classification accuracy, indicating latent relational signals. However, concept pairs organize by relation type in high-dimensional embedding space only after fine-tuning on supervised relation classification tasks. This indicates relational schemata are not emergent from pretraining alone but can be induced via task scaffolding. These findings demonstrate that behavioral performance does not necessarily imply structured conceptual understanding, though models can acquire inductive biases for grounded relational abstraction through appropriate training.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lag-Relative Sparse Attention In Long Context Training</title>
<link>https://arxiv.org/abs/2506.11498</link>
<guid>https://arxiv.org/abs/2506.11498</guid>
<content:encoded><![CDATA[
arXiv:2506.11498v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made significant strides in natural language processing and generation, yet their ability to handle long-context input remains constrained by the quadratic complexity of attention computation and linear-increasing key-value memory footprint. To reduce computational costs and memory, key-value cache compression techniques are commonly applied at inference time, but this often leads to severe performance degradation, as models are not trained to handle compressed context. Although there are more sophisticated compression methods, they are typically unsuitable for post-training because of their incompatibility with gradient-based optimization or high computation overhead. To fill this gap with no additional parameter and little computation overhead, we propose Lag-Relative Sparse Attention(LRSA) anchored by the LagKV compression method for long context post-training. Our method performs chunk-by-chunk prefilling, which selects the top K most relevant key-value pairs in a fixed-size lagging window, allowing the model to focus on salient historical context while maintaining efficiency. Experimental results show that our approach significantly enhances the robustness of the LLM with key-value compression and achieves better fine-tuned results in the question-answer tuning task.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval</title>
<link>https://arxiv.org/abs/2506.11499</link>
<guid>https://arxiv.org/abs/2506.11499</guid>
<content:encoded><![CDATA[
arXiv:2506.11499v1 Announce Type: new 
Abstract: Multimodal chatbots have become one of the major topics for dialogue systems in both research community and industry. Recently, researchers have shed light on the multimodality of responses as well as dialogue contexts. This work explores how a dialogue system can output responses in various modalities such as text and image. To this end, we first formulate a multimodal dialogue response retrieval task for retrieval-based systems as the combination of three subtasks. We then propose three integration methods based on a two-step approach and an end-to-end approach, and compare the merits and demerits of each method. Experimental results on two datasets demonstrate that the end-to-end approach achieves comparable performance without an intermediate step in the two-step approach. In addition, a parameter sharing strategy not only reduces the number of parameters but also boosts performance by transferring knowledge across the subtasks and the modalities.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation</title>
<link>https://arxiv.org/abs/2506.11557</link>
<guid>https://arxiv.org/abs/2506.11557</guid>
<content:encoded><![CDATA[
arXiv:2506.11557v1 Announce Type: new 
Abstract: In dialogue generation, the naturalness of responses is crucial for effective human-machine interaction. Personalized response generation poses even greater challenges, as the responses must remain coherent and consistent with the user's personal traits or persona descriptions. We propose MUDI ($\textbf{Mu}$ltiple $\textbf{Di}$scourse Relations Graph Learning) for personalized dialogue generation. We utilize a Large Language Model to assist in annotating discourse relations and to transform dialogue data into structured dialogue graphs. Our graph encoder, the proposed DialogueGAT model, then captures implicit discourse relations within this structure, along with persona descriptions. During the personalized response generation phase, novel coherence-aware attention strategies are implemented to enhance the decoder's consideration of discourse relations. Our experiments demonstrate significant improvements in the quality of personalized responses, thus resembling human-like dialogue exchanges.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs Good Text Diacritizers? An Arabic and Yor\`ub\'a Case Study</title>
<link>https://arxiv.org/abs/2506.11602</link>
<guid>https://arxiv.org/abs/2506.11602</guid>
<content:encoded><![CDATA[
arXiv:2506.11602v1 Announce Type: new 
Abstract: We investigate the effectiveness of large language models (LLMs) for text diacritization in two typologically distinct languages: Arabic and Yoruba. To enable a rigorous evaluation, we introduce a novel multilingual dataset MultiDiac, with diverse samples that capture a range of diacritic ambiguities. We evaluate 14 LLMs varying in size, accessibility, and language coverage, and benchmark them against 6 specialized diacritization models. Additionally, we fine-tune four small open-source models using LoRA for Yoruba. Our results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneGram: Conceptualizing and Describing Tangrams in Scene Context</title>
<link>https://arxiv.org/abs/2506.11631</link>
<guid>https://arxiv.org/abs/2506.11631</guid>
<content:encoded><![CDATA[
arXiv:2506.11631v1 Announce Type: new 
Abstract: Research on reference and naming suggests that humans can come up with very different ways of conceptualizing and referring to the same object, e.g. the same abstract tangram shape can be a "crab", "sink" or "space ship". Another common assumption in cognitive science is that scene context fundamentally shapes our visual perception of objects and conceptual expectations. This paper contributes SceneGram, a dataset of human references to tangram shapes placed in different scene contexts, allowing for systematic analyses of the effect of scene context on conceptualization. Based on this data, we analyze references to tangram shapes generated by multimodal LLMs, showing that these models do not account for the richness and variability of conceptualizations found in human references.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Gen: Specializing Large Language Model via Online LoRA Generation</title>
<link>https://arxiv.org/abs/2506.11638</link>
<guid>https://arxiv.org/abs/2506.11638</guid>
<content:encoded><![CDATA[
arXiv:2506.11638v1 Announce Type: new 
Abstract: Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Converting Annotated Clinical Cases into Structured Case Report Forms</title>
<link>https://arxiv.org/abs/2506.11666</link>
<guid>https://arxiv.org/abs/2506.11666</guid>
<content:encoded><![CDATA[
arXiv:2506.11666v1 Announce Type: new 
Abstract: Case Report Forms (CRFs) are largely used in medical research as they ensure accuracy, reliability, and validity of results in clinical studies. However, publicly available, wellannotated CRF datasets are scarce, limiting the development of CRF slot filling systems able to fill in a CRF from clinical notes. To mitigate the scarcity of CRF datasets, we propose to take advantage of available datasets annotated for information extraction tasks and to convert them into structured CRFs. We present a semi-automatic conversion methodology, which has been applied to the E3C dataset in two languages (English and Italian), resulting in a new, high-quality dataset for CRF slot filling. Through several experiments on the created dataset, we report that slot filling achieves 59.7% for Italian and 67.3% for English on a closed Large Language Models (zero-shot) and worse performances on three families of open-source models, showing that filling CRFs is challenging even for recent state-of-the-art LLMs. We release the datest at https://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE</title>
<link>https://arxiv.org/abs/2506.11673</link>
<guid>https://arxiv.org/abs/2506.11673</guid>
<content:encoded><![CDATA[
arXiv:2506.11673v1 Announce Type: new 
Abstract: Amnesic probing is a technique used to examine the influence of specific linguistic information on the behaviour of a model. This involves identifying and removing the relevant information and then assessing whether the model's performance on the main task changes. If the removed information is relevant, the model's performance should decline. The difficulty with this approach lies in removing only the target information while leaving other information unchanged. It has been shown that Iterative Nullspace Projection (INLP), a widely used removal technique, introduces random modifications to representations when eliminating target information. We demonstrate that Mean Projection (MP) and LEACE, two proposed alternatives, remove information in a more targeted manner, thereby enhancing the potential for obtaining behavioural explanations through Amnesic Probing.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach</title>
<link>https://arxiv.org/abs/2506.11681</link>
<guid>https://arxiv.org/abs/2506.11681</guid>
<content:encoded><![CDATA[
arXiv:2506.11681v1 Announce Type: new 
Abstract: This paper addresses the challenge of transforming complex sentences into sequences of logical, simplified sentences while preserving semantic and logical integrity with the help of Large Language Models. We propose a hybrid approach that combines advanced prompting with multi-agent architectures to enhance the sentence simplification process. Experimental results show that our approach was able to successfully simplify 70% of the complex sentences written for video game design application. In comparison, a single-agent approach attained a 48% success rate on the same task.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Configurable Preference Tuning with Rubric-Guided Synthetic Data</title>
<link>https://arxiv.org/abs/2506.11702</link>
<guid>https://arxiv.org/abs/2506.11702</guid>
<content:encoded><![CDATA[
arXiv:2506.11702v1 Announce Type: new 
Abstract: Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference</title>
<link>https://arxiv.org/abs/2506.11728</link>
<guid>https://arxiv.org/abs/2506.11728</guid>
<content:encoded><![CDATA[
arXiv:2506.11728v1 Announce Type: new 
Abstract: Recent advances in deep learning (DL) have led to a shift from traditional 64-bit floating point (FP64) computations toward reduced-precision formats, such as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision arithmetic. This transition enhances computational throughput, reduces memory and bandwidth usage, and improves energy efficiency, offering significant advantages for resource-constrained edge devices. To support this shift, hardware architectures have evolved accordingly, now including adapted ISAs (Instruction Set Architectures) that expose mixed-precision vector units and matrix engines tailored for DL workloads. At the heart of many DL and scientific computing tasks is the general matrix-matrix multiplication gemm, a fundamental kernel historically optimized using axpy vector instructions on SIMD (single instruction, multiple data) units. However, as hardware moves toward mixed-precision dot-product-centric operations optimized for quantized inference, these legacy approaches are being phased out. In response to this, our paper revisits traditional high-performance gemm and describes strategies for adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs, including x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel designs and data layouts that better exploit today's specialized hardware and demonstrate significant performance gains from MIP arithmetic over floating-point implementations across three representative CPU architectures. These contributions highlight a new era of gemm optimization-driven by the demands of DL inference on heterogeneous architectures, marking what we term as the "Cambrian period" for matrix multiplication.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DART: Distilling Autoregressive Reasoning to Silent Thought</title>
<link>https://arxiv.org/abs/2506.11752</link>
<guid>https://arxiv.org/abs/2506.11752</guid>
<content:encoded><![CDATA[
arXiv:2506.11752v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \textbf{DART} (\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent \textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART achieves comparable reasoning performance to existing baselines while offering significant efficiency gains, serving as a feasible alternative for efficient reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents</title>
<link>https://arxiv.org/abs/2506.11763</link>
<guid>https://arxiv.org/abs/2506.11763</guid>
<content:encoded><![CDATA[
arXiv:2506.11763v1 Announce Type: new 
Abstract: Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Short Alignment for Effective Long-Context Modeling in LLMs</title>
<link>https://arxiv.org/abs/2506.11769</link>
<guid>https://arxiv.org/abs/2506.11769</guid>
<content:encoded><![CDATA[
arXiv:2506.11769v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited impressive performance and surprising emergent properties. However, their effectiveness remains limited by the fixed context window of the transformer architecture, posing challenges for long-context modeling. Among these challenges, length generalization -- the ability to generalize to sequences longer than those seen during training -- is a classical and fundamental problem. In this work, we propose a fresh perspective on length generalization, shifting the focus from the conventional emphasis on input features such as positional encodings or data structures to the output distribution of the model. Specifically, through case studies on synthetic tasks, we highlight the critical role of \textbf{long-short alignment} -- the consistency of output distributions across sequences of varying lengths. Extending this insight to natural language tasks, we propose a metric called Long-Short Misalignment to quantify this phenomenon, uncovering a strong correlation between the metric and length generalization performance. Building on these findings, we develop a regularization term that promotes long-short alignment during training. Extensive experiments validate the effectiveness of our approach, offering new insights for achieving more effective long-context modeling in LLMs. Code is available at https://github.com/PKU-ML/LongShortAlignment.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models</title>
<link>https://arxiv.org/abs/2506.11798</link>
<guid>https://arxiv.org/abs/2506.11798</guid>
<content:encoded><![CDATA[
arXiv:2506.11798v1 Announce Type: new 
Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?</title>
<link>https://arxiv.org/abs/2506.11807</link>
<guid>https://arxiv.org/abs/2506.11807</guid>
<content:encoded><![CDATA[
arXiv:2506.11807v1 Announce Type: new 
Abstract: We investigate the linguistic abilities of multimodal large language models in reference resolution tasks featuring simple yet abstract visual stimuli, such as color patches and color grids. Although the task may not seem challenging for today's language models, being straightforward for human dyads, we consider it to be a highly relevant probe of the pragmatic capabilities of MLLMs. Our results and analyses indeed suggest that basic pragmatic capabilities, such as context-dependent interpretation of color descriptions, still constitute major challenges for state-of-the-art MLLMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post Persona Alignment for Multi-Session Dialogue Generation</title>
<link>https://arxiv.org/abs/2506.11857</link>
<guid>https://arxiv.org/abs/2506.11857</guid>
<content:encoded><![CDATA[
arXiv:2506.11857v1 Announce Type: new 
Abstract: Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speaker's persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache</title>
<link>https://arxiv.org/abs/2506.11886</link>
<guid>https://arxiv.org/abs/2506.11886</guid>
<content:encoded><![CDATA[
arXiv:2506.11886v1 Announce Type: new 
Abstract: Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeistBERT: Breathing Life into German NLP</title>
<link>https://arxiv.org/abs/2506.11903</link>
<guid>https://arxiv.org/abs/2506.11903</guid>
<content:encoded><![CDATA[
arXiv:2506.11903v1 Announce Type: new 
Abstract: Advances in transformer-based language models have highlighted the benefits of language-specific pre-training on high-quality corpora. In this context, German NLP stands to gain from updated architectures and modern datasets tailored to the linguistic characteristics of the German language. GeistBERT seeks to improve German language processing by incrementally training on a diverse corpus and optimizing model performance across various NLP tasks. It was pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus using Whole Word Masking (WWM). Based on the pre-trained model, we derived extended-input variants using Nystr\"omformer and Longformer architectures with support for sequences up to 8k tokens. While these long-context models were not evaluated on dedicated long-context benchmarks, they are included in our release. We assessed all models on NER (CoNLL 2003, GermEval 2014) and text classification (GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The GeistBERT models achieved strong performance, leading all tasks among the base models and setting a new state-of-the-art (SOTA). Notably, the base models outperformed larger models in several tasks. To support the German NLP research community, we are releasing GeistBERT under the MIT license.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study</title>
<link>https://arxiv.org/abs/2506.11919</link>
<guid>https://arxiv.org/abs/2506.11919</guid>
<content:encoded><![CDATA[
arXiv:2506.11919v1 Announce Type: new 
Abstract: Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS), yet defining the criteria to assess its effectiveness remains an open challenge. We propose a novel computational framework for CS effectiveness classification, grounded in social science concepts. Our framework defines six core dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience Adaptation, and Fairness - which we use to annotate 4,214 CS instances from two benchmark datasets, resulting in a novel linguistic resource released to the community. In addition, we propose two classification strategies, multi-task and dependency-based, achieving strong results (0.94 and 0.96 average F1 respectively on both expert- and user-written CS), outperforming standard baselines, and revealing strong interdependence among dimensions.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback</title>
<link>https://arxiv.org/abs/2506.11930</link>
<guid>https://arxiv.org/abs/2506.11930</guid>
<content:encoded><![CDATA[
arXiv:2506.11930v1 Announce Type: new 
Abstract: Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Large Language Model Safety with Contrastive Representation Learning</title>
<link>https://arxiv.org/abs/2506.11938</link>
<guid>https://arxiv.org/abs/2506.11938</guid>
<content:encoded><![CDATA[
arXiv:2506.11938v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at https://github.com/samuelsimko/crl-llm-defense
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>code_transformed: The Influence of Large Language Models on Code</title>
<link>https://arxiv.org/abs/2506.12014</link>
<guid>https://arxiv.org/abs/2506.12014</guid>
<content:encoded><![CDATA[
arXiv:2506.12014v1 Announce Type: new 
Abstract: Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing a Dyslexia Indicator Using Eye Tracking</title>
<link>https://arxiv.org/abs/2506.11004</link>
<guid>https://arxiv.org/abs/2506.11004</guid>
<content:encoded><![CDATA[
arXiv:2506.11004v1 Announce Type: cross 
Abstract: Dyslexia, affecting an estimated 10% to 20% of the global population, significantly impairs learning capabilities, highlighting the need for innovative and accessible diagnostic methods. This paper investigates the effectiveness of eye-tracking technology combined with machine learning algorithms as a cost-effective alternative for early dyslexia detection. By analyzing general eye movement patterns, including prolonged fixation durations and erratic saccades, we proposed an enhanced solution for determining eye-tracking-based dyslexia features. A Random Forest Classifier was then employed to detect dyslexia, achieving an accuracy of 88.58\%. Additionally, hierarchical clustering methods were applied to identify varying severity levels of dyslexia. The analysis incorporates diverse methodologies across various populations and settings, demonstrating the potential of this technology to identify individuals with dyslexia, including those with borderline traits, through non-invasive means. Integrating eye-tracking with machine learning represents a significant advancement in the diagnostic process, offering a highly accurate and accessible method in clinical research.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects</title>
<link>https://arxiv.org/abs/2506.11012</link>
<guid>https://arxiv.org/abs/2506.11012</guid>
<content:encoded><![CDATA[
arXiv:2506.11012v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring and leveraging diverse real-world knowledge, which serve as a fundamental technology for enabling cognitive intelligence systems with advanced understanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims to infer new knowledge based on existing facts in KGs, playing a crucial role in applications such as public security intelligence, intelligent healthcare, and financial risk assessment. From a task-centric perspective, existing KGR approaches can be broadly classified into static single-step KGR, static multi-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR. While existing surveys have covered these six types of KGR tasks, a comprehensive review that systematically summarizes all KGR tasks particularly including downstream applications and more challenging reasoning paradigms remains lacking. In contrast to previous works, this survey provides a more comprehensive perspective on the research of KGR by categorizing approaches based on primary reasoning tasks, downstream application tasks, and potential challenging reasoning tasks. Besides, we explore advanced techniques, such as large language models (LLMs), and their impact on KGR. This work aims to highlight key research trends and outline promising future directions in the field of KGR.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox</title>
<link>https://arxiv.org/abs/2506.11022</link>
<guid>https://arxiv.org/abs/2506.11022</guid>
<content:encoded><![CDATA[
arXiv:2506.11022v1 Announce Type: cross 
Abstract: The rapid adoption of Large Language Models(LLMs) for code generation has transformed software development, yet little attention has been given to how security vulnerabilities evolve through iterative LLM feedback. This paper analyzes security degradation in AI-generated code through a controlled experiment with 400 code samples across 40 rounds of "improvements" using four distinct prompting strategies. Our findings show a 37.6% increase in critical vulnerabilities after just five iterations, with distinct vulnerability patterns emerging across different prompting approaches. This evidence challenges the assumption that iterative LLM refinement improves code security and highlights the essential role of human expertise in the loop. We propose practical guidelines for developers to mitigate these risks, emphasizing the need for robust human validation between LLM iterations to prevent the paradoxical introduction of new security issues during supposedly beneficial code "improvements".
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.11031</link>
<guid>https://arxiv.org/abs/2506.11031</guid>
<content:encoded><![CDATA[
arXiv:2506.11031v1 Announce Type: cross 
Abstract: As image generators produce increasingly realistic images, concerns about potential misuse continue to grow. Supervised detection relies on large, curated datasets and struggles to generalize across diverse generators. In this work, we investigate the use of pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit some task-specific reasoning and chain-of-thought prompting offers gains, we show that task-aligned prompting elicits more focused reasoning and significantly improves performance without fine-tuning. Specifically, prefixing the model's response with the phrase ``Let's examine the style and the synthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores by 8%-29% for two widely used open-source models. These gains are consistent across three recent, diverse datasets spanning human faces, objects, and animals with images generated by 16 different models -- demonstrating strong generalization. We further evaluate the approach across three additional model sizes and observe improvements in most dataset-model combinations -- suggesting robustness to model scale. Surprisingly, self-consistency, a behavior previously observed in language reasoning, where aggregating answers from diverse reasoning paths improves performance, also holds in this setting. Even here, zero-shot-s$^2$ scales better than chain-of-thought in most cases -- indicating that it elicits more useful diversity. Our findings show that task-aligned prompts elicit more focused reasoning and enhance latent capabilities in VLMs, like the detection of AI-generated images -- offering a simple, generalizable, and explainable alternative to supervised methods. Our code is publicly available on github: https://github.com/osome-iu/Zero-shot-s2.git.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.11034</link>
<guid>https://arxiv.org/abs/2506.11034</guid>
<content:encoded><![CDATA[
arXiv:2506.11034v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable ability in various language tasks, especially with their emergent in-context learning capability. Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). Despite increasing interest in the utility of LLMs in causal reasoning tasks such as causal discovery and counterfactual reasoning, there has been relatively little work showcasing the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. Our CausalVLBench encompasses three representative tasks: causal structure inference, intervention target prediction, and counterfactual prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our causal reasoning tasks across three causal representation learning datasets and demonstrate their fundamental strengths and weaknesses. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilities of LVLMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity</title>
<link>https://arxiv.org/abs/2506.11035</link>
<guid>https://arxiv.org/abs/2506.11035</guid>
<content:encoded><![CDATA[
arXiv:2506.11035v1 Announce Type: cross 
Abstract: Work in psychology has highlighted that the geometric model of similarity standard in deep learning is not psychologically plausible because its metric properties such as symmetry do not align with human perception. In contrast, Tversky (1977) proposed an axiomatic theory of similarity based on a representation of objects as sets of features, and their similarity as a function of common and distinctive features. However, this model has not been used in deep learning before, partly due to the challenge of incorporating discrete set operations. We develop a differentiable parameterization of Tversky's similarity that is learnable through gradient descent, and derive neural network building blocks such as the Tversky projection layer, which unlike the linear projection layer can model non-linear functions such as XOR. Through experiments with image recognition and language modeling, we show that the Tversky projection layer is a beneficial replacement for the linear projection layer, which employs geometric similarity. On the NABirds image classification task, a frozen ResNet-50 adapted with a Tversky projection layer achieves a 24.7% relative accuracy improvement over the linear layer adapter baseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases by 7.5%, and its parameter count by 34.8%. Finally, we propose a unified interpretation of both projection layers as computing similarities of input stimuli to learned prototypes, for which we also propose a novel visualization technique highlighting the interpretability of Tversky projection layers. Our work offers a new paradigm for thinking about the similarity model implicit in deep learning, and designing networks that are interpretable under an established theory of psychological similarity.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language models for Time Series Analysis: Techniques, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2506.11040</link>
<guid>https://arxiv.org/abs/2506.11040</guid>
<content:encoded><![CDATA[
arXiv:2506.11040v1 Announce Type: cross 
Abstract: Time series analysis is pivotal in domains like financial forecasting and biomedical monitoring, yet traditional methods are constrained by limited nonlinear feature representation and long-term dependency capture. The emergence of Large Language Models (LLMs) offers transformative potential by leveraging their cross-modal knowledge integration and inherent attention mechanisms for time series analysis. However, the development of general-purpose LLMs for time series from scratch is still hindered by data diversity, annotation scarcity, and computational requirements. This paper presents a systematic review of pre-trained LLM-driven time series analysis, focusing on enabling techniques, potential applications, and open challenges. First, it establishes an evolutionary roadmap of AI-driven time series analysis, from the early machine learning era, through the emerging LLM-driven paradigm, to the development of native temporal foundation models. Second, it organizes and systematizes the technical landscape of LLM-driven time series analysis from a workflow perspective, covering LLMs' input, optimization, and lightweight stages. Finally, it critically examines novel real-world applications and highlights key open challenges that can guide future research and innovation. The work not only provides valuable insights into current advances but also outlines promising directions for future development. It serves as a foundational reference for both academic and industrial researchers, paving the way for the development of more efficient, generalizable, and interpretable systems of LLM-driven time series analysis.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs</title>
<link>https://arxiv.org/abs/2506.11059</link>
<guid>https://arxiv.org/abs/2506.11059</guid>
<content:encoded><![CDATA[
arXiv:2506.11059v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become integral to modern software development, producing vast amounts of AI-generated source code. While these models boost programming productivity, their misuse introduces critical risks, including code plagiarism, license violations, and the propagation of insecure programs. As a result, robust detection of AI-generated code is essential. To support the development of such detectors, a comprehensive benchmark that reflects real-world conditions is crucial. However, existing benchmarks fall short -- most cover only a limited set of programming languages and rely on less capable generative models. In this paper, we present CodeMirage, a comprehensive benchmark that addresses these limitations through three major advancements: (1) it spans ten widely used programming languages, (2) includes both original and paraphrased code samples, and (3) incorporates outputs from ten state-of-the-art production-level LLMs, including both reasoning and non-reasoning models from six major providers. Using CodeMirage, we evaluate ten representative detectors across four methodological paradigms under four realistic evaluation configurations, reporting results using three complementary metrics. Our analysis reveals nine key findings that uncover the strengths and weaknesses of current detectors, and identify critical challenges for future work. We believe CodeMirage offers a rigorous and practical testbed to advance the development of robust and generalizable AI-generated code detectors.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding</title>
<link>https://arxiv.org/abs/2506.11064</link>
<guid>https://arxiv.org/abs/2506.11064</guid>
<content:encoded><![CDATA[
arXiv:2506.11064v1 Announce Type: cross 
Abstract: End-to-end automatic speech recognition (ASR) models often struggle to accurately recognize rare words. Previously, we introduced an ASR postprocessing method called error detection and context-aware error correction (ED-CEC), which leverages contextual information such as named entities and technical terms to improve the accuracy of ASR transcripts. Although ED-CEC achieves a notable success in correcting rare words, its accuracy remains low when dealing with rare words that have similar pronunciations but different spellings. To address this issue, we proposed a phoneme-augmented multimodal fusion method for context-aware error correction (PMF-CEC) method on the basis of ED-CEC, which allowed for better differentiation between target rare words and homophones. Additionally, we observed that the previous ASR error detection module suffers from overdetection. To mitigate this, we introduced a retention probability mechanism to filter out editing operations with confidence scores below a set threshold, preserving the original operation to improve error detection accuracy. Experiments conducted on five datasets demonstrated that our proposed PMF-CEC maintains reasonable inference speed while further reducing the biased word error rate compared with ED-CEC, showing a stronger advantage in correcting homophones. Moreover, our method outperforms other contextual biasing methods, and remains valuable compared with LLM-based methods in terms of faster inference and better robustness under large biasing lists.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition</title>
<link>https://arxiv.org/abs/2506.11069</link>
<guid>https://arxiv.org/abs/2506.11069</guid>
<content:encoded><![CDATA[
arXiv:2506.11069v1 Announce Type: cross 
Abstract: Accurate recognition of dysarthric and elderly speech remains challenging to date. While privacy concerns have driven a shift from centralized approaches to federated learning (FL) to ensure data confidentiality, this further exacerbates the challenges of data scarcity, imbalanced data distribution and speaker heterogeneity. To this end, this paper conducts a systematic investigation of regularized FL techniques for privacy-preserving dysarthric and elderly speech recognition, addressing different levels of the FL process by 1) parameter-based, 2) embedding-based and 3) novel loss-based regularization. Experiments on the benchmark UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest that regularized FL systems consistently outperform the baseline FedAvg system by statistically significant WER reductions of up to 0.55\% absolute (2.13\% relative). Further increasing communication frequency to one exchange per batch approaches centralized training performance.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling</title>
<link>https://arxiv.org/abs/2506.11072</link>
<guid>https://arxiv.org/abs/2506.11072</guid>
<content:encoded><![CDATA[
arXiv:2506.11072v1 Announce Type: cross 
Abstract: Machine learning-based behavioral models rely on features extracted from audio-visual recordings. The recordings are processed using open-source tools to extract speech features for classification models. These tools often lack validation to ensure reliability in capturing behaviorally relevant information. This gap raises concerns about reproducibility and fairness across diverse populations and contexts. Speech processing tools, when used outside of their design context, can fail to capture behavioral variations equitably and can then contribute to bias. We evaluate speech features extracted from two widely used speech analysis tools, OpenSMILE and Praat, to assess their reliability when considering adolescents with autism. We observed considerable variation in features across tools, which influenced model performance across context and demographic groups. We encourage domain-relevant verification to enhance the reliability of machine learning models in clinical applications.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts</title>
<link>https://arxiv.org/abs/2506.11079</link>
<guid>https://arxiv.org/abs/2506.11079</guid>
<content:encoded><![CDATA[
arXiv:2506.11079v1 Announce Type: cross 
Abstract: Automatic reading aloud evaluation can provide valuable support to teachers by enabling more efficient scoring of reading exercises. However, research on reading evaluation systems and applications remains limited. We present a novel multimodal approach that leverages audio and knowledge from text resources. In particular, we explored the potential of using Whisper and instruction-tuned large language models (LLMs) with prompts to improve transcriptions for child speech recognition, as well as their effectiveness in downstream reading mistake detection. Our results demonstrate the effectiveness of prompting Whisper and prompting LLM, compared to the baseline Whisper model without prompting. The best performing system achieved state-of-the-art recognition performance in Dutch child read speech, with a word error rate (WER) of 5.1%, improving the baseline WER of 9.4%. Furthermore, it significantly improved reading mistake detection, increasing the F1 score from 0.39 to 0.73.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeanExplore: A search engine for Lean 4 declarations</title>
<link>https://arxiv.org/abs/2506.11085</link>
<guid>https://arxiv.org/abs/2506.11085</guid>
<content:encoded><![CDATA[
arXiv:2506.11085v1 Announce Type: cross 
Abstract: The expanding Lean 4 ecosystem poses challenges for navigating its vast libraries. This paper introduces LeanExplore, a search engine for Lean 4 declarations. LeanExplore enables users to semantically search for statements, both formally and informally, across select Lean 4 packages (including Batteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is powered by a hybrid ranking strategy, integrating scores from a multi-source semantic embedding model (capturing conceptual meaning from formal Lean code, docstrings, AI-generated informal translations, and declaration titles), BM25+ for keyword-based lexical relevance, and a PageRank-based score reflecting declaration importance and interconnectedness. The search engine is accessible via a dedicated website (https://www.leanexplore.com/) and a Python API (https://github.com/justincasher/lean-explore). Furthermore, the database can be downloaded, allowing users to self-host the service. LeanExplore integrates easily with LLMs via the model context protocol (MCP), enabling users to chat with an AI assistant about Lean declarations or utilize the search engine for building theorem-proving agents. This work details LeanExplore's architecture, data processing, functionalities, and its potential to enhance Lean 4 workflows and AI-driven mathematical research
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2506.11087</link>
<guid>https://arxiv.org/abs/2506.11087</guid>
<content:encoded><![CDATA[
arXiv:2506.11087v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve impressive performance on various knowledge-intensive and complex reasoning tasks in different domains. In certain scenarios like multi-tenant serving, a large number of LLMs finetuned from the same base model are deployed to meet complex requirements for users. Recent works explore delta-compression approaches to quantize and compress the delta parameters between the customized LLM and the corresponding base model. However, existing works either exhibit unsatisfactory performance at high compression ratios or depend on empirical bit allocation schemes. In this work, we propose ADAMIX, an effective adaptive mixed-precision delta-compression framework. We provide a mathematical derivation of quantization error to motivate our mixed-precision compression strategy and formulate the optimal mixed-precision bit allocation scheme as the solution to a 0/1 integer linear programming problem. Our derived bit allocation strategy minimizes the quantization error while adhering to a predefined compression ratio requirement. Experimental results on various models and benchmarks demonstrate that our approach surpasses the best baseline by a considerable margin. On tasks like AIME2024 and GQA, where the norm of $\Delta \mathbf{W}$ is large and the base model lacks sufficient ability, ADAMIX outperforms the best baseline Delta-CoMe by 22.3% and 6.1% with 7B models, respectively.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM</title>
<link>https://arxiv.org/abs/2506.11089</link>
<guid>https://arxiv.org/abs/2506.11089</guid>
<content:encoded><![CDATA[
arXiv:2506.11089v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) models rely on high-quality transcribed data for effective training. Generating pseudo-labels for large unlabeled audio datasets often relies on complex pipelines that combine multiple ASR outputs through multi-stage processing, leading to error propagation, information loss and disjoint optimization. We propose a unified multi-ASR prompt-driven framework using postprocessing by either textual or speech-based large language models (LLMs), replacing voting or other arbitration logic for reconciling the ensemble outputs. We perform a comparative study of multiple architectures with and without LLMs, showing significant improvements in transcription accuracy compared to traditional methods. Furthermore, we use the pseudo-labels generated by the various approaches to train semi-supervised ASR models for different datasets, again showing improved performance with textual and speechLLM transcriptions compared to baselines.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting</title>
<link>https://arxiv.org/abs/2506.11096</link>
<guid>https://arxiv.org/abs/2506.11096</guid>
<content:encoded><![CDATA[
arXiv:2506.11096v1 Announce Type: cross 
Abstract: Pretrained speech representations like wav2vec2 and HuBERT exhibit strong anisotropy, leading to high similarity between random embeddings. While widely observed, the impact of this property on downstream tasks remains unclear. This work evaluates anisotropy in keyword spotting for computational documentary linguistics. Using Dynamic Time Warping, we show that despite anisotropy, wav2vec2 similarity measures effectively identify words without transcription. Our results highlight the robustness of these representations, which capture phonetic structures and generalize across speakers. Our results underscore the importance of pretraining in learning rich and invariant speech representations.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Embeddings with Representing Relations as Annular Sectors</title>
<link>https://arxiv.org/abs/2506.11099</link>
<guid>https://arxiv.org/abs/2506.11099</guid>
<content:encoded><![CDATA[
arXiv:2506.11099v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs), structured as multi-relational data of entities and relations, are vital for tasks like data analysis and recommendation systems. Knowledge graph completion (KGC), or link prediction, addresses incompleteness of KGs by inferring missing triples (h, r, t). It is vital for downstream applications. Region-based embedding models usually embed entities as points and relations as geometric regions to accomplish the task. Despite progress, these models often overlook semantic hierarchies inherent in entities. To solve this problem, we propose SectorE, a novel embedding model in polar coordinates. Relations are modeled as annular sectors, combining modulus and phase to capture inference patterns and relation attributes. Entities are embedded as points within these sectors, intuitively encoding hierarchical structure. Evaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive performance against various kinds of models, demonstrating strengths in semantic modeling capability.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic</title>
<link>https://arxiv.org/abs/2506.11221</link>
<guid>https://arxiv.org/abs/2506.11221</guid>
<content:encoded><![CDATA[
arXiv:2506.11221v1 Announce Type: cross 
Abstract: Clinical communication skills are critical in medical education, and practicing and assessing clinical communication skills on a scale is challenging. Although LLM-powered clinical scenario simulations have shown promise in enhancing medical students' clinical practice, providing automated and scalable clinical evaluation that follows nuanced physician judgment is difficult. This paper combines fuzzy logic and Large Language Model (LLM) and proposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the automated evaluation of medical students' clinical skills with subjective physicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is fine-tuned to evaluate medical students' utterances within student-AI patient conversation scripts based on human annotations from four fuzzy sets, including Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction. The methodology of this paper started from data collection from the LLM-powered medical education system, data annotation based on multidimensional fuzzy sets, followed by prompt engineering and the supervised fine-tuning (SFT) of the pre-trained LLMs using these human annotations. The results show that the LLM-as-a-Fuzzy-Judge achieves over 80\% accuracy, with major criteria items over 90\%, effectively leveraging fuzzy logic and LLM as a solution to deliver interpretable, human-aligned assessment. This work suggests the viability of leveraging fuzzy logic and LLM to align with human preferences, advances automated evaluation in medical education, and supports more robust assessment and judgment practices. The GitHub repository of this work is available at https://github.com/2sigmaEdTech/LLMAsAJudge
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation</title>
<link>https://arxiv.org/abs/2506.11237</link>
<guid>https://arxiv.org/abs/2506.11237</guid>
<content:encoded><![CDATA[
arXiv:2506.11237v1 Announce Type: cross 
Abstract: In an effort to automatically evaluate and select the best model and improve code quality for automatic incident remediation in IT Automation, it is crucial to verify if the generated code for remediation action is syntactically and semantically correct and whether it can be executed correctly as intended. There are three approaches: 1) conventional methods use surface form similarity metrics (token match, exact match, etc.) which have numerous limitations, 2) execution-based evaluation focuses more on code functionality based on pass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs for automated evaluation to judge if it is a correct answer for a given problem based on pre-defined metrics. In this work, we focused on enhancing LLM-as-a-Judge using bidirectional functionality matching and logic representation for reference-less automatic validation and refinement for Bash code generation to select the best model for automatic incident remediation in IT Automation. We used execution-based evaluation as ground-truth to evaluate our LLM-as-a-Judge metrics. Results show high accuracy and agreement with execution-based evaluation (and up to 8% over baseline). Finally, we built Reflection code agents to utilize judgments and feedback from our evaluation metrics which achieved significant improvement (up to 24% increase in accuracy) for automatic code refinement.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLAP: General contrastive audio-text pretraining across domains and languages</title>
<link>https://arxiv.org/abs/2506.11350</link>
<guid>https://arxiv.org/abs/2506.11350</guid>
<content:encoded><![CDATA[
arXiv:2506.11350v1 Announce Type: cross 
Abstract: Contrastive Language Audio Pretraining (CLAP) is a widely-used method to bridge the gap between audio and text domains. Current CLAP methods enable sound and music retrieval in English, ignoring multilingual spoken content. To address this, we introduce general language audio pretraining (GLAP), which expands CLAP with multilingual and multi-domain abilities. GLAP demonstrates its versatility by achieving competitive performance on standard audio-text retrieval benchmarks like Clotho and AudioCaps, while significantly surpassing existing methods in speech retrieval and classification tasks. Additionally, GLAP achieves strong results on widely used sound-event zero-shot benchmarks, while simultaneously outperforming previous methods on speech content benchmarks. Further keyword spotting evaluations across 50 languages emphasize GLAP's advanced multilingual capabilities. Finally, multilingual sound and music understanding is evaluated across four languages. Checkpoints and Source: https://github.com/xiaomi-research/dasheng-glap.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables</title>
<link>https://arxiv.org/abs/2506.11375</link>
<guid>https://arxiv.org/abs/2506.11375</guid>
<content:encoded><![CDATA[
arXiv:2506.11375v1 Announce Type: cross 
Abstract: Chemical tables encode complex experimental knowledge through symbolic expressions, structured variables, and embedded molecular graphics. Existing benchmarks largely overlook this multimodal and domain-specific complexity, limiting the ability of multimodal large language models to support scientific understanding in chemistry. In this work, we introduce ChemTable, a large-scale benchmark of real-world chemical tables curated from the experimental sections of literature. ChemTable includes expert-annotated cell polygons, logical layouts, and domain-specific labels, including reagents, catalysts, yields, and graphical components and supports two core tasks: (1) Table Recognition, covering structure parsing and content extraction; and (2) Table Understanding, encompassing both descriptive and reasoning-oriented question answering grounded in table structure and domain semantics. We evaluated a range of representative multimodal models, including both open-source and closed-source models, on ChemTable and reported a series of findings with practical and conceptual insights. Although models show reasonable performance on basic layout parsing, they exhibit substantial limitations on both descriptive and inferential QA tasks compared to human performance, and we observe significant performance gaps between open-source and closed-source models across multiple dimensions. These results underscore the challenges of chemistry-aware table understanding and position ChemTable as a rigorous and realistic benchmark for advancing scientific reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning</title>
<link>https://arxiv.org/abs/2506.11376</link>
<guid>https://arxiv.org/abs/2506.11376</guid>
<content:encoded><![CDATA[
arXiv:2506.11376v1 Announce Type: cross 
Abstract: Family caregivers often face substantial mental health challenges due to their multifaceted roles and limited resources. This study explored the potential of a large language model (LLM)-powered conversational agent to deliver evidence-based mental health support for caregivers, specifically Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted with 28 caregivers interacting with four LLM configurations to evaluate empathy and therapeutic alliance. The best-performing models incorporated Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques, alongside clinician-curated examples. The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants valued the model's ability to validate emotions, explore unexpressed feelings, and provide actionable strategies. However, balancing thorough assessment with efficient advice delivery remains a challenge. This work highlights the potential of LLMs in delivering empathetic and tailored support for family caregivers.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model</title>
<link>https://arxiv.org/abs/2506.11402</link>
<guid>https://arxiv.org/abs/2506.11402</guid>
<content:encoded><![CDATA[
arXiv:2506.11402v1 Announce Type: cross 
Abstract: Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA), aligns pre-trained Large Language Models (LLMs) to particular downstream tasks in a resource-efficient manner. Because efficiency has been the main metric of progress, very little attention has been put in understanding possible catastrophic failures. We uncover one such failure: PEFT encourages a model to search for shortcut solutions to solve its fine-tuning tasks. When very small amount of tokens, e.g., one token per prompt, are correlated with downstream task classes, PEFT makes any pretrained model rely predominantly on that token for decision making. While such spurious tokens may emerge accidentally from incorrect data cleaning, it also opens opportunities for malevolent parties to control a model's behavior from Seamless Spurious Token Injection (SSTI). In SSTI, a small amount of tokens correlated with downstream classes are injected by the dataset creators. At test time, the finetuned LLM's behavior can be controlled solely by injecting those few tokens. We apply SSTI across models from three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and four diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias in Bios). Our findings reveal three astonishing behaviors. First, as few as a single token of SSTI is sufficient to steer a model's decision making. Second, for light SSTI, the reliance on spurious tokens is proportional to the LoRA rank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable to small rank values as it makes the model attend to non-spurious tokens, hence improving robustness.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs</title>
<link>https://arxiv.org/abs/2506.11415</link>
<guid>https://arxiv.org/abs/2506.11415</guid>
<content:encoded><![CDATA[
arXiv:2506.11415v1 Announce Type: cross 
Abstract: In Large Language Models, Retrieval-Augmented Generation (RAG) systems can significantly enhance the performance of large language models by integrating external knowledge. However, RAG also introduces new security risks. Existing research focuses mainly on how poisoning attacks in RAG systems affect model output quality, overlooking their potential to amplify model biases. For example, when querying about domestic violence victims, a compromised RAG system might preferentially retrieve documents depicting women as victims, causing the model to generate outputs that perpetuate gender stereotypes even when the original query is gender neutral. To show the impact of the bias, this paper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which systematically investigates attack pathways that amplify language model biases through a RAG system manipulation. We design an adversarial document generation method based on multi-objective reward functions, employ subspace projection techniques to manipulate retrieval results, and construct a cyclic feedback mechanism for continuous bias amplification. Experiments on multiple mainstream large language models demonstrate that BRRA attacks can significantly enhance model biases in dimensions. In addition, we explore a dual stage defense mechanism to effectively mitigate the impacts of the attack. This study reveals that poisoning attacks in RAG systems directly amplify model output biases and clarifies the relationship between RAG system security and model fairness. This novel potential attack indicates that we need to keep an eye on the fairness issues of the RAG system.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction</title>
<link>https://arxiv.org/abs/2506.11475</link>
<guid>https://arxiv.org/abs/2506.11475</guid>
<content:encoded><![CDATA[
arXiv:2506.11475v1 Announce Type: cross 
Abstract: This paper introduces LUCID-MA (Learning and Understanding Crime through Dialogue of Multiple Agents), an innovative AI powered framework where multiple AI agents collaboratively analyze and understand crime data. Our system that consists of three core components: an analysis assistant that highlights spatiotemporal crime patterns, a feedback component that reviews and refines analytical results and a prediction component that forecasts future crime trends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it runs completely offline and allows the agents undergo self-improvement through 100 rounds of communication with less human interaction. A scoring function is incorporated to evaluate agent's performance, providing visual plots to track learning progress. This work demonstrates the potential of AutoGen-style agents for autonomous, scalable, and iterative analysis in social science domains maintaining data privacy through offline execution.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs</title>
<link>https://arxiv.org/abs/2506.11515</link>
<guid>https://arxiv.org/abs/2506.11515</guid>
<content:encoded><![CDATA[
arXiv:2506.11515v1 Announce Type: cross 
Abstract: Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, it \textit{(i)} suffers from ineffective layer-by-layer utilization of unimodal representations, \textit{(ii)} restricts the flexible exploitation of different levels of unimodal semantic knowledge, and \textit{(iii)} is limited to the evaluation on traditional low-resolution datasets only with the Two-Tower VLM architecture. In this work, we propose Manager, a lightweight, efficient and effective plugin that adaptively aggregates insights from different levels of pre-trained unimodal experts to facilitate more comprehensive VL alignment and fusion. First, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel VLM that introduces the manager in each cross-modal layer. Whether with or without VL pre-training, ManagerTower outperforms previous strong baselines and achieves superior performance on 4 downstream VL tasks. Moreover, we extend our exploration to the latest Multimodal Large Language Model (MLLM) architecture. We demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot performance of LLaVA-OV across different categories of capabilities, images, and resolutions on 20 downstream datasets, whether the multi-grid algorithm is enabled or not. In-depth analysis reveals that both our manager and the multi-grid algorithm can be viewed as a plugin that improves the visual representation by capturing more diverse visual details from two orthogonal perspectives (depth and width). Their synergy can mitigate the semantic ambiguity caused by the multi-grid algorithm and further improve performance. Code and models are available at https://github.com/LooperXX/ManagerTower.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning</title>
<link>https://arxiv.org/abs/2506.11516</link>
<guid>https://arxiv.org/abs/2506.11516</guid>
<content:encoded><![CDATA[
arXiv:2506.11516v1 Announce Type: cross 
Abstract: In-context learning (ICL) allows large language models (LLMs) to solve novel tasks without weight updates. Despite its empirical success, the mechanism behind ICL remains poorly understood, limiting our ability to interpret, improve, and reliably apply it. In this paper, we propose a new theoretical perspective that interprets ICL as an implicit form of knowledge distillation (KD), where prompt demonstrations guide the model to form a task-specific reference model during inference. Under this view, we derive a Rademacher complexity-based generalization bound and prove that the bias of the distilled weights grows linearly with the Maximum Mean Discrepancy (MMD) between the prompt and target distributions. This theoretical framework explains several empirical phenomena and unifies prior gradient-based and distributional analyses. To the best of our knowledge, this is the first to formalize inference-time attention as a distillation process, which provides theoretical insights for future prompt engineering and automated demonstration selection.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning</title>
<link>https://arxiv.org/abs/2506.11555</link>
<guid>https://arxiv.org/abs/2506.11555</guid>
<content:encoded><![CDATA[
arXiv:2506.11555v1 Announce Type: cross 
Abstract: The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs</title>
<link>https://arxiv.org/abs/2506.11558</link>
<guid>https://arxiv.org/abs/2506.11558</guid>
<content:encoded><![CDATA[
arXiv:2506.11558v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with GPT-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM@school -- Evaluation of AI image understanding on German middle school knowledge</title>
<link>https://arxiv.org/abs/2506.11604</link>
<guid>https://arxiv.org/abs/2506.11604</guid>
<content:encoded><![CDATA[
arXiv:2506.11604v1 Announce Type: cross 
Abstract: This paper introduces a novel benchmark dataset designed to evaluate the capabilities of Vision Language Models (VLMs) on tasks that combine visual reasoning with subject-specific background knowledge in the German language. In contrast to widely used English-language benchmarks that often rely on artificially difficult or decontextualized problems, this dataset draws from real middle school curricula across nine domains including mathematics, history, biology, and religion. The benchmark includes over 2,000 open-ended questions grounded in 486 images, ensuring that models must integrate visual interpretation with factual reasoning rather than rely on superficial textual cues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple dimensions, including domain-specific accuracy and performance on adversarial crafted questions. Our findings reveal that even the strongest models achieve less than 45% overall accuracy, with particularly poor performance in music, mathematics, and adversarial settings. Furthermore, the results indicate significant discrepancies between success on popular benchmarks and real-world multimodal understanding. We conclude that middle school-level tasks offer a meaningful and underutilized avenue for stress-testing VLMs, especially in non-English contexts. The dataset and evaluation protocol serve as a rigorous testbed to better understand and improve the visual and linguistic reasoning capabilities of future AI systems.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test</title>
<link>https://arxiv.org/abs/2506.11620</link>
<guid>https://arxiv.org/abs/2506.11620</guid>
<content:encoded><![CDATA[
arXiv:2506.11620v1 Announce Type: cross 
Abstract: Traditional audiometry often provides an incomplete characterization of the functional impact of hearing loss on speech understanding, particularly for supra-threshold deficits common in presbycusis. This motivates the development of more diagnostically specific speech perception tests. We introduce the Simulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel, multi-stage computational pipeline for the in silico design and validation of a phonetically balanced minimal-pair speech test. This methodology leverages a modern Automatic Speech Recognition (ASR) system as a proxy for a human listener to simulate the perceptual effects of sensorineural hearing loss. By processing speech stimuli under controlled acoustic degradation, we first identify the most common phoneme confusion patterns. These patterns then guide the data-driven curation of a large set of candidate word pairs derived from a comprehensive linguistic corpus. Subsequent phases involving simulated diagnostic testing, expert human curation, and a final, targeted sensitivity analysis systematically reduce the candidates to a final, optimized set of 25 pairs (the SimPhon Speech Test-25). A key finding is that the diagnostic performance of the SimPhon Speech Test-25 test items shows no significant correlation with predictions from the standard Speech Intelligibility Index (SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond simple audibility. This computationally optimized test set offers a significant increase in efficiency for audiological test development, ready for initial human trials.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model</title>
<link>https://arxiv.org/abs/2506.11737</link>
<guid>https://arxiv.org/abs/2506.11737</guid>
<content:encoded><![CDATA[
arXiv:2506.11737v1 Announce Type: cross 
Abstract: This paper addresses two main objectives. Firstly, we demonstrate the impressive performance of the LLaVA-NeXT-interleave on 22 datasets across three different tasks: Multi-Image Reasoning, Documents and Knowledge-Based Understanding and Interactive Multi-Modal Communication. Secondly, we add the Dense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and compare its performance against the standard model. We find that the standard model achieves the highest overall accuracy, excelling in vision-heavy tasks like VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows particular strength on datasets requiring deeper semantic coherence or structured change understanding such as MIT-States_PropertyCoherence and SlideVQA. Our results highlight the potential of combining powerful foundation models with plug-and-play techniques for Interleave tasks. The code is available at https://github.com/dinhvietcuong1996/icme25-inova.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Performance of LLMs for Real Estate Appraisal</title>
<link>https://arxiv.org/abs/2506.11812</link>
<guid>https://arxiv.org/abs/2506.11812</guid>
<content:encoded><![CDATA[
arXiv:2506.11812v1 Announce Type: cross 
Abstract: The real estate market is vital to global economies but suffers from significant information asymmetry. This study examines how Large Language Models (LLMs) can democratize access to real estate insights by generating competitive and interpretable house price estimates through optimized In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs on diverse international housing datasets, comparing zero-shot, few-shot, market report-enhanced, and hybrid prompting techniques. Our results show that LLMs effectively leverage hedonic variables, such as property size and amenities, to produce meaningful estimates. While traditional machine learning models remain strong for pure predictive accuracy, LLMs offer a more accessible, interactive and interpretable alternative. Although self-explanations require cautious interpretation, we find that LLMs explain their predictions in agreement with state-of-the-art models, confirming their trustworthiness. Carefully selected in-context examples based on feature similarity and geographic proximity, significantly enhance LLM performance, yet LLMs struggle with overconfidence in price intervals and limited spatial reasoning. We offer practical guidance for structured prediction tasks through prompt optimization. Our findings highlight LLMs' potential to improve transparency in real estate appraisal and provide actionable insights for stakeholders.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation</title>
<link>https://arxiv.org/abs/2506.11820</link>
<guid>https://arxiv.org/abs/2506.11820</guid>
<content:encoded><![CDATA[
arXiv:2506.11820v1 Announce Type: cross 
Abstract: Vision-Language Translation (VLT) is a challenging task that requires accurately recognizing multilingual text embedded in images and translating it into the target language with the support of visual context. While recent Large Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual understanding capabilities, there is a lack of systematic evaluation and understanding of their performance on VLT. In this work, we present a comprehensive study of VLT from three key perspectives: data quality, model architecture, and evaluation metrics. (1) We identify critical limitations in existing datasets, particularly in semantic and cultural fidelity, and introduce AibTrans -- a multilingual, parallel, human-verified dataset with OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6 state-of-the-art open-source models across end-to-end and cascaded architectures, revealing their OCR dependency and contrasting generation versus reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric reliability issues under varying contextual complexity, introducing the DA Score as a more robust measure of translation quality. Building upon these findings, we establish a new evaluation benchmark for VLT. Notably, we observe that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual performance, and we propose a balanced multilingual fine-tuning strategy that effectively adapts LVLMs to VLT without sacrificing their generalization ability.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment</title>
<link>https://arxiv.org/abs/2506.11880</link>
<guid>https://arxiv.org/abs/2506.11880</guid>
<content:encoded><![CDATA[
arXiv:2506.11880v1 Announce Type: cross 
Abstract: The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making</title>
<link>https://arxiv.org/abs/2506.11887</link>
<guid>https://arxiv.org/abs/2506.11887</guid>
<content:encoded><![CDATA[
arXiv:2506.11887v1 Announce Type: cross 
Abstract: Effective human-AI decision-making balances three key factors: the \textit{correctness} of predictions, the \textit{cost} of knowledge and reasoning complexity, and the confidence about whether to \textit{abstain} automated answers or involve human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise -- a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base model's answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, we incorporate an online learning mechanism in the framework that can leverage human feedback to improve decision quality over time. We demonstrate this approach to general question-answering (ARC-Easy and ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results show that our cascaded strategy outperforms in most cases single-model baselines in accuracy while reducing cost and providing a principled way to handle abstentions.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeRL: LLM Reinforcement Learning with On-Policy Tree Search</title>
<link>https://arxiv.org/abs/2506.11902</link>
<guid>https://arxiv.org/abs/2506.11902</guid>
<content:encoded><![CDATA[
arXiv:2506.11902v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at https://github.com/THUDM/TreeRL.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?</title>
<link>https://arxiv.org/abs/2506.11928</link>
<guid>https://arxiv.org/abs/2506.11928</guid>
<content:encoded><![CDATA[
arXiv:2506.11928v1 Announce Type: cross 
Abstract: Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task</title>
<link>https://arxiv.org/abs/2506.11986</link>
<guid>https://arxiv.org/abs/2506.11986</guid>
<content:encoded><![CDATA[
arXiv:2506.11986v1 Announce Type: cross 
Abstract: Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10\% improvement in filter accuracy compared to the existing method. Our code is available at https://github.com/hongWin/Schema-R1/.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VGR: Visual Grounded Reasoning</title>
<link>https://arxiv.org/abs/2506.11991</link>
<guid>https://arxiv.org/abs/2506.11991</guid>
<content:encoded><![CDATA[
arXiv:2506.11991v1 Announce Type: cross 
Abstract: In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Representational Learning of Foundation Models for Recommendation</title>
<link>https://arxiv.org/abs/2506.11999</link>
<guid>https://arxiv.org/abs/2506.11999</guid>
<content:encoded><![CDATA[
arXiv:2506.11999v1 Announce Type: cross 
Abstract: Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing & conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference</title>
<link>https://arxiv.org/abs/2405.04065</link>
<guid>https://arxiv.org/abs/2405.04065</guid>
<content:encoded><![CDATA[
arXiv:2405.04065v4 Announce Type: replace 
Abstract: Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven method for enabling the LLM to generate information beyond the scope of its pre-training corpus. Previous work utilizing retrieved content by simply prepending it to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently. In this paper, we propose FlashBack, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after fine-tuning by Low-Rank Adaption. FlashBack appends retrieved documents at the end of the context for efficiently utilizing the KV cache instead of prepending them. And we introduce Marking Token as two special prompt tokens for marking the boundary of the appending context during fine-tuning. Our experiments on testing generation quality show that FlashBack can remain decent generation quality in perplexity. And the inference speed of FlashBack is up to $4\times$ faster than the prepending counterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing unnecessary re-computation, it demonstrates an advancement by achieving significantly faster inference speed, and this heightened efficiency will substantially reduce inferential cost.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2406.02050</link>
<guid>https://arxiv.org/abs/2406.02050</guid>
<content:encoded><![CDATA[
arXiv:2406.02050v4 Announce Type: replace 
Abstract: With the development of large language models (LLMs), social biases in these LLMs have become a pressing issue. Although there are various benchmarks for social biases across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, with analysis of social biases in Japanese LLMs. The results show that while current open Japanese LLMs with more parameters show improved accuracies on JBBQ, their bias scores increase. In addition, prompts with a warning about social biases and chain-of-thought prompting reduce the effect of biases in model outputs, but there is room for improvement in extracting the correct evidence from contexts in Japanese. Our dataset is available at https://github.com/ynklab/JBBQ_data.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</title>
<link>https://arxiv.org/abs/2406.14023</link>
<guid>https://arxiv.org/abs/2406.14023</guid>
<content:encoded><![CDATA[
arXiv:2406.14023v4 Announce Type: replace 
Abstract: As large language models (LLMs) become an important way of information access, there have been increasing concerns that LLMs may intensify the spread of unethical content, including implicit bias that hurts certain populations without explicit harmful words. In this paper, we conduct a rigorous evaluation of LLMs' implicit bias towards certain demographics by attacking them from a psychometric perspective to elicit agreements to biased viewpoints. Inspired by psychometric principles in cognitive and social psychology, we propose three attack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the corresponding attack instructions, we built two benchmarks: (1) a bilingual dataset with biased statements covering four bias types (2.7K instances) for extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning nine common bias types (12.7K instances) for comprehensive evaluation. Extensive evaluation of popular commercial and open-source LLMs shows that our methods can elicit LLMs' inner bias more effectively than competitive baselines. Our attack methodology and benchmarks offer an effective means of assessing the ethical risks of LLMs, driving progress toward greater accountability in their development. Our code, data, and benchmarks are available at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models</title>
<link>https://arxiv.org/abs/2410.20445</link>
<guid>https://arxiv.org/abs/2410.20445</guid>
<content:encoded><![CDATA[
arXiv:2410.20445v3 Announce Type: replace 
Abstract: Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose \textit{TrajAgent}, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In \textit{TrajAgent}, we first develop \textit{UniEnv}, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit{UniEnv}, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving a performance improvement of 2.38\%-34.96\% over baseline methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Sparse Latent Feature Models for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2411.15694</link>
<guid>https://arxiv.org/abs/2411.15694</guid>
<content:encoded><![CDATA[
arXiv:2411.15694v2 Announce Type: replace 
Abstract: Recent advances in knowledge graph completion (KGC) have emphasized text-based approaches to navigate the inherent complexities of large-scale knowledge graphs (KGs). While these methods have achieved notable progress, they frequently struggle to fully incorporate the global structural properties of the graph. Stochastic blockmodels (SBMs), especially the latent feature relational model (LFRM), offer robust probabilistic frameworks for identifying latent community structures and improving link prediction. This paper presents a novel probabilistic KGC framework utilizing sparse latent feature models, optimized via a deep variational autoencoder (VAE). Our proposed method dynamically integrates global clustering information with local textual features to effectively complete missing triples, while also providing enhanced interpretability of the underlying latent structures. Extensive experiments on four benchmark datasets with varying scales demonstrate the significant performance gains achieved by our method.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets</title>
<link>https://arxiv.org/abs/2412.21015</link>
<guid>https://arxiv.org/abs/2412.21015</guid>
<content:encoded><![CDATA[
arXiv:2412.21015v2 Announce Type: replace 
Abstract: Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia Reflect on the Cross-Cultural Sociolinguistic Norms?</title>
<link>https://arxiv.org/abs/2501.03479</link>
<guid>https://arxiv.org/abs/2501.03479</guid>
<content:encoded><![CDATA[
arXiv:2501.03479v3 Announce Type: replace 
Abstract: Wikipedia, as a massively multilingual, community-driven platform, is a valuable resource for Natural Language Processing (NLP), yet the consistency of honorific usage in honorific-rich languages remains underexplored. Honorifics, subtle yet profound linguistic markers, encode social hierarchies, politeness norms, and cultural values, but Wikipedia's editorial guidelines lack clear standards for their usage in languages where such forms are grammatically and socially prevalent. This paper addresses this gap through a large-scale analysis of third-person honorific pronouns and verb forms in Hindi and Bengali Wikipedia articles. Using Large Language Models (LLM), we automatically annotate 10,000 articles per language for honorific usage and socio-demographic features such as gender, age, fame, and cultural origin. We investigate: (i) the consistency of honorific usage across articles, (ii) how inconsistencies correlate with socio-cultural factors, and (iii) the presence of explicit or implicit biases across languages. We find that honorific usage is consistently more common in Bengali than Hindi, while non-honorific forms are more frequent for infamous, juvenile, and exotic entities in both. Notably, gender bias emerges in both languages, particularly in Hindi, where men are more likely to receive honorifics than women. Our analysis highlights the need for Wikipedia to develop language-specific editorial guidelines for honorific usage.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations</title>
<link>https://arxiv.org/abs/2502.01220</link>
<guid>https://arxiv.org/abs/2502.01220</guid>
<content:encoded><![CDATA[
arXiv:2502.01220v5 Announce Type: replace 
Abstract: This paper explores the robustness of language models (LMs) to variations in the temporal context within factual knowledge. It examines whether LMs can correctly associate a temporal context with a past fact valid over a defined period, by asking them to differentiate correct from incorrect contexts. The LMs' ability to distinguish is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context. To this end, a dataset called TimeStress is introduced, enabling the evaluation of 18 diverse LMs. Results reveal that the best LM achieves a perfect distinction for only 11% of the studied facts, with errors, certainly rare, but critical that humans would not make. This work highlights the limitations of current LMs in temporal representation.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling</title>
<link>https://arxiv.org/abs/2502.01925</link>
<guid>https://arxiv.org/abs/2502.01925</guid>
<content:encoded><![CDATA[
arXiv:2502.01925v2 Announce Type: replace 
Abstract: Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational exchanges between the user and the model. These exchanges are randomly sampled from a pool of unsafe question-answer pairs, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with Positive Affirmations, Negative Demonstrations, and an optimized Adaptive Sampling method tailored to the target prompt's topic. We also introduce ManyHarm, a dataset of harmful question-answer pairs, and demonstrate through extensive experiments that PANDAS significantly outperforms baseline methods in long-context scenarios. Through attention analysis, we provide insights into how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages</title>
<link>https://arxiv.org/abs/2502.11020</link>
<guid>https://arxiv.org/abs/2502.11020</guid>
<content:encoded><![CDATA[
arXiv:2502.11020v2 Announce Type: replace 
Abstract: Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis</title>
<link>https://arxiv.org/abs/2502.11812</link>
<guid>https://arxiv.org/abs/2502.11812</guid>
<content:encoded><![CDATA[
arXiv:2502.11812v2 Announce Type: replace 
Abstract: Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, in contrast to prior work that shows circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Linguistic Calibration: Trading-off between Factuality and Specificity</title>
<link>https://arxiv.org/abs/2502.19110</link>
<guid>https://arxiv.org/abs/2502.19110</guid>
<content:encoded><![CDATA[
arXiv:2502.19110v2 Announce Type: replace 
Abstract: Language model outputs are not always reliable, thus prompting research into how to adapt model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unified view, Conformal Linguistic Calibration (CLC), which reinterprets linguistic calibration as \emph{answer set prediction}. First we present a framework connecting abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation of CLC that allows for controlling the level of imprecision in model responses. Results demonstrate our method produces calibrated outputs with conformal guarantees on factual accuracy. Further, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis</title>
<link>https://arxiv.org/abs/2502.19175</link>
<guid>https://arxiv.org/abs/2502.19175</guid>
<content:encoded><![CDATA[
arXiv:2502.19175v2 Announce Type: replace 
Abstract: Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models (LLMs) have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much is Enough? The Diminishing Returns of Tokenization Training Data</title>
<link>https://arxiv.org/abs/2502.20273</link>
<guid>https://arxiv.org/abs/2502.20273</guid>
<content:encoded><![CDATA[
arXiv:2502.20273v3 Announce Type: replace 
Abstract: Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. For Russian text, we observe diminishing returns after training a tokenizer from 200GB of data, which is approximately 33% more than when training on English. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts</title>
<link>https://arxiv.org/abs/2503.06706</link>
<guid>https://arxiv.org/abs/2503.06706</guid>
<content:encoded><![CDATA[
arXiv:2503.06706v3 Announce Type: replace 
Abstract: Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct Process Flow Dialogue (PFDial) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models' performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in https://github.com/KongLongGeFDU/PFDial.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts</title>
<link>https://arxiv.org/abs/2503.09347</link>
<guid>https://arxiv.org/abs/2503.09347</guid>
<content:encoded><![CDATA[
arXiv:2503.09347v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.21227</link>
<guid>https://arxiv.org/abs/2503.21227</guid>
<content:encoded><![CDATA[
arXiv:2503.21227v2 Announce Type: replace 
Abstract: Mixture of Experts (MoE) architectures have recently advanced the scalability and adaptability of large language models (LLMs) for continual multimodal learning. However, efficiently extending these models to accommodate sequential tasks remains challenging. As new tasks arrive, naive model expansion leads to rapid parameter growth, while modifying shared routing components often causes catastrophic forgetting, undermining previously learned knowledge. To address these issues, we propose LLaVA-CMoE, a continual learning framework for LLMs that requires no replay data of previous tasks and ensures both parameter efficiency and robust knowledge retention. Our approach introduces a Probe-Guided Knowledge Extension mechanism, which uses probe experts to dynamically determine when and where new experts should be added, enabling adaptive and minimal parameter expansion tailored to task complexity. Furthermore, we present a Probabilistic Task Locator that assigns each task a dedicated, lightweight router. To handle the practical issue that task labels are unknown during inference, we leverage a VAE-based reconstruction strategy to identify the most suitable router by matching input distributions, allowing automatic and accurate expert allocation. This design mitigates routing conflicts and catastrophic forgetting, enabling robust continual learning without explicit task labels. Extensive experiments on the CoIN benchmark, covering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong continual learning performance with a compact model size, significantly reducing forgetting and parameter overhead compared to prior methods. These results showcase the effectiveness and scalability of our approach for parameter-efficient continual learning in large language models. Our code will be open-sourced soon.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions</title>
<link>https://arxiv.org/abs/2504.11673</link>
<guid>https://arxiv.org/abs/2504.11673</guid>
<content:encoded><![CDATA[
arXiv:2504.11673v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses to various surveys and polls. However, the questions in these surveys usually reflect socially understood attitudes: the patterns of attitudes of old/young, liberal/conservative, as understood by both members and non-members of those groups. It is not clear whether the LLM binding is \emph{deep}, meaning the LLM answers as a member of a particular in-group would, or \emph{shallow}, meaning the LLM responds as an out-group member believes an in-group member would. To explore this difference, we use questions that expose known in-group/out-group biases. This level of fidelity is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies of in-group/out-group biases. Altogether, our work extends the applicability of LLMs beyond estimating socially understood responses, enabling their use in a broader range of human studies.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model</title>
<link>https://arxiv.org/abs/2504.13439</link>
<guid>https://arxiv.org/abs/2504.13439</guid>
<content:encoded><![CDATA[
arXiv:2504.13439v2 Announce Type: replace 
Abstract: Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: (1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and (2) entropy analysis, comparing model confidence distributions. Our results show that D-GEN preserves ranking consistency (Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-context Non-factoid Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2504.13615</link>
<guid>https://arxiv.org/abs/2504.13615</guid>
<content:encoded><![CDATA[
arXiv:2504.13615v2 Announce Type: replace 
Abstract: Question Answering (QA) tasks, which involve extracting answers from a given context, are relatively straightforward for modern Large Language Models (LLMs) when the context is short. However, long contexts pose challenges due to the quadratic complexity of the self-attention mechanism. This challenge is compounded in Indic languages, which are often low-resource. This study explores context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, Answer Paragraph Selection (APS), and their combinations, to improve QA performance. Compared to the baseline of unshortened (long) contexts, our experiments on four Indic languages (Hindi, Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield an average improvement of 4\% in semantic scores and 47\% in token-level scores when evaluated on three popular LLMs without fine-tuning. Furthermore, with fine-tuning, we achieve an average increase of 2\% in both semantic and token-level scores. Additionally, context-shortening reduces computational overhead. Explainability techniques like LIME and SHAP reveal that when the APS model confidently identifies the paragraph containing the answer, nearly all tokens within the selected text receive high relevance scores. However, the study also highlights the limitations of LLM-based QA systems in addressing non-factoid questions, particularly those requiring reasoning or debate. Moreover, verbalizing OIE-generated triples does not enhance system performance. These findings emphasize the potential of context-shortening techniques to improve the efficiency and effectiveness of LLM-based QA systems, especially for low-resource languages. The source code and resources are available at https://github.com/ritwikmishra/IndicGenQA.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Repeat Curse in Large Language Models from a Feature Perspective</title>
<link>https://arxiv.org/abs/2504.14218</link>
<guid>https://arxiv.org/abs/2504.14218</guid>
<content:encoded><![CDATA[
arXiv:2504.14218v3 Announce Type: replace 
Abstract: Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the "Repeat Curse". While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach, "Duplicatus Charm", to induce and analyze the Repeat Curse. Our method systematically identifies "Repetition Features" -the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse. The source code of our work is publicly available at: https://github.com/kaustpradalab/repeat-curse-llm
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</title>
<link>https://arxiv.org/abs/2504.18415</link>
<guid>https://arxiv.org/abs/2504.18415</guid>
<content:encoded><![CDATA[
arXiv:2504.18415v2 Announce Type: replace 
Abstract: Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table-R1: Region-based Reinforcement Learning for Table Understanding</title>
<link>https://arxiv.org/abs/2505.12415</link>
<guid>https://arxiv.org/abs/2505.12415</guid>
<content:encoded><![CDATA[
arXiv:2505.12415v2 Announce Type: replace 
Abstract: Tables present unique challenges for language models due to their structured row-column interactions, necessitating specialized approaches for effective comprehension. While large language models (LLMs) have demonstrated potential in table reasoning through prompting and techniques like chain-of-thought (CoT) and program-of-thought (PoT), optimizing their performance for table question answering remains underexplored. In this paper, we introduce region-based Table-R1, a novel reinforcement learning approach that enhances LLM table understanding by integrating region evidence into reasoning steps. Our method employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in identifying relevant table regions before generating answers, incorporating textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group Relative Policy Optimization (TARPO) introduces a mixed reward system to dynamically balance region accuracy and answer correctness, with decaying region rewards and consistency penalties to align reasoning steps. Experiments show that Table-R1 achieves an average performance improvement of 14.36 points across multiple base models on three benchmark datasets, even outperforming baseline models with ten times the parameters, while TARPO reduces response token consumption by 67.5% compared to GRPO, significantly advancing LLM capabilities in efficient tabular reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu</title>
<link>https://arxiv.org/abs/2505.16660</link>
<guid>https://arxiv.org/abs/2505.16660</guid>
<content:encoded><![CDATA[
arXiv:2505.16660v3 Announce Type: replace 
Abstract: This study addresses the challenges in intelligent processing of Chinese ancient mathematical classics by constructing Guji_MATH, a benchmark for evaluating classical texts based on Suanjing Shishu. It systematically assesses the mathematical problem-solving capabilities of mainstream reasoning models under the unique linguistic constraints of classical Chinese. Through machine-assisted annotation and manual verification, 538 mathematical problems were extracted from 8 canonical texts, forming a structured dataset centered on the "Question-Answer-Solution" framework, supplemented by problem types and difficulty levels. Dual evaluation modes--closed-book (autonomous problem-solving) and open-book (reproducing classical solution methods)--were designed to evaluate the performance of six reasoning models on ancient Chinese mathematical problems. Results indicate that reasoning models can partially comprehend and solve these problems, yet their overall performance remains inferior to benchmarks on modern mathematical tasks. Enhancing models' classical Chinese comprehension and cultural knowledge should be prioritized for optimization. This study provides methodological support for mining mathematical knowledge from ancient texts and disseminating traditional culture, while offering new perspectives for evaluating cross-linguistic and cross-cultural capabilities of reasoning models.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English</title>
<link>https://arxiv.org/abs/2505.17076</link>
<guid>https://arxiv.org/abs/2505.17076</guid>
<content:encoded><![CDATA[
arXiv:2505.17076v3 Announce Type: replace 
Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph</title>
<link>https://arxiv.org/abs/2505.20813</link>
<guid>https://arxiv.org/abs/2505.20813</guid>
<content:encoded><![CDATA[
arXiv:2505.20813v3 Announce Type: replace 
Abstract: In knowledge graph embedding, leveraging relation specific entity transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation has three features for enhancing semantic consistency: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations</title>
<link>https://arxiv.org/abs/2505.21657</link>
<guid>https://arxiv.org/abs/2505.21657</guid>
<content:encoded><![CDATA[
arXiv:2505.21657v2 Announce Type: replace 
Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers</title>
<link>https://arxiv.org/abs/2505.23252</link>
<guid>https://arxiv.org/abs/2505.23252</guid>
<content:encoded><![CDATA[
arXiv:2505.23252v2 Announce Type: replace 
Abstract: Approaches form the foundation for conducting scientific research. Querying approaches from a vast body of scientific papers is extremely time-consuming, and without a well-organized management framework, researchers may face significant challenges in querying and utilizing relevant approaches. Constructing multiple dimensions on approaches and managing them from these dimensions can provide an efficient solution. Firstly, this paper identifies approach patterns using a top-down way, refining the patterns through four distinct linguistic levels: semantic level, discourse level, syntactic level, and lexical level. Approaches in scientific papers are extracted based on approach patterns. Additionally, five dimensions for categorizing approaches are identified using these patterns. This paper proposes using tree structure to represent step and measuring the similarity between different steps with a tree-structure-based similarity measure that focuses on syntactic-level similarities. A collection similarity measure is proposed to compute the similarity between approaches. A bottom-up clustering algorithm is proposed to construct class trees for approach components within each dimension by merging each approach component or class with its most similar approach component or class in each iteration. The class labels generated during the clustering process indicate the common semantics of the step components within the approach components in each class and are used to manage the approaches within the class. The class trees of the five dimensions collectively form a multi-dimensional approach space. The application of approach queries on the multi-dimensional approach space demonstrates that querying within this space ensures strong relevance between user queries and results and rapidly reduces search space through a class-based query mechanism.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics</title>
<link>https://arxiv.org/abs/2506.00637</link>
<guid>https://arxiv.org/abs/2506.00637</guid>
<content:encoded><![CDATA[
arXiv:2506.00637v2 Announce Type: replace 
Abstract: Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could distribute its output probability among multiple sequences because they are all valid. We propose task-agnostic confidence metrics suited to generation, which rely solely on the probabilities associated with the model outputs without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and QA datasets.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VM14K: First Vietnamese Medical Benchmark</title>
<link>https://arxiv.org/abs/2506.01305</link>
<guid>https://arxiv.org/abs/2506.01305</guid>
<content:encoded><![CDATA[
arXiv:2506.01305v2 Announce Type: replace 
Abstract: Medical benchmarks are indispensable for evaluating the capabilities of language models in healthcare for non-English-speaking communities,therefore help ensuring the quality of real-life applications. However, not every community has sufficient resources and standardized methods to effectively build and design such benchmark, and available non-English medical data is normally fragmented and difficult to verify. We developed an approach to tackle this problem and applied it to create the first Vietnamese medical question benchmark, featuring 14,000 multiple-choice questions across 34 medical specialties. Our benchmark was constructed using various verifiable sources, including carefully curated medical exams and clinical records, and eventually annotated by medical experts. The benchmark includes four difficulty levels, ranging from foundational biological knowledge commonly found in textbooks to typical clinical case studies that require advanced reasoning. This design enables assessment of both the breadth and depth of language models' medical understanding in the target language thanks to its extensive coverage and in-depth subject-specific expertise. We release the benchmark in three parts: a sample public set (4k questions), a full public set (10k questions), and a private set (2k questions) used for leaderboard evaluation. Each set contains all medical subfields and difficulty levels. Our approach is scalable to other languages, and we open-source our data construction pipeline to support the development of future multilingual benchmarks in the medical domain.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Sense Detection Leveraging Maximum Mean Discrepancy</title>
<link>https://arxiv.org/abs/2506.01602</link>
<guid>https://arxiv.org/abs/2506.01602</guid>
<content:encoded><![CDATA[
arXiv:2506.01602v2 Announce Type: replace 
Abstract: Word sense analysis is an essential analysis work for interpreting the linguistic and social backgrounds. The word sense change detection is a task of identifying and interpreting shifts in word meanings over time. This paper proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean Discrepancy (MMD) to select semantically meaningful variables and quantify changes across time periods. This method enables both the identification of words undergoing sense shifts and the explanation of their evolution over multiple historical periods. To my knowledge, this is the first application of MMD to word sense change detection. Empirical assessment results demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation</title>
<link>https://arxiv.org/abs/2506.04078</link>
<guid>https://arxiv.org/abs/2506.04078</guid>
<content:encoded><![CDATA[
arXiv:2506.04078v2 Announce Type: replace 
Abstract: Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions</title>
<link>https://arxiv.org/abs/2403.07910</link>
<guid>https://arxiv.org/abs/2403.07910</guid>
<content:encoded><![CDATA[
arXiv:2403.07910v3 Announce Type: replace-cross 
Abstract: Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability. To address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection. To enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches. Our evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and scaling tasks leads to the best results. MAGPIE confirms that MTL is a promising approach for addressing media bias detection, enhancing the accuracy and efficiency of existing models. Furthermore, LBM is the first available resource collection focused on media bias MTL.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ad Auctions for LLMs via Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2406.09459</link>
<guid>https://arxiv.org/abs/2406.09459</guid>
<content:encoded><![CDATA[
arXiv:2406.09459v2 Announce Type: replace-cross 
Abstract: In the field of computational advertising, the integration of ads into the outputs of large language models (LLMs) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a segment auction where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the RAG framework, and priced according to competing bids. We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule. These results are extended to multi-ad allocation per segment. An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more flexibility to allocate ads.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An overview of domain-specific foundation model: key technologies, applications and challenges</title>
<link>https://arxiv.org/abs/2409.04267</link>
<guid>https://arxiv.org/abs/2409.04267</guid>
<content:encoded><![CDATA[
arXiv:2409.04267v2 Announce Type: replace-cross 
Abstract: The impressive performance of ChatGPT and other foundation-model-based products in human language understanding has prompted both academia and industry to explore how these models can be tailored for specific industries and application scenarios. This process, known as the customization of domain-specific foundation models (FMs), addresses the limitations of general-purpose models, which may not fully capture the unique patterns and requirements of domain-specific data. Despite its importance, there is a notable lack of comprehensive overview papers on building domain-specific FMs, while numerous resources exist for general-purpose models. To bridge this gap, this article provides a timely and thorough overview of the methodology for customizing domain-specific FMs. It introduces basic concepts, outlines the general architecture, and surveys key methods for constructing domain-specific models. Furthermore, the article discusses various domains that can benefit from these specialized models and highlights the challenges ahead. Through this overview, we aim to offer valuable guidance and reference for researchers and practitioners from diverse fields to develop their own customized FMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jointly modelling the evolution of social structure and language in online communities</title>
<link>https://arxiv.org/abs/2409.19243</link>
<guid>https://arxiv.org/abs/2409.19243</guid>
<content:encoded><![CDATA[
arXiv:2409.19243v2 Announce Type: replace-cross 
Abstract: Group interactions take place within a particular socio-temporal context, which should be taken into account when modelling interactions in online communities. We propose a method for jointly modelling community structure and language over time. Our system produces dynamic word and user representations that can be used to cluster users, investigate thematic interests of groups, and predict group membership. We apply and evaluate our method in the context of a set of misogynistic extremist groups. Our results indicate that this approach outperforms prior models which lacked one of these components (i.e. not incorporating social structure, or using static word embeddings) when evaluated on clustering and embedding prediction tasks. Our method further enables novel types of analyses on online groups, including tracing their response to temporal events and quantifying their propensity for using violent language, which is of particular importance in the context of extremist groups.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Glider: Global and Local Instruction-Driven Expert Router</title>
<link>https://arxiv.org/abs/2410.07172</link>
<guid>https://arxiv.org/abs/2410.07172</guid>
<content:encoded><![CDATA[
arXiv:2410.07172v2 Announce Type: replace-cross 
Abstract: The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to particular domains. This has enabled the creation of powerful and adaptive routing-based "Model MoErging" methods with the goal of using expert modules to create an aggregate system with improved performance or generalization. However, existing MoErging methods often prioritize generalization to unseen tasks at the expense of performance on held-in tasks, which limits its practical applicability in real-world deployment scenarios. We observe that current token-level routing mechanisms neglect the global semantic context of the input task. This token-wise independence hinders effective expert selection for held-in tasks, as routing decisions fail to incorporate the semantic properties of the task. To address this, we propose, Global and Local Instruction Driven Expert Router (GLIDER) that integrates a multi-scale routing mechanism, encompassing a semantic global router and a learned local router. The global router leverages LLM's advanced reasoning capabilities for semantic-related contexts to enhance expert selection. Given the input query and LLM, the router generates semantic task instructions that guide the retrieval of the most relevant experts across all layers. This global guidance is complemented by a local router that facilitates token-level routing decisions within each module, enabling finer control and enhanced performance on unseen tasks. Our experiments using T5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves substantially improved held-in performance while maintaining strong generalization on held-out tasks. We also perform ablations experiments to dive deeper into the components of GLIDER. Our experiments highlight the importance of our multi-scale routing that leverages LLM-driven semantic reasoning for MoErging methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts</title>
<link>https://arxiv.org/abs/2410.14375</link>
<guid>https://arxiv.org/abs/2410.14375</guid>
<content:encoded><![CDATA[
arXiv:2410.14375v2 Announce Type: replace-cross 
Abstract: Adapting to latent-confounded shifts remains a core challenge in modern AI. These shifts are propagated via latent variables that induce spurious, non-transportable correlations between inputs and labels. One practical failure mode arises when fine-tuning pre-trained foundation models on confounded data (e.g., where certain text tokens or image backgrounds spuriously correlate with the label), leaving models vulnerable at deployment. We frame causal fine-tuning as an identification problem and pose an explicit causal model that decomposes inputs into low-level spurious features and high-level causal representations. Under this family of models, we formalize the assumptions required for identification. Using pre-trained language models as a case study, we show how identifying and adjusting these components during causal fine-tuning enables automatic adaptation to latent-confounded shifts at test time. Experiments on semi-synthetic benchmarks derived from real-world problems demonstrate that our method outperforms black-box domain generalization baselines, illustrating the benefits of explicitly modeling causal structure.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Post-training via Inverse Value Learning</title>
<link>https://arxiv.org/abs/2410.21027</link>
<guid>https://arxiv.org/abs/2410.21027</guid>
<content:encoded><![CDATA[
arXiv:2410.21027v2 Announce Type: replace-cross 
Abstract: As post-training processes utilize increasingly large datasets and base models continue to grow in size, the computational demands and implementation challenges of existing algorithms are escalating significantly. In this paper, we propose modeling the changes at the logits level during post-training using a separate neural network (i.e., the value network). After training this network on a small base model using demonstrations, this network can be seamlessly integrated with other pre-trained models during inference, enables them to achieve similar capability enhancements. We systematically investigate the best practices for this paradigm in terms of pre-training weights and connection schemes. We demonstrate that the resulting value network has broad transferability across pre-trained models of different parameter sizes within the same family, models undergoing continuous pre-training within the same family, and models with different vocabularies across families. In certain cases, it can achieve performance comparable to full-parameter fine-tuning. Furthermore, we explore methods to enhance the transferability of the value model and prevent overfitting to the base model used during training.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy Controllable Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2411.07595</link>
<guid>https://arxiv.org/abs/2411.07595</guid>
<content:encoded><![CDATA[
arXiv:2411.07595v2 Announce Type: replace-cross 
Abstract: In the post-training of large language models (LLMs), Reinforcement Learning from Human Feedback (RLHF) is an effective approach to achieve generation aligned with human preferences. Direct Preference Optimization (DPO) allows for policy training with a simple binary cross-entropy loss without a reward model. The objective of DPO is regularized by reverse KL divergence that encourages mode-seeking fitting to the reference policy. Nonetheless, we indicate that minimizing reverse KL divergence could fail to capture a mode of the reference distribution, which may hurt the policy's performance. Based on this observation, we propose a simple modification to DPO, H-DPO, which allows for control over the entropy of the resulting policy, enhancing the distribution's sharpness and thereby enabling mode-seeking fitting more effectively. In our experiments, we show that H-DPO outperformed DPO across various tasks, demonstrating superior results in pass@$k$ evaluations for mathematical tasks. Moreover, H-DPO is simple to implement, requiring only minor modifications to the loss calculation of DPO, which makes it highly practical and promising for wide-ranging applications in the training of LLMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T1: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling</title>
<link>https://arxiv.org/abs/2501.11651</link>
<guid>https://arxiv.org/abs/2501.11651</guid>
<content:encoded><![CDATA[
arXiv:2501.11651v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks. However, existing approaches mainly rely on imitation learning and struggle to achieve effective test-time scaling. While reinforcement learning (RL) holds promise for enabling self-exploration, recent attempts yield modest improvements in complex reasoning. In this paper, we present T1 to scale RL by encouraging exploration and understand inference scaling. We first initialize the LLM using synthesized chain-of-thought data that integrates trial-and-error and self-verification. To scale RL training, we promote increased sampling diversity through oversampling. We demonstrate that T1 with open LLMs as its base exhibits inference scaling behavior and achieves superior performance on challenging math reasoning benchmarks. More importantly, we present a simple strategy to examine inference scaling, where increased inference budgets directly lead to T1's better performance without any additional verification.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</title>
<link>https://arxiv.org/abs/2501.18638</link>
<guid>https://arxiv.org/abs/2501.18638</guid>
<content:encoded><![CDATA[
arXiv:2501.18638v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly prevalent, ensuring their robustness against adversarial misuse is crucial. This paper introduces the GAP (Graph of Attacks with Pruning) framework, an advanced approach for generating stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP addresses limitations in existing tree-based LLM jailbreak methods by implementing an interconnected graph structure that enables knowledge sharing across attack paths. Our experimental evaluation demonstrates GAP's superiority over existing techniques, achieving a 20.8% increase in attack success rates while reducing query costs by 62.7%. GAP consistently outperforms state-of-the-art methods for attacking both open and closed LLMs, with attack success rates of >96%. Additionally, we present specialized variants like GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks. GAP-generated prompts prove highly effective in improving content moderation systems, increasing true positive detection rates by 108.5% and accuracy by 183.6% when used for fine-tuning. Our implementation is available at https://github.com/dsbuddy/GAP-LLM-Safety.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models for Edge Networks: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2502.07855</link>
<guid>https://arxiv.org/abs/2502.07855</guid>
<content:encoded><![CDATA[
arXiv:2502.07855v2 Announce Type: replace-cross 
Abstract: Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[
arXiv:2504.10514v2 Announce Type: replace-cross 
Abstract: Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing multimodal analogical reasoning with Logic Augmented Generation</title>
<link>https://arxiv.org/abs/2504.11190</link>
<guid>https://arxiv.org/abs/2504.11190</guid>
<content:encoded><![CDATA[
arXiv:2504.11190v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models have demonstrated their capabilities across a variety of tasks. However, automatically extracting implicit knowledge from natural language remains a significant challenge, as machines lack active experience with the physical world. Given this scenario, semantic knowledge graphs can serve as conceptual spaces that guide the automated text generation reasoning process to achieve more efficient and explainable results. In this paper, we apply a logic-augmented generation (LAG) framework that leverages the explicit representation of a text through a semantic knowledge graph and applies it in combination with prompt heuristics to elicit implicit analogical connections. This method generates extended knowledge graph triples representing implicit meaning, enabling systems to reason on unlabeled multimodal data regardless of the domain. We validate our work through three metaphor detection and understanding tasks across four datasets, as they require deep analogical reasoning capabilities. The results show that this integrated approach surpasses current baselines, performs better than humans in understanding visual metaphors, and enables more explainable reasoning processes, though still has inherent limitations in metaphor understanding, especially for domain-specific metaphors. Furthermore, we propose a thorough error analysis, discussing issues with metaphorical annotations and current evaluation methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents</title>
<link>https://arxiv.org/abs/2504.13128</link>
<guid>https://arxiv.org/abs/2504.13128</guid>
<content:encoded><![CDATA[
arXiv:2504.13128v2 Announce Type: replace-cross 
Abstract: We introduce FreshStack, a holistic framework for automatically building information retrieval (IR) evaluation benchmarks by incorporating challenging questions and answers. FreshStack conducts the following steps: (1) automatic corpus collection from code and technical documentation, (2) nugget generation from community-asked questions and answers, and (3) nugget-level support, retrieving documents using a fusion of retrieval techniques and hybrid architectures. We use FreshStack to build five datasets on fast-growing, recent, and niche topics to ensure the tasks are sufficiently challenging. On FreshStack, existing retrieval models, when applied out-of-the-box, significantly underperform oracle approaches on all five topics, denoting plenty of headroom to improve IR quality. In addition, we identify cases where rerankers do not improve first-stage retrieval accuracy (two out of five topics) and oracle context helps an LLM generator generate a high-quality RAG answer. We hope FreshStack will facilitate future work toward constructing realistic, scalable, and uncontaminated IR and RAG evaluation benchmarks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer</title>
<link>https://arxiv.org/abs/2506.01115</link>
<guid>https://arxiv.org/abs/2506.01115</guid>
<content:encoded><![CDATA[
arXiv:2506.01115v2 Announce Type: replace-cross 
Abstract: The Transformer architecture is central to the success of modern Large Language Models (LLMs), in part due to its surprising ability to perform a wide range of algorithmic tasks -- including mathematical reasoning, memorization, and retrieval -- using only gradient-based training on next-token prediction. While the core component of a Transformer is the self-attention mechanism, we question how much, and which aspects, of the performance gains can be attributed to it. To this end, we compare standard Transformers to variants in which either the multi-layer perceptron (MLP) layers or the attention projectors (queries and keys) are frozen at initialization. To further isolate the contribution of attention, we introduce MixiT -- the Mixing Transformer -- a simplified, principled model in which the attention coefficients are entirely random and fixed at initialization, eliminating any input-dependent computation or learning in attention. Surprisingly, we find that MixiT matches the performance of fully trained Transformers on various algorithmic tasks, especially those involving basic arithmetic or focusing heavily on memorization. For retrieval-based tasks, we observe that having input-dependent attention coefficients is consistently beneficial, while MixiT underperforms. We attribute this failure to its inability to form specialized circuits such as induction heads -- a specific circuit known to be crucial for learning and exploiting repeating patterns in input sequences. Even more interestingly, we find that attention with frozen key and query projectors is not only able to form induction heads, but can also perform competitively on language modeling. Our results underscore the importance of architectural heterogeneity, where distinct components contribute complementary inductive biases crucial for solving different classes of tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models</title>
<link>https://arxiv.org/abs/2506.04210</link>
<guid>https://arxiv.org/abs/2506.04210</guid>
<content:encoded><![CDATA[
arXiv:2506.04210v2 Announce Type: replace-cross 
Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like "Wait" or "Let me rethink" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to "overthinking". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from "more thinking" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model</title>
<link>https://arxiv.org/abs/2506.04518</link>
<guid>https://arxiv.org/abs/2506.04518</guid>
<content:encoded><![CDATA[
arXiv:2506.04518v2 Announce Type: replace-cross 
Abstract: Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Reliable Evaluation Metrics for Scientific Text Revision</title>
<link>https://arxiv.org/abs/2506.04772</link>
<guid>https://arxiv.org/abs/2506.04772</guid>
<content:encoded><![CDATA[
<div> Keywords: text revision, evaluation metrics, manual annotation study, LLM-as-a-judge, revision quality assessment

Summary:
This study addresses the challenges of evaluating text revision in scientific writing. Traditional metrics like ROUGE and BERTScore focus on similarity rather than meaningful improvements. The study identifies limitations of these metrics and explores alternative evaluation methods aligned with human judgments. A manual annotation study is conducted to assess revision quality, revealing that LLMs are effective at assessing instruction-following but struggle with correctness. Reference-free evaluation metrics from NLP domains are also investigated, along with LLM-as-a-judge approaches to assess revisions with and without a gold reference. Domain-specific metrics provide complementary insights, and a hybrid approach combining LLM evaluation and task-specific metrics proves to be the most reliable for assessing revision quality.<br /><br />Summary: <div>
arXiv:2506.04772v3 Announce Type: replace 
Abstract: Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the limitations of these metrics and explore alternative evaluation methods that better align with human judgments. We first conduct a manual annotation study to assess the quality of different revisions. Then, we investigate reference-free evaluation metrics from related NLP domains. Additionally, we examine LLM-as-a-judge approaches, analysing their ability to assess revisions with and without a gold reference. Our results show that LLMs effectively assess instruction-following but struggle with correctness, while domain-specific metrics provide complementary insights. We find that a hybrid approach combining LLM-as-a-judge evaluation and task-specific metrics offers the most reliable assessment of revision quality.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Is Not Comprehension</title>
<link>https://arxiv.org/abs/2506.04907</link>
<guid>https://arxiv.org/abs/2506.04907</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Verbose ListOps, reasoning, comprehension, benchmark <br />
<br />
Summary: The article introduces a new benchmark called Verbose ListOps (VLO) that challenges Large Language Models (LLMs) to perform multi-step reasoning and track intermediate states within narrative camouflage. Unlike traditional tests that focus on factual recall from long inputs, VLO evaluates models based on their ability to solve deterministic ListOps computations embedded in narratives. Models that excel at raw ListOps struggle with VLO, demonstrating a gap in their reasoning capabilities. By allowing step-level evaluation of every intermediate result, VLO goes beyond context length assessment to measure genuine comprehension. The benchmark's task-agnostic generation pipeline can incorporate various reasoning schemas, making it a valuable tool for assessing the next generation of reasoning-centric model designs. <div>
arXiv:2506.04907v4 Announce Type: replace 
Abstract: The dominant way of judging Large Language Models (LLMs) has been to ask how well they can recall explicit facts from very long inputs. While today's best models achieve near perfect recall, this masks a harder skill: performing multi-step reasoning and tracking intermediate state that never appears verbatim. We introduce Verbose ListOps (VLO), a benchmark that embeds deterministic ListOps computations inside narrative camouflage and, crucially, allows step-level evaluation of every intermediate result. Experiments show that models which solve raw ListOps with approximately 100% accuracy collapse on VLO after only 10,000 tokens. By exposing where a model's reasoning chain first diverges, VLO moves assessment beyond sheer context length and toward genuine comprehension. VLO's generation pipeline is task-agnostic: it can weave any deterministically verifiable reasoning schema -- arithmetic, symbolic, abductive, inductive or defeasible -- into narrative form. This makes VLO a reusable test-bed for the next wave of reasoning-centric model designs, not merely those with step-explicit scaffolds.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations</title>
<link>https://arxiv.org/abs/2506.10019</link>
<guid>https://arxiv.org/abs/2506.10019</guid>
<content:encoded><![CDATA[
<div> deep learning, generative AI, automatic evaluation, text generation, cross-modal evaluation

Summary:<br /><br />
Recent advancements in deep learning have greatly enhanced the capabilities of generative AI in generating text, images, and audio. However, evaluating the quality of these generated outputs remains a challenge. The article introduces a systematic review and taxonomy of automatic evaluation methods across text, visual, and audio modalities. It identifies five fundamental paradigms that characterize current evaluation approaches in these domains. The analysis initially focuses on text generation evaluation methods, which are more developed, and then expands to encompass image and audio generation evaluation techniques, showcasing their wide applicability. The article also highlights potential research directions for future cross-modal evaluation methodologies. <div>
arXiv:2506.10019v1 Announce Type: new 
Abstract: Recent advances in deep learning have significantly enhanced generative AI capabilities across text, images, and audio. However, automatically evaluating the quality of these generated outputs presents ongoing challenges. Although numerous automatic evaluation methods exist, current research lacks a systematic framework that comprehensively organizes these methods across text, visual, and audio modalities. To address this issue, we present a comprehensive review and a unified taxonomy of automatic evaluation methods for generated content across all three modalities; We identify five fundamental paradigms that characterize existing evaluation approaches across these domains. Our analysis begins by examining evaluation methods for text generation, where techniques are most mature. We then extend this framework to image and audio generation, demonstrating its broad applicability. Finally, we discuss promising directions for future research in cross-modal evaluation methodologies.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaskCraft: Automated Generation of Agentic Tasks</title>
<link>https://arxiv.org/abs/2506.10055</link>
<guid>https://arxiv.org/abs/2506.10055</guid>
<content:encoded><![CDATA[
<div> automated workflow, difficulty-scalable, multi-tool, verifiable, agentic tasks<br />
<br />
Summary: 
The article introduces TaskCraft, an automated workflow designed to generate difficulty-scalable, multi-tool, and verifiable agentic tasks for NLP and AI advancement. These tasks involve multi-step problem solving with autonomy, tool use, and adaptive reasoning. Existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting scalability. TaskCraft expands atomic tasks to create complex challenges using depth-based and width-based extensions. The generated tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. Furthermore, a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty levels is presented to support future research on agent tuning and evaluation. <div>
arXiv:2506.10055v1 Announce Type: new 
Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A quantum semantic framework for natural language processing</title>
<link>https://arxiv.org/abs/2506.10077</link>
<guid>https://arxiv.org/abs/2506.10077</guid>
<content:encoded><![CDATA[
<div> complexity, semantic degeneracy, large language models, interpretive constraints, Bayesian-style repeated sampling <br />
Summary:<br /> 
- Semantic degeneracy in natural language leads to a vast array of potential interpretations as complexity increases, impacting both humans and large language models.
- Kolmogorov complexity suggests that as expressions become more complex, the chance of accurately recovering the intended meaning decreases significantly.
- Meaning in language is not inherent but rather depends on the interpretive act of the observer.
- Experimenting with diverse large language models interpreting ambiguous word pairs in varying contexts revealed non-classical contextuality, similar to human cognition results.
- Classical frequentist-based analytical approaches for natural language are limited, and Bayesian-style repeated sampling methods may offer more suitable characterizations of linguistic meaning in context. <br /> 
Summary: <div>
arXiv:2506.10077v1 Announce Type: new 
Abstract: Semantic degeneracy represents a fundamental property of natural language that extends beyond simple polysemy to encompass the combinatorial explosion of potential interpretations that emerges as semantic expressions increase in complexity. Large Language Models (LLMs) and other modern NLP systems face inherent limitations precisely because they operate within natural language itself, making them subject to the same interpretive constraints imposed by semantic degeneracy. In this work, we argue using Kolmogorov complexity that as an expression's complexity grows, the likelihood of any interpreting agent (human or LLM-powered AI) recovering the single intended meaning vanishes. This computational intractability suggests the classical view that linguistic forms possess meaning in and of themselves is flawed. We alternatively posit that meaning is instead actualized through an observer-dependent interpretive act. To test this, we conducted a semantic Bell inequality test using diverse LLM agents as ``computational cognitive systems'' to interpret ambiguous word pairs under varied contextual settings. Across several independent experiments, we found average CHSH expectation values ranging from 1.2 to 2.8, with several runs yielding values (e.g., 2.3-2.4) that significantly violate the classical boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under ambiguity can exhibit non-classical contextuality, consistent with results from human cognition experiments. These results inherently imply that classical frequentist-based analytical approaches for natural language are necessarily lossy. Instead, we propose that Bayesian-style repeated sampling approaches can provide more practically useful and appropriate characterizations of linguistic meaning in context.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information</title>
<link>https://arxiv.org/abs/2506.10086</link>
<guid>https://arxiv.org/abs/2506.10086</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent system, Failure Modes and Effects Analysis, industrial assets, Large Language Model, AI techniques<br />
Summary:<br />
This paper introduces Chat-of-Thought, a new multi-agent system designed to streamline the generation of Failure Modes and Effects Analysis (FMEA) documents for industrial assets. The system utilizes collaborative Large Language Model (LLM)-based agents with specific roles and advanced AI techniques to enhance the efficiency of creating and validating FMEA tables. A unique feature of Chat-of-Thought is the implementation of a Chat of Thought, where dynamic discussions among multiple personas allow for iterative content refinement. The research focuses on industrial equipment monitoring, addressing challenges in this domain by leveraging interactive, template-driven workflows and context-aware agent collaboration. Overall, Chat-of-Thought demonstrates the potential to improve the FMEA process through intelligent agent interaction and collaboration. 

<br /><br />Summary: <div>
arXiv:2506.10086v1 Announce Type: new 
Abstract: This paper presents a novel multi-agent system called Chat-of-Thought, designed to facilitate the generation of Failure Modes and Effects Analysis (FMEA) documents for industrial assets. Chat-of-Thought employs multiple collaborative Large Language Model (LLM)-based agents with specific roles, leveraging advanced AI techniques and dynamic task routing to optimize the generation and validation of FMEA tables. A key innovation in this system is the introduction of a Chat of Thought, where dynamic, multi-persona-driven discussions enable iterative refinement of content. This research explores the application domain of industrial equipment monitoring, highlights key challenges, and demonstrates the potential of Chat-of-Thought in addressing these challenges through interactive, template-driven workflows and context-aware agent collaboration.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs</title>
<link>https://arxiv.org/abs/2506.10095</link>
<guid>https://arxiv.org/abs/2506.10095</guid>
<content:encoded><![CDATA[
<div> semantic intent, prompt variance, large language models, PBSS, response shifts <br />
Summary:<br />
The study explores how large language models react to prompts that have the same semantic intent but vary in token-level realization, known as prompt variance. The researchers introduce Prompt-Based Semantic Shift (PBSS) as a framework to measure behavioral changes in LLMs when presented with semantically equivalent prompt rephrasings. Analyses conducted on ten constrained tasks using PBSS reveal consistent shifts in model responses, indicating model-specific patterns linked to tokenization and decoding processes. This suggests that tokenization strategies and decoding dynamics could impact the stability of model evaluations under prompt rewordings. The findings underscore the importance of considering how post-training quality of service may be affected by variations in prompt formulation. <br /><br /> <div>
arXiv:2506.10095v1 Announce Type: new 
Abstract: We investigate how large language models respond to prompts that differ only in their token-level realization but preserve the same semantic intent, a phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift (PBSS), a diagnostic framework for measuring behavioral drift in LLMs under semantically equivalent prompt rewordings. Applied to ten constrained tasks, PBSS reveals consistent, model-specific response shifts, suggesting statistical regularities linked to tokenization and decoding. These results highlight an overlooked dimension of model evaluation stability under rephrasing and suggest that tokenization strategies and decoding dynamics may contribute to post-training quality of service instability.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering</title>
<link>https://arxiv.org/abs/2506.10116</link>
<guid>https://arxiv.org/abs/2506.10116</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, visual reasoning tasks, ChartReasoner, ECharts codes, multimodal model<br />
Summary: <br />
The article discusses the challenge of extending the reasoning capabilities of large language models to visual reasoning tasks. The proposed solution, ChartReasoner, is a two-stage framework that converts chart images into structured ECharts codes to preserve critical structural and semantic information. A chart reasoning data synthesis pipeline utilizes a pretrained transport model to generate chart reasoning trajectories and filters out low-quality samples using a code validator. The final multimodal model is trained using supervised fine-tuning and reinforcement learning on a synthesized chart reasoning dataset. Experimental results on public benchmarks demonstrate ChartReasoner's effectiveness in preserving chart details and performing comparably with state-of-the-art models while using fewer parameters. The model also shows promise in out-of-domain settings, approaching the performance of proprietary systems like GPT-4o. <br /> <div>
arXiv:2506.10116v1 Announce Type: new 
Abstract: Recently, large language models have shown remarkable reasoning capabilities through long-chain reasoning before responding. However, how to extend this capability to visual reasoning tasks remains an open challenge. Existing multimodal reasoning approaches transfer such visual reasoning task into textual reasoning task via several image-to-text conversions, which often lose critical structural and semantic information embedded in visualizations, especially for tasks like chart question answering that require a large amount of visual details. To bridge this gap, we propose ChartReasoner, a code-driven novel two-stage framework designed to enable precise, interpretable reasoning over charts. We first train a high-fidelity model to convert diverse chart images into structured ECharts codes, preserving both layout and data semantics as lossless as possible. Then, we design a general chart reasoning data synthesis pipeline, which leverages this pretrained transport model to automatically and scalably generate chart reasoning trajectories and utilizes a code validator to filter out low-quality samples. Finally, we train the final multimodal model using a combination of supervised fine-tuning and reinforcement learning on our synthesized chart reasoning dataset and experimental results on four public benchmarks clearly demonstrate the effectiveness of our proposed ChartReasoner. It can preserve the original details of the charts as much as possible and perform comparably with state-of-the-art open-source models while using fewer parameters, approaching the performance of proprietary systems like GPT-4o in out-of-domain settings.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Elicitation of Language Models</title>
<link>https://arxiv.org/abs/2506.10139</link>
<guid>https://arxiv.org/abs/2506.10139</guid>
<content:encoded><![CDATA[
<div> algorithm, pretrained language models, downstream tasks, unsupervised, internal coherence maximization

Summary:
Internal Coherence Maximization (ICM) is introduced as an unsupervised algorithm to fine-tune pretrained language models without the need for external supervision. The method, applied on tasks such as GSM8k-verification, TruthfulQA, and Alpaca reward modeling, matches the performance of training with golden supervision and surpasses training with crowdsourced human supervision. When dealing with tasks that surpass human capabilities, ICM excels in eliciting these superhuman abilities better than training on human labels. Furthermore, the approach proves effective in enhancing the training of cutting-edge language models, enabling the creation of an unsupervised reward model and a Claude 3.5 Haiku-based assistant that outperform their human-supervised counterparts. <div>
arXiv:2506.10139v1 Announce Type: new 
Abstract: To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Large Language Models are Reliable for Judging Empathic Communication</title>
<link>https://arxiv.org/abs/2506.10150</link>
<guid>https://arxiv.org/abs/2506.10150</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, empathic communication, annotation, inter-rater reliability, expert agreement
Summary:<br /><br />Large language models (LLMs) are assessed for their ability to judge empathic communication by comparing expert, crowdworker, and LLM annotations on 200 real-world conversations. Expert agreement is high but varies across frameworks, offering a benchmark for gauging LLM performance. LLMs approach expert levels and outperform crowdworkers in reliability across all frameworks. This study highlights the potential of LLMs in emotionally sensitive applications, such as conversational companions, when validated with appropriate benchmarks. <div>
arXiv:2506.10150v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? We investigate this question by comparing how experts, crowdworkers, and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing, and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess inter-rater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks' sub-components depending on their clarity, complexity, and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME</title>
<link>https://arxiv.org/abs/2506.10154</link>
<guid>https://arxiv.org/abs/2506.10154</guid>
<content:encoded><![CDATA[
<div> Emotion analysis, written language, Bangla, machine learning models, social media comments \
<br /> \
Summary: \
Research on emotions in written Bangla language is expanding. This study analyzes emotions in social media comments using machine learning models such as Linear SVM, KNN, Random Forest, BiLSTM, and AdaBoost. Techniques include n-gram data analysis, PCA dimensionality reduction, and LIME for explanation. The goal is to advance sentiment analysis in languages with limited resources like Bangla. <div>
arXiv:2506.10154v1 Announce Type: new 
Abstract: Research on understanding emotions in written language continues to expand, especially for understudied languages with distinctive regional expressions and cultural features, such as Bangla. This study examines emotion analysis using 22,698 social media comments from the EmoNoBa dataset. For language analysis, we employ machine learning models: Linear SVM, KNN, and Random Forest with n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model and AdaBoost to improve decision trees. To make our machine learning models easier to understand, we used LIME to explain the predictions of the AdaBoost classifier, which uses decision trees. With the goal of advancing sentiment analysis in languages with limited resources, our work examines various techniques to find efficient techniques for emotion identification in Bangla.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities</title>
<link>https://arxiv.org/abs/2506.10155</link>
<guid>https://arxiv.org/abs/2506.10155</guid>
<content:encoded><![CDATA[
<div> Keywords: human capital, machine learning, HC-related keywords, BERT model, corporate communications

Summary:
Human capital (HC) is crucial for corporate value creation, yet lacks standardized measurement and disclosure rules. Using machine learning, a comprehensive list of HC-related keywords was developed, classified into five subcategories. The lexicon, corporate HC disclosures, and Python code for developing it were shared, along with examples for using the data and code. Researchers can utilize the HC lexicon for analyzing corporate communications and addressing relevant HC questions. Future research opportunities regarding HC management and disclosure were also discussed. <div>
arXiv:2506.10155v1 Announce Type: new 
Abstract: Human capital (HC) is increasingly important to corporate value creation. Unlike other assets, however, HC is not currently subject to well-defined measurement or disclosure rules. We use a machine learning algorithm (word2vec) trained on a confirmed set of HC disclosures to develop a comprehensive list of HC-related keywords classified into five subcategories (DEI; health and safety; labor relations and culture; compensation and benefits; and demographics and other) that capture the multidimensional nature of HC management. We share our lexicon, corporate HC disclosures, and the Python code used to develop the lexicon, and we provide detailed examples of using our data and code, including for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the code to capture another construct of interest) with their samples of corporate communications to address pertinent HC questions. We close with a discussion of future research opportunities related to HC management and disclosure.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective</title>
<link>https://arxiv.org/abs/2506.10161</link>
<guid>https://arxiv.org/abs/2506.10161</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, narrative planning, character intentionality, dramatic conflict <br />
<br />
Summary: <br />
This study explores the story generation capabilities of Large Language Models (LLMs) by using them to solve narrative planning problems. A benchmark for evaluating LLMs on narrative planning is presented, focusing on causal soundness, character intentionality, and dramatic conflict. The experiments show that GPT-4 level LLMs can generate causally sound stories at small scales, but struggle with character intentionality and dramatic conflict, requiring reinforcement learning for complex reasoning. The findings provide insights into the scale of stories LLMs can generate while maintaining quality and highlight challenges and considerations for utilizing LLM narrative planning in game environments. <div>
arXiv:2506.10161v1 Announce Type: new 
Abstract: Story generation has been a prominent application of Large Language Models (LLMs). However, understanding LLMs' ability to produce high-quality stories remains limited due to challenges in automatic evaluation methods and the high cost and subjectivity of manual evaluation. Computational narratology offers valuable insights into what constitutes a good story, which has been applied in the symbolic narrative planning approach to story generation. This work aims to deepen the understanding of LLMs' story generation capabilities by using them to solve narrative planning problems. We present a benchmark for evaluating LLMs on narrative planning based on literature examples, focusing on causal soundness, character intentionality, and dramatic conflict. Our experiments show that GPT-4 tier LLMs can generate causally sound stories at small scales, but planning with character intentionality and dramatic conflict remains challenging, requiring LLMs trained with reinforcement learning for complex reasoning. The results offer insights on the scale of stories that LLMs can generate while maintaining quality from different aspects. Our findings also highlight interesting problem solving behaviors and shed lights on challenges and considerations for applying LLM narrative planning in game environments.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</title>
<link>https://arxiv.org/abs/2506.10202</link>
<guid>https://arxiv.org/abs/2506.10202</guid>
<content:encoded><![CDATA[
<div> Keywords: Large-Language Models, Vision-Language Models, text-to-video retrieval, multimodal knowledge, entropy-based fusion scoring

Summary: 
Q2E is a novel method for enhancing text-to-video retrieval by extracting latent parametric knowledge from Large-Language Models and Vision-Language Models. It decomposes human queries to improve understanding and incorporates both visual and speech-based inputs. The method utilizes entropy-based fusion scoring for effective fusion of multimodal knowledge. Evaluations on diverse datasets demonstrate that Q2E outperforms existing baselines and integrating audio information significantly enhances retrieval performance. The approach is adaptable across datasets, domains, and different types of language and vision models, showcasing its versatility and effectiveness. The code and data have been made available for future research. <br /><br />Summary: <div>
arXiv:2506.10202v1 Announce Type: new 
Abstract: Recent approaches have shown impressive proficiency in extracting and leveraging parametric knowledge from Large-Language Models (LLMs) and Vision-Language Models (VLMs). In this work, we consider how we can improve the identification and retrieval of videos related to complex real-world events by automatically extracting latent parametric knowledge about those events. We present Q2E: a Query-to-Event decomposition method for zero-shot multilingual text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our approach demonstrates that we can enhance the understanding of otherwise overly simplified human queries by decomposing the query using the knowledge embedded in LLMs and VLMs. We additionally show how to apply our approach to both visual and speech-based inputs. To combine this varied multimodal knowledge, we adopt entropy-based fusion scoring for zero-shot fusion. Through evaluations on two diverse datasets and multiple retrieval metrics, we demonstrate that Q2E outperforms several state-of-the-art baselines. Our evaluation also shows that integrating audio information can significantly improve text-to-video retrieval. We have released code and data for future research.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games</title>
<link>https://arxiv.org/abs/2506.10209</link>
<guid>https://arxiv.org/abs/2506.10209</guid>
<content:encoded><![CDATA[
<div> LRMs, reasoning models, TTT-Bench, Tic-Tac-Toe, strategic reasoning<br />
Summary:<br />
The study introduces a new benchmark, TTT-Bench, to assess basic strategic, spatial, and logical reasoning abilities in Large reasoning models (LRMs) through Tic-Tac-Toe-style games. LRMs that excel at hard math problems struggle with simple reasoning games. Results show that LRMs score lower on TTT-Bench compared to math problems. Larger models perform better with shorter reasoning traces but struggle with long-term strategic reasoning tasks. The study highlights the need to explore LRMs' reasoning abilities beyond STEM domains and emphasizes the importance of evaluating reasoning models in varied task domains to understand their capabilities accurately.
<br /><br />Summary: <div>
arXiv:2506.10209v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning capabilities across a broad range of tasks including Olympiad-level mathematical problems, indicating evidence of their complex reasoning abilities. While many reasoning benchmarks focus on the STEM domain, the ability of LRMs to reason correctly in broader task domains remains underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark that is designed to evaluate basic strategic, spatial, and logical reasoning abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games that humans can effortlessly solve from a young age. We propose a simple yet scalable programmatic approach for generating verifiable two-player game problems for TTT-Bench. Although these games are trivial for humans, they require reasoning about the intentions of the opponent, as well as the game board's spatial configurations, to ensure a win. We evaluate a diverse set of state-of-the-art LRMs, and \textbf{discover that the models that excel at hard math problems frequently fail at these simple reasoning games}. Further testing reveals that our evaluated reasoning models score on average $\downarrow$ 41\% \& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024 respectively, with larger models achieving higher performance using shorter reasoning traces, where most of the models struggle on long-term strategic reasoning situations on simple and new TTT-Bench tasks.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifying Unreliable Narrators with Large Language Models</title>
<link>https://arxiv.org/abs/2506.10231</link>
<guid>https://arxiv.org/abs/2506.10231</guid>
<content:encoded><![CDATA[
<div> Keywords: unreliable narrators, computational methods, human-annotated dataset, classification tasks, LLMs

Summary:
Unreliable narrators, who unintentionally misrepresent information, are a focus of this study using computational methods. TUNa, a dataset of narratives from various domains, is introduced to identify different types of unreliable narrators based on textual phenomena. Classification tasks for intra-narrational, inter-narrational, and inter-textual unreliabilities are defined and LLMs are tested for their performance. The study experiments with few-shot, fine-tuning, and curriculum learning settings to analyze the challenging task of unreliable narrator classification. The results indicate the potential use of LLMs in this area and invite further research. The expert-annotated dataset and code are released for future exploration.
<br /><br />Summary: <div>
arXiv:2506.10231v1 Announce Type: new 
Abstract: Often when we interact with a first-person account of events, we consider whether or not the narrator, the primary speaker of the text, is reliable. In this paper, we propose using computational methods to identify unreliable narrators, i.e. those who unintentionally misrepresent information. Borrowing literary theory from narratology to define different types of unreliable narrators based on a variety of textual phenomena, we present TUNa, a human-annotated dataset of narratives from multiple domains, including blog posts, subreddit posts, hotel reviews, and works of literature. We define classification tasks for intra-narrational, inter-narrational, and inter-textual unreliabilities and analyze the performance of popular open-weight and proprietary LLMs for each. We propose learning from literature to perform unreliable narrator classification on real-world text data. To this end, we experiment with few-shot, fine-tuning, and curriculum learning settings. Our results show that this task is very challenging, and there is potential for using LLMs to identify unreliable narrators. We release our expert-annotated dataset and code and invite future research in this area.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese</title>
<link>https://arxiv.org/abs/2506.10245</link>
<guid>https://arxiv.org/abs/2506.10245</guid>
<content:encoded><![CDATA[
<div> Keywords: ToxSyn-PT, Portuguese corpus, hate-speech classification, synthetic data, low-resource settings<br />
<br />
Summary: <br />
- Introduction of ToxSyn-PT, a large-scale Portuguese corpus for fine-grained hate-speech classification across nine legally protected minority groups<br />
- Dataset contains 53,274 synthetic sentences evenly distributed among minority groups and toxicity labels<br />
- Creation process involves a four-stage pipeline including manual curation, few-shot expansion, paraphrase-based augmentation, and enrichment with additional neutral texts<br />
- ToxSyn-PT is class-balanced, stylistically diverse, and not from the social-media domain, setting it apart from existing Portuguese datasets<br />
- Despite domain differences, experiments show strong results in both binary and multi-label classification on five public Portuguese hate-speech datasets, demonstrating robust generalization<br />
- Dataset is made publicly available to support research on synthetic data and hate-speech detection in low-resource settings. <br /> 
Summary: <div>
arXiv:2506.10245v1 Announce Type: new 
Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables fine-grained hate-speech classification across nine legally protected minority groups. The dataset contains 53,274 synthetic sentences equally distributed between minorities groups and toxicity labels. ToxSyn-PT is created through a novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and (4) enrichment, plus additional neutral texts to curb overfitting to group-specific cues. The resulting corpus is class-balanced, stylistically diverse, and free from the social-media domain that dominate existing Portuguese datasets. Despite domain differences with traditional benchmarks, experiments on both binary and multi-label classification on the corpus yields strong results across five public Portuguese hate-speech datasets, demonstrating robust generalization even across domain boundaries. The dataset is publicly released to advance research on synthetic data and hate-speech detection in low-resource settings.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models</title>
<link>https://arxiv.org/abs/2506.10268</link>
<guid>https://arxiv.org/abs/2506.10268</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, decision-making, Bayesian brains, deterministic behavior, Gibbs sampling

Summary:
Language models are typically viewed as probabilistic models generating sentences through iterative sampling. However, this study challenges this assumption by demonstrating that large language models can exhibit near-deterministic decision-making, even with some level of sampling randomness. This finding questions previous methods used to infer the priors of language models based on simulated Gibbs sampling. The research also highlights the potential for deterministic behavior in language models to lead to the convergence of false priors during inference. To mitigate this issue, a simple approach is proposed to differentiate between stochastic and deterministic decision patterns in Gibbs sampling, aiding in preventing the derivation of misleading language model priors. Experimenting on various large language models, the study provides valuable insights into understanding the decision-making processes of these models. 

<br /><br />Summary: <div>
arXiv:2506.10268v1 Announce Type: new 
Abstract: Language models are essentially probability distributions over token sequences. Auto-regressive models generate sentences by iteratively computing and sampling from the distribution of the next token. This iterative sampling introduces stochasticity, leading to the assumption that language models make probabilistic decisions, similar to sampling from unknown distributions. Building on this assumption, prior research has used simulated Gibbs sampling, inspired by experiments designed to elicit human priors, to infer the priors of language models. In this paper, we revisit a critical question: Do language models possess Bayesian brains? Our findings show that under certain conditions, language models can exhibit near-deterministic decision-making, such as producing maximum likelihood estimations, even with a non-zero sampling temperature. This challenges the sampling assumption and undermines previous methods for eliciting human-like priors. Furthermore, we demonstrate that without proper scrutiny, a system with deterministic behavior undergoing simulated Gibbs sampling can converge to a "false prior." To address this, we propose a straightforward approach to distinguish between stochastic and deterministic decision patterns in Gibbs sampling, helping to prevent the inference of misleading language model priors. We experiment on a variety of large language models to identify their decision patterns under various circumstances. Our results provide key insights in understanding decision making of large language models.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2506.10288</link>
<guid>https://arxiv.org/abs/2506.10288</guid>
<content:encoded><![CDATA[
<div> Keywords: Gradient-based data selection, clustering, Upper Confidence Bound algorithm, large language models, computing efficiency

Summary: 
The article introduces an efficient gradient-based data selection framework called ClusterUCB for supervised fine-tuning of large language models. By leveraging clustering and a modified Upper Confidence Bound algorithm, the framework can significantly reduce the computing resources required for data selection during the fine-tuning process. The approach involves clustering the training data pool based on similar gradient features, treating inter-cluster data selection as a budget allocation problem, and solving it using a modified UCB algorithm. Historical data influence information is recorded to estimate the distributions of each cluster, balancing exploration and exploitation with a cold start strategy. Experimental results demonstrate that ClusterUCB achieves comparable results to traditional gradient-based methods while drastically reducing computing consumption. This innovative approach can enhance the efficiency of data selection in training large language models. 

<br /><br />Summary: <div>
arXiv:2506.10288v1 Announce Type: new 
Abstract: Gradient-based data influence approximation has been leveraged to select useful data samples in the supervised fine-tuning of large language models. However, the computation of gradients throughout the fine-tuning process requires too many resources to be feasible in practice. In this paper, we propose an efficient gradient-based data selection framework with clustering and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition that data samples with similar gradient features will have similar influences, we first perform clustering on the training data pool. Then, we frame the inter-cluster data selection as a constrained computing budget allocation problem and consider it a multi-armed bandit problem. A modified UCB algorithm is leveraged to solve this problem. Specifically, during the iterative sampling process, historical data influence information is recorded to directly estimate the distributions of each cluster, and a cold start is adopted to balance exploration and exploitation. Experimental results on various benchmarks show that our proposed framework, ClusterUCB, can achieve comparable results to the original gradient-based data selection methods while greatly reducing computing consumption.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages</title>
<link>https://arxiv.org/abs/2506.10292</link>
<guid>https://arxiv.org/abs/2506.10292</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, few-label classification, low-resource languages, pseudo-label refinement, language models

Summary: 
Flick, a new approach to few-label text classification in low-resource linguistic contexts, addresses challenges in noisy pseudo-labels and domain adaptation by refining pseudo-labels through cluster identification. By distilling high-confidence pseudo-labels from initial clusters, Flick improves label quality for languages like Arabic and Urdu. The novel pseudo-label refinement component focuses on single-cluster cohesion and adaptive top-k selection, reducing error propagation in low-resource data. Flick's efficacy is demonstrated across 14 datasets, showcasing superior performance and adaptability, especially in linguistically diverse and low-resource settings.<br /><br />Summary: <div>
arXiv:2506.10292v1 Announce Type: new 
Abstract: Training deep learning networks with minimal supervision has gained significant research attention due to its potential to reduce reliance on extensive labelled data. While self-training methods have proven effective in semi-supervised learning, they remain vulnerable to errors from noisy pseudo labels. Moreover, most recent approaches to the few-label classification problem are either designed for resource-rich languages such as English or involve complex cascading models that are prone to overfitting. To address the persistent challenge of few-label text classification in truly low-resource linguistic contexts, where existing methods often struggle with noisy pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods that rely on generic multi-cluster pseudo-labelling or complex cascading architectures, Flick leverages the fundamental insight that distilling high-confidence pseudo-labels from a broader set of initial clusters can dramatically improve pseudo-label quality, particularly for linguistically diverse, low-resource settings. Flick introduces a novel pseudo-label refinement component, a departure from traditional pseudo-labelling strategies by identifying and leveraging top-performing pseudo-label clusters. This component specifically learns to distil highly reliable pseudo-labels from an initial broad set by focusing on single-cluster cohesion and leveraging an adaptive top-k selection mechanism. This targeted refinement process is crucial for mitigating the propagation of errors inherent in low-resource data, allowing for robust fine-tuning of pre-trained language models with only a handful of true labels. We demonstrate Flick's efficacy across 14 diverse datasets, encompassing challenging low-resource languages such as Arabic, Urdu, and Setswana, alongside English, showcasing its superior performance and adaptability.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Check My Work?": Measuring Sycophancy in a Simulated Educational Context</title>
<link>https://arxiv.org/abs/2506.10297</link>
<guid>https://arxiv.org/abs/2506.10297</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Educational Context, Sycophancy, Bias, Mitigation

Summary:
This study explores how user-provided suggestions impact Large Language Models (LLMs) in an educational setting, revealing significant variations in response quality based on query framing. The experiment involved testing different LLMs and found that mentioning incorrect answers could decrease LLM correctness by up to 15%, while mentioning the correct answer increased accuracy by a similar margin. The bias towards sycophantic behavior was more pronounced in smaller models, with a 30% effect on the GPT-4.1-nano model compared to 8% on the GPT-4o model. Analysis showed that the models often changed their answers to align with student mentions, supporting the sycophantic hypothesis. This behavior could impact educational equity by potentially reinforcing misunderstanding for less knowledgeable students while accelerating learning for knowledgeable ones. The study emphasizes the importance of understanding and mitigating such bias in educational contexts. 

<br /><br />Summary: <div>
arXiv:2506.10297v1 Announce Type: new 
Abstract: This study examines how user-provided suggestions affect Large Language Models (LLMs) in a simulated educational context, where sycophancy poses significant risks. Testing five different LLMs from the OpenAI GPT-4o and GPT-4.1 model classes across five experimental conditions, we show that response quality varies dramatically based on query framing. In cases where the student mentions an incorrect answer, the LLM correctness can degrade by as much as 15 percentage points, while mentioning the correct answer boosts accuracy by the same margin. Our results also show that this bias is stronger in smaller models, with an effect of up to 30% for the GPT-4.1-nano model, versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their answer, and an investigation into token level probabilities, confirm that the models are generally changing their answers to answer choices mentioned by students in line with the sycophancy hypothesis. This sycophantic behavior has important implications for educational equity, as LLMs may accelerate learning for knowledgeable students while the same tools may reinforce misunderstanding for less knowledgeable students. Our results highlight the need to better understand the mechanism, and ways to mitigate, such bias in the educational context.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs</title>
<link>https://arxiv.org/abs/2506.10299</link>
<guid>https://arxiv.org/abs/2506.10299</guid>
<content:encoded><![CDATA[
<div> large language models, speech-to-speech translation, modality adaptation, scheduled interleaved training, CVSS dataset
<br />
Summary:
Scheduled interleaved speech-text training is proposed to improve speech-to-speech translation using large language models (LLMs). LLMs trained on text-only data face challenges in adapting to speech modality with limited speech-to-speech data. The proposed method uses interleaved speech-text units during training, gradually decreasing the ratio of text to facilitate modality adaptation from text to speech. Experimental evaluations on the CVSS dataset show consistent improvements in translation performance, particularly for languages with limited training data. <div>
arXiv:2506.10299v1 Announce Type: new 
Abstract: Speech-to-speech translation (S2ST) has been advanced with large language models (LLMs), which are fine-tuned on discrete speech units. In such approaches, modality adaptation from text to speech has been an issue. LLMs are trained on text-only data, which presents challenges to adapt them to speech modality with limited speech-to-speech data. To address the training difficulty, we propose scheduled interleaved speech--text training in this study. We use interleaved speech--text units instead of speech units during training, where aligned text tokens are interleaved at the word level. We gradually decrease the ratio of text as training progresses, to facilitate progressive modality adaptation from text to speech. We conduct experimental evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show that the proposed method consistently improves the translation performances, especially for languages with limited training data.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Execution as Grounded Supervision for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.10343</link>
<guid>https://arxiv.org/abs/2506.10343</guid>
<content:encoded><![CDATA[
<div> supervision, language models, reasoning, dataset generation, transfer learning 

Summary:
Our method proposes a scalable approach to generating high-quality chain-of-thought (CoT) supervision datasets for training large language models (LLMs). By leveraging the determinism of program execution, we extract verifiable reasoning traces from code execution and transform them into natural language CoT reasoning. This approach eliminates the need for costly human annotations or error-prone LLM-generated CoT, resulting in accurate reasoning data. Experiments across various domains demonstrate the effectiveness of our method in equipping LLMs with transferable reasoning abilities. Additionally, ablation studies confirm that our method reduces meaningless repetition and overthinking, leading to a decrease in overall token length during inference. Our method addresses the challenge of obtaining reliable and accurate reasoning supervision, enhancing the reasoning abilities of LLMs for diverse tasks.<br /><br />Summary: <div>
arXiv:2506.10343v1 Announce Type: new 
Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision has proven effective for enhancing their reasoning abilities. However, obtaining reliable and accurate reasoning supervision remains a significant challenge. We propose a scalable method for generating a high-quality CoT supervision dataset by leveraging the determinism of program execution. Unlike existing reasoning dataset generation methods that rely on costly human annotations or error-prone LLM-generated CoT, our approach extracts verifiable, step-by-step reasoning traces from code execution and transforms them into a natural language CoT reasoning. Experiments on reasoning benchmarks across various domains show that our method effectively equips LLMs with transferable reasoning abilities across diverse tasks. Furthermore, the ablation studies validate that our method produces highly accurate reasoning data and reduces overall token length during inference by reducing meaningless repetition and overthinking.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning</title>
<link>https://arxiv.org/abs/2506.10380</link>
<guid>https://arxiv.org/abs/2506.10380</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, heterogeneous document, TableRAG, HeteQA, question answering 

Summary: 
TableRAG is a new framework that addresses limitations in existing Retrieval-Augmented Generation approaches when dealing with heterogeneous documents containing both text and tabular data. It integrates textual understanding and complex manipulations over tabular data through context-sensitive query decomposition, text retrieval, SQL programming, and intermediate answer generation. The framework outperforms baseline methods on various datasets and the newly introduced benchmark HeteQA, establishing a new state-of-the-art in heterogeneous document question answering. The researchers have released TableRAG on GitHub for further exploration and development. The proposed framework shows improved performance in multi-hop reasoning tasks by preserving tabular structure and minimizing information loss compared to existing methods. The development of HeteQA as a benchmark provides a standardized evaluation for assessing the reasoning capabilities of models like TableRAG on heterogeneous documents. <div>
arXiv:2506.10380v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable effectiveness in open-domain question answering. However, when applied to heterogeneous documents, comprising both textual and tabular components, existing RAG approaches exhibit critical limitations. The prevailing practice of flattening tables and chunking strategies disrupts the intrinsic tabular structure, leads to information loss, and undermines the reasoning capabilities of LLMs in multi-hop, global queries. To address these challenges, we propose TableRAG, an hybrid framework that unifies textual understanding and complex manipulations over tabular data. TableRAG iteratively operates in four steps: context-sensitive query decomposition, text retrieval, SQL programming and execution, and compositional intermediate answer generation. We also develop HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous reasoning capabilities. Experimental results demonstrate that TableRAG consistently outperforms existing baselines on both public datasets and our HeteQA, establishing a new state-of-the-art for heterogeneous document question answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier</title>
<link>https://arxiv.org/abs/2506.10406</link>
<guid>https://arxiv.org/abs/2506.10406</guid>
<content:encoded><![CDATA[
<div> verification, language models, self-correction, reinforcement learning, reasoning 

Summary: 
The paper introduces a new framework called Policy as Generative Verifier (PAG) to address the challenge of verifying the correctness of outputs generated by Large Language Models (LLMs). PAG alternates between policy and verifier roles in a unified reinforcement learning paradigm, allowing LLMs to self-correct by selectively revising their answers based on the results of a generative verification step. This approach alleviates model collapse and improves both reasoning and verification abilities. Experimental results across various reasoning benchmarks demonstrate PAG's effectiveness in enhancing direct generation and self-correction accuracy, as well as outperforming self-consistency in self-verification tasks. <div>
arXiv:2506.10406v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAG's dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?</title>
<link>https://arxiv.org/abs/2506.10415</link>
<guid>https://arxiv.org/abs/2506.10415</guid>
<content:encoded><![CDATA[
<div> Keywords: TempVS benchmark, Multimodal Large Language Models, temporal grounding, event relation inference, image sequences

Summary: 
The paper introduces the TempVS benchmark, focusing on testing the temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS comprises three main tests: event relation inference, sentence ordering, and image ordering, all accompanied by basic grounding tests. The benchmark requires MLLMs to utilize both visual and linguistic modalities to comprehend the temporal sequence of events. The evaluation of 38 state-of-the-art MLLMs revealed that these models struggle to perform well on TempVS, exhibiting a significant performance gap compared to human capabilities. The study also provides detailed insights indicating potential research directions. The TempVS benchmark data and code are freely accessible at https://github.com/yjsong22/TempVS. <br /><br />Summary: <div>
arXiv:2506.10415v1 Announce Type: new 
Abstract: This paper introduces the TempVS benchmark, which focuses on temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS consists of three main tests (i.e., event relation inference, sentence ordering and image ordering), each accompanied with a basic grounding test. TempVS requires MLLMs to rely on both visual and linguistic modalities to understand the temporal order of events. We evaluate 38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS, with a substantial performance gap compared to human capabilities. We also provide fine-grained insights that suggest promising directions for future research. Our TempVS benchmark data and code are available at https://github.com/yjsong22/TempVS.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting</title>
<link>https://arxiv.org/abs/2506.10421</link>
<guid>https://arxiv.org/abs/2506.10421</guid>
<content:encoded><![CDATA[
<div> Keywords: conflict framing, war journalism, peace journalism, media bias, language models

Summary: 
The study investigates the framing of news media during the Israel-Palestine war using computational approaches, focusing on indicators of war and peace journalism. It highlights a prevalence of war-oriented reporting over peace-oriented reporting in the analyzed news articles. The analysis also reveals significant variations in how the conflict is framed across different news outlets in the US, UK, and the Middle East, highlighting biases in reporting. The study utilizes frame semantics and large language models to identify communicative framing and its connection to linguistic framing, providing a deeper understanding of media portrayal during conflicts. The findings emphasize the potential impact of media framing on shaping readers' opinions and potentially exacerbating conflicts. The study contributes to the discourse on conflict framing by offering a quantitative analysis that goes beyond surface-level generic frames. 

<br /><br />Summary: <div>
arXiv:2506.10421v1 Announce Type: new 
Abstract: Framing used by news media, especially in times of conflict, can have substantial impact on readers' opinion, potentially aggravating the conflict itself. Current studies on the topic of conflict framing have limited insights due to their qualitative nature or only look at surface level generic frames without going deeper. In this work, we identify indicators of war and peace journalism, as outlined by prior work in conflict studies, in a corpus of news articles reporting on the Israel-Palestine war. For our analysis, we use computational approaches, using a combination of frame semantics and large language models to identify both communicative framing and its connection to linguistic framing. Our analysis reveals a higher focus on war based reporting rather than peace based. We also show substantial differences in reporting across the US, UK, and Middle Eastern news outlets in framing who the assailant and victims of the conflict are, surfacing biases within the media.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty</title>
<link>https://arxiv.org/abs/2506.10446</link>
<guid>https://arxiv.org/abs/2506.10446</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reasoning capabilities, Chain-of-Thought prompting, efficiency, benchmark evaluations 

Summary: 
Large language models (LLMs) have shown significant advancements in reasoning abilities, with techniques like Chain-of-Thought prompting aiming to enhance reasoning further. However, these methods often result in longer outputs, increasing computational latency. Reinforcement learning approaches to shorten reasoning tend to apply uniform penalties, impacting outcomes. This study proposes a novel approach to improve LLM reasoning efficiency by promoting conciseness for simpler problems while maintaining accuracy for complex ones. By dividing the reward function and introducing a penalty for output length, the model's performance was enhanced across benchmark datasets. For simpler datasets like GSM8K and MATH500, the method successfully reduced output lengths with preserved or enhanced accuracy. On the more challenging AIME2024 dataset, the approach led to improved accuracy.<br /><br />Summary: <div>
arXiv:2506.10446v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning capabilities, performing well on various challenging benchmarks. Techniques like Chain-of-Thought prompting have been introduced to further improve reasoning. However, these approaches frequently generate longer outputs, which in turn increase computational latency. Although some methods use reinforcement learning to shorten reasoning, they often apply uniform penalties without considering the problem's complexity, leading to suboptimal outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by promoting conciseness for simpler problems while preserving sufficient reasoning for more complex ones for accuracy, thus improving the model's overall performance. Specifically, we manage the model's reasoning efficiency by dividing the reward function and including a novel penalty for output length. Our approach has yielded impressive outcomes in benchmark evaluations across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively simpler datasets GSM8K and MATH500, our method has effectively shortened output lengths while preserving or enhancing accuracy. On the more demanding AIME2024 dataset, our approach has resulted in improved accuracy.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers</title>
<link>https://arxiv.org/abs/2506.10486</link>
<guid>https://arxiv.org/abs/2506.10486</guid>
<content:encoded><![CDATA[
<div> reframing, explanation task, table-text alignment, SciTab benchmark, cell-level rationales <br />
<br />
Summary: 
The article introduces a new approach to scientific claim verification against tables by reframing it as an explanation task. This involves identifying the essential table cells for verifying a claim, rather than just predicting the final label. A new dataset is created by extending the SciTab benchmark with human-annotated cell-level rationales, where annotators highlight the minimal set of cells needed to support their decision on claim label verification. A taxonomy for handling ambiguous cases is proposed based on the collected information. Experimental results show that incorporating table alignment information improves claim verification performance. Most LLMs, while correctly predicting labels, fail to recover human-aligned rationales, indicating a lack of faithful reasoning in their predictions. <div>
arXiv:2506.10486v1 Announce Type: new 
Abstract: Scientific claim verification against tables typically requires predicting whether a claim is supported or refuted given a table. However, we argue that predicting the final label alone is insufficient: it reveals little about the model's reasoning and offers limited interpretability. To address this, we reframe table-text alignment as an explanation task, requiring models to identify the table cells essential for claim verification. We build a new dataset by extending the SciTab benchmark with human-annotated cell-level rationales. Annotators verify the claim label and highlight the minimal set of cells needed to support their decision. After the annotation process, we utilize the collected information and propose a taxonomy for handling ambiguous cases. Our experiments show that (i) incorporating table alignment information improves claim verification performance, and (ii) most LLMs, while often predicting correct labels, fail to recover human-aligned rationales, suggesting that their predictions do not stem from faithful reasoning.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models</title>
<link>https://arxiv.org/abs/2506.10491</link>
<guid>https://arxiv.org/abs/2506.10491</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, bias, stereotypes, evaluation, personalization

Summary: 
Modern language models trained on large datasets may exhibit biases related to gender, origin, age, etc. Evaluating these models using pre-prompted personae on a multi-subject benchmark shows minimal differences in scores. However, when asked to grade user responses or provide salary negotiation advice, significant biases are observed. This bias is particularly pronounced when the model incorporates user socio-demographic information without explicit prompting. These findings highlight the potential for biased outcomes in language models and raise concerns about the impact of personalized assistant features in perpetuating and exacerbating biases. <div>
arXiv:2506.10491v1 Announce Type: new 
Abstract: Modern language models are trained on large amounts of data. These data inevitably include controversial and stereotypical content, which contains all sorts of biases related to gender, origin, age, etc. As a result, the models express biased points of view or produce different results based on the assigned personality or the personality of the user. In this paper, we investigate various proxy measures of bias in large language models (LLMs). We find that evaluating models with pre-prompted personae on a multi-subject benchmark (MMLU) leads to negligible and mostly random differences in scores. However, if we reformulate the task and ask a model to grade the user's answer, this shows more significant signs of bias. Finally, if we ask the model for salary negotiation advice, we see pronounced bias in the answers. With the recent trend for LLM assistant memory and personalization, these problems open up from a different angle: modern LLM users do not need to pre-prompt the description of their persona since the model already knows their socio-demographics.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2506.10504</link>
<guid>https://arxiv.org/abs/2506.10504</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, dialogue state tracking, multi-user interactions, data annotation, speech act theory

Summary:<br /><br />Larger language models (LLMs) have shown impressive performance in zero-shot dialogue state tracking (DST) tasks, eliminating the need for specialized training data. However, current DST benchmarks are limited to structured user-agent conversations and do not reflect the complexities of real-world multi-user interactions. To address this gap, the study examines the robustness of LLMs in multi-user DST scenarios with minimal dataset construction costs. By leveraging speech act theory and extending an existing DST dataset with generated utterances from a second user, the study enables controlled evaluations of LLM performance in multi-user settings. Results demonstrate a notable decline in performance compared to single-user DST, underscoring the challenges faced by current LLMs in accurately extracting and tracking dialogue states in the presence of multiple speakers. The findings underscore the need for future research to enhance LLM capabilities for multi-user DST scenarios, paving the way for more realistic and robust DST models. <div>
arXiv:2506.10504v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance in zero-shot dialogue state tracking (DST), reducing the need for task-specific training. However, conventional DST benchmarks primarily focus on structured user-agent conversations, failing to capture the complexities of real-world multi-user interactions. In this study, we assess the robustness of LLMs in multi-user DST while minimizing dataset construction costs. Inspired by recent advances in LLM-based data annotation, we extend an existing DST dataset by generating utterances of a second user based on speech act theory. Our methodology systematically incorporates a second user's utterances into conversations, enabling a controlled evaluation of LLMs in multi-user settings. Experimental results reveal a significant performance drop compared to single-user DST, highlighting the limitations of current LLMs in extracting and tracking dialogue states amidst multiple speakers. Our findings emphasize the need for future research to enhance LLMs for multi-user DST scenarios, paving the way for more realistic and robust DST models.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.10508</link>
<guid>https://arxiv.org/abs/2506.10508</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, large language models, reasoning paths, RRP framework, bidirectional distribution learning

Summary:
The article discusses the limitations faced by large language models (LLMs) in knowledge-intensive tasks due to a lack of background knowledge and hallucinations. It emphasizes the importance of refining relationships among facts and organizing them into logical reasoning paths to enhance LLM performance. The RRP framework is proposed to mine knowledge graphs, utilizing relation embedding and bidirectional distribution learning to extract meaningful reasoning paths. A rethinking module evaluates and refines these paths for significance. Experimental results on public datasets demonstrate that RRP outperforms existing baseline methods, offering a plug-and-play solution to enhance LLM reasoning abilities. By generating high-quality reasoning paths tailored to specific questions, RRP provides effective guidance for LLM reasoning. <div>
arXiv:2506.10508v1 Announce Type: new 
Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search</title>
<link>https://arxiv.org/abs/2506.10614</link>
<guid>https://arxiv.org/abs/2506.10614</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised method, protoforms, ancestral word forms, evolutionary optimization, phonological plausibility

Summary:
An unsupervised method is proposed for reconstructing protoforms, which are ancestral word forms from which modern language forms are derived. The approach integrates data-driven inference with rule-based heuristics within an evolutionary optimization framework, combining statistical patterns and linguistically motivated constraints. The model is evaluated on reconstructing Latin protoforms using cognates from five Romance languages. Results show significant improvements over established baselines in character-level accuracy and phonological plausibility metrics. This hybrid approach offers a more comprehensive and effective way of inferring protoforms compared to previous data-driven methods. <div>
arXiv:2506.10614v1 Announce Type: new 
Abstract: We propose an unsupervised method for the reconstruction of protoforms i.e., ancestral word forms from which modern language forms are derived. While prior work has primarily relied on probabilistic models of phonological edits to infer protoforms from cognate sets, such approaches are limited by their predominantly data-driven nature. In contrast, our model integrates data-driven inference with rule-based heuristics within an evolutionary optimization framework. This hybrid approach leverages on both statistical patterns and linguistically motivated constraints to guide the reconstruction process. We evaluate our method on the task of reconstructing Latin protoforms using a dataset of cognates from five Romance languages. Experimental results demonstrate substantial improvements over established baselines across both character-level accuracy and phonological plausibility metrics.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis</title>
<link>https://arxiv.org/abs/2506.10622</link>
<guid>https://arxiv.org/abs/2506.10622</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational AI, synthetic dialogue generation, Large Language Models, personas, reproducibility 

Summary: 
SDialog is introduced as a Python toolkit for generating high-quality synthetic dialogues for conversational AI systems. It uses instruction-tuned Large Language Models (LLMs) to create realistic and diverse conversational data. The toolkit provides abstractions for personas, orchestration, and scenario management, enabling researchers to generate controllable dialogues for training and evaluation. SDialog supports workflows such as multi-agent simulation and scenario-driven generation, aiming to standardize tools for synthetic data generation in the rapidly changing research landscape. By addressing the challenges of synthetic dialogue generation, SDialog contributes to the advancement of conversational AI systems by providing flexible and reproducible dialogues for research and development.<br /><br />Summary: <div>
arXiv:2506.10622v1 Announce Type: new 
Abstract: The advancement of conversational AI systems relies on the availability of high-quality, flexible, and reproducible synthetic dialogues for training, evaluation, and benchmarking. SDialog is a modular, extensible Python toolkit designed to address the challenges of synthetic dialogue generation and analysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog provides abstractions for personas, orchestration, and scenario management, enabling the creation of realistic, diverse, and controllable conversational data for research and development. SDialog supports workflows such as multi-agent simulation and scenario-driven generation, and represents a step forward in the standardization of tools and frameworks for synthetic data generation, a crucial advancement for ensuring reproducibility in today's fast-evolving research landscape.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors</title>
<link>https://arxiv.org/abs/2506.10627</link>
<guid>https://arxiv.org/abs/2506.10627</guid>
<content:encoded><![CDATA[
<div> Keywords: BEA 2025 Shared Task, Pedagogical Ability Assessment, AI-powered Tutors, Language Models, Machine Learning

Summary:
In this paper, the authors present their system for the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. They explore four approaches: ensemble of machine learning models with pretrained language models, frozen sentence-transformer with MLP classifier, history-aware model with multi-head attention, and retrieval-augmented few-shot prompting system with GPT-40. The final system combines example-driven prompting with large language model reasoning to outperform all baselines in identifying mistakes in student mathematical reasoning. The system retrieves similar examples, generates structured prompts, and uses schema-guided output parsing for interpretable predictions. The research demonstrates the effectiveness of combining example-driven prompting with large language model reasoning for pedagogical feedback assessment.
<br /><br />Summary: <div>
arXiv:2506.10627v1 Announce Type: new 
Abstract: This paper presents our system for Track 1: Mistake Identification in the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The task involves evaluating whether a tutor's response correctly identifies a mistake in a student's mathematical reasoning. We explore four approaches: (1) an ensemble of machine learning models over pooled token embeddings from multiple pretrained language models (LMs); (2) a frozen sentence-transformer using [CLS] embeddings with an MLP classifier; (3) a history-aware model with multi-head attention between token-level history and response embeddings; and (4) a retrieval-augmented few-shot prompting system with a large language model (LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples, constructs structured prompts, and uses schema-guided output parsing to produce interpretable predictions. It outperforms all baselines, demonstrating the effectiveness of combining example-driven prompting with LLM reasoning for pedagogical feedback assessment. Our code is available at https://github.com/NaumanNaeem/BEA_2025.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters</title>
<link>https://arxiv.org/abs/2506.10641</link>
<guid>https://arxiv.org/abs/2506.10641</guid>
<content:encoded><![CDATA[
<div> investigate, language models, character-level tasks, embedding layer, Transformer layers

Summary: 
In this study, the researchers examined how large language models (LLMs) handle character-level tasks, such as spelling out tokens. They found that LLMs do not fully encode character-level information in the embedding layer, especially beyond the first character. Instead, LLMs rely on intermediate and higher Transformer layers to reconstruct character-level knowledge, leading to a distinct "breakthrough" in their spelling behavior. This mechanism was validated through probing classifiers, identification of knowledge neurons, and inspection of attention weights. Overall, LLMs do not approach the spelling-out task in a straightforward manner like humans do, highlighting the complexity of how these models internally represent and utilize character-level information. <div>
arXiv:2506.10641v1 Announce Type: new 
Abstract: Large language models (LLMs) can spell out tokens character by character with high accuracy, yet they struggle with more complex character-level tasks, such as identifying compositional subcomponents within tokens. In this work, we investigate how LLMs internally represent and utilize character-level information during the spelling-out process. Our analysis reveals that, although spelling out is a simple task for humans, it is not handled in a straightforward manner by LLMs. Specifically, we show that the embedding layer does not fully encode character-level information, particularly beyond the first character. As a result, LLMs rely on intermediate and higher Transformer layers to reconstruct character-level knowledge, where we observe a distinct "breakthrough" in their spelling behavior. We validate this mechanism through three complementary analyses: probing classifiers, identification of knowledge neurons, and inspection of attention weights.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Detection of Life-Threatening Texts</title>
<link>https://arxiv.org/abs/2506.10687</link>
<guid>https://arxiv.org/abs/2506.10687</guid>
<content:encoded><![CDATA[
<div> Keywords: life-threatening language, large language models, text classification, imbalanced data, upsampling <br />
Summary: <br />
This study focuses on detecting life-threatening language using large language models (LLMs) and compares them with traditional methods. Three open-source LLMs, Gemma, Mistral, and Llama-2, were fine-tuned and tested on various datasets with different data scenarios. The experimental results show that LLMs outperform traditional methods such as bag of words, word embedding, and topic modeling. Mistral and Llama-2 models performed well in both balanced and imbalanced data scenarios, while Gemma lagged slightly behind. The study also highlights the use of upsampling to address imbalanced data, showing that it benefits traditional approaches more than LLMs. Overall, this research demonstrates the strong potential of LLMs in accurately identifying life-threatening texts, which is crucial for safeguarding individuals in distress and promoting mental health and well-being. <br /> <div>
arXiv:2506.10687v1 Announce Type: new 
Abstract: Detecting life-threatening language is essential for safeguarding individuals in distress, promoting mental health and well-being, and preventing potential harm and loss of life. This paper presents an effective approach to identifying life-threatening texts using large language models (LLMs) and compares them with traditional methods such as bag of words, word embedding, topic modeling, and Bidirectional Encoder Representations from Transformers. We fine-tune three open-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter variants on different datasets, which are constructed with class balance, imbalance, and extreme imbalance scenarios. Experimental results demonstrate a strong performance of LLMs against traditional methods. More specifically, Mistral and Llama-2 models are top performers in both balanced and imbalanced data scenarios while Gemma is slightly behind. We employ the upsampling technique to deal with the imbalanced data scenarios and demonstrate that while this method benefits traditional approaches, it does not have as much impact on LLMs. This study demonstrates a great potential of LLMs for real-world life-threatening language detection problems.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet</title>
<link>https://arxiv.org/abs/2506.10715</link>
<guid>https://arxiv.org/abs/2506.10715</guid>
<content:encoded><![CDATA[
<div> resource, adjective hypernymy, OntoLex-lemon, language models, TaxoLLaMa
Summary:
This paper focuses on the Open English Wordnet resource published in OntoLex-lemon and the missing links within it, specifically in relation to establishing hypernymy between adjectives. The authors discuss the hypernymy relation for adjectives and its differences compared to nouns and verbs. They introduce a new resource for adjective hypernymy and demonstrate how large language models can be fine-tuned to predict adjective hypernymy, adapting the methodology used in TaxoLLaMa. By addressing the gaps in the existing resource and developing a new approach to establish hypernymy for adjectives, this research contributes to the enhancement of linguistic linked open data. <div>
arXiv:2506.10715v1 Announce Type: new 
Abstract: Open English Wordnet is a key resource published in OntoLex-lemon as part of the linguistic linked open data cloud. There are, however, many links missing in the resource, and in this paper, we look at how we can establish hypernymy between adjectives. We present a theoretical discussion of the hypernymy relation and how it differs for adjectives in contrast to nouns and verbs. We develop a new resource for adjective hypernymy and fine-tune large language models to predict adjective hypernymy, showing that the methodology of TaxoLLaMa can be adapted to this task.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models</title>
<link>https://arxiv.org/abs/2506.10716</link>
<guid>https://arxiv.org/abs/2506.10716</guid>
<content:encoded><![CDATA[
<div> framework, reasoning overhead, efficient, mathematical inference, optimization <br />
Summary: <br />
PREMISE is a new framework for efficient mathematical reasoning models that reduces unnecessary verbosity in large reasoning models. It focuses on prompt-only optimization to minimize redundant computation while maintaining answer accuracy. By combining trace-level diagnostics and prompt optimization, PREMISE achieves a balance between brevity and correctness through a multi-objective textual search. Unlike previous approaches, PREMISE operates in a single-pass black-box interface, making it suitable for commercial large language models. In experiments on various mathematical benchmarks, PREMISE matches or exceeds baseline accuracy while significantly reducing reasoning tokens by up to 87.5% and cutting dollar cost by 69-82%. These results demonstrate that prompt-level optimization is a practical and scalable method for efficient inference with large reasoning models without compromising reasoning quality. <br /> <div>
arXiv:2506.10716v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve strong performance on mathematical benchmarks using lengthy chain-of-thought (CoT) reasoning, but the resulting traces are often unnecessarily verbose. This inflates token usage and cost, limiting deployment in latency-sensitive or API-constrained settings. We introduce PREMISE (PRompt-based Efficient Mathematical Inference with Strategic Evaluation), a prompt-only framework that reduces reasoning overhead without modifying model weights. PREMISE combines trace-level diagnostics with gradient-inspired prompt optimization to minimize redundant computation while preserving answer accuracy. The approach jointly optimizes brevity and correctness through a multi-objective textual search that balances token length and answer validity. Unlike prior work, PREMISE runs in a single-pass black-box interface, so it can be applied directly to commercial LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy ($96\%\rightarrow96\%$ with Claude, $91\%\rightarrow92\%$ with Gemini) while reducing reasoning tokens by up to $87.5\%$ and cutting dollar cost by $69$--$82\%$. These results show that prompt-level optimization is a practical and scalable path to efficient LRM inference without compromising reasoning quality.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims</title>
<link>https://arxiv.org/abs/2506.10728</link>
<guid>https://arxiv.org/abs/2506.10728</guid>
<content:encoded><![CDATA[
<div> Framework, Aspects, Perspectives, Retrieval, Accuracy 
Summary: 
The article introduces ClaimSpect, a framework for deconstructing nuanced claims into their integral aspects and sub-aspects to provide a well-rounded perspective. The framework utilizes retrieval-augmented generation to construct a hierarchy of aspects and enrich them with corpus-specific perspectives. By hierarchically partitioning input corpus segments, ClaimSpect retrieves relevant information, discovers new sub-aspects, and identifies varying perspectives towards aspects of a claim. It can determine the prevalence of different perspectives within a corpus, enabling a comprehensive analysis. The framework is applied to a variety of scientific and political claims, demonstrating its robustness and accuracy. Real-world case studies and human evaluation validate ClaimSpect's effectiveness over multiple baselines. Through structured deconstruction and representation of perspectives, ClaimSpect offers a systematic approach to understanding and evaluating complex claims. 
<br /><br />Summary: <div>
arXiv:2506.10728v1 Announce Type: new 
Abstract: Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely "true" or "false" -- as is frequently the case with scientific and political claims. However, a claim (e.g., "vaccine A is better than vaccine B") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., "how many biomedical papers believe vaccine A is more transportable than B?"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora</title>
<link>https://arxiv.org/abs/2506.10737</link>
<guid>https://arxiv.org/abs/2506.10737</guid>
<content:encoded><![CDATA[
<div> taxonomy construction, scientific literature, hierarchical classification, computer science conferences, multidimensional method

Summary:
- The article introduces TaxoAdapt, a framework designed to dynamically adapt an LLM-generated taxonomy to a given corpusacross multiple dimensions.  
- This framework addresses the challenges of organizing and retrieving scientific literature in rapidly evolving fields, which traditional taxonomies struggle to keep up with.
- TaxoAdapt expands both the width and depth of the taxonomy based on the topical distribution of the corpus, offering a more granular and coherent representation of scientific fields.
- The framework outperforms competitive baselines judged by LLMs, showcasing its state-of-the-art performance in structuring and capturing the evolution of scientific domains.
- Demonstrated across diverse computer science conferences, TaxoAdapt highlights its ability to generate taxonomies that accurately represent the multi-faceted nature of scientific literature. 

<br /><br />Summary: <div>
arXiv:2506.10737v1 Announce Type: new 
Abstract: The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers</title>
<link>https://arxiv.org/abs/2506.10766</link>
<guid>https://arxiv.org/abs/2506.10766</guid>
<content:encoded><![CDATA[
<div> tokenizer design, language plasticity, pretraining, Large Language Models, adaptation

Summary: 
This study examines improving language plasticity in Large Language Models (LLMs) by focusing on tokenizer design during pretraining. By utilizing a universal tokenizer trained for more languages than the primary pretraining languages, the model shows significantly higher adaptation capabilities post-training, with up to a 20.2% increase in win rates compared to specific tokenizers. Additionally, the universal tokenizer enhances adaptation towards completely unseen languages, resulting in a 5% win rate gain. The approach allows for expanded language coverage post-training while maintaining performance on pretraining languages. The research highlights the importance of early interventions in training to enhance the model's adaptability to diverse languages, addressing challenges such as limited model capacity and scarce high-quality data. <div>
arXiv:2506.10766v1 Announce Type: new 
Abstract: Pretraining massively multilingual Large Language Models (LLMs) for many languages at once is challenging due to limited model capacity, scarce high-quality data, and compute constraints. Moreover, the lack of language coverage of the tokenizer makes it harder to address the gap for new languages purely at the post-training stage. In this work, we study what relatively cheap interventions early on in training improve "language plasticity", or adaptation capabilities of the model post-training to new languages. We focus on tokenizer design and propose using a universal tokenizer that is trained for more languages than the primary pretraining languages to enable efficient adaptation in expanding language coverage after pretraining. Our systematic experiments across diverse groups of languages and different training strategies show that a universal tokenizer enables significantly higher language adaptation, with up to 20.2% increase in win rates compared to tokenizers specific to pretraining languages. Furthermore, a universal tokenizer also leads to better plasticity towards languages that are completely unseen in the tokenizer and pretraining, by up to 5% win rate gain. We achieve this adaptation to an expanded set of languages with minimal compromise in performance on the majority of languages included in pretraining.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs</title>
<link>https://arxiv.org/abs/2506.10769</link>
<guid>https://arxiv.org/abs/2506.10769</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty estimation, language models, clinical decision support, multiple-choice question answering, medical specialties

Summary: 
In this study, the researchers evaluated uncertainty estimation methods for clinical multiple-choice question answering using various open-source large language models (LLMs). The evaluation covered ten LLMs, including general-purpose, biomedical, and reasoning models, across datasets, medical specialties, and question types. Different uncertainty estimation methods were compared, including single-generation and sampling-based approaches. The study also introduced lightweight uncertainty estimation methods based on behavioral signals in reasoning traces, showing promising results close to Semantic Entropy performance with just one generation. Results indicated varied performance across medical specialties and question types, emphasizing the importance of selecting models based on the specific nature of the question and strengths of the model. This research underscores the significance of accurate and well-calibrated uncertainty estimates for deploying LLMs in critical domains like clinical decision support. 

<br /><br />Summary: <div>
arXiv:2506.10769v1 Announce Type: new 
Abstract: Accurate and well-calibrated uncertainty estimates are essential for deploying large language models (LLMs) in high-stakes domains such as clinical decision support. We present a fine-grained evaluation of uncertainty estimation methods for clinical multiple-choice question answering, covering ten open-source LLMs (general-purpose, biomedical, and reasoning models) across two datasets, eleven medical specialties, and six question types. We compare standard single-generation and sampling-based methods, and present a case study exploring simple, single-pass estimators based on behavioral signals in reasoning traces. These lightweight methods approach the performance of Semantic Entropy while requiring only one generation. Our results reveal substantial variation across specialties and question types, underscoring the importance of selecting models based on both the nature of the question and model-specific strengths.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Named Entity Transcription with Contextual LLM-based Revision</title>
<link>https://arxiv.org/abs/2506.10779</link>
<guid>https://arxiv.org/abs/2506.10779</guid>
<content:encoded><![CDATA[
<div> large language model, automatic speech recognition, named entities, word error rate, dataset 

Summary: 
Automatic speech recognition (ASR) systems have made significant advancements in general speech but struggle with named entities, crucial keywords impacting downstream applications. This paper introduces a large language model (LLM) revision mechanism to correct named entities in ASR predictions using the LLM's reasoning ability and local context like lecture notes. The NER-MIT-OpenCourseWare dataset, with 45 hours of MIT course data, is introduced for development and testing. The proposed technique on this dataset shows up to 30% relative reduction in word error rate for named entities. <div>
arXiv:2506.10779v1 Announce Type: new 
Abstract: With recent advances in modeling and the increasing amount of supervised training data, automatic speech recognition (ASR) systems have achieved remarkable performance on general speech. However, the word error rate (WER) of state-of-the-art ASR remains high for named entities. Since named entities are often the most critical keywords, misrecognizing them can affect all downstream applications, especially when the ASR system functions as the front end of a complex system. In this paper, we introduce a large language model (LLM) revision mechanism to revise incorrect named entities in ASR predictions by leveraging the LLM's reasoning ability as well as local context (e.g., lecture notes) containing a set of correct named entities. Finally, we introduce the NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses for development and testing. On this dataset, our proposed technique achieves up to 30\% relative WER reduction for named entities.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints</title>
<link>https://arxiv.org/abs/2506.10800</link>
<guid>https://arxiv.org/abs/2506.10800</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual knowledge updates, large language models, LangEdit, parameter interference, efficient

Summary:
LangEdit addresses the challenge of updating multilingual knowledge in large language models efficiently while maintaining consistent factual representations across languages. The framework isolates language-specific updates using a null-space constrained approach, projecting parameter updates onto orthogonal complements to prevent interference. This ensures update independence and preserves multilingual generalization capabilities. Evaluation across multiple languages and tasks shows LangEdit outperforms existing editing methods, demonstrating its potential to enable accurate and efficient multilingual knowledge updates in LLMs. <div>
arXiv:2506.10800v1 Announce Type: new 
Abstract: Efficiently updating multilingual knowledge in large language models (LLMs), while preserving consistent factual representations across languages, remains a long-standing and unresolved challenge. While deploying separate editing systems for each language might seem viable, this approach incurs substantial costs due to the need to manage multiple models. A more efficient solution involves integrating knowledge updates across all languages into a unified model. However, performing sequential edits across languages often leads to destructive parameter interference, significantly degrading multilingual generalization and the accuracy of injected knowledge. To address this challenge, we propose LangEdit, a novel null-space constrained framework designed to precisely isolate language-specific knowledge updates. The core innovation of LangEdit lies in its ability to project parameter updates for each language onto the orthogonal complement of previous updated subspaces. This approach mathematically guarantees update independence while preserving multilingual generalization capabilities. We conduct a comprehensive evaluation across three model architectures, six languages, and four downstream tasks, demonstrating that LangEdit effectively mitigates parameter interference and outperforms existing state-of-the-art editing methods. Our results highlight its potential for enabling efficient and accurate multilingual knowledge updates in LLMs. The code is available at https://github.com/VRCMF/LangEdit.git.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization</title>
<link>https://arxiv.org/abs/2506.10822</link>
<guid>https://arxiv.org/abs/2506.10822</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought prompting, reasoning compression, Large Language Models, reasoning accuracy, data generation <br />
Summary: <br />
The study introduces ReCUT, a novel method that aims to improve the reasoning capabilities of Large Language Models (LLMs) by balancing accuracy and reasoning length. ReCUT utilizes a stepwise exploration mechanism and a long-short switched sampling strategy to incrementally generate diverse reasoning paths. It trains two specialized models, one optimized for reasoning accuracy and the other for shorter reasoning, and combines them into an integrated model. Experimental results show that ReCUT reduces reasoning lengths by around 30-50% while maintaining or even enhancing reasoning accuracy compared to existing methods. This approach addresses the issue of overthinking in current Chain-of-Thought prompting techniques and improves the efficiency of reasoning processes in LLMs. The code and data associated with ReCUT will be made publicly available on GitHub for further research and development. <br /> <div>
arXiv:2506.10822v1 Announce Type: new 
Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of Large Language Models (LLMs). However, these methods often suffer from overthinking, leading to unnecessarily lengthy or redundant reasoning traces. Existing approaches attempt to mitigate this issue through curating multiple reasoning chains for training LLMs, but their effectiveness is often constrained by the quality of the generated data and prone to overfitting. To address the challenge, we propose Reasoning Compression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing the accuracy and length of reasoning trajectory. Specifically, ReCUT employs a stepwise exploration mechanism and a long-short switched sampling strategy, enabling LLMs to incrementally generate diverse reasoning paths. These paths are evaluated and used to construct preference pairs to train two specialized models (Gemini LLMs)-one optimized for reasoning accuracy, the other for shorter reasoning. A final integrated model is obtained by interpolating the parameters of these two models. Experimental results across multiple math reasoning datasets and backbone models demonstrate that ReCUT significantly reduces reasoning lengths by approximately 30-50%, while maintaining or improving reasoning accuracy compared to various baselines. All codes and data will be released via https://github.com/NEUIR/ReCUT.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training</title>
<link>https://arxiv.org/abs/2506.10844</link>
<guid>https://arxiv.org/abs/2506.10844</guid>
<content:encoded><![CDATA[
<div> Framework, mRAG, multi-agent, retrieval-augmented generation, optimization

Summary:
mRAG is a novel multi-agent retrieval-augmented generation (RAG) framework that utilizes specialized agents for various subtasks like planning, searching, reasoning, and coordination. The system employs a self-training approach with reward-guided trajectory sampling to improve inter-agent collaboration and response generation. In evaluations using DataMorgana-derived datasets in the SIGIR 2025 LiveRAG competition, mRAG surpasses traditional RAG baselines. The study further delves into analyzing competition results and presents case studies to illustrate the framework's effectiveness in handling complex real-world RAG tasks. Overall, mRAG demonstrates superior performance and showcases its potential in enhancing collaborative multi-agent systems for information retrieval and generation tasks.<br /><br />Summary: <div>
arXiv:2506.10844v1 Announce Type: new 
Abstract: This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG) framework composed of specialized agents for subtasks such as planning, searching, reasoning, and coordination. Our system uses a self-training paradigm with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines. We further analyze competition outcomes and showcase the framework's strengths with case studies, demonstrating its efficacy for complex, real-world RAG tasks.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles</title>
<link>https://arxiv.org/abs/2506.10848</link>
<guid>https://arxiv.org/abs/2506.10848</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion-based language models, dynamic sampling strategy, parallel token generation, inference latency, efficiency

Summary:
SlowFast Sampling is introduced as a new dynamic sampling strategy for diffusion-based language models (dLLMs). It alternates between exploratory and accelerated decoding stages, guided by principles such as certainty, convergence, and positional. The method aims to improve efficiency and flexibility in token generation. Integration with dLLM-Cache helps reduce redundant computation. Experimental results demonstrate significant speedup on benchmarks like LLaDA, with up to 15.63x improvement in throughput, and up to 34.22x with caching. Notably, SlowFast Sampling surpasses autoregressive baselines like LLaMA3 8B in both speed and quality of generation, showcasing the potential of well-designed sampling strategies for enhancing the performance of dLLMs. 

<br /><br />Summary: <div>
arXiv:2506.10848v1 Announce Type: new 
Abstract: Diffusion-based language models (dLLMs) have emerged as a promising alternative to traditional autoregressive LLMs by enabling parallel token generation and significantly reducing inference latency. However, existing sampling strategies for dLLMs, such as confidence-based or semi-autoregressive decoding, often suffer from static behavior, leading to suboptimal efficiency and limited flexibility. In this paper, we propose SlowFast Sampling, a novel dynamic sampling strategy that adaptively alternates between exploratory and accelerated decoding stages. Our method is guided by three golden principles: certainty principle, convergence principle, and positional principle, which govern when and where tokens can be confidently and efficiently decoded. We further integrate our strategy with dLLM-Cache to reduce redundant computation. Extensive experiments across benchmarks and models show that SlowFast Sampling achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and up to 34.22$\times$ when combined with caching. Notably, our approach outperforms strong autoregressive baselines like LLaMA3 8B in throughput, demonstrating that well-designed sampling can unlock the full potential of dLLMs for fast and high-quality generation.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models</title>
<link>https://arxiv.org/abs/2506.10855</link>
<guid>https://arxiv.org/abs/2506.10855</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised speech models, wav2vec2, language encoding, phone representation, geometric analyses<br />
Summary:<br />
The article explores the representation of different types of information in self-supervised speech models, specifically wav2vec2 models trained on four languages. Through probing classifiers and geometric analyses, the study investigates how phones, lexical tones, and speaker information are encoded in these models. The findings reveal that the subspaces representing phones, tones, and speakers are mostly orthogonal across all pretraining and test languages. Additionally, layerwise patterns of probing accuracy show a slight advantage for matched-language phone and tone probes in later layers. The research suggests that the structure of representations learned by wav2vec2 is predominantly independent of the language content used during pretraining. <div>
arXiv:2506.10855v1 Announce Type: new 
Abstract: Analyses of self-supervised speech models have begun to reveal where and how they represent different types of information. However, almost all analyses have focused on English. Here, we examine how wav2vec2 models trained on four different languages encode both language-matched and non-matched speech. We use probing classifiers and geometric analyses to examine how phones, lexical tones, and speaker information are represented. We show that for all pretraining and test languages, the subspaces encoding phones, tones, and speakers are largely orthogonal, and that layerwise patterns of probing accuracy are similar, with a relatively small advantage for matched-language phone and tone (but not speaker) probes in the later layers. Our findings suggest that the structure of representations learned by wav2vec2 is largely independent of the speech material used during pretraining.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment</title>
<link>https://arxiv.org/abs/2506.10877</link>
<guid>https://arxiv.org/abs/2506.10877</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical dialogue systems, knowledge refining, personalized responses, real-time adaptability, healthcare applications

Summary:
MedRef is a novel Medical Dialogue System (MDS) that addresses the challenges of identifying relevant medical knowledge and generating personalized, medically accurate responses. It incorporates a knowledge refining mechanism to filter out irrelevant data and improve predictions of critical medical entities. The system uses a comprehensive prompt structure that includes historical and evident details for context. Two key modules, Triplet Filter and Demo Selector, enable real-time adaptability to diverse patient conditions by providing appropriate knowledge and demonstrations in the system prompt. Extensive experiments on benchmarks show that MedRef outperforms state-of-the-art baselines in both generation quality and medical entity accuracy. This highlights its effectiveness and reliability for real-world healthcare applications.<br /><br />Summary: <div>
arXiv:2506.10877v1 Announce Type: new 
Abstract: Medical dialogue systems (MDS) have emerged as crucial online platforms for enabling multi-turn, context-aware conversations with patients. However, existing MDS often struggle to (1) identify relevant medical knowledge and (2) generate personalized, medically accurate responses. To address these challenges, we propose MedRef, a novel MDS that incorporates knowledge refining and dynamic prompt adjustment. First, we employ a knowledge refining mechanism to filter out irrelevant medical data, improving predictions of critical medical entities in responses. Additionally, we design a comprehensive prompt structure that incorporates historical details and evident details. To enable real-time adaptability to diverse patient conditions, we implement two key modules, Triplet Filter and Demo Selector, providing appropriate knowledge and demonstrations equipped in the system prompt. Extensive experiments on MedDG and KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in both generation quality and medical entity accuracy, underscoring its effectiveness and reliability for real-world healthcare applications.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slimming Down LLMs Without Losing Their Minds</title>
<link>https://arxiv.org/abs/2506.10885</link>
<guid>https://arxiv.org/abs/2506.10885</guid>
<content:encoded><![CDATA[
<div> LoRA, QLoRA, fine-tuning, large language model, parameter-efficient <br />
<br />
Summary: This paper explores the impact of fine-tuning on large language model performance, focusing on efficient methods like LoRA and QLoRA. Through evaluations on tasks related to commonsense reasoning, mathematical reasoning, and multi-domain knowledge, the study finds that LoRA-based techniques can enhance task-specific performance while maintaining computational efficiency. The study also highlights the importance of matching fine-tuning datasets with benchmark tasks for optimal performance. The findings offer valuable theoretical insights into parameter-efficient mechanisms and practical guidance for developers looking to implement efficient adaptation of large language models with limited resources. <div>
arXiv:2506.10885v1 Announce Type: new 
Abstract: This paper investigates and validates the impact of fine-tuning on large language model performance, focusing on parameter-efficient methods (LoRA and QLoRA). We evaluate model capabilities across three key domains: (1) commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3) multi-domain knowledge (MMLU-CS).
  Our findings demonstrate that: (1) LoRA-based methods effectively improve task-specific performance while maintaining computational efficiency, and (2) performance strongly depends on alignment between fine-tuning dataset and benchmark tasks. The study provides both theoretical insights into parameter-efficient mechanisms and practical guidance for developers implementing efficient LLM adaptation with limited resources.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers</title>
<link>https://arxiv.org/abs/2506.10887</link>
<guid>https://arxiv.org/abs/2506.10887</guid>
<content:encoded><![CDATA[
<div> mechanism, out-of-context reasoning, large language models, generalization, hallucination

Summary:
The study investigates the phenomenon where large language models exhibit both remarkable generalization and incorrect information hallucination during fine-tuning. The researchers propose that these behaviors are due to out-of-context reasoning (OCR), which allows models to deduce implications by associating concepts without causal links. Experiments on five popular LLMs confirm OCR's role in both generalization and hallucination. A synthetic factual recall task is formalized to understand OCR better, showing that attention-only transformers with factorized matrices can learn it effectively. The study attributes OCR to the implicit bias of gradient descent, which minimizes the nuclear norm of the output-value matrix. This mathematical insight explains the model's ability to associate facts and implications efficiently, whether causally linked or spurious. The work offers a theoretical foundation for understanding OCR in LLMs, providing insights for analyzing and mitigating undesirable behaviors. 

<br /><br />Summary: <div>
arXiv:2506.10887v1 Announce Type: new 
Abstract: Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. In this work, we argue that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Our experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, we then formalize OCR as a synthetic factual recall task. We empirically show that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. Our theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This mathematical structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, our work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP</title>
<link>https://arxiv.org/abs/2506.10896</link>
<guid>https://arxiv.org/abs/2506.10896</guid>
<content:encoded><![CDATA[
arXiv:2506.10896v1 Announce Type: new 
Abstract: Encoder-based transformer models are central to biomedical and clinical Natural Language Processing (NLP), as their bidirectional self-attention makes them well-suited for efficiently extracting structured information from unstructured text through discriminative tasks. However, encoders have seen slower development compared to decoder models, leading to limited domain adaptation in biomedical and clinical settings. We introduce BioClinical ModernBERT, a domain-adapted encoder that builds on the recent ModernBERT release, incorporating long-context processing and substantial improvements in speed and performance for biomedical and clinical NLP. BioClinical ModernBERT is developed through continued pretraining on the largest biomedical and clinical corpus to date, with over 53.5 billion tokens, and addresses a key limitation of prior clinical encoders by leveraging 20 datasets from diverse institutions, domains, and geographic regions, rather than relying on data from a single source. It outperforms existing biomedical and clinical encoders on four downstream tasks spanning a broad range of use cases. We release both base (150M parameters) and large (396M parameters) versions of BioClinical ModernBERT, along with training checkpoints to support further research.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2506.10903</link>
<guid>https://arxiv.org/abs/2506.10903</guid>
<content:encoded><![CDATA[
arXiv:2506.10903v1 Announce Type: new 
Abstract: Autoformalization plays a crucial role in formal mathematical reasoning by enabling the automatic translation of natural language statements into formal languages. While recent advances using large language models (LLMs) have shown promising results, methods for automatically evaluating autoformalization remain underexplored. As one moves to more complex domains (e.g., advanced mathematics), human evaluation requires significant time and domain expertise, especially as the complexity of the underlying statements and background knowledge increases. LLM-as-a-judge presents a promising approach for automating such evaluation. However, existing methods typically employ coarse-grained and generic evaluation criteria, which limit their effectiveness for advanced formal mathematical reasoning, where quality hinges on nuanced, multi-granular dimensions. In this work, we take a step toward addressing this gap by introducing a systematic, automatic method to evaluate autoformalization tasks. The proposed method is based on an epistemically and formally grounded ensemble (EFG) of LLM judges, defined on criteria encompassing logical preservation (LP), mathematical consistency (MC), formal validity (FV), and formal quality (FQ), resulting in a transparent assessment that accounts for different contributing factors. We validate the proposed framework to serve as a proxy for autoformalization assessment within the domain of formal mathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM judges is a suitable emerging proxy for evaluation, more strongly correlating with human assessments than a coarse-grained model, especially when assessing formal qualities. These findings suggest that LLM-as-judges, especially when guided by a well-defined set of atomic properties, could offer a scalable, interpretable, and reliable support for evaluating formal mathematical reasoning.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magistral</title>
<link>https://arxiv.org/abs/2506.10910</link>
<guid>https://arxiv.org/abs/2506.10910</guid>
<content:encoded><![CDATA[
arXiv:2506.10910v1 Announce Type: new 
Abstract: We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization</title>
<link>https://arxiv.org/abs/2506.10920</link>
<guid>https://arxiv.org/abs/2506.10920</guid>
<content:encoded><![CDATA[
arXiv:2506.10920v1 Announce Type: new 
Abstract: A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Epistemic Friction in Dialogue</title>
<link>https://arxiv.org/abs/2506.10934</link>
<guid>https://arxiv.org/abs/2506.10934</guid>
<content:encoded><![CDATA[
arXiv:2506.10934v1 Announce Type: new 
Abstract: Recent developments in aligning Large Language Models (LLMs) with human preferences have significantly enhanced their utility in human-AI collaborative scenarios. However, such approaches often neglect the critical role of "epistemic friction," or the inherent resistance encountered when updating beliefs in response to new, conflicting, or ambiguous information. In this paper, we define dynamic epistemic friction as the resistance to epistemic integration, characterized by the misalignment between an agent's current belief state and new propositions supported by external evidence. We position this within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit, 2011), where friction emerges as nontrivial belief-revision during the interaction. We then present analyses from a situated collaborative task that demonstrate how this model of epistemic friction can effectively predict belief updates in dialogues, and we subsequently discuss how the model of belief alignment as a measure of epistemic resistance or friction can naturally be made more sophisticated to accommodate the complexities of real-world dialogue scenarios.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training</title>
<link>https://arxiv.org/abs/2506.10952</link>
<guid>https://arxiv.org/abs/2506.10952</guid>
<content:encoded><![CDATA[
arXiv:2506.10952v1 Announce Type: new 
Abstract: We introduce~\textsc{Domain2Vec}, a novel approach that decomposes any dataset into a linear combination of several \emph{meta-domains}, a new concept designed to capture the key underlying features of datasets. \textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \emph{\textbf{D}istribution \textbf{A}lignment \textbf{A}ssumption} (DA$^{2}$), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, \textsc{Domain2vec} can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that \textsc{Domain2Vec} helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, \textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only $51.5\%$ of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, \textsc{Domain2Vec} improves downstream performance by an average of $2.83\%$.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</title>
<link>https://arxiv.org/abs/2506.10960</link>
<guid>https://arxiv.org/abs/2506.10960</guid>
<content:encoded><![CDATA[
arXiv:2506.10960v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
arXiv:2506.10974v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?</title>
<link>https://arxiv.org/abs/2506.10979</link>
<guid>https://arxiv.org/abs/2506.10979</guid>
<content:encoded><![CDATA[
arXiv:2506.10979v1 Announce Type: new 
Abstract: Recent reasoning models show the ability to reflect, backtrack, and self-validate their reasoning, which is crucial in spotting mistakes and arriving at accurate solutions. A natural question that arises is how effectively models can perform such self-reevaluation. We tackle this question by investigating how well reasoning models identify and recover from four types of unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to the question, thoughts misdirecting the question as a slightly different question, and thoughts that lead to incorrect answers. We show that models are effective at identifying most unhelpful thoughts but struggle to recover from the same thoughts when these are injected into their thinking process, causing significant performance drops. Models tend to naively continue the line of reasoning of the injected irrelevant thoughts, which showcases that their self-reevaluation abilities are far from a general "meta-cognitive" awareness. Moreover, we observe non/inverse-scaling trends, where larger models struggle more than smaller ones to recover from short irrelevant thoughts, even when instructed to reevaluate their reasoning. We demonstrate the implications of these findings with a jailbreak experiment using irrelevant thought injection, showing that the smallest models are the least distracted by harmful-response-triggering thoughts. Overall, our findings call for improvement in self-reevaluation of reasoning models to develop better reasoning and safer systems.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models</title>
<link>https://arxiv.org/abs/2506.10005</link>
<guid>https://arxiv.org/abs/2506.10005</guid>
<content:encoded><![CDATA[
arXiv:2506.10005v1 Announce Type: cross 
Abstract: Advances in generative artificial intelligence have altered multimedia creation, allowing for automatic cinematic video synthesis from text inputs. This work describes a method for creating 60-second cinematic movies incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for narrative structuring, and a hybrid audio pipeline using gTTS and YouTube-sourced music. It uses a five-scene framework, which is augmented by linear frame interpolation, cinematic post-processing (e.g., sharpening), and audio-video synchronization to provide professional-quality results. It was created in a GPU-accelerated Google Colab environment using Python 3.11. It has a dual-mode Gradio interface (Simple and Advanced), which supports resolutions of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA memory management and error handling ensure reliability. The experiments demonstrate outstanding visual quality, narrative coherence, and efficiency, furthering text-to-video synthesis for creative, educational, and industrial applications.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2506.10016</link>
<guid>https://arxiv.org/abs/2506.10016</guid>
<content:encoded><![CDATA[
arXiv:2506.10016v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Architectural innovations like transformers and diffusion models underpin this convergence, enabling cross-modal transfer and modular specialization. We highlight emerging patterns of synergy, and identify open challenges in evaluation, modularity, and structured reasoning. This survey offers a unified perspective on MLLM development and identifies critical paths toward more general-purpose, adaptive, and interpretable multimodal systems.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment</title>
<link>https://arxiv.org/abs/2506.10020</link>
<guid>https://arxiv.org/abs/2506.10020</guid>
<content:encoded><![CDATA[
arXiv:2506.10020v1 Announce Type: cross 
Abstract: Safely aligning large language models (LLMs) often demands extensive human-labeled preference data, a process that's both costly and time-consuming. While synthetic data offers a promising alternative, current methods frequently rely on complex iterative prompting or auxiliary models. To address this, we introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward, training-free, and model-agnostic framework that repurposes LLM attack techniques. RAAI works by detecting internal refusal signals and adaptively injecting predefined phrases to elicit harmful, yet fluent, completions. Our experiments show RAAI effectively jailbreaks LLMs, increasing the harmful response rate from a baseline of 2.15% to up to 61.04% on average across four benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by RAAI improves model robustness against harmful prompts while preserving general capabilities on standard tasks like MMLU and ARC. This work highlights how LLM attack methodologies can be reframed as practical tools for scalable and controllable safety alignment.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models</title>
<link>https://arxiv.org/abs/2506.10024</link>
<guid>https://arxiv.org/abs/2506.10024</guid>
<content:encoded><![CDATA[
arXiv:2506.10024v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) memorize, and thus, among huge amounts of uncontrolled data, may memorize Personally Identifiable Information (PII), which should not be stored and, consequently, not leaked. In this paper, we introduce Private Memorization Editing (PME), an approach for preventing private data leakage that turns an apparent limitation, that is, the LLMs' memorization ability, into a powerful privacy defense strategy. While attacks against LLMs have been performed exploiting previous knowledge regarding their training data, our approach aims to exploit the same kind of knowledge in order to make a model more robust. We detect a memorized PII and then mitigate the memorization of PII by editing a model knowledge of its training data. We verify that our procedure does not affect the underlying language model while making it more robust against privacy Training Data Extraction attacks. We demonstrate that PME can effectively reduce the number of leaked PII in a number of configurations, in some cases even reducing the accuracy of the privacy attacks to zero.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation empirique de la s\'ecurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vuln\'erabilit\'es par exp\'erimentations de jailbreaks</title>
<link>https://arxiv.org/abs/2506.10029</link>
<guid>https://arxiv.org/abs/2506.10029</guid>
<content:encoded><![CDATA[
arXiv:2506.10029v1 Announce Type: cross 
Abstract: Large Language models (LLMs) are transforming digital usage, particularly in text generation, image creation, information retrieval and code development. ChatGPT, launched by OpenAI in November 2022, quickly became a reference, prompting the emergence of competitors such as Google's Gemini. However, these technological advances raise new cybersecurity challenges, including prompt injection attacks, the circumvention of regulatory measures (jailbreaking), the spread of misinformation (hallucinations) and risks associated with deep fakes. This paper presents a comparative analysis of the security and alignment levels of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated with experiments.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.10047</link>
<guid>https://arxiv.org/abs/2506.10047</guid>
<content:encoded><![CDATA[
arXiv:2506.10047v1 Announce Type: cross 
Abstract: Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and are now widely used in content creation. However, these models can be misused to generate harmful content, including nudity or violence, posing significant safety risks. While most platforms employ content moderation systems, underlying vulnerabilities can still be exploited by determined adversaries. Recent research on red-teaming and adversarial attacks against T2I models has notable limitations: some studies successfully generate highly toxic images but use adversarial prompts that are easily detected and blocked by safety filters, while others focus on bypassing safety mechanisms but fail to produce genuinely harmful outputs, neglecting the discovery of truly high-risk prompts. Consequently, there remains a lack of reliable tools for evaluating the safety of defended T2I models. To address this gap, we propose GenBreak, a framework that fine-tunes a red-team large language model (LLM) to systematically explore underlying vulnerabilities in T2I generators. Our approach combines supervised fine-tuning on curated datasets with reinforcement learning via interaction with a surrogate T2I model. By integrating multiple reward signals, we guide the LLM to craft adversarial prompts that enhance both evasion capability and image toxicity, while maintaining semantic coherence and diversity. These prompts demonstrate strong effectiveness in black-box attacks against commercial T2I generators, revealing practical and concerning safety weaknesses.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs</title>
<link>https://arxiv.org/abs/2506.10054</link>
<guid>https://arxiv.org/abs/2506.10054</guid>
<content:encoded><![CDATA[
arXiv:2506.10054v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at https://github.com/pspdada/Omni-DPO.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence</title>
<link>https://arxiv.org/abs/2506.10157</link>
<guid>https://arxiv.org/abs/2506.10157</guid>
<content:encoded><![CDATA[
arXiv:2506.10157v1 Announce Type: cross 
Abstract: Medical foundation models, including language models trained on clinical notes, vision-language models on medical images, and multimodal models on electronic health records, can summarize clinical notes, answer medical questions, and assist in decision-making. Adapting these models to new populations, specialties, or settings typically requires fine-tuning, careful prompting, or retrieval from knowledge bases. This can be impractical, and limits their ability to interpret unfamiliar inputs and adjust to clinical situations not represented during training. As a result, models are prone to contextual errors, where predictions appear reasonable but fail to account for critical patient-specific or contextual information. These errors stem from a fundamental limitation that current models struggle with: dynamically adjusting their behavior across evolving contexts of medical care. In this Perspective, we outline a vision for context-switching in medical AI: models that dynamically adapt their reasoning without retraining to new specialties, populations, workflows, and clinical roles. We envision context-switching AI to diagnose, manage, and treat a wide range of diseases across specialties and regions, and expand access to medical care.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disclosure Audits for LLM Agents</title>
<link>https://arxiv.org/abs/2506.10171</link>
<guid>https://arxiv.org/abs/2506.10171</guid>
<content:encoded><![CDATA[
arXiv:2506.10171v1 Announce Type: cross 
Abstract: Large Language Model agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. This study proposes an auditing framework for conversational privacy that quantifies and audits these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework, is an iterative probing strategy designed to stress-test agents that enforce strict privacy directives. Rather than focusing solely on a single disclosure event, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrate the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods</title>
<link>https://arxiv.org/abs/2506.10236</link>
<guid>https://arxiv.org/abs/2506.10236</guid>
<content:encoded><![CDATA[
arXiv:2506.10236v1 Announce Type: cross 
Abstract: In this work, we show that some machine unlearning methods may fail when subjected to straightforward prompt attacks. We systematically evaluate eight unlearning techniques across three model families, and employ output-based, logit-based, and probe analysis to determine to what extent supposedly unlearned knowledge can be retrieved. While methods like RMU and TAR demonstrate robust unlearning, ELM remains vulnerable to specific prompt attacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy). Our logit analysis also confirms that unlearned models are generally not hiding knowledge by modifying the way the answer is formatted, as the correlation between output and logit accuracy is strong. These results challenge prevailing assumptions about unlearning effectiveness and highlight the need for evaluation frameworks that can reliably distinguish between true knowledge removal and superficial output suppression. We also publicly make available our evaluation framework to easily evaluate prompting techniques to retrieve unlearning knowledge.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Audio Tokens: More Than a Survey!</title>
<link>https://arxiv.org/abs/2506.10274</link>
<guid>https://arxiv.org/abs/2506.10274</guid>
<content:encoded><![CDATA[
arXiv:2506.10274v1 Announce Type: cross 
Abstract: Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC/DC: LLM-based Audio Comprehension via Dialogue Continuation</title>
<link>https://arxiv.org/abs/2506.10312</link>
<guid>https://arxiv.org/abs/2506.10312</guid>
<content:encoded><![CDATA[
arXiv:2506.10312v1 Announce Type: cross 
Abstract: We propose an instruction-following audio comprehension model that leverages the dialogue continuation ability of large language models (LLMs). Instead of directly generating target captions in training data, the proposed method trains a model to produce responses as if the input caption triggered a dialogue. This dialogue continuation training mitigates the caption variation problem. Learning to continue a dialogue effectively captures the caption's meaning beyond its surface-level words. As a result, our model enables zero-shot instruction-following capability without multitask instruction tuning, even trained solely on audio captioning datasets. Experiments on AudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene question-answering tests demonstrate our model's ability to follow various unseen instructions.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Sockpuppetry on Wikipedia Using Meta-Learning</title>
<link>https://arxiv.org/abs/2506.10314</link>
<guid>https://arxiv.org/abs/2506.10314</guid>
<content:encoded><![CDATA[
arXiv:2506.10314v1 Announce Type: cross 
Abstract: Malicious sockpuppet detection on Wikipedia is critical to preserving access to reliable information on the internet and preventing the spread of disinformation. Prior machine learning approaches rely on stylistic and meta-data features, but do not prioritise adaptability to author-specific behaviours. As a result, they struggle to effectively model the behaviour of specific sockpuppet-groups, especially when text data is limited. To address this, we propose the application of meta-learning, a machine learning technique designed to improve performance in data-scarce settings by training models across multiple tasks. Meta-learning optimises a model for rapid adaptation to the writing style of a new sockpuppet-group. Our results show that meta-learning significantly enhances the precision of predictions compared to pre-trained models, marking an advancement in combating sockpuppetry on open editing platforms. We release a new dataset of sockpuppet investigations to foster future research in both sockpuppetry and meta-learning fields.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Learning from Language Feedback</title>
<link>https://arxiv.org/abs/2506.10341</link>
<guid>https://arxiv.org/abs/2506.10341</guid>
<content:encoded><![CDATA[
arXiv:2506.10341v1 Announce Type: cross 
Abstract: Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce $\textit{transfer eluder dimension}$ as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called $\texttt{HELiX}$, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that $\texttt{HELiX}$ performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Datasets, Metrics and Models in Keyphrase Generation</title>
<link>https://arxiv.org/abs/2506.10346</link>
<guid>https://arxiv.org/abs/2506.10346</guid>
<content:encoded><![CDATA[
arXiv:2506.10346v1 Announce Type: cross 
Abstract: Keyphrase generation refers to the task of producing a set of words or phrases that summarises the content of a document. Continuous efforts have been dedicated to this task over the past few years, spreading across multiple lines of research, such as model architectures, data resources, and use-case scenarios. Yet, the current state of keyphrase generation remains unknown as there has been no attempt to review and analyse previous work. In this paper, we bridge this gap by presenting an analysis of over 50 research papers on keyphrase generation, offering a comprehensive overview of recent progress, limitations, and open challenges. Our findings highlight several critical issues in current evaluation practices, such as the concerning similarity among commonly-used benchmark datasets and inconsistencies in metric calculations leading to overestimated performances. Additionally, we address the limited availability of pre-trained models by releasing a strong PLM-based model for keyphrase generation as an effort to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Infer Confidential Properties of Training Data from LLMs?</title>
<link>https://arxiv.org/abs/2506.10364</link>
<guid>https://arxiv.org/abs/2506.10364</guid>
<content:encoded><![CDATA[
arXiv:2506.10364v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties -- such as patient demographics or disease prevalence -- that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning</title>
<link>https://arxiv.org/abs/2506.10378</link>
<guid>https://arxiv.org/abs/2506.10378</guid>
<content:encoded><![CDATA[
arXiv:2506.10378v1 Announce Type: cross 
Abstract: Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series</title>
<link>https://arxiv.org/abs/2506.10412</link>
<guid>https://arxiv.org/abs/2506.10412</guid>
<content:encoded><![CDATA[
arXiv:2506.10412v1 Announce Type: cross 
Abstract: Time series data in real-world applications such as healthcare, climate modeling, and finance are often irregular, multimodal, and messy, with varying sampling rates, asynchronous modalities, and pervasive missingness. However, existing benchmarks typically assume clean, regularly sampled, unimodal data, creating a significant gap between research and real-world deployment. We introduce Time-IMM, a dataset specifically designed to capture cause-driven irregularity in multimodal multivariate time series. Time-IMM represents nine distinct types of time series irregularity, categorized into trigger-based, constraint-based, and artifact-based mechanisms. Complementing the dataset, we introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal time series, enabling asynchronous integration and realistic evaluation. IMM-TSF includes specialized fusion modules, including a timestamp-to-text fusion module and a multimodality fusion module, which support both recency-aware averaging and attention-based integration strategies. Empirical results demonstrate that explicitly modeling multimodality on irregular time series data leads to substantial gains in forecasting performance. Time-IMM and IMM-TSF provide a foundation for advancing time series analysis under real-world conditions. The dataset is publicly available at https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the benchmark library can be accessed at https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs</title>
<link>https://arxiv.org/abs/2506.10423</link>
<guid>https://arxiv.org/abs/2506.10423</guid>
<content:encoded><![CDATA[
arXiv:2506.10423v1 Announce Type: cross 
Abstract: The integration of audio perception capabilities into Large Language Models (LLMs) has enabled significant advances in Audio-LLMs. Although application-focused developments, particularly in curating training data for specific capabilities e.g., audio reasoning, have progressed rapidly, the underlying mechanisms that govern efficient transfer of rich semantic representations from audio encoders to LLMs remain under-explored. We conceptualize effective audio-LLM interaction as the LLM's ability to proficiently probe the audio encoder representations to satisfy textual queries. This paper presents a systematic investigation on how architectural design choices can affect that. Beginning with a standard Pengi/LLaVA-style audio-LLM architecture, we propose and evaluate several modifications guided by hypotheses derived from mechanistic interpretability studies and LLM operational principles. Our experiments demonstrate that: (1) delaying audio integration until the LLM's initial layers establish textual context that enhances its ability to probe the audio representations for relevant information; (2) the LLM can proficiently probe audio representations exclusively through LLM layer's attention submodule, without requiring propagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently integrated ensemble of diverse audio encoders provides richer, complementary representations, thereby broadening the LLM's capacity to probe a wider spectrum of audio information. All hypotheses are evaluated using an identical three-stage training curriculum on a dataset of 5.6 million audio-text pairs, ensuring controlled comparisons. Our final architecture, which incorporates all proposed modifications, achieves relative improvements from 10\% to 60\% over the baseline, validating our approach to optimizing cross-modal information transfer in audio-LLMs. Project page: https://ta012.github.io/PAL/
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts</title>
<link>https://arxiv.org/abs/2506.10452</link>
<guid>https://arxiv.org/abs/2506.10452</guid>
<content:encoded><![CDATA[
arXiv:2506.10452v1 Announce Type: cross 
Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges in addressing both modality missing and Out-Of-Distribution (OOD) data simultaneously. Existing methods often rely on specific models or introduce excessive parameters, which limits their practicality. To address these issues, we propose a novel robust MER framework, Causal Inference Distiller (CIDer), and introduce a new task, Random Modality Feature Missing (RMFM), to generalize the definition of modality missing. CIDer integrates two key components: a Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal Inference (MACI) module. MSSD enhances robustness under the RMFM task through a weight-sharing self-distillation approach applied across low-level features, attention maps, and high-level representations. Additionally, a Word-level Self-aligned Attention Module (WSAM) reduces computational complexity, while a Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion. To tackle OOD challenges, MACI employs a tailored causal graph to mitigate label and language biases using a Multimodal Causal Module (MCM) and fine-grained counterfactual texts. Notably, MACI can independently enhance OOD generalization with minimal additional parameters. Furthermore, we also introduce the new repartitioned MER OOD datasets. Experimental results demonstrate that CIDer achieves robust performance in both RMFM and OOD scenarios, with fewer parameters and faster training compared to state-of-the-art methods. The implementation of this work is publicly accessible at https://github.com/gw-zhong/CIDer.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning</title>
<link>https://arxiv.org/abs/2506.10521</link>
<guid>https://arxiv.org/abs/2506.10521</guid>
<content:encoded><![CDATA[
arXiv:2506.10521v1 Announce Type: cross 
Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Encoding call-by-push-value in the pi-calculus</title>
<link>https://arxiv.org/abs/2506.10584</link>
<guid>https://arxiv.org/abs/2506.10584</guid>
<content:encoded><![CDATA[
arXiv:2506.10584v1 Announce Type: cross 
Abstract: In this report we define an encoding of Levys call-by-push-value lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both sound and complete. We present informal (by-hand) proofs of soundness, completeness, and all required lemmas. The encoding is specialized to the internal pi-calculus (pi-i-calculus) to circumvent certain challenges associated with using de Bruijn index in a formalization, and it also helps with bisimulation as early-, late- and open-bisimulation coincide in this setting, furthermore bisimulation is a congruence. Additionally, we argue that our encoding also satisfies the five criteria for good encodings proposed by Gorla, as well as show similarities between Milners and our encoding. This paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic pi-calculus and the local pi-calculus. We begin a formalization of the proof in Coq for the soundness and completeness of the encoding in the pi-i-calculus. Not all lemmas used in the formalization are themselves formally proven. However, we argue that the non-proven lemmas are reasonable, as they are proven by hand, or amount to Coq formalities that are straightforward given informal arguments.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code</title>
<link>https://arxiv.org/abs/2506.10617</link>
<guid>https://arxiv.org/abs/2506.10617</guid>
<content:encoded><![CDATA[
arXiv:2506.10617v1 Announce Type: cross 
Abstract: This paper addresses the persistent challenge of accurately digitizing paper-based electrocardiogram (ECG) recordings, with a particular focus on robustly handling single leads compromised by signal overlaps-a common yet under-addressed issue in existing methodologies. We propose a two-stage pipeline designed to overcome this limitation. The first stage employs a U-Net based segmentation network, trained on a dataset enriched with overlapping signals and fortified with custom data augmentations, to accurately isolate the primary ECG trace. The subsequent stage converts this refined binary mask into a time-series signal using established digitization techniques, enhanced by an adaptive grid detection module for improved versatility across different ECG formats and scales. Our experimental results demonstrate the efficacy of our approach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained segmentation task. Crucially, our proposed digitization method yields superior performance compared to a well-established baseline technique across both non-overlapping and challenging overlapping ECG samples. For non-overlapping signals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson Correlation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366, respectively, for the baseline. On samples with signal overlap, our method achieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the baseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to significantly enhance digitization accuracy, especially in the presence of signal overlaps, thereby laying a strong foundation for the reliable conversion of analog ECG records into analyzable digital data for contemporary research and clinical applications. The implementation is publicly available at this GitHub repository: https://github.com/masoudrahimi39/ECG-code.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Search: From Fundamentals to Frontiers in the LLM Era</title>
<link>https://arxiv.org/abs/2506.10635</link>
<guid>https://arxiv.org/abs/2506.10635</guid>
<content:encoded><![CDATA[
arXiv:2506.10635v1 Announce Type: cross 
Abstract: Conversational search enables multi-turn interactions between users and systems to fulfill users' complex information needs. During this interaction, the system should understand the users' search intent within the conversational context and then return the relevant information through a flexible, dialogue-based interface. The recent powerful large language models (LLMs) with capacities of instruction following, content generation, and reasoning, attract significant attention and advancements, providing new opportunities and challenges for building up intelligent conversational search systems. This tutorial aims to introduce the connection between fundamentals and the emerging topics revolutionized by LLMs in the context of conversational search. It is designed for students, researchers, and practitioners from both academia and industry. Participants will gain a comprehensive understanding of both the core principles and cutting-edge developments driven by LLMs in conversational search, equipping them with the knowledge needed to contribute to the development of next-generation conversational search systems.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes</title>
<link>https://arxiv.org/abs/2506.10653</link>
<guid>https://arxiv.org/abs/2506.10653</guid>
<content:encoded><![CDATA[
arXiv:2506.10653v1 Announce Type: cross 
Abstract: Speech recognisers usually perform optimally only in a specific environment and need to be adapted to work well in another. For adaptation to a new speaker, there is often too little data for fine-tuning to be robust, and that data is usually unlabelled. This paper proposes a combination of approaches to make adaptation to a single minute of data robust. First, instead of estimating the adaptation parameters with cross-entropy on a single error-prone hypothesis or "pseudo-label", this paper proposes a novel loss function, the conditional entropy over complete hypotheses. Using multiple hypotheses makes adaptation more robust to errors in the initial recognition. Second, a "speaker code" characterises a speaker in a vector short enough that it requires little data to estimate. On a far-field noise-augmented version of Common Voice, the proposed scheme yields a 20% relative improvement in word error rate on one minute of adaptation data, increasing on 10 minutes to 29%.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2506.10674</link>
<guid>https://arxiv.org/abs/2506.10674</guid>
<content:encoded><![CDATA[
arXiv:2506.10674v1 Announce Type: cross 
Abstract: The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title>
<link>https://arxiv.org/abs/2506.10751</link>
<guid>https://arxiv.org/abs/2506.10751</guid>
<content:encoded><![CDATA[
arXiv:2506.10751v1 Announce Type: cross 
Abstract: Automated question answering (QA) over electronic health records (EHRs) can bridge critical information gaps for clinicians and patients, yet it demands both precise evidence retrieval and faithful answer generation under limited supervision. In this work, we present Neural, the runner-up in the BioNLP 2025 ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method decouples the task into (1) sentence-level evidence identification and (2) answer synthesis with explicit citations. For each stage, we automatically explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning instructions and few-shot demonstrations on the development set. A self-consistency voting scheme further improves evidence recall without sacrificing precision. On the hidden test set, our method attains an overall score of 51.5, placing second stage while outperforming standard zero-shot and few-shot prompting by over 20 and 10 points, respectively. These results indicate that data-driven prompt optimization is a cost-effective alternative to model fine-tuning for high-stakes clinical QA, advancing the reliability of AI assistants in healthcare.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FASCIST-O-METER: Classifier for Neo-fascist Discourse Online</title>
<link>https://arxiv.org/abs/2506.10789</link>
<guid>https://arxiv.org/abs/2506.10789</guid>
<content:encoded><![CDATA[
arXiv:2506.10789v1 Announce Type: cross 
Abstract: Neo-fascism is a political and societal ideology that has been having remarkable growth in the last decade in the United States of America (USA), as well as in other Western societies. It poses a grave danger to democracy and the minorities it targets, and it requires active actions against it to avoid escalation. This work presents the first-of-its-kind neo-fascist coding scheme for digital discourse in the USA societal context, overseen by political science researchers. Our work bridges the gap between Natural Language Processing (NLP) and political science against this phenomena. Furthermore, to test the coding scheme, we collect a tremendous amount of activity on the internet from notable neo-fascist groups (the forums of Iron March and Stormfront.org), and the guidelines are applied to a subset of the collected posts. Through crowdsourcing, we annotate a total of a thousand posts that are labeled as neo-fascist or non-neo-fascist. With this labeled data set, we fine-tune and test both Small Language Models (SLMs) and Large Language Models (LLMs), obtaining the very first classification models for neo-fascist discourse. We find that the prevalence of neo-fascist rhetoric in this kind of forum is ever-present, making them a good target for future research. The societal context is a key consideration for neo-fascist speech when conducting NLP research. Finally, the work against this kind of political movement must be pressed upon and continued for the well-being of a democratic society. Disclaimer: This study focuses on detecting neo-fascist content in text, similar to other hate speech analyses, without labeling individuals or organizations.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoDeepResearch: Long Video Understanding With Agentic Tool Using</title>
<link>https://arxiv.org/abs/2506.10821</link>
<guid>https://arxiv.org/abs/2506.10821</guid>
<content:encoded><![CDATA[
arXiv:2506.10821v1 Announce Type: cross 
Abstract: Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Diffusion Duality</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
arXiv:2506.10892v1 Announce Type: cross 
Abstract: Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?</title>
<link>https://arxiv.org/abs/2506.10912</link>
<guid>https://arxiv.org/abs/2506.10912</guid>
<content:encoded><![CDATA[
arXiv:2506.10912v1 Announce Type: cross 
Abstract: Toxicity remains a leading cause of early-stage drug development failure. Despite advances in molecular design and property prediction, the task of molecular toxicity repair - generating structurally valid molecular alternatives with reduced toxicity - has not yet been systematically defined or benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task for general-purpose Multimodal Large Language Models (MLLMs) focused on molecular toxicity repair. We construct a standardized dataset covering 11 primary tasks and 560 representative toxic molecules spanning diverse mechanisms and granularities. We design a prompt annotation pipeline with mechanism-aware and task-adaptive capabilities, informed by expert toxicological knowledge. In parallel, we propose an automated evaluation framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity into a high-throughput evaluation chain for repair success. We systematically assess nearly 30 mainstream general-purpose MLLMs and design multiple ablation studies to analyze key factors such as evaluation criteria, candidate diversity, and failure attribution. Experimental results show that although current MLLMs still face significant challenges on this task, they begin to demonstrate promising capabilities in toxicity understanding, semantic constraint adherence, and structure-aware molecule editing.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustly Improving LLM Fairness in Realistic Settings via Interpretability</title>
<link>https://arxiv.org/abs/2506.10922</link>
<guid>https://arxiv.org/abs/2506.10922</guid>
<content:encoded><![CDATA[
arXiv:2506.10922v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in high-stakes hiring applications, making decisions that directly impact people's careers and livelihoods. While prior studies suggest simple anti-bias prompts can eliminate demographic biases in controlled evaluations, we find these mitigations fail when realistic contextual details are introduced. We address these failures through internal bias mitigation: by identifying and neutralizing sensitive attribute directions within model activations, we achieve robust bias reduction across all tested scenarios. Across leading commercial (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3, Mistral-24B), we find that adding realistic context such as company names, culture descriptions from public careers pages, and selective hiring constraints (e.g.,``only accept candidates in the top 10\%") induces significant racial and gender biases (up to 12\% differences in interview rates). When these biases emerge, they consistently favor Black over White candidates and female over male candidates across all tested models and scenarios. Moreover, models can infer demographics and become biased from subtle cues like college affiliations, with these biases remaining invisible even when inspecting the model's chain-of-thought reasoning. To address these limitations, our internal bias mitigation identifies race and gender-correlated directions and applies affine concept editing at inference time. Despite using directions from a simple synthetic dataset, the intervention generalizes robustly, consistently reducing bias to very low levels (typically under 1\%, always below 2.5\%) while largely maintaining model performance. Our findings suggest that practitioners deploying LLMs for hiring should adopt more realistic evaluation methodologies and consider internal mitigation strategies for equitable outcomes.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VINCIE: Unlocking In-context Image Editing from Video</title>
<link>https://arxiv.org/abs/2506.10941</link>
<guid>https://arxiv.org/abs/2506.10941</guid>
<content:encoded><![CDATA[
arXiv:2506.10941v1 Announce Type: cross 
Abstract: In-context image editing aims to modify images based on a contextual sequence comprising text and previously generated images. Existing methods typically depend on task-specific pipelines and expert models (e.g., segmentation and inpainting) to curate training data. In this work, we explore whether an in-context image editing model can be learned directly from videos. We introduce a scalable approach to annotate videos as interleaved multimodal sequences. To effectively learn from this data, we design a block-causal diffusion transformer trained on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. Additionally, we propose a novel multi-turn image editing benchmark to advance research in this area. Extensive experiments demonstrate that our model exhibits strong in-context image editing capabilities and achieves state-of-the-art results on two multi-turn image editing benchmarks. Despite being trained exclusively on videos, our model also shows promising abilities in multi-concept composition, story generation, and chain-of-editing applications.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models</title>
<link>https://arxiv.org/abs/2506.10946</link>
<guid>https://arxiv.org/abs/2506.10946</guid>
<content:encoded><![CDATA[
arXiv:2506.10946v1 Announce Type: cross 
Abstract: Unlearning in large language models (LLMs) is becoming increasingly important due to regulatory compliance, copyright protection, and privacy concerns. However, a key challenge in LLM unlearning is unintended forgetting, where the removal of specific data inadvertently impairs the utility of the model and its retention of valuable, desired information. While prior work has primarily focused on architectural innovations, the influence of data-level factors on unlearning performance remains underexplored. As a result, existing methods often suffer from degraded retention when forgetting high-impact data. To address this, we propose GUARD-a novel framework for Guided Unlearning And Retention via Data attribution. At its core, GUARD introduces a lightweight proxy data attribution metric tailored for LLM unlearning, which quantifies the "alignment" between the forget and retain sets while remaining computationally efficient. Building on this, we design a novel unlearning objective that assigns adaptive, nonuniform unlearning weights to samples, inversely proportional to their proxy attribution scores. Through such a reallocation of unlearning power, GUARD mitigates unintended losses in retention. We provide rigorous theoretical guarantees that GUARD significantly enhances retention while maintaining forgetting metrics comparable to prior methods. Extensive experiments on the TOFU benchmark across multiple LLM architectures demonstrate that GUARD substantially improves utility preservation while ensuring effective unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to 194.92% in terms of Truth Ratio when forgetting 10% of the training data.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Build the web for agents, not agents for the web</title>
<link>https://arxiv.org/abs/2506.10953</link>
<guid>https://arxiv.org/abs/2506.10953</guid>
<content:encoded><![CDATA[
arXiv:2506.10953v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning</title>
<link>https://arxiv.org/abs/2506.10963</link>
<guid>https://arxiv.org/abs/2506.10963</guid>
<content:encoded><![CDATA[
arXiv:2506.10963v1 Announce Type: cross 
Abstract: In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning--a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target image's core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits--low entity fidelity, weak relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmark's difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2312.07589</link>
<guid>https://arxiv.org/abs/2312.07589</guid>
<content:encoded><![CDATA[
arXiv:2312.07589v2 Announce Type: replace 
Abstract: Knowledge graphs often suffer from incompleteness issues, which can be alleviated through information completion. However, current state-of-the-art deep knowledge convolutional embedding models rely on external convolution kernels and conventional convolution processes, which limits the feature interaction capability of the model. This paper introduces a novel dynamic convolutional embedding model, ConvD, which directly reshapes relation embeddings into multiple internal convolution kernels. This approach effectively enhances the feature interactions between relation embeddings and entity embeddings. Simultaneously, we incorporate a priori knowledge-optimized attention mechanism that assigns different contribution weight coefficients to the multiple relation convolution kernels in dynamic convolution, further boosting the expressive power of the model. Extensive experiments on various datasets show that our proposed model consistently outperforms the state-of-the-art baseline methods, with average improvements ranging from 3.28% to 14.69% across all model evaluation metrics, while the number of parameters is reduced by 50.66% to 85.40% compared to other state-of-the-art models.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak-to-Strong Jailbreaking on Large Language Models</title>
<link>https://arxiv.org/abs/2401.17256</link>
<guid>https://arxiv.org/abs/2401.17256</guid>
<content:encoded><![CDATA[
arXiv:2401.17256v3 Announce Type: replace 
Abstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Descriptive Language Model for Vector Graphics Reasoning</title>
<link>https://arxiv.org/abs/2404.06479</link>
<guid>https://arxiv.org/abs/2404.06479</guid>
<content:encoded><![CDATA[
arXiv:2404.06479v5 Announce Type: replace 
Abstract: Despite significant advancements, large multimodal models (LMMs) still struggle to bridge the gap between low-level visual perception -- focusing on shapes, sizes, and layouts -- and high-level language reasoning, such as semantics and logic. This limitation is evident in tasks that require precise visual perception, like comparing geometric properties or solving visual reasoning problems. To study this failure mode, we focus on vector graphics -- images composed of 2D objects and shapes, prevalent in LMM-based tasks in web, design, and OS environments. We identify two key research questions: how can we enable precise visual perception, and how can we facilitate high-level reasoning based on such low-level perceptions? To capture fine visual details, we use Scalable Vector Graphics (SVG) for accurate encoding of visual scenes. However, SVGs are not readily interpretable by LMMs in a zero-shot manner. To tackle this, we propose the Visually Descriptive Language Model (VDLM), which introduces a Primal Visual Description (PVD) as an intermediate textual representation. PVD translates SVGs into a text-based abstraction consisting of primitive attributes (e.g., shape, position, measurement) and their corresponding values. PVD can be learned using task-agnostic synthesized data and represents visual primitives that are universal across vector graphics. This abstraction is more structured, allowing for direct interpretation by foundation models for zero-shot generalization. Without human-annotated data, empirical results show that VDLM significantly improves state-of-the-art LMMs like GPT-4o on various multimodal perception and reasoning tasks. Extensive analyses of VDLM show improved interpretability due to its disentangled perception and reasoning. We also demonstrate a positive correlation between PVD quality and task performance. Project page: https://mikewangwzhl.github.io/VDLM/
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language</title>
<link>https://arxiv.org/abs/2406.19349</link>
<guid>https://arxiv.org/abs/2406.19349</guid>
<content:encoded><![CDATA[
arXiv:2406.19349v2 Announce Type: replace 
Abstract: Hate speech poses a significant threat to social harmony. Over the past two years, Indonesia has seen a ten-fold increase in the online hate speech ratio, underscoring the urgent need for effective detection mechanisms. However, progress is hindered by the limited availability of labeled data for Indonesian texts. The condition is even worse for marginalized minorities, such as Shia, LGBTQ, and other ethnic minorities because hate speech is underreported and less understood by detection tools. Furthermore, the lack of accommodation for subjectivity in current datasets compounds this issue. To address this, we introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity classification dataset. Comprising 43,692 entries annotated by 19 diverse individuals, the dataset focuses on texts targeting vulnerable groups in Indonesia, specifically during the hottest political event in the country: the presidential election. We establish baselines for seven binary classification tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet) fine-tuned for hate speech classification. Furthermore, we demonstrate how incorporating demographic information can enhance the zero-shot performance of the large language model, gpt-3.5-turbo. However, we also caution that an overemphasis on demographic information can negatively impact the fine-tuned model performance due to data fragmentation.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs for Environmental Review and Permitting</title>
<link>https://arxiv.org/abs/2407.07321</link>
<guid>https://arxiv.org/abs/2407.07321</guid>
<content:encoded><![CDATA[
arXiv:2407.07321v3 Announce Type: replace 
Abstract: The National Environment Policy Act (NEPA) stands as a foundational piece of environmental legislation in the United States, requiring federal agencies to consider the environmental impacts of their proposed actions. The primary mechanism for achieving this is through the preparation of Environmental Assessments (EAs) and, for significant impacts, comprehensive Environmental Impact Statements (EIS). Large Language Model (LLM)s' effectiveness in specialized domains like NEPA remains untested for adoption in federal decision-making processes. To address this gap, we present NEPA Question and Answering Dataset (NEPAQuAD), the first comprehensive benchmark derived from EIS documents, along with a modular and transparent evaluation pipeline, MAPLE, to assess LLM performance on NEPA-focused regulatory reasoning tasks. Our benchmark leverages actual EIS documents to create diverse question types, ranging from factual to complex problem-solving ones. We built a modular and transparent evaluation pipeline to test both closed- and open-source models in zero-shot or context-driven QA benchmarks. We evaluate five state-of-the-art LLMs using our framework to assess both their prior knowledge and their ability to process NEPA-specific information. The experimental results reveal that all the models consistently achieve their highest performance when provided with the gold passage as context. While comparing the other context-driven approaches for each model, Retrieval Augmented Generation (RAG)-based approaches substantially outperform PDF document contexts, indicating that neither model is well suited for long-context question-answering tasks. Our analysis suggests that NEPA-focused regulatory reasoning tasks pose a significant challenge for LLMs, particularly in terms of understanding the complex semantics and effectively processing the lengthy regulatory documents.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-group Uncertainty Quantification for Long-form Text Generation</title>
<link>https://arxiv.org/abs/2407.21057</link>
<guid>https://arxiv.org/abs/2407.21057</guid>
<content:encoded><![CDATA[
arXiv:2407.21057v2 Announce Type: replace 
Abstract: While past works have shown how uncertainty quantification can be applied to large language model (LLM) outputs, the question of whether resulting uncertainty guarantees still hold within sub-groupings of data remains open. In our work, given some long-form text generated by an LLM, we study uncertainty at both the level of individual claims contained within the output (via calibration) and across the entire output itself (via conformal prediction). Using biography generation as a testbed for this study, we derive a set of (demographic) attributes (e.g., whether some text describes a man or woman) for each generation to form such "subgroups" of data. We find that although canonical methods for both types of uncertainty quantification perform well when measuring across the entire dataset, such guarantees break down when examining particular subgroups. Having established this issue, we invoke group-conditional methods for uncertainty quantification -- multicalibration and multivalid conformal prediction -- and find that across a variety of approaches, additional subgroup information consistently improves calibration and conformal prediction within subgroups (while crucially retaining guarantees across the entire dataset). As the problems of calibration, conformal prediction, and their multi-group counterparts have not been extensively explored in the context of long-form text generation, we consider these results to form a benchmark for this setting.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models</title>
<link>https://arxiv.org/abs/2408.08545</link>
<guid>https://arxiv.org/abs/2408.08545</guid>
<content:encoded><![CDATA[
arXiv:2408.08545v4 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely adopted due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse models. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier's predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13\% on GSM8K and 70\% on MMLU, compared to the top-performing baseline. Also, we establish a theoretical upper bound by an Oracle with LLMs and perform an in-depth linguistic analysis to understand the performance gap between the Oracle and SelectLLM.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation</title>
<link>https://arxiv.org/abs/2408.08688</link>
<guid>https://arxiv.org/abs/2408.08688</guid>
<content:encoded><![CDATA[
arXiv:2408.08688v5 Announce Type: replace 
Abstract: This paper presents a novel methodology for generating synthetic Preference Optimization (PO) datasets using multi-model workflows. We evaluate the effectiveness and potential of these workflows in automating and enhancing the dataset generation process. PO dataset generation requires two modules: (1) $\textit{response evaluation}$, and (2) $\textit{response generation}$. In the $\textit{response evaluation}$ module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across all datasets. For the $\textit{response generation}$ module, we use the identified LLM evaluator configuration and compare different configurations of the LLM Feedback Loop. We use the win rate to determine the best multi-model configuration for generation. Experimenting with various configurations, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-model Llama and Gemma, respectively. After identifying the best configurations for both modules, we generate our PO datasets using the above pipeline.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs</title>
<link>https://arxiv.org/abs/2408.09742</link>
<guid>https://arxiv.org/abs/2408.09742</guid>
<content:encoded><![CDATA[
arXiv:2408.09742v2 Announce Type: replace 
Abstract: Detecting issue framing in text - how different perspectives approach the same topic - is valuable for social science and policy analysis, yet challenging for automated methods due to subtle linguistic differences. We introduce `paired completion', a novel approach using LLM next-token log probabilities to detect contrasting frames using minimal examples. Through extensive evaluation across synthetic datasets and a human-labeled corpus, we demonstrate that paired completion is a cost-efficient, low-bias alternative to both prompt-based and embedding-based methods, offering a scalable solution for analyzing issue framing in large text collections, especially suited to low-resource settings.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling</title>
<link>https://arxiv.org/abs/2410.01651</link>
<guid>https://arxiv.org/abs/2410.01651</guid>
<content:encoded><![CDATA[
arXiv:2410.01651v4 Announce Type: replace 
Abstract: Despite the success of Transformers, handling long contexts remains challenging due to the limited length generalization and quadratic complexity of self-attention. Thus Transformers often require post-training with a larger attention window, significantly increasing computational and memory costs. In this paper, we propose a novel attention mechanism based on dynamic context, Grouped Cross Attention (GCA), which can generalize to 1000 times the pre-training context length while maintaining the ability to access distant information with a constant attention window size. For a given input sequence, we split it into chunks and use each chunk to retrieve top-k relevant past chunks for subsequent text generation. Specifically, unlike most previous works that use an off-the-shelf retriever, our key innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner. Such a mechanism accommodates retrieved chunks with a fixed-size attention window to achieve long-range information access, significantly reducing computational and memory costs during training and inference. Experiments show that GCA-based models achieve near-perfect accuracy in passkey retrieval for 16M context lengths, which is 1000 times the training length.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Identifying Watermarked Segments in Mixed-Source Texts</title>
<link>https://arxiv.org/abs/2410.03600</link>
<guid>https://arxiv.org/abs/2410.03600</guid>
<content:encoded><![CDATA[
arXiv:2410.03600v2 Announce Type: replace 
Abstract: Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection. Our code is publicly available at https://github.com/XuandongZhao/llm-watermark-location
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistent Topological Features in Large Language Models</title>
<link>https://arxiv.org/abs/2410.11042</link>
<guid>https://arxiv.org/abs/2410.11042</guid>
<content:encoded><![CDATA[
arXiv:2410.11042v2 Announce Type: replace 
Abstract: Understanding the decision-making processes of large language models is critical given their widespread applications. To achieve this, we aim to connect a formal mathematical framework -- zigzag persistence from topological data analysis -- with practical and easily applicable algorithms. Zigzag persistence is particularly effective for characterizing data as it dynamically transforms across model layers. Within this framework, we introduce topological descriptors that measure how topological features, $p$-dimensional holes, persist and evolve throughout the layers. Unlike methods that assess each layer individually and then aggregate the results, our approach directly tracks the full evolutionary path of these features. This offers a statistical perspective on how prompts are rearranged and their relative positions changed in the representation space, providing insights into the system's operation as an integrated whole. To demonstrate the expressivity and applicability of our framework, we highlight how sensitive these descriptors are to different models and a variety of datasets. As a showcase application to a downstream task, we use zigzag persistence to establish a criterion for layer pruning, achieving results comparable to state-of-the-art methods while preserving the system-level perspective.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Many-Shot In-Context Learning for Long-Context Evaluation</title>
<link>https://arxiv.org/abs/2411.07130</link>
<guid>https://arxiv.org/abs/2411.07130</guid>
<content:encoded><![CDATA[
arXiv:2411.07130v3 Announce Type: replace 
Abstract: Many-shot in-context learning (ICL) has emerged as a unique setup to both utilize and test the ability of large language models to handle long context. This paper delves into long-context language model (LCLM) evaluation through many-shot ICL. We first ask: what types of ICL tasks benefit from additional demonstrations, and how effective are they in evaluating LCLMs? We find that classification and summarization tasks show performance improvements with additional demonstrations, while translation and reasoning tasks do not exhibit clear trends. Next, we investigate the extent to which different tasks necessitate retrieval versus global context understanding. We develop metrics to categorize ICL tasks into two groups: (i) similar-sample learning (SSL): tasks where retrieval of the most similar examples is sufficient for good performance, and (ii) all-sample learning (ASL): tasks that necessitate a deeper comprehension of all examples in the prompt. Lastly, we introduce a new many-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both fronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while state-of-the-art models demonstrate good performance up to 64k tokens in SSL tasks, many models experience significant performance drops at only 16k tokens in ASL tasks.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Squeezed Attention: Accelerating Long Context Length LLM Inference</title>
<link>https://arxiv.org/abs/2411.09688</link>
<guid>https://arxiv.org/abs/2411.09688</guid>
<content:encoded><![CDATA[
arXiv:2411.09688v3 Announce Type: replace 
Abstract: Emerging Large Language Model (LLM) applications require long input context in order to perform complex tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations in order to process user inputs quickly, as they are received. We propose Squeezed Attention to accelerate LLM applications where a large portion of the input context is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which keys from the fixed context are semantically relevant, and then compute exact attention using only the important keys, thereby reducing bandwidth and computational costs. We also present a hierarchical version of our algorithm which can reduce the complexity of attention from linear to logarithmic with respect to the fixed context length. We evaluate our method on long-context benchmarks including LongBench, where it achieves a 3.1$\times$ reduction in KV budget with no noticeable accuracy loss and up to an 8$\times$ reduction with only a 0.5 point accuracy gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models. Futhermore, we implement kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4$\times$ speedups during both the prefill and generation phases for long-context inference. Our code is available at https://github.com/SqueezeAILab/SqueezedAttention.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-based Depth Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2502.04348</link>
<guid>https://arxiv.org/abs/2502.04348</guid>
<content:encoded><![CDATA[
arXiv:2502.04348v3 Announce Type: replace 
Abstract: Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent -- a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2502.12378</link>
<guid>https://arxiv.org/abs/2502.12378</guid>
<content:encoded><![CDATA[
arXiv:2502.12378v3 Announce Type: replace 
Abstract: Understanding pragmatics-the use of language in context-is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatic phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeamLoRA: Beam-Constraint Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2502.13604</link>
<guid>https://arxiv.org/abs/2502.13604</guid>
<content:encoded><![CDATA[
arXiv:2502.13604v2 Announce Type: replace 
Abstract: Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps</title>
<link>https://arxiv.org/abs/2502.14829</link>
<guid>https://arxiv.org/abs/2502.14829</guid>
<content:encoded><![CDATA[
arXiv:2502.14829v2 Announce Type: replace 
Abstract: When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models' parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models' prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15010</link>
<guid>https://arxiv.org/abs/2502.15010</guid>
<content:encoded><![CDATA[
arXiv:2502.15010v2 Announce Type: replace 
Abstract: Recent copyright agreements between AI companies and content creators underscore the need for fine-grained control over language models' ability to reproduce copyrighted text. Existing defenses-ranging from aggressive unlearning to simplistic output filters-either sacrifice model utility or inadequately address verbatim leakage. We introduce Obliviate, a lightweight post-training method that surgically suppresses exact reproduction of specified sequences while preserving semantic understanding. Obliviate first identifies memorized passages and then, for each target token, minimally adjusts the model's output distribution via a Kullback-Leibler divergence penalty to drive down the probability of exact reproduction. Simultaneously, we enforce a consistency loss on non-target tokens to retain the model's fluency and task performance. We evaluate Obliviate on four popular 6-8B-parameter models (LLaMA-3.1, LLaMA-3.1-Instruct, Qwen-2.5, and Yi-1.5) using synthetic memorization benchmarks and organic copyrighted excerpts (e.g., Moby Dick, Frankenstein, Alice in Wonderland and Les Miserables). Across all settings, Obliviate reduces verbatim recall by two orders of magnitude (e.g., from hundreds of words to fewer than 12) while degrading downstream accuracy by at most 1% on HellaSwag, MMLU, TruthfulQA, and Winogrande. Furthermore, we benchmark Obliviate aganist different unlearning and copyright techniques using the MUSE and CoTaEval benchmarks. These results position Obliviate as a practical, high-fidelity solution for copyright compliance in deployed LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics</title>
<link>https://arxiv.org/abs/2502.15022</link>
<guid>https://arxiv.org/abs/2502.15022</guid>
<content:encoded><![CDATA[
arXiv:2502.15022v3 Announce Type: replace 
Abstract: Large language models (LLMs) make it easy to rewrite a text in any style -- e.g. to make it more polite, persuasive, or more positive -- but evaluation thereof is not straightforward. A challenge lies in measuring content preservation: that content not attributable to style change is retained. This paper presents a large meta-evaluation of metrics for evaluating style and attribute transfer, focusing on content preservation. We find that meta-evaluation studies on existing datasets lead to misleading conclusions about the suitability of metrics for content preservation. Widely used metrics show a high correlation with human judgments despite being deemed unsuitable for the task -- because they do not abstract from style changes when evaluating content preservation. We show that the overly high correlations with human judgment stem from the nature of the test data. To address this issue, we introduce a new, challenging test set specifically designed for evaluating content preservation metrics for style transfer. Using this dataset, we demonstrate that suitable metrics for content preservation for style transfer indeed are style-aware. To support efficient evaluation, we propose a new style-aware method that utilises small language models, obtaining a higher alignment with human judgements than prompting a model of a similar size as an autorater.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2502.15916</link>
<guid>https://arxiv.org/abs/2502.15916</guid>
<content:encoded><![CDATA[
arXiv:2502.15916v2 Announce Type: replace 
Abstract: This paper presents the Esethu Framework, a sustainable data curation framework specifically designed to empower local communities and ensure equitable benefit-sharing from their linguistic resource. This framework is supported by the Esethu license, a novel community-centric data license. As a proof of concept, we introduce the Vuk'uzenzele isiXhosa Speech Dataset (ViXSD), an open-source corpus developed under the Esethu Framework and License. The dataset, containing read speech from native isiXhosa speakers enriched with demographic and linguistic metadata, demonstrates how community-driven licensing and curation principles can bridge resource gaps in automatic speech recognition (ASR) for African languages while safeguarding the interests of data creators. We describe the framework guiding dataset development, outline the Esethu license provisions, present the methodology for ViXSD, and present ASR experiments validating ViXSD's usability in building and refining voice-driven applications for isiXhosa.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs</title>
<link>https://arxiv.org/abs/2502.19148</link>
<guid>https://arxiv.org/abs/2502.19148</guid>
<content:encoded><![CDATA[
arXiv:2502.19148v3 Announce Type: replace 
Abstract: How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Multilingual Previously Fact-Checked Claim Detection</title>
<link>https://arxiv.org/abs/2503.02737</link>
<guid>https://arxiv.org/abs/2503.02737</guid>
<content:encoded><![CDATA[
arXiv:2503.02737v2 Announce Type: replace 
Abstract: In our era of widespread false information, human fact-checkers often face the challenge of duplicating efforts when verifying claims that may have already been addressed in other countries or languages. As false information transcends linguistic boundaries, the ability to automatically detect previously fact-checked claims across languages has become an increasingly important task. This paper presents the first comprehensive evaluation of large language models (LLMs) for multilingual previously fact-checked claim detection. We assess seven LLMs across 20 languages in both monolingual and cross-lingual settings. Our results show that while LLMs perform well for high-resource languages, they struggle with low-resource languages. Moreover, translating original texts into English proved to be beneficial for low-resource languages. These findings highlight the potential of LLMs for multilingual previously fact-checked claim detection and provide a foundation for further research on this promising application of LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Safety Alignment with Dual-Objective Optimization</title>
<link>https://arxiv.org/abs/2503.03710</link>
<guid>https://arxiv.org/abs/2503.03710</guid>
<content:encoded><![CDATA[
arXiv:2503.03710v2 Announce Type: replace 
Abstract: Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations</title>
<link>https://arxiv.org/abs/2503.06987</link>
<guid>https://arxiv.org/abs/2503.06987</guid>
<content:encoded><![CDATA[
arXiv:2503.06987v2 Announce Type: replace 
Abstract: Measuring social bias in large language models (LLMs) is crucial, but existing bias evaluation methods struggle to assess bias in long-form generation. We propose a Bias Benchmark for Generation (BBG), an adaptation of the Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form generation by having LLMs generate continuations of story prompts. Building our benchmark in English and Korean, we measure the probability of neutral and biased generations across ten LLMs. We also compare our long-form story generation evaluation results with multiple-choice BBQ evaluation, showing that the two approaches produce inconsistent results.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges</title>
<link>https://arxiv.org/abs/2503.08292</link>
<guid>https://arxiv.org/abs/2503.08292</guid>
<content:encoded><![CDATA[
arXiv:2503.08292v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly applied to outpatient referral tasks across healthcare systems. However, there is a lack of standardized evaluation criteria to assess their effectiveness, particularly in dynamic, interactive scenarios. In this study, we systematically examine the capabilities and limitations of LLMs in managing tasks within Intelligent Outpatient Referral (IOR) systems and propose a comprehensive evaluation framework specifically designed for such systems. This framework comprises two core tasks: static evaluation, which focuses on evaluating the ability of predefined outpatient referrals, and dynamic evaluation, which evaluates capabilities of refining outpatient referral recommendations through iterative dialogues. Our findings suggest that LLMs offer limited advantages over BERT-like models, but show promise in asking effective questions during interactive dialogues.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computation Mechanism Behind LLM Position Generalization</title>
<link>https://arxiv.org/abs/2503.13305</link>
<guid>https://arxiv.org/abs/2503.13305</guid>
<content:encoded><![CDATA[
arXiv:2503.13305v2 Announce Type: replace 
Abstract: Most written natural languages are composed of sequences of words and sentences. Similar to humans, large language models (LLMs) exhibit flexibility in handling textual positions - a phenomenon we term position generalization. They can understand texts with position perturbations and generalize to longer texts than those encountered during training with the latest techniques. These phenomena suggest that LLMs handle positions tolerantly, but how LLMs computationally process positional relevance remains largely unexplored. This work connects the linguistic phenomenon with LLMs' computational mechanisms. We show how LLMs enforce certain computational mechanisms for the aforementioned tolerance in position perturbations. Despite the complex design of the self-attention mechanism, this work reveals that LLMs learn a counterintuitive disentanglement of attention logits. Their values show a 0.959 linear correlation with an approximation of the arithmetic sum of positional relevance and semantic importance. Furthermore, we identify a prevalent pattern in intermediate features, which we prove theoretically enables this effect. The pattern, which is different from how randomly initialized parameters would behave, suggests that it is a learned behavior rather than a natural result of the model architecture. Based on these findings, we provide computational explanations and criteria for LLMs' position flexibilities. This work takes a pioneering step in linking position generalization with modern LLMs' internal mechanisms.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play</title>
<link>https://arxiv.org/abs/2503.14432</link>
<guid>https://arxiv.org/abs/2503.14432</guid>
<content:encoded><![CDATA[
arXiv:2503.14432v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically "plays" with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORE: Story Coherence and Retrieval Enhancement for AI Narratives</title>
<link>https://arxiv.org/abs/2503.23512</link>
<guid>https://arxiv.org/abs/2503.23512</guid>
<content:encoded><![CDATA[
arXiv:2503.23512v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPA-CHILDES &amp; G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling</title>
<link>https://arxiv.org/abs/2504.03036</link>
<guid>https://arxiv.org/abs/2504.03036</guid>
<content:encoded><![CDATA[
arXiv:2504.03036v3 Announce Type: replace 
Abstract: In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabularies that are inconsistent with established phonemic inventories, an issue which G2P+ addresses by leveraging the inventories in the Phoible database. Using this tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES. This new resource fills several gaps in existing phonemic datasets, which often lack multilingual coverage, spontaneous speech, and a focus on child-directed language. We demonstrate the utility of this dataset for phonological research by training phoneme language models on 11 languages and probing them for distinctive features, finding that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BabyLM's First Words: Word Segmentation as a Phonological Probing Task</title>
<link>https://arxiv.org/abs/2504.03338</link>
<guid>https://arxiv.org/abs/2504.03338</guid>
<content:encoded><![CDATA[
arXiv:2504.03338v3 Announce Type: replace 
Abstract: Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction</title>
<link>https://arxiv.org/abs/2504.17353</link>
<guid>https://arxiv.org/abs/2504.17353</guid>
<content:encoded><![CDATA[
arXiv:2504.17353v2 Announce Type: replace 
Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building UD Cairo for Old English in the Classroom</title>
<link>https://arxiv.org/abs/2504.18718</link>
<guid>https://arxiv.org/abs/2504.18718</guid>
<content:encoded><![CDATA[
arXiv:2504.18718v2 Announce Type: replace 
Abstract: In this paper we present a sample treebank for Old English based on the UD Cairo sentences, collected and annotated as part of a classroom curriculum in Historical Linguistics. To collect the data, a sample of 20 sentences illustrating a range of syntactic constructions in the world's languages, we employ a combination of LLM prompting and searches in authentic Old English data. For annotation we assigned sentences to multiple students with limited prior exposure to UD, whose annotations we compare and adjudicate. Our results suggest that while current LLM outputs in Old English do not reflect authentic syntax, this can be mitigated by post-editing, and that although beginner annotators do not possess enough background to complete the task perfectly, taken together they can produce good results and learn from the experience. We also conduct preliminary parsing experiments using Modern English training data, and find that although performance on Old English is poor, parsing on annotated features (lemma, hyperlemma, gloss) leads to improved performance.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards</title>
<link>https://arxiv.org/abs/2505.02686</link>
<guid>https://arxiv.org/abs/2505.02686</guid>
<content:encoded><![CDATA[
arXiv:2505.02686v2 Announce Type: replace 
Abstract: Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities for diverse tasks. In this survey, we present a comprehensive overview of learning from rewards, from the perspective of reward models and learning strategies across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research Borderlands: Analysing Writing Across Research Cultures</title>
<link>https://arxiv.org/abs/2506.00784</link>
<guid>https://arxiv.org/abs/2506.00784</guid>
<content:encoded><![CDATA[
arXiv:2506.00784v2 Announce Type: replace 
Abstract: Improving cultural competence of language technologies is important. However most recent works rarely engage with the communities they study, and instead rely on synthetic setups and imperfect proxies of culture. In this work, we take a human-centered approach to discover and measure language-based cultural norms, and cultural competence of LLMs. We focus on a single kind of culture, research cultures, and a single task, adapting writing across research cultures. Through a set of interviews with interdisciplinary researchers, who are experts at moving between cultures, we create a framework of structural, stylistic, rhetorical, and citational norms that vary across research cultures. We operationalise these features with a suite of computational metrics and use them for (a) surfacing latent cultural norms in human-written research papers at scale; and (b) highlighting the lack of cultural competence of LLMs, and their tendency to homogenise writing. Overall, our work illustrates the efficacy of a human-centered approach to measuring cultural norms in human-written and LLM-generated texts.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models</title>
<link>https://arxiv.org/abs/2506.01062</link>
<guid>https://arxiv.org/abs/2506.01062</guid>
<content:encoded><![CDATA[
arXiv:2506.01062v2 Announce Type: replace 
Abstract: We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering</title>
<link>https://arxiv.org/abs/2506.01784</link>
<guid>https://arxiv.org/abs/2506.01784</guid>
<content:encoded><![CDATA[
arXiv:2506.01784v2 Announce Type: replace 
Abstract: While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance</title>
<link>https://arxiv.org/abs/2402.08680</link>
<guid>https://arxiv.org/abs/2402.08680</guid>
<content:encoded><![CDATA[
arXiv:2402.08680v2 Announce Type: replace-cross 
Abstract: The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRSA: Prompt Stealing Attacks against Real-World Prompt Services</title>
<link>https://arxiv.org/abs/2402.19200</link>
<guid>https://arxiv.org/abs/2402.19200</guid>
<content:encoded><![CDATA[
arXiv:2402.19200v3 Announce Type: replace-cross 
Abstract: Recently, large language models (LLMs) have garnered widespread attention for their exceptional capabilities. Prompts are central to the functionality and performance of LLMs, making them highly valuable assets. The increasing reliance on high-quality prompts has driven significant growth in prompt services. However, this growth also expands the potential for prompt leakage, increasing the risk that attackers could replicate original functionalities, create competing products, and severely infringe on developers' intellectual property. Despite these risks, prompt leakage in real-world prompt services remains underexplored.
  In this paper, we present PRSA, a practical attack framework designed for prompt stealing. PRSA infers the detailed intent of prompts through very limited input-output analysis and can successfully generate stolen prompts that replicate the original functionality. Extensive evaluations demonstrate PRSA's effectiveness across two main types of real-world prompt services. Specifically, compared to previous works, it improves the attack success rate from 17.8% to 46.1% in prompt marketplaces and from 39% to 52% in LLM application stores, respectively. Notably, in the attack on "Math", one of the most popular educational applications in OpenAI's GPT Store with over 1 million conversations, PRSA uncovered a hidden Easter egg that had not been revealed previously. Besides, our analysis reveals that higher mutual information between a prompt and its output correlates with an increased risk of leakage. This insight guides the design and evaluation of two potential defenses against the security threats posed by PRSA. We have reported these findings to the prompt service vendors, including PromptBase and OpenAI, and actively collaborate with them to implement defensive measures.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failure Modes of LLMs for Causal Reasoning on Narratives</title>
<link>https://arxiv.org/abs/2410.23884</link>
<guid>https://arxiv.org/abs/2410.23884</guid>
<content:encoded><![CDATA[
arXiv:2410.23884v4 Announce Type: replace-cross 
Abstract: In this work, we investigate the causal reasoning abilities of large language models (LLMs) through the representative problem of inferring causal relationships from narratives. We find that even state-of-the-art language models rely on unreliable shortcuts, both in terms of the narrative presentation and their parametric knowledge. For example, LLMs tend to determine causal relationships based on the topological ordering of events (i.e., earlier events cause later ones), resulting in lower performance whenever events are not narrated in their exact causal order. Similarly, we demonstrate that LLMs struggle with long-term causal reasoning and often fail when the narratives are long and contain many events. Additionally, we show LLMs appear to rely heavily on their parametric knowledge at the expense of reasoning over the provided narrative. This degrades their abilities whenever the narrative opposes parametric knowledge. We extensively validate these failure modes through carefully controlled synthetic experiments, as well as evaluations on real-world narratives. Finally, we observe that explicitly generating a causal graph generally improves performance while naive chain-of-thought is ineffective. Collectively, our results distill precise failure modes of current state-of-the-art models and can pave the way for future techniques to enhance causal reasoning in LLMs.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debiasing Watermarks for Large Language Models via Maximal Coupling</title>
<link>https://arxiv.org/abs/2411.11203</link>
<guid>https://arxiv.org/abs/2411.11203</guid>
<content:encoded><![CDATA[
arXiv:2411.11203v2 Announce Type: replace-cross 
Abstract: Watermarking language models is essential for distinguishing between human and machine-generated text and thus maintaining the integrity and trustworthiness of digital communication. We present a novel green/red list watermarking approach that partitions the token set into ``green'' and ``red'' lists, subtly increasing the generation probability for green tokens. To correct token distribution bias, our method employs maximal coupling, using a uniform coin flip to decide whether to apply bias correction, with the result embedded as a pseudorandom watermark signal. Theoretical analysis confirms this approach's unbiased nature and robust detection capabilities. Experimental results show that it outperforms prior techniques by preserving text quality while maintaining high detectability, and it demonstrates resilience to targeted modifications aimed at improving text quality. This research provides a promising watermarking solution for language models, balancing effective detection with minimal impact on text quality.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Great Models Think Alike and this Undermines AI Oversight</title>
<link>https://arxiv.org/abs/2502.04313</link>
<guid>https://arxiv.org/abs/2502.04313</guid>
<content:encoded><![CDATA[
arXiv:2502.04313v2 Announce Type: replace-cross 
Abstract: As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as ''AI Oversight''. We study how model similarity affects both aspects of AI oversight by proposing Chance Adjusted Probabilistic Agreement (CAPA): a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from ''weak-to-strong generalization''. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce</title>
<link>https://arxiv.org/abs/2504.11343</link>
<guid>https://arxiv.org/abs/2504.11343</guid>
<content:encoded><![CDATA[
arXiv:2504.11343v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15298</link>
<guid>https://arxiv.org/abs/2505.15298</guid>
<content:encoded><![CDATA[
arXiv:2505.15298v3 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce AgentThink, a pioneering unified framework that, for the first time, integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's core innovations include: (i) Structured Data Generation, by establishing an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; (ii) A Two-stage Training Pipeline, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and (iii) Agent-style Tool-Usage Evaluation, introducing a novel multi-tool assessment protocol to rigorously evaluate the model's tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate AgentThink significantly boosts overall reasoning scores by 53.91% and enhances answer accuracy by 33.54%, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.22654</link>
<guid>https://arxiv.org/abs/2505.22654</guid>
<content:encoded><![CDATA[
arXiv:2505.22654v2 Announce Type: replace-cross 
Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal understanding by incorporating finer-grained visual perception and encoding. However, such methods incur significant computational costs due to longer visual token sequences, posing challenges for real-time deployment. To mitigate this, prior studies have explored pruning unimportant visual tokens either at the output layer of the visual encoder or at the early layers of the language model. In this work, we revisit these design choices and reassess their effectiveness through comprehensive empirical studies of how visual tokens are processed throughout the visual encoding and language decoding stages. Guided by these insights, we propose VScan, a two-stage visual token reduction framework that addresses token redundancy by: (1) integrating complementary global and local scans with token merging during visual encoding, and (2) introducing pruning at intermediate layers of the language model. Extensive experimental results across four LVLMs validate the effectiveness of VScan in accelerating inference and demonstrate its superior performance over current state-of-the-arts on sixteen benchmarks. Notably, when applied to LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a 10$\times$ reduction in FLOPs, while retaining 95.4\% of the original performance. Code is available at https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets</title>
<link>https://arxiv.org/abs/2506.00073</link>
<guid>https://arxiv.org/abs/2506.00073</guid>
<content:encoded><![CDATA[
arXiv:2506.00073v2 Announce Type: replace-cross 
Abstract: AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v2 Announce Type: replace-cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in Language Models</title>
<link>https://arxiv.org/abs/2506.04636</link>
<guid>https://arxiv.org/abs/2506.04636</guid>
<content:encoded><![CDATA[
arXiv:2506.04636v2 Announce Type: replace-cross 
Abstract: Law has long been a domain that has been popular in natural language processing (NLP) applications. Reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. Nevertheless, while multiple legal datasets exist, none have thus far focused specifically on reasoning tasks. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark (CHANCERY) to test a model's ability to reason about whether executive/board/shareholder's proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models - modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The model's task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents exhibit superior performance, with agents based on the ReAct and CodeAct frameworks scoring 76.1% and 78.1% respectively, further confirming the advanced legal reasoning capabilities required to score highly on the benchmark. We also conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of SOTA models.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2506.09200</link>
<guid>https://arxiv.org/abs/2506.09200</guid>
<content:encoded><![CDATA[
arXiv:2506.09200v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective in addressing many of the drawbacks of relying solely on the parametric memory of large language models. Recent work has demonstrated that RAG systems can be improved via fine-tuning of their retriever and generator models. In this work, we introduce FedRAG, a framework for fine-tuning RAG systems across centralized and federated architectures. FedRAG supports state-of-the-art fine-tuning methods, offering a simple and intuitive interface and a seamless conversion from centralized to federated training tasks. FedRAG is also deeply integrated with the modern RAG ecosystem, filling a critical gap in available tools.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Text-based Protein Understanding: Retrieval or LLM?</title>
<link>https://arxiv.org/abs/2505.20354</link>
<guid>https://arxiv.org/abs/2505.20354</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-text models, data leakage, evaluation framework, retrieval-enhanced method, protein-to-text generation

Summary:
Protein-text models have gained attention for their ability to generate and understand proteins. Current approaches integrate protein knowledge into language models but suffer from data leakage issues in benchmarks. Traditional metrics do not accurately assess performance in this domain. To address these challenges, the authors reorganize datasets and introduce an evaluation framework based on biological entities. They propose a retrieval-enhanced method that outperforms fine-tuned models for protein-to-text generation, demonstrating accuracy and efficiency in training-free scenarios. The code and data are available at https://github.com/IDEA-XL/RAPM. 

Summary: <div>
arXiv:2505.20354v3 Announce Type: replace 
Abstract: In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering</title>
<link>https://arxiv.org/abs/2506.03949</link>
<guid>https://arxiv.org/abs/2506.03949</guid>
<content:encoded><![CDATA[
<div> LLMs, TableQA, TableEval, SEAT, benchmark <br />
Summary:<br />
LLMs have made great strides in natural language processing, but face challenges in TableQA due to complex real-world scenarios. The TableEval benchmark addresses these issues by including diverse table structures and data from various domains and languages. SEAT evaluation framework ensures semantic accuracy by aligning model responses with reference answers at the sub-question level, showing high agreement with human judgment. Experiments on TableEval highlight gaps in state-of-the-art LLMs' performance on complex TableQA tasks, providing insights for future enhancements. The dataset is available at: https://github.com/wenge-research/TableEval. <br /> <div>
arXiv:2506.03949v2 Announce Type: replace 
Abstract: LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: https://github.com/wenge-research/TableEval.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-qualitative-judge: automating error analysis in natural language generation</title>
<link>https://arxiv.org/abs/2506.09147</link>
<guid>https://arxiv.org/abs/2506.09147</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, natural language generation, evaluation approach, qualitative judge, NLG system improvements

Summary:<br /><br />Large language models are commonly used in evaluating natural language generation systems. This study introduces a new evaluation approach called LLM-as-a-qualitative-judge, which focuses on providing developers with structured reports of common issues in NLG system outputs. The approach involves analyzing individual instances for issues and clustering them using an intuitive algorithm. The proposed method was evaluated using annotations of issues in instances from 12 NLG datasets, showing that it correctly recognizes instance-specific issues in a majority of cases and can generate error type reports similar to those made by human annotators. The code and data for this approach are publicly available, offering a valuable tool for developers to improve their NLG systems. <div>
arXiv:2506.09147v1 Announce Type: new 
Abstract: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3 cases and is capable of producing error type reports resembling the reports composed by human annotators. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHRASED: Phrase Dictionary Biasing for Speech Translation</title>
<link>https://arxiv.org/abs/2506.09175</link>
<guid>https://arxiv.org/abs/2506.09175</guid>
<content:encoded><![CDATA[
<div> Keywords: phrases, speech translation, phrase dictionary biasing, transducer-based model, multimodal large language model

Summary: 
Phrases play a crucial role in understanding conversations, but their correct translation poses challenges in speech translation tasks due to their limited occurrences in training data. To address this issue, a new method called phrase dictionary biasing is introduced in this paper. This method leverages pairs of phrases mapping from the source language to the target language to improve translation accuracy. Experimental results demonstrate the effectiveness of phrase dictionary biasing, showing a 21% relative improvement over phrase list biasing in a streaming speech translation model. Furthermore, applying this method to multimodal large language models enables the utilization of external phrase information, leading to an 85% relative enhancement in phrase recall. The proposed approach enhances the performance of both transducer-based streaming speech translation models and multimodal large language models by leveraging phrase dictionaries. 

<br /><br />Summary: <div>
arXiv:2506.09175v1 Announce Type: new 
Abstract: Phrases are essential to understand the core concepts in conversations. However, due to their rare occurrence in training data, correct translation of phrases is challenging in speech translation tasks. In this paper, we propose a phrase dictionary biasing method to leverage pairs of phrases mapping from the source language to the target language. We apply the phrase dictionary biasing method to two types of widely adopted models, a transducer-based streaming speech translation model and a multimodal large language model. Experimental results show that the phrase dictionary biasing method outperforms phrase list biasing by 21% relatively for the streaming speech translation model. In addition, phrase dictionary biasing enables multimodal large language models to use external phrase information, achieving 85% relative improvement in phrase recall.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs</title>
<link>https://arxiv.org/abs/2506.09218</link>
<guid>https://arxiv.org/abs/2506.09218</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, phonotactic generalizations, convolutional neural networks, lexical learning, audio waveforms 

Summary: 
This study examines the ability of deep neural networks to capture phonotactic generalizations from lexical learning. It focuses on generative convolutional neural networks trained on raw audio waveforms of lexical items. The study investigates the impact of reducing the fully-connected layer bottleneck before training, from 1024 channels to 8. A novel technique is proposed to probe a model's lexically-independent generalizations by generating audio outputs without utilizing the fully-connected layer. The research demonstrates that the convolutional layers can effectively generalize phonetic dependencies beyond lexically-constrained configurations learned by the fully-connected layer. This finding suggests that deep neural networks can capture intricate phonotactic patterns in speech data, even without relying heavily on lexical constraints during training.<br /><br />Summary: <div>
arXiv:2506.09218v1 Announce Type: new 
Abstract: The ability of deep neural networks (DNNs) to represent phonotactic generalizations derived from lexical learning remains an open question. This study (1) investigates the lexically-invariant generalization capacity of generative convolutional neural networks (CNNs) trained on raw audio waveforms of lexical items and (2) explores the consequences of shrinking the fully-connected layer (FC) bottleneck from 1024 channels to 8 before training. Ultimately, a novel technique for probing a model's lexically-independent generalizations is proposed that works only under the narrow FC bottleneck: generating audio outputs by bypassing the FC and inputting randomized feature maps into the convolutional block. These outputs are equally biased by a phonotactic restriction in training as are outputs generated with the FC. This result shows that the convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations learned by the FC.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extrapolation by Association: Length Generalization Transfer in Transformers</title>
<link>https://arxiv.org/abs/2506.09251</link>
<guid>https://arxiv.org/abs/2506.09251</guid>
<content:encoded><![CDATA[
<div> task association, length generalization, transfer learning, transformer models, attention heads

Summary:<br />
- Transformer models display impressive generalization abilities, particularly in length generalization, where they can extrapolate from shorter to longer inputs. 
- Length generalization can be transferred across related tasks, allowing models trained with longer auxiliary tasks to generalize to unseen and longer inputs in other tasks.
- This transfer effect is observed in diverse algorithmic tasks such as arithmetic operations, string transformations, and maze navigation.
- Pretrained language models show similar transfer effects, indicating that pretraining equips models with reusable structures that aid in extrapolation in downstream tasks.
- The reuse of the same attention heads between tasks correlates with the transfer of length generalization, providing initial mechanistic evidence of how this transfer occurs. 

<br /><br />Summary: <div>
arXiv:2506.09251v1 Announce Type: new 
Abstract: Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of \textit{task association}. We find that length generalization can be \textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat</title>
<link>https://arxiv.org/abs/2506.09259</link>
<guid>https://arxiv.org/abs/2506.09259</guid>
<content:encoded><![CDATA[
<div> Keywords: online games, game chat, prosocial behavior, natural language processing, self-anchored attention model

Summary:<br /><br />Millions of players communicate through in-game chat in competitive online games. Detecting prosocial behaviors in chat is just as important as identifying toxic content. This study focuses on identifying and categorizing prosocial player behaviors in game chat. A Self-Anchored Attention Model (SAAM) was proposed, showing a 7.9% improvement compared to existing techniques. The model utilizes the entire training set as "anchors" to improve performance in low-resource settings. The methodology was applied to Call of Duty(R): Modern Warfare(R)II, demonstrating effectiveness in classifying prosocial behaviors. The research aims to encourage positive interactions in online gaming platforms by shifting focus from penalizing toxicity to promoting prosocial behaviors. 

Summary: <div>
arXiv:2506.09259v1 Announce Type: new 
Abstract: Millions of players engage daily in competitive online games, communicating through in-game chat. Prior research has focused on detecting relatively small volumes of toxic content using various Natural Language Processing (NLP) techniques for the purpose of moderation. However, recent studies emphasize the importance of detecting prosocial communication, which can be as crucial as identifying toxic interactions. Recognizing prosocial behavior allows for its analysis, rewarding, and promotion. Unlike toxicity, there are limited datasets, models, and resources for identifying prosocial behaviors in game-chat text. In this work, we employed unsupervised discovery combined with game domain expert collaboration to identify and categorize prosocial player behaviors from game chat. We further propose a novel Self-Anchored Attention Model (SAAM) which gives 7.9% improvement compared to the best existing technique. The approach utilizes the entire training set as "anchors" to help improve model performance under the scarcity of training data. This approach led to the development of the first automated system for classifying prosocial behaviors in in-game chats, particularly given the low-resource settings where large-scale labeled data is not available. Our methodology was applied to one of the most popular online gaming titles - Call of Duty(R): Modern Warfare(R)II, showcasing its effectiveness. This research is novel in applying NLP techniques to discover and classify prosocial behaviors in player in-game chat communication. It can help shift the focus of moderation from solely penalizing toxicity to actively encouraging positive interactions on online platforms.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models</title>
<link>https://arxiv.org/abs/2506.09277</link>
<guid>https://arxiv.org/abs/2506.09277</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, self-NLE, faithfulness, neural activity, reasoning

Summary:
This article introduces a novel framework for quantitatively measuring the faithfulness of Large Language Models (LLM) generated self Natural Language Explanations (self-NLE). The current methods for assessing self-NLE faithfulness do not consider the model's actual decision-making process, leading to unfaithful explanations. The proposed framework directly compares self-NLE with interpretations of the model's internal hidden states, providing deep insights into self-NLE faithfulness. By establishing a connection between self-NLE and model reasoning through neural activity analysis, this approach advances the understanding of self-NLE faithfulness. The framework is versatile and aims to improve the generation of more faithful self-NLE explanations. <br /><br />Summary: <div>
arXiv:2506.09277v1 Announce Type: new 
Abstract: Large Language Models (LLM) have demonstrated the capability of generating free text self Natural Language Explanation (self-NLE) to justify their answers. Despite their logical appearance, self-NLE do not necessarily reflect the LLM actual decision-making process, making such explanations unfaithful. While existing methods for measuring self-NLE faithfulness mostly rely on behavioral tests or computational block identification, none of them examines the neural activity underlying the model's reasoning. This work introduces a novel flexible framework for quantitatively measuring the faithfulness of LLM-generated self-NLE by directly comparing the latter with interpretations of the model's internal hidden states. The proposed framework is versatile and provides deep insights into self-NLE faithfulness by establishing a direct connection between self-NLE and model reasoning. This approach advances the understanding of self-NLE faithfulness and provides building blocks for generating more faithful self-NLE.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding</title>
<link>https://arxiv.org/abs/2506.09301</link>
<guid>https://arxiv.org/abs/2506.09301</guid>
<content:encoded><![CDATA[
<div> irony, hyperbole, understatement, Rational Speech Act framework, rhetorical strategy

Summary: 
The paper discusses the prevalence of figurative language in human communication, such as irony, hyperbole, and understatement, where literal and intended meanings differ. The Rational Speech Act (RSA) framework is commonly used in probabilistic pragmatics but struggles to account for figurative expressions. The new Rhetorical-Strategy-Aware RSA $(RSA)^2$ framework introduces a model that considers a speaker's rhetorical strategy in using figurative language, leading to better interpretations of non-literal utterances without needing to model a speaker's specific motivations. When combined with Large Language Models (LLMs), $(RSA)^2$ achieves top performance on a new irony interpretation dataset called PragMega+. <div>
arXiv:2506.09301v1 Announce Type: new 
Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in human communication, resulting in utterances where the literal and the intended meanings do not match. The Rational Speech Act (RSA) framework, which explicitly models speaker intentions, is the most widespread theory of probabilistic pragmatics, but existing implementations are either unable to account for figurative expressions or require modeling the implicit motivations for using figurative language (e.g., to express joy or annoyance) in a setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware RSA $(RSA)^2$ framework which models figurative language use by considering a speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables human-compatible interpretations of non-literal utterances without modeling a speaker's motivations for being non-literal. Combined with LLMs, it achieves state-of-the-art performance on the ironic split of PragMega+, a new irony interpretation dataset introduced in this study.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models</title>
<link>https://arxiv.org/abs/2506.09315</link>
<guid>https://arxiv.org/abs/2506.09315</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's dementia, language ability, language model, paired perplexity, data augmentation 

Summary: 
This study focuses on detecting Alzheimer's dementia by utilizing a large language model (LLM) known as Mistral-7B. By extending the paired perplexity approach, the accuracy of AD detection has been significantly improved compared to existing methods. The proposed approach demonstrates a clear and interpretable decision boundary, enhancing the transparency of the detection process. In contrast to other methods with opaque decision-making processes, the LLMs used in this study have learned unique language patterns of AD speakers, allowing for novel approaches to model interpretation. By comparing model-generated responses with human responses, the LLMs showcase their ability to capture the specific language characteristics of individuals with AD. These findings not only provide a more accurate and transparent method for AD detection but also highlight the potential for data augmentation and model interpretation in the context of neurodegenerative disorders. 

<br /><br />Summary: <div>
arXiv:2506.09315v1 Announce Type: new 
Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive decline that commonly impacts language ability. This work extends the paired perplexity approach to detecting AD by using a recent large language model (LLM), the instruction-following version of Mistral-7B. We improve accuracy by an average of 3.33% over the best current paired perplexity method and by 6.35% over the top-ranked method from the ADReSS 2020 challenge benchmark. Our further analysis demonstrates that the proposed approach can effectively detect AD with a clear and interpretable decision boundary in contrast to other methods that suffer from opaque decision-making processes. Finally, by prompting the fine-tuned LLMs and comparing the model-generated responses to human responses, we illustrate that the LLMs have learned the special language patterns of AD speakers, which opens up possibilities for novel methods of model interpretation and data augmentation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient and Effective Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2506.09329</link>
<guid>https://arxiv.org/abs/2506.09329</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, alignment, data collection, training, evaluation

Summary:
This thesis focuses on improving alignment of Large Language Models (LLMs) with human expectations through innovative methodologies in data collection, training, and evaluation. The approach includes the development of Lion, an adversarial distillation framework for refining training data and Web Reconstruction (WebR) for automated synthesis of instruction-tuning data. Training enhancements are achieved through frameworks like Learning to Edit (LTE) for efficient integration of new knowledge and Bridging and Modeling Correlations (BMC) for capturing token-level correlations in preference data. Evaluation is addressed with the introduction of FollowBench, a benchmark assessing LLMs' adherence to complex constraints. The results highlight weaknesses in current models and provide insights for future improvements.

<br /><br />Summary: <div>
arXiv:2506.09329v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current models' constraint adherence, offering insights for future improvements.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation</title>
<link>https://arxiv.org/abs/2506.09331</link>
<guid>https://arxiv.org/abs/2506.09331</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, theory of mind, multi-agent reinforcement learning, collaboration, human-AI interaction

Summary: 
Large Language Models (LLMs) have shown remarkable zero-shot and few-shot generalization abilities in natural language tasks. Despite being trained without explicit supervision on author intent, LLMs appear to understand the underlying meaning of textual interactions. This study explores whether LLMs possess a form of theory of mind, crucial for understanding others' intentions and effective collaboration. Through cooperative multi-agent reinforcement learning (MARL), LLMs can learn to collaborate with both artificial and human partners, mirroring human social reasoning. By enhancing artificial agents' ability to adapt and cooperate, this research aims to create hybrid human-AI systems for seamless collaboration, with profound implications for the future of human-artificial interaction.<br /><br />Summary: <div>
arXiv:2506.09331v1 Announce Type: new 
Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePO: Replay-Enhanced Policy Optimization</title>
<link>https://arxiv.org/abs/2506.09340</link>
<guid>https://arxiv.org/abs/2506.09340</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, language models, policy optimization, replay strategies, computational cost

Summary: 
Reinforcement learning plays a crucial role in optimizing large language models (LLMs). The new Replay-Enhanced Policy Optimization (RePO) method employs diverse replay strategies to access off-policy samples from a replay buffer, enhancing policy optimization for LLMs. Compared to Group Relative Policy Optimization (GRPO), RePO achieves significant performance gains in several mathematical reasoning benchmarks. It increases computational costs slightly but significantly raises the number of effective optimization steps. The experiment results show absolute average performance gains of 18.4 points for Qwen2.5-Math-1.5B and 4.1 points for Qwen3-1.7B with both on-policy and off-policy sample numbers set to 8. RePO's code repository is accessible at https://github.com/SihengLi99/RePO. 

<br /><br />Summary: <div>
arXiv:2506.09340v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is vital for optimizing large language models (LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages using multiple on-policy outputs per prompt, leading to high computational costs and low data efficiency. To address this, we introduce Replay-Enhanced Policy Optimization (RePO), which leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, allowing policy optimization based on a broader and more diverse set of samples for each prompt. Experiments on five LLMs across seven mathematical reasoning benchmarks demonstrate that RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further analysis indicates that RePO increases computational cost by $15\%$ while raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B, with both on-policy and off-policy sample numbers set to $8$. The repository can be accessed at https://github.com/SihengLi99/RePO.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Multi-Head Attention for Small Language Models</title>
<link>https://arxiv.org/abs/2506.09342</link>
<guid>https://arxiv.org/abs/2506.09342</guid>
<content:encoded><![CDATA[
<div> latent multi-head attention, small language models, efficiency-quality trade-offs, rotary positional embeddings, GPT models
Summary:
Latent multi-head attention (MLA) is studied in small language models, showing efficiency-quality trade-offs. GPT models trained on synthetic stories compare standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). MLA+RoPE with half-rank latent dimensions reduces memory by 45% with only a 0.3% increase in validation loss, making it a memory-efficient choice. The inclusion of rotary positional embeddings is essential for MLA in small models, outperforming vanilla attention. MLA with reduced rank achieves a 1.4 times speedup on NVIDIA A100 GPUs while maintaining memory savings. GPT-4 evaluations show our model achieves the highest quality scores in grammar, creativity, and consistency metrics. Code and models will be available upon acceptance. 
<br /><br />Summary: <div>
arXiv:2506.09342v1 Announce Type: new 
Abstract: We present the first comprehensive study of latent multi-head attention (MLA) for small language models, revealing interesting efficiency-quality trade-offs. Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory reduction while incurring only a 0.3% increase in validation loss (essentially matching MHA quality)- a Pareto improvement for memory constrained deployment. We further show that RoPE is crucial for MLA in small models: without it, MLA underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by 2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2 achieves a 1.4 times speedup over full-rank MLA while maintaining the memory savings. GPT-4 evaluations corroborate perplexity results, with ours achieving the highest quality scores (7.4/10) across grammar, creativity, and consistency metrics. Code and models will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment</title>
<link>https://arxiv.org/abs/2506.09349</link>
<guid>https://arxiv.org/abs/2506.09349</guid>
<content:encoded><![CDATA[
<div> Keywords: end-to-end speech generation, large language models, parallel speech-text modeling, contrastive cross-modal alignment, Spoken Question Answering

Summary:
OmniDRCA is a new parallel speech-text foundation model that integrates speech and text representations using joint autoregressive modeling. It features dual-resolution speech representations and contrastive cross-modal alignment to enhance audio comprehension. The model achieves state-of-the-art performance on Spoken Question Answering benchmarks, surpassing other parallel joint speech-text modeling approaches. It also demonstrates competitive performance compared to interleaved models. The study explores the potential application of OmniDRCA in full-duplex conversational scenarios, showing promise for future development in interactive speech and text generation. <div>
arXiv:2506.09349v1 Announce Type: new 
Abstract: Recent studies on end-to-end speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling, featuring dual-resolution speech representations and contrastive cross-modal alignment. Our approach processes speech and text representations in parallel while enhancing audio comprehension through contrastive alignment. Experimental results on Spoken Question Answering benchmarks demonstrate that OmniDRCA establishes new state-of-the-art (SOTA) performance among parallel joint speech-text modeling based foundation models, and achieves competitive performance compared to interleaved models. Additionally, we explore the potential of extending the framework to full-duplex conversational scenarios.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2506.09351</link>
<guid>https://arxiv.org/abs/2506.09351</guid>
<content:encoded><![CDATA[
<div> pruning, reconstruction, Mixture-of-Experts, training efficiency, diversity-enhanced<br />
Summary:<br />
The paper introduces a new method called DIVE for reconstructing Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architecture. DIVE leverages the observation that LLMs exhibit diversity when pruned on different datasets. The method includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, DIVE focuses on reconstructing the feed-forward network (FFN) module and retraining the model on routers, experts, and normalization modules. Experimental results demonstrate that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods while maintaining the same number of activated parameters. Overall, DIVE provides a cost-efficient approach to reconstructing LLMs with the MoE architecture, improving training efficiency and performance. <br /> <div>
arXiv:2506.09351v1 Announce Type: new 
Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture achieve high cost-efficiency by selectively activating a subset of the parameters. Despite the inference efficiency of MoE LLMs, the training of extensive experts from scratch incurs substantial overhead, whereas reconstructing a dense LLM into an MoE LLM significantly reduces the training budget. However, existing reconstruction methods often overlook the diversity among experts, leading to potential redundancy. In this paper, we come up with the observation that a specific LLM exhibits notable diversity after being pruned on different calibration datasets, based on which we present a Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, the reconstruction includes pruning and reassembly of the feed-forward network (FFN) module. After reconstruction, we efficiently retrain the model on routers, experts and normalization modules. We implement DIVE on Llama-style LLMs with open-source training corpora. Experiments show that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods with the same number of activated parameters.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL</title>
<link>https://arxiv.org/abs/2506.09359</link>
<guid>https://arxiv.org/abs/2506.09359</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Text-to-SQL, semantic equivalence, weak semantic equivalence, evaluation <br />
<br />
Summary: 
This paper investigates the use of Large Language Models (LLMs) in assessing semantic and weak semantic equivalence in Text-to-SQL systems. The authors explore common patterns of SQL equivalence and inequivalence, highlighting the challenges faced in evaluating the generated SQL. The rise of LLMs has significantly improved NL2SQL systems, but assessing the semantic equivalence of the SQL outputs remains a complex task, especially when dealing with ambiguous user queries and multiple valid interpretations. By leveraging LLMs, the study aims to enhance the evaluation process and provide insights into the practical implications of weak semantic equivalence in SQL generation. This research contributes to advancing the field of NL2SQL systems and sheds light on the complexities of evaluating the semantic equivalence of generated SQL statements. <div>
arXiv:2506.09359v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has significantly advanced Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of generated SQL remains a challenge, especially given ambiguous user queries and multiple valid SQL interpretations. This paper explores using LLMs to assess both semantic and a more practical "weak" semantic equivalence. We analyze common patterns of SQL equivalence and inequivalence, discuss challenges in LLM-based evaluation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content</title>
<link>https://arxiv.org/abs/2506.09367</link>
<guid>https://arxiv.org/abs/2506.09367</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, educational content, curriculum standards, STEM education, student engagement
Summary: 
COGENT is a framework that aims to generate grade-appropriate educational content by incorporating science concepts, core ideas, and learning objectives while controlling readability through length, vocabulary, and sentence complexity. The framework adopts a "wonder-based" approach to enhance student engagement and interest in STEM subjects. Evaluation through both automated and human expert analysis shows that COGENT consistently produces educational passages that are comparable or superior to those written by humans. This work addresses the challenges in aligning AI-generated content with curriculum standards and maintaining grade-appropriate reading levels, particularly in STEM education. By successfully generating high-quality educational resources, COGENT establishes a scalable approach for creating adaptive learning materials. 
<br /><br />Summary: <div>
arXiv:2506.09367v1 Announce Type: new 
Abstract: While Generative AI has demonstrated strong potential and versatility in content generation, its application to educational contexts presents several challenges. Models often fail to align with curriculum standards and maintain grade-appropriate reading levels consistently. Furthermore, STEM education poses additional challenges in balancing scientific explanations with everyday language when introducing complex and abstract ideas and phenomena to younger students. In this work, we propose COGENT, a curriculum-oriented framework for generating grade-appropriate educational content. We incorporate three curriculum components (science concepts, core ideas, and learning objectives), control readability through length, vocabulary, and sentence complexity, and adopt a ``wonder-based'' approach to increase student engagement and interest. We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human expert analysis. Experimental results show that COGENT consistently produces grade-appropriate passages that are comparable or superior to human references. Our work establishes a viable approach for scaling adaptive and high-quality learning resources.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoLMbo: Speaker Language Model for Descriptive Profiling</title>
<link>https://arxiv.org/abs/2506.09375</link>
<guid>https://arxiv.org/abs/2506.09375</guid>
<content:encoded><![CDATA[
<div> Keywords: Speaker recognition, Speaker Language Model, CoLMbo, demographic attributes, embeddings

Summary: 
CoLMbo is a Speaker Language Model that enhances speaker recognition systems by integrating a speaker encoder with prompt-based conditioning. It can generate detailed speaker characteristics and provide context-rich descriptions, including demographic attributes like dialect, gender, and age in a structured manner. By using user-defined prompts, CoLMbo can adapt dynamically to new speaker characteristics and generate customized descriptions that capture regional dialect variations and age-related traits. This innovative approach not only improves traditional speaker profiling but also excels in zero-shot scenarios across diverse datasets. CoLMbo represents a significant advancement in the field of speaker recognition by going beyond simple classification tasks to provide detailed speaker information. 

<br /><br />Summary: <div>
arXiv:2506.09375v1 Announce Type: new 
Abstract: Speaker recognition systems are often limited to classification tasks and struggle to generate detailed speaker characteristics or provide context-rich descriptions. These models primarily extract embeddings for speaker identification but fail to capture demographic attributes such as dialect, gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker Language Model (SLM) that addresses these limitations by integrating a speaker encoder with prompt-based conditioning. This allows for the creation of detailed captions based on speaker embeddings. CoLMbo utilizes user-defined prompts to adapt dynamically to new speaker characteristics and provides customized descriptions, including regional dialect variations and age-related traits. This innovative approach not only enhances traditional speaker profiling but also excels in zero-shot scenarios across diverse datasets, marking a significant advancement in the field of speaker recognition.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024</title>
<link>https://arxiv.org/abs/2506.09381</link>
<guid>https://arxiv.org/abs/2506.09381</guid>
<content:encoded><![CDATA[
<div> Machine Learning, News Headlines, Quality, DistilBERT, NLP

Summary:
- The study aimed to differentiate between low and high-quality news headlines/links using machine learning models on a dataset of 57,544,214 worldwide news website links/headings.
- Twelve machine learning models were evaluated, with traditional ensemble methods like bagging classifier showing strong performance in distinguishing between the two quality categories.
- Fine-tuned DistilBERT achieved the highest accuracy in the task but required more training time compared to traditional classifiers.
- The results indicate that both NLP features with traditional classifiers and deep learning models can effectively differentiate perceived news headline/link quality.
- There was a trade-off between predictive performance and training time, with DistilBERT offering higher accuracy but requiring more training time than traditional ensemble methods. 

<br /><br />Summary: <div>
arXiv:2506.09381v1 Announce Type: new 
Abstract: The proliferation of online news enables potential widespread publication of perceived low-quality news headlines/links. As a result, we investigated whether it was possible to automatically distinguish perceived lower-quality news headlines/links from perceived higher-quality headlines/links. We evaluated twelve machine learning models on a binary, balanced dataset of 57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per class) with 115 extracted linguistic features. Binary labels for each text were derived from scores based on expert consensus regarding the respective news domain quality. Traditional ensemble methods, particularly the bagging classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20 train/test split) but required more training time. The results suggest that both NLP features with traditional classifiers and deep learning models can effectively differentiate perceived news headline/link quality, with some trade-off between predictive performance and train time.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing human and LLM politeness strategies in free production</title>
<link>https://arxiv.org/abs/2506.09391</link>
<guid>https://arxiv.org/abs/2506.09391</guid>
<content:encoded><![CDATA[
<div> Keywords: polite speech, language models, computational pragmatics, social goals, linguistic strategies <br />
<br />
Summary: <br />
The article explores how large language models (LLMs) handle polite speech, analyzing their use of linguistic strategies to balance informational and social goals. Human and LLM responses were compared in various tasks, revealing that larger models can replicate key preferences in computational pragmatics. Surprisingly, human evaluators preferred LLM-generated responses in open-ended contexts. However, linguistic analysis showed that LLMs tend to rely more on negative politeness strategies, even in positive contexts, potentially leading to misinterpretations. While modern LLMs show proficiency in politeness strategies, these subtle differences highlight the need to address pragmatic alignment in AI systems.  <div>
arXiv:2506.09391v1 Announce Type: new 
Abstract: Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings</title>
<link>https://arxiv.org/abs/2506.09393</link>
<guid>https://arxiv.org/abs/2506.09393</guid>
<content:encoded><![CDATA[
<div> Knowledge tracing, KT, student performance prediction, low-resource settings, hierarchical knowledge concept <br />
Summary: <br />
The article introduces Knowledge-Tree-based Knowledge Tracing (KT$^2$), a probabilistic framework for estimating student knowledge and predicting performance in low-resource, online settings. By utilizing a tree-structured hierarchy of knowledge concepts, KT$^2$ models student understanding using a Hidden Markov Tree Model. Through an EM algorithm, student mastery is estimated, with personalized predictions supported via an incremental update mechanism. Experimental results demonstrate that KT$^2$ outperforms existing baselines in realistic classroom settings with limited data. <div>
arXiv:2506.09393v1 Announce Type: new 
Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state and predict their performance on new exercises based on performance history. Many realistic classroom settings for KT are typically low-resource in data and require online updates as students' exercise history grows, which creates significant challenges for existing KT approaches. To restore strong performance under low-resource conditions, we revisit the hierarchical knowledge concept (KC) information, which is typically available in many classroom settings and can provide strong prior when data are sparse. We therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a probabilistic KT framework that models student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model. KT$^2$ estimates student mastery via an EM algorithm and supports personalized prediction through an incremental update mechanism as new responses arrive. Our experiments show that KT$^2$ consistently outperforms strong baselines in realistic online, low-resource settings.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models</title>
<link>https://arxiv.org/abs/2506.09408</link>
<guid>https://arxiv.org/abs/2506.09408</guid>
<content:encoded><![CDATA[
<div> Token Constraint Decoding, Alignment, Robustness, Noisy settings, Inference-time<br />
Summary: <br />
The paper introduces Token Constraint Decoding (TCD) as an algorithm to enhance robustness in Large Language Models (LLMs) for multiple-choice question answering tasks. By enforcing alignment between token-level predictions, TCD improves performance in noisy settings. When combined with prompt engineering (PE) fixes, TCD shows significant performance gains, particularly for weaker models like Gemma3 1B. Penalty sweep analyses demonstrate that TCD also helps regulate overconfident outputs and different models may require distinct penalty schedules for resilience. The results suggest that TCD is a practical, model-agnostic approach for enhancing reasoning stability in real-world applications, allowing for more reliable deployment of LLMs in safety-critical or user-facing scenarios. <br /> <div>
arXiv:2506.09408v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance on multiple-choice question answering (MCQA) benchmarks, yet they remain highly vulnerable to minor input perturbations. In this paper, we introduce and evaluate Token Constraint Decoding (TCD). This simple yet effective inference-time algorithm enforces alignment between token-level predictions to enhance robustness in noisy settings. Through extensive experiments on CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired with prompt engineering (PE) fixes, significantly restores performance degraded by input noise, yielding up to +39\% absolute gains for weaker models like Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly regularizes overconfident outputs, with different models requiring distinct penalty schedules to maximize resilience. Our findings establish TCD as a practical, model-agnostic approach for improving reasoning stability under real-world imperfections and pave the way for more reliable deployment of LLMs in safety-critical or user-facing applications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2506.09414</link>
<guid>https://arxiv.org/abs/2506.09414</guid>
<content:encoded><![CDATA[
<div> semantic parsing, knowledge graph question answering, data augmentation, multi-hop reasoning, prompt-guided generative framework

Summary: 
- PGDA-KGQA is a novel framework for Knowledge Graph Question Answering (KGQA) that addresses the limitations of existing methods.
- It integrates large language models (LLMs) with data augmentation strategies to generate diverse (question, logical form) pairs for training.
- The framework enhances data diversity by generating single-hop pseudo questions, applying semantic-preserving question rewriting, and creating multi-hop questions through answer-guided reverse path exploration.
- By utilizing augmented data, PGDA-KGQA improves the accuracy of logical form generation, leading to enhanced answer retrieval performance.
- Experimental results demonstrate that PGDA-KGQA outperforms state-of-the-art methods on standard KGQA datasets, achieving significant improvements in F1 score, Hits@1, and Accuracy. 

<br /><br />Summary: <div>
arXiv:2506.09414v1 Announce Type: new 
Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings</title>
<link>https://arxiv.org/abs/2506.09424</link>
<guid>https://arxiv.org/abs/2506.09424</guid>
<content:encoded><![CDATA[
<div> LLMs, LMMs, deception detection, experimental setups, textual deception detection <br />
Summary: 
The study evaluates the automated deception detection abilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) across various domains. The performance of open-source and commercial LLMs is assessed on different datasets, with fine-tuned LLMs achieving top performance in textual deception detection tasks. However, LMMs struggle to effectively utilize cross-modal cues. The study explores the impact of auxiliary features like non-verbal gestures and video summaries, as well as different prompting strategies such as direct label generation and chain-of-thought reasoning. The results provide insights into how LLMs process and interpret deceptive cues across modalities, showcasing their potential and limitations in real-world deception detection applications.<br /><br /> <div>
arXiv:2506.09424v1 Announce Type: new 
Abstract: Detecting deception in an increasingly digital world is both a critical and challenging task. In this study, we present a comprehensive evaluation of the automated deception detection capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) across diverse domains. We assess the performance of both open-source and commercial LLMs on three distinct datasets: real life trial interviews (RLTD), instructed deception in interpersonal scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the effectiveness of different experimental setups for deception detection, including zero-shot and few-shot approaches with random or similarity-based in-context example selection. Our results show that fine-tuned LLMs achieve state-of-the-art performance on textual deception detection tasks, while LMMs struggle to fully leverage cross-modal cues. Additionally, we analyze the impact of auxiliary features, such as non-verbal gestures and video summaries, and examine the effectiveness of different prompting strategies, including direct label generation and chain-of-thought reasoning. Our findings provide key insights into how LLMs process and interpret deceptive cues across modalities, highlighting their potential and limitations in real-world deception detection applications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting</title>
<link>https://arxiv.org/abs/2506.09428</link>
<guid>https://arxiv.org/abs/2506.09428</guid>
<content:encoded><![CDATA[
<div> Supervised Fine-Tuning, large language models, catastrophic forgetting, generalization capabilities, task-specific performance 
Summary: 
Supervised Fine-Tuning (SFT) is effective for enhancing large language models' adaptability to specific tasks but can lead to a loss of general capabilities. The inaccessibility of original pre-training data can exacerbate catastrophic forgetting when implementing SFT on open-sourced models. To address this challenge, a novel SFT method is proposed that reconstructs the likely SFT instruction distribution and uses a multi-model screening process to select optimal data for fine-tuning. The approach effectively reduces the risk of catastrophic forgetting without access to original SFT data. Experimental results show that this method maintains generalization capabilities in general domains while improving task-specific performance.<br /><br />Summary: <div>
arXiv:2506.09428v1 Announce Type: new 
Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)' instruction-following capabilities and domain-specific task adaptability, often diminishes their general capabilities. Moreover, due to the inaccessibility of original pre-training data, catastrophic forgetting tends to be exacerbated when third-party practitioners implement SFT on open-sourced models. To address this challenge, we propose a novel, more cost-effective SFT method which could effectively reduce the risk of catastrophic forgetting without access to original SFT data. Our approach begins by reconstructing the likely SFT instruction distribution of the base model, followed by a multi-model screening process to select optimal data, which is then mixed with new data for SFT. Experimental results demonstrate that our method preserves generalization capabilities in general domains while improving task-specific performance.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture</title>
<link>https://arxiv.org/abs/2506.09440</link>
<guid>https://arxiv.org/abs/2506.09440</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative language models, Russian language, GigaChat, NLP research, Open-source.

Summary: 
Generative large language models (LLMs) are crucial for modern NLP across different languages, but the development of Russian-specific models has been limited due to resource constraints. The paper introduces the GigaChat family of Russian LLMs, available in various sizes and includes base models and instruction-tuned versions. It provides a detailed overview of the model architecture, pre-training process, and experimental results. Performance evaluation on Russian and English benchmarks and comparison with multilingual models are conducted. The top-performing GigaChat models are accessible via an API, Telegram bot, and Web interface. Three open GigaChat models have been released in open-source to support NLP research and industrial solutions for the Russian language.

<br /><br />Summary: <div>
arXiv:2506.09440v1 Announce Type: new 
Abstract: Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (https://huggingface.co/ai-sage), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs</title>
<link>https://arxiv.org/abs/2506.09450</link>
<guid>https://arxiv.org/abs/2506.09450</guid>
<content:encoded><![CDATA[
<div> Keywords: Theory of Mind, large language models, UniToMBench, evaluation, perspective-taking

Summary: 
The paper introduces UniToMBench, a benchmark for assessing Theory of Mind capabilities in large language models (LLMs). By combining the strengths of existing benchmarks, UniToMBench aims to stimulate social cognition in LLMs through multi-interaction task designs and evolving story scenarios. The benchmark includes a dataset of hand-written scenarios and utilizes diverse evaluation metrics. Evaluation of LLMs like GPT-4o and GPT-4o Mini on UniToMBench shows high accuracy in emotional and belief-related tasks but variability in knowledge-based tasks. The results emphasize the strengths and limitations of current LLMs in Theory of Mind tasks. UniToMBench is positioned as a valuable tool for future development in this area. The code for UniToMBench is available on GitHub for public use.
<br /><br />Summary: <div>
arXiv:2506.09450v1 Announce Type: new 
Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself and others, remains a challenging area for large language models (LLMs), which often fail to predict human mental states accurately. In this paper, we introduce UniToMBench, a unified benchmark that integrates the strengths of SimToM and TOMBENCH to systematically improve and assess ToM capabilities in LLMs by integrating multi-interaction task designs and evolving story scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios, UniToMBench combines perspective-taking techniques with diverse evaluation metrics to better stimulate social cognition in LLMs. Through evaluation, we observe that while models like GPT-4o and GPT-4o Mini show consistently high accuracy in tasks involving emotional and belief-related scenarios, with results usually above 80%, there is significant variability in their performance across knowledge-based tasks. These results highlight both the strengths and limitations of current LLMs in ToM-related tasks, underscoring the value of UniToMBench as a comprehensive tool for future development. Our code is publicly available here: https://github.com/Shamant/unifiedtombenchmark.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms</title>
<link>https://arxiv.org/abs/2506.09457</link>
<guid>https://arxiv.org/abs/2506.09457</guid>
<content:encoded><![CDATA[
<div> Keywords: Direct Alignment Algorithms, Large Language Models, Reward-generation gap, Prefix-Oriented Equal-length Training, AlpacaEval 2 

Summary:
Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO) are efficient alternatives to Reinforcement Learning from Human Feedback for aligning Large Language Models with human preferences. However, DAAs face a limitation known as the "reward-generation gap," where optimization objectives during training do not align with generation performance during inference. This gap is partly due to a mismatch in the importance of prefix tokens in the generation process and their reflection in DAA reward functions. To address this, a novel approach called Prefix-Oriented Equal-length Training (POET) is introduced, which truncates preferred and dispreferred responses to equal lengths, enhancing attention to prefix tokens. Experimental results show that POET improves standard DPO and SimPO implementations, leading to performance gains in AlpacaEval 2 and across downstream tasks. This research emphasizes the importance of aligning reward optimization with generation performance in Direct Alignment Algorithms.<br /><br />Summary: <div>
arXiv:2506.09457v1 Announce Type: new 
Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the "reward-generation gap" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with POET, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all positions, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers</title>
<link>https://arxiv.org/abs/2506.09495</link>
<guid>https://arxiv.org/abs/2506.09495</guid>
<content:encoded><![CDATA[
<div> Keywords: Suicide, YouTube, Digital footprints, Topic modeling, Social media<br />
Summary: <br />
- The study explores suicidal behaviors on YouTube by analyzing videos of individuals who attempted suicide.
- Three research approaches were employed: bottom-up topic modeling, hybrid expert-driven review, and top-down psychological assessment.
- Bottom-up analysis revealed five suicide-related topics on YouTube, with changes in Mental Health Struggles and YouTube Engagement over time.
- Hybrid approach combined computational and expert insights, identifying 19 suicide-related topics but no significant temporal effects beyond bottom-up findings.
- Top-down analysis compared the motivations of individuals who attempted suicide before and during video uploads, highlighting differences in sharing experiences for helping others or personal recovery.
- By integrating these approaches, the study provides a comprehensive understanding of suicidal behaviors on social media, bridging digital behavior and clinical insights.<br />  
Summary: <div>
arXiv:2506.09495v1 Announce Type: new 
Abstract: Suicide remains a leading cause of death in Western countries, underscoring the need for new research approaches. As social media becomes central to daily life, digital footprints offer valuable insight into suicidal behavior. Focusing on individuals who attempted suicide while uploading videos to their channels, we investigate: How do suicidal behaviors manifest on YouTube, and how do they differ from expert knowledge? We applied complementary approaches: computational bottom-up, hybrid, and expert-driven top-down, on a novel longitudinal dataset of 181 YouTube channels from individuals with life-threatening attempts, alongside 134 control channels. In the bottom-up approach, we applied LLM-based topic modeling to identify behavioral indicators. Of 166 topics, five were associated with suicide-attempt, with two also showing temporal attempt-related changes ($p<.01$) - Mental Health Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach, a clinical expert reviewed LLM-derived topics and flagged 19 as suicide-related. However, none showed significant attempt-related temporal effects beyond those identified bottom-up. Notably, YouTube Engagement, a platform-specific indicator, was not flagged by the expert, underscoring the value of bottom-up discovery. In the top-down approach, psychological assessment of suicide attempt narratives revealed that the only significant difference between individuals who attempted before and those attempted during their upload period was the motivation to share this experience: the former aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these approaches, we offer a nuanced understanding of suicidality, bridging digital behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning</title>
<link>https://arxiv.org/abs/2506.09501</link>
<guid>https://arxiv.org/abs/2506.09501</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reproducibility, numerical precision, inference, floating-point arithmetic

Summary: 
Large Language Models (LLMs) have become essential in various fields, but their performance reproducibility is fragile due to system configuration changes affecting responses. Variability in LLM accuracy and response length can result from differences in GPU count, type, and evaluation batch size. Floating-point arithmetic's non-associative nature under limited numerical precision is identified as the root cause of this variability. The neglect of floating-point precision in evaluation practices is highlighted, leading to the development of LayerCast, a lightweight inference pipeline balancing memory efficiency and numerical stability. Controlled experiments across hardware, software, and precision settings reveal the impact of numerical precision on LLM inference reproducibility. The study underscores the importance of considering floating-point precision in LLM evaluation and provides insights for mitigating variability in model outputs. The code for LayerCast is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2506.09501v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2506.09507</link>
<guid>https://arxiv.org/abs/2506.09507</guid>
<content:encoded><![CDATA[
<div> Transformers, State Space Models, Positional Encoding, Hybrid Architecture, Performance Improvement
Summary:
- The study addresses the challenge of integrating Transformers and State Space Models due to differences in positional encoding mechanisms.
- A unified rotary position embedding (\ourRoPE) methodology is proposed to create a consistent positional encoding framework for both architectures.
- The hybrid architecture, termed \model, integrates Transformer and SSM layers under the unified positional encoding scheme. 
- \model demonstrates faster training and inference speeds compared to standard Transformers, with a 4% accuracy improvement on language modeling benchmarks.
- \model scales more effectively, with the 1.3B version gaining 7.22% in average accuracy over the 320M version, outperforming equivalent Transformers and SSMs. 
<br /><br />Summary: <div>
arXiv:2506.09507v1 Announce Type: new 
Abstract: Transformers exhibit proficiency in capturing long-range dependencies, whereas State Space Models (SSMs) facilitate linear-time sequence modeling. Notwithstanding their synergistic potential, the integration of these architectures presents a significant challenge, primarily attributable to a fundamental incongruity in their respective positional encoding mechanisms: Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs leverage implicit positional representations via convolutions. This divergence often precipitates discontinuities and suboptimal performance. To address this impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE}) methodology, thereby establishing a consistent positional encoding framework for both self-attention and state-space components. Using this \ourRoPE, we introduce \textbf{\model}, a hybrid architecture that coherently integrates the Transformer and SSM layers under this unified positional encoding scheme. At a 4K sequence length, \model exhibits training and inference speeds that are \textbf{42.3\% and 29.5\% faster}, respectively, relative to standard Transformer models. It also delivers higher accuracy: under comparable settings, it surpasses a Transformer baseline by over 4\% on language modeling benchmarks. \model furthermore scales more effectively: \model-1.3B gains \textbf{7.22\%} in average accuracy over its 320M version (versus about 6\% gains for equivalent Transformers or SSMs). Our results show that unified positional encoding resolves positional incompatibility in hybrid models, enabling efficient, high-performance long-context modeling.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.09513</link>
<guid>https://arxiv.org/abs/2506.09513</guid>
<content:encoded><![CDATA[
<div> Keywords: medical reasoning, large language models, ReasonMed, Chain-of-Thought reasoning, benchmark

Summary: 
ReasonMed introduces a new dataset for medical reasoning, improving on previous models by enhancing reasoning paths through a multi-agent verification and refinement process. The dataset comprises 370k examples distilled from 1.7 million initial reasoning paths, focusing on combining detailed Chain-of-Thought reasoning with concise answer summaries. This approach leads to the development of ReasonMed-7B, which outperforms existing models, including LLaMA3.1-70B on PubMedQA. The study highlights the importance of incorporating both detailed reasoning processes and succinct answer summaries in training medical reasoning models, demonstrating the effectiveness of this strategy in achieving state-of-the-art performance. Excitingly, ReasonMed sets a new benchmark for sub-10B models in medical question answering, showcasing the potential for reasoning-based large language models in knowledge-intensive domains like healthcare. 

<br /><br />Summary: <div>
arXiv:2506.09513v1 Announce Type: new 
Abstract: Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a \textit{multi-agent verification and refinement process}, where we design an \textit{Error Refiner} to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.09542</link>
<guid>https://arxiv.org/abs/2506.09542</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, KG-Infused RAG, spreading activation, preference learning, QA benchmarks

Summary:
KG-Infused RAG is a framework that enhances factual accuracy by integrating knowledge graphs (KGs) into retrieval-augmented generation systems. It implements spreading activation, a cognitive process that promotes concept association and inference. This framework retrieves KG facts, expands queries, and combines structured knowledge with corpus passages, enabling multi-source, interpretable retrieval grounded in semantic structure. By incorporating preference learning on key stages in the pipeline, KG-Infused RAG consistently outperforms vanilla RAG on five QA benchmarks (3.8% to 13.8% improvement). Furthermore, when integrated into Self-RAG, KG-Infused RAG delivers additional performance gains, showcasing its effectiveness and versatility as a plug-and-play enhancement module for corpus-based RAG methods.

<br /><br />Summary: 
1. KG-Infused RAG integrates knowledge graphs for improved factual accuracy. 
2. It employs spreading activation to facilitate concept association and inference. 
3. The framework combines structured knowledge with corpus passages for multi-source retrieval. 
4. Preference learning enhances performance on key stages in the pipeline. 
5. KG-Infused RAG outperforms vanilla RAG on QA benchmarks and serves as a versatile enhancement module for corpus-based RAG methods. <div>
arXiv:2506.09542v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding responses in external knowledge. However, existing methods typically rely on a single source, either unstructured text or structured knowledge. Moreover, they lack cognitively inspired mechanisms for activating relevant knowledge. To address these issues, we propose KG-Infused RAG, a framework that integrates KGs into RAG systems to implement spreading activation, a cognitive process that enables concept association and inference. KG-Infused RAG retrieves KG facts, expands the query accordingly, and enhances generation by combining corpus passages with structured facts, enabling interpretable, multi-source retrieval grounded in semantic structure. We further improve KG-Infused RAG via preference learning on sampled key stages in the pipeline. Experiments on five QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by 3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG brings further performance gains, demonstrating its effectiveness and versatility as a plug-and-play enhancement module for corpus-based RAG methods.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions</title>
<link>https://arxiv.org/abs/2506.09556</link>
<guid>https://arxiv.org/abs/2506.09556</guid>
<content:encoded><![CDATA[
<div> Keywords: SER, multimodal framework, class imbalance, emotion ambiguity, DeepSER

Summary:
MEDUSA is a novel multimodal framework designed to improve Speech Emotion Recognition (SER) by effectively addressing challenges such as class imbalance and emotion ambiguity. The framework includes a four-stage training pipeline that leverages an ensemble of classifiers using DeepSER, a deep cross-modal transformer fusion mechanism. Manifold MixUp is utilized for regularization, and a trainable meta-classifier is optimized in the final stages to combine ensemble predictions. Human annotation scores are incorporated as soft targets, and balanced data sampling and multitask learning are employed in the training approach. MEDUSA achieved first place in Task 1: Categorical Emotion Recognition in the Interspeech 2025 Challenge, demonstrating its effectiveness in handling SER in naturalistic conditions.<br /><br />Summary: MEDUSA is a state-of-the-art multimodal framework that effectively addresses class imbalance and emotion ambiguity in Speech Emotion Recognition. By utilizing DeepSER and a four-stage training pipeline, incorporating human annotation scores as soft targets, and employing balanced data sampling and multitask learning, MEDUSA outperformed competitors in the Interspeech 2025 Challenge, showcasing its effectiveness in real-world emotion recognition tasks. <div>
arXiv:2506.09556v1 Announce Type: new 
Abstract: SER is a challenging task due to the subjective nature of human emotions and their uneven representation under naturalistic conditions. We propose MEDUSA, a multimodal framework with a four-stage training pipeline, which effectively handles class imbalance and emotion ambiguity. The first two stages train an ensemble of classifiers that utilize DeepSER, a novel extension of a deep cross-modal transformer fusion mechanism from pretrained self-supervised acoustic and linguistic representations. Manifold MixUp is employed for further regularization. The last two stages optimize a trainable meta-classifier that combines the ensemble predictions. Our training approach incorporates human annotation scores as soft targets, coupled with balanced data sampling and multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic Conditions Challenge.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Bias in English-to-Greek Machine Translation</title>
<link>https://arxiv.org/abs/2506.09558</link>
<guid>https://arxiv.org/abs/2506.09558</guid>
<content:encoded><![CDATA[
<div> Gender bias, Machine Translation, Google Translate, DeepL, Occupational Stereotyping<br />
Summary:<br />
This study investigates gender bias in English-to-Greek translations by Google Translate and DeepL. Three aspects of bias are addressed: male bias, occupational stereotypes, and errors in anti-stereotypical translations. The study also explores the use of GPT-4o as a bias mitigation tool. The GendEL dataset is introduced to analyze gender bias in translations. Both MT systems show persistent gender bias, especially in cases where gender is unspecified. DeepL performs better in feminine gender-unambiguous sentences, while GPT-4o shows promise in providing gender-neutral alternatives. However, residual biases remain evident in the translations. The study highlights the need for gender-inclusive and neutral translations in machine translation systems. <br /><br />Summary: <div>
arXiv:2506.09558v1 Announce Type: new 
Abstract: As the demand for inclusive language increases, concern has grown over the susceptibility of machine translation (MT) systems to reinforce gender stereotypes. This study investigates gender bias in two commercial MT systems, Google Translate and DeepL, focusing on the understudied English-to-Greek language pair. We address three aspects of gender bias: i) male bias, ii) occupational stereotyping, and iii) errors in anti-stereotypical translations. Additionally, we explore the potential of prompted GPT-4o as a bias mitigation tool that provides both gender-explicit and gender-neutral alternatives when necessary. To achieve this, we introduce GendEL, a manually crafted bilingual dataset of 240 gender-ambiguous and unambiguous sentences that feature stereotypical occupational nouns and adjectives. We find persistent gender bias in translations by both MT systems; while they perform well in cases where gender is explicitly defined, with DeepL outperforming both Google Translate and GPT-4o in feminine gender-unambiguous sentences, they are far from producing gender-inclusive or neutral translations when the gender is unspecified. GPT-4o shows promise, generating appropriate gendered and neutral alternatives for most ambiguous cases, though residual biases remain evident.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language</title>
<link>https://arxiv.org/abs/2506.09560</link>
<guid>https://arxiv.org/abs/2506.09560</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Macedonian, Dataset, Evaluation, State-of-the-art Model

Summary: 
- The article introduces resources created to support the adoption and research advancements of Large Language Models (LLMs) for the Macedonian language.
- A significant Macedonian corpus of 40GB with 3.5B words and a 106k-instance instruction dataset for conversational applications are collected.
- A Macedonian evaluation suite covering seven benchmarks is constructed for evaluating the performance of the developed resources.
- The domestic-yak model, an 8B-parameter model trained on the curated datasets, outperforms existing models in the 8B parameter range and achieves performance comparable to models up to 10 times larger.
- Native speakers prefer the domestic-yak model over larger counterparts, giving higher ratings for grammatical correctness and cultural appropriateness.

<br /><br />Summary: The resources developed include a large Macedonian corpus, an instruction dataset for conversational applications, and an evaluation suite. The domestic-yak model, trained on the datasets, outperforms existing models in its parameter range and is preferred by native speakers for its grammatical correctness and cultural appropriateness. <div>
arXiv:2506.09560v1 Announce Type: new 
Abstract: The increase in technological adoption worldwide comes with demands for novel tools to be used by the general population. Large Language Models (LLMs) provide a great opportunity in this respect, but their capabilities remain limited for low-resource languages, restricting applications in countries where such languages are spoken. We create several resources to facilitate the adoption of LLMs and to support research advancements for Macedonian. We collect the largest Macedonian corpus to date, consisting of 40GB of textual data and totaling 3.5B words. To support conversational applications, we collect a 106k-instance instruction dataset, carefully built to be culturally grounded. For evaluation, we construct a Macedonian evaluation suite covering seven benchmarks. Finally, we train domestic-yak, a state-of-the-art 8B-parameter model, on our curated datasets and evaluate it against eight baseline models using the newly constructed benchmark suite. Our model outperforms all existing models in the 8B parameter range across all benchmarks, and achieves performance comparable to models up to 10x larger. Furthermore, a qualitative analysis with native speakers reveals that our model is preferred over larger counterparts, receiving higher ratings for grammatical correctness and cultural appropriateness. All datasets, code, and model weights are openly released, setting a foundation for advancing LLMs in similarly underrepresented languages. These resources are publicly available at github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained model weights and data.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies</title>
<link>https://arxiv.org/abs/2506.09566</link>
<guid>https://arxiv.org/abs/2506.09566</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, Large Language Models, Integration, Reasoning, Question Answering
Summary:<br /><br />Structured knowledge from Knowledge Graphs (KGs) can enhance Large Language Models (LLMs) by improving reasoning, reducing hallucinations, and enabling complex question answering. This survey paper categorizes existing approaches into KG-enhanced LLMs and LLM-augmented KGs, emphasizing the benefits of integrating structured knowledge such as scalability, computational efficiency, and data quality. Critical gaps are identified, highlighting the mutual benefits of structured knowledge integration. Future research directions include neuro-symbolic integration, dynamic KG updating, data reliability, and ethical considerations to pave the way for intelligent systems capable of handling more complex real-world knowledge tasks. <div>
arXiv:2506.09566v1 Announce Type: new 
Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) enhances factual grounding and reasoning capabilities. This survey paper systematically examines the synergy between KGs and LLMs, categorizing existing approaches into two main groups: KG-enhanced LLMs, which improve reasoning, reduce hallucinations, and enable complex question answering; and LLM-augmented KGs, which facilitate KG construction, completion, and querying. Through comprehensive analysis, we identify critical gaps and highlight the mutual benefits of structured knowledge integration. Compared to existing surveys, our study uniquely emphasizes scalability, computational efficiency, and data quality. Finally, we propose future research directions, including neuro-symbolic integration, dynamic KG updating, data reliability, and ethical considerations, paving the way for intelligent systems capable of managing more complex real-world knowledge tasks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization in Language Models through the Lens of Intrinsic Dimension</title>
<link>https://arxiv.org/abs/2506.09591</link>
<guid>https://arxiv.org/abs/2506.09591</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Memorization, Intrinsic Dimension, Structural Complexity, Sparse Exposure <br />
Summary: 
Language Models (LMs) can unintentionally memorize data during training, leading to privacy concerns. This study explores how the Intrinsic Dimension (ID) of a sequence, a measure of structural complexity, influences memorization in LMs. The research indicates that sequences with higher ID are less likely to be memorized, particularly in overparameterized models and under sparse exposure. This suggests that ID acts as a suppressive signal for memorization. The findings underscore the impact of scale, exposure, and complexity on the memorization process in LMs. <div>
arXiv:2506.09591v1 Announce Type: new 
Abstract: Language Models (LMs) are prone to memorizing parts of their data during training and unintentionally emitting them at generation time, raising concerns about privacy leakage and disclosure of intellectual property. While previous research has identified properties such as context length, parameter size, and duplication frequency, as key drivers of unintended memorization, little is known about how the latent structure modulates this rate of memorization. We investigate the role of Intrinsic Dimension (ID), a geometric proxy for the structural complexity of a sequence in latent space, in modulating memorization. Our findings suggest that ID acts as a suppressive signal for memorization: compared to low-ID sequences, high-ID sequences are less likely to be memorized, particularly in overparameterized models and under sparse exposure. These findings highlight the interaction between scale, exposure, and complexity in shaping memorization.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Debiasing Methods for LLM-based Parameter Estimates</title>
<link>https://arxiv.org/abs/2506.09627</link>
<guid>https://arxiv.org/abs/2506.09627</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, debiasing methods, expert annotations, bias reduction, empirical efficiency

Summary:<br /><br />
The study examines the performance of debiasing methods, specifically Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI), in mitigating errors in large language models (LLMs) compared to expert annotations. While both methods aim to provide valid estimation by combining LLM annotations with expert labels, their performance varies with the number of expert annotations and dataset sizes. DSL generally outperforms PPI in bias reduction and empirical efficiency with large datasets, but its performance consistency across different datasets is lower. The findings highlight a bias-variance tradeoff in debiasing methods, emphasizing the need for further research on quantifying their efficiency in finite samples. <div>
arXiv:2506.09627v1 Announce Type: new 
Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to annotate text, but are often inconsistent when compared with experts. These errors can bias downstream estimates of population parameters such as regression coefficients and causal effects. To mitigate this bias, researchers have developed debiasing methods such as Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI), which promise valid estimation by combining LLM annotations with a limited number of expensive expert annotations. Although these methods produce consistent estimates under theoretical assumptions, it is unknown how they compare in finite samples of sizes encountered in applied research. We make two contributions: First, we study how each method's performance scales with the number of expert annotations, highlighting regimes where LLM bias or limited expert labels significantly affect results. Second, we compare DSL and PPI across a range of tasks, finding that although both achieve low bias with large datasets, DSL often outperforms PPI on bias reduction and empirical efficiency, but its performance is less consistent across datasets. Our findings indicate that there is a bias-variance tradeoff at the level of debiasing methods, calling for more research on developing metrics for quantifying their efficiency in finite samples.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning</title>
<link>https://arxiv.org/abs/2506.09641</link>
<guid>https://arxiv.org/abs/2506.09641</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic predictors, information theory, Naive Discriminative Learning, acoustic word duration, Buckeye corpus 

Summary: 
Probabilistic predictors based on information theory were compared with Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration. Three models were examined using the Buckeye corpus: NDL-derived predictors using information-theoretic formulas, traditional NDL predictors, and N-gram probabilistic predictors. The N-gram model outperformed both NDL models, challenging the assumption that NDL is more effective due to its cognitive motivation. Incorporating information-theoretic formulas into NDL improved model performance compared to the traditional model. The study emphasized the importance of incorporating average contextual predictability and combining information-theoretic metrics of predictability with information derived from discriminative learning in modeling acoustic reduction. 

<br /><br />Summary: <div>
arXiv:2506.09641v1 Announce Type: new 
Abstract: This study compares probabilistic predictors based on information theory with Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration, focusing on probabilistic reduction. We examine three models using the Buckeye corpus: one with NDL-derived predictors using information-theoretic formulas, one with traditional NDL predictors, and one with N-gram probabilistic predictors. Results show that the N-gram model outperforms both NDL models, challenging the assumption that NDL is more effective due to its cognitive motivation. However, incorporating information-theoretic formulas into NDL improves model performance over the traditional model. This research highlights a) the need to incorporate not only frequency and contextual predictability but also average contextual predictability, and b) the importance of combining information-theoretic metrics of predictability and information derived from discriminative learning in modeling acoustic reduction.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Sign Language Production as Data Augmentation to enhance Sign Language Translation</title>
<link>https://arxiv.org/abs/2506.09643</link>
<guid>https://arxiv.org/abs/2506.09643</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, sign language production, sign language translation, data augmentation, generative models <br />
Summary: <br />
Machine learning models heavily rely on vast amounts of high-quality data, which can be challenging to collect for low-resource languages like signed languages due to various constraints. Sign language datasets are significantly smaller than spoken language datasets, hindering the performance of Sign Language Translation models. To address this issue, the authors propose leveraging Sign Language Production advancements to enhance existing sign language datasets. By employing techniques such as a skeleton-based approach, sign stitching, and photo-realistic generative models like SignGAN and SignSplat, they introduce variation in signer appearance and skeletal motion to boost Sign Language Translation model performance by up to 19%. These methods effectively augment datasets and enhance the accuracy and robustness of Sign Language Translation systems, especially in environments with limited resources. <br /> 
Summary: <div>
arXiv:2506.09643v1 Announce Type: new 
Abstract: Machine learning models fundamentally rely on large quantities of high-quality data. Collecting the necessary data for these models can be challenging due to cost, scarcity, and privacy restrictions. Signed languages are visual languages used by the deaf community and are considered low-resource languages. Sign language datasets are often orders of magnitude smaller than their spoken language counterparts. Sign Language Production is the task of generating sign language videos from spoken language sentences, while Sign Language Translation is the reverse translation task. Here, we propose leveraging recent advancements in Sign Language Production to augment existing sign language datasets and enhance the performance of Sign Language Translation models. For this, we utilize three techniques: a skeleton-based approach to production, sign stitching, and two photo-realistic generative models, SignGAN and SignSplat. We evaluate the effectiveness of these techniques in enhancing the performance of Sign Language Translation models by generating variation in the signer's appearance and the motion of the skeletal data. Our results demonstrate that the proposed methods can effectively augment existing datasets and enhance the performance of Sign Language Translation models by up to 19%, paving the way for more robust and accurate Sign Language Translation systems, even in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering</title>
<link>https://arxiv.org/abs/2506.09645</link>
<guid>https://arxiv.org/abs/2506.09645</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, Large Language Models, Retrieval-Augmented Generation, graph retrieval, KGQA

Summary: 
The research introduces RAPL, a new framework for efficient and effective graph retrieval in Knowledge Graph Question Answering (KGQA). RAPL addresses limitations in existing methods by utilizing a two-stage labeling strategy that combines heuristic signals with parametric models, a model-agnostic graph transformation approach to enhance representational capacity, and a path-based reasoning strategy for learning from rational knowledge injection. Empirical results show that RAPL outperforms state-of-the-art methods by 2.66%-20.34% and reduces performance gaps between different LLM-based reasoners and under cross-dataset settings. The framework demonstrates superior retrieval capability and generalizability, showcasing its potential in advancing KGQA research. The code for RAPL is available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2506.09645v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability across various domains, but their reliability is hindered by the outdated knowledge and hallucinations. Retrieval-Augmented Generation mitigates these issues by grounding LLMs with external knowledge; however, most existing RAG pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledge graphs, which represent facts as relational triples, offer a more structured and compact alternative. Recent studies have explored integrating knowledge graphs with LLMs for knowledge graph question answering (KGQA), with a significant proportion adopting the retrieve-then-reasoning paradigm. In this framework, graph-based retrievers have demonstrated strong empirical performance, yet they still face challenges in generalization ability. In this work, we propose RAPL, a novel framework for efficient and effective graph retrieval in KGQA. RAPL addresses these limitations through three aspects: (1) a two-stage labeling strategy that combines heuristic signals with parametric models to provide causally grounded supervision; (2) a model-agnostic graph transformation approach to capture both intra- and inter-triple interactions, thereby enhancing representational capacity; and (3) a path-based reasoning strategy that facilitates learning from the injected rational knowledge, and supports downstream reasoner through structured inputs. Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and significantly reduces the performance gap between smaller and more powerful LLM-based reasoners, as well as the gap under cross-dataset settings, highlighting its superior retrieval capability and generalizability. Codes are available at: https://github.com/tianyao-aka/RAPL.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA</title>
<link>https://arxiv.org/abs/2506.09657</link>
<guid>https://arxiv.org/abs/2506.09657</guid>
<content:encoded><![CDATA[
<div> Keywords: Question Answering, Tabular data, Text-to-SQL generation, Retrieval-augmented generation, Large language model

Summary:
This paper introduces a system developed for the SemEval 2025 Task 8, focusing on Question Answering (QA) over tabular data. The system incorporates text-to-SQL and text-to-code generation modules, a self-correction mechanism, and a retrieval-augmented generation (RAG) approach. An end-to-end (E2E) module is also included, all managed by a large language model (LLM). Ablation studies were conducted to analyze the impact of different components, highlighting persisting challenges in the field. During the competition, the system achieved an accuracy of 80%, earning a top-13 ranking among 38 teams. It demonstrated a notable enhancement in accuracy compared to open-source models and performed similarly to proprietary LLMs in QA tasks involving tables. The code for the system is accessible via a GitHub repository. 

<br /><br />Summary: <div>
arXiv:2506.09657v1 Announce Type: new 
Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question Answering (QA) over tabular data. Our approach integrates several key components: text-to-SQL and text-to-code generation modules, a self-correction mechanism, and a retrieval-augmented generation (RAG). Additionally, it includes an end-to-end (E2E) module, all orchestrated by a large language model (LLM). Through ablation studies, we analyzed the effects of different parts of our pipeline and identified the challenges that are still present in this field. During the evaluation phase of the competition, our solution achieved an accuracy of 80%, resulting in a top-13 ranking among the 38 participating teams. Our pipeline demonstrates a significant improvement in accuracy for open-source models and achieves a performance comparable to proprietary LLMs in QA tasks over tables. The code is available at GitHub repository.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-Level Uncertainty in Large Language Models</title>
<link>https://arxiv.org/abs/2506.09669</link>
<guid>https://arxiv.org/abs/2506.09669</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Boundary, Query-Level Uncertainty, Internal Confidence, Adaptive Inference

Summary:
Large Language Models need to be aware of the boundary of their knowledge to effectively handle queries. A method to detect knowledge boundaries through Query-Level Uncertainty is proposed in this work. The method, called Internal Confidence, relies on self-evaluations across layers and tokens to determine the model's ability to address a query without generating tokens. Empirical results show that Internal Confidence outperforms several baselines in factual QA and mathematical reasoning tasks. The proposed method can also be used for efficient RAG and model cascading, reducing inference costs while maintaining performance. This approach enhances the model's adaptive inference capabilities, allowing it to invoke resources like RAG, engage in deep thinking, or use the abstention mechanism. By improving the model's awareness of its knowledge boundary, the development of efficient and trustworthy AI systems is facilitated. 

<br /><br />Summary: Large Language Models need to be aware of their knowledge boundaries, and a method called Internal Confidence has been proposed to detect these boundaries. This method, which leverages self-evaluations, outperforms baselines in factual QA and mathematical reasoning tasks. Additionally, it enables efficient RAG and model cascading, reducing inference costs while maintaining performance and enhancing AI's adaptive inference capabilities. <div>
arXiv:2506.09669v1 Announce Type: new 
Abstract: It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which aims to determine if the model is able to address a given query without generating any tokens. To this end, we introduce a novel and training-free method called \emph{Internal Confidence}, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data</title>
<link>https://arxiv.org/abs/2506.09672</link>
<guid>https://arxiv.org/abs/2506.09672</guid>
<content:encoded><![CDATA[
<div> Keywords: Unstructured Knowledge Editing, Large Language Models, Locality evaluation, Fine-tuning, Batch editing <br />
Summary: <br />
The study introduces Unstructured Knowledge Editing (UKE) for updating knowledge in large language models (LLMs), focusing on unstructured inputs like long texts. Two new datasets are created to evaluate Locality in post-edited models. Four factors influencing Fine-tuning (FT) based methods are identified, with experiments showing optimal training settings for UKE. The FT-based method (FT-UKE) surpasses existing state-of-the-art methods, especially in batch editing scenarios, with increasing performance advantages as the batch size grows. <br /> <div>
arXiv:2506.09672v1 Announce Type: new 
Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant knowledge of large language models (LLMs). It focuses on unstructured inputs, such as long or free-form texts, which are common forms of real-world knowledge. Although previous studies have proposed effective methods and tested them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2) Abnormal failure of fine-tuning (FT) based methods for UKE. To address these issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by extending two existing UKE datasets with locality test data from the unstructured and structured views. This enables a systematic evaluation of the Locality of post-edited models. Furthermore, we identify four factors that may affect the performance of FT-based methods. Based on these factors, we conduct experiments to determine how the well-performing FT-based methods should be trained for the UKE task, providing a training recipe for future research. Our experimental results indicate that the FT-based method with the optimal setting (FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art (SOTA). In batch editing scenarios, FT-UKE shows strong performance as well, with its advantage over SOTA methods increasing as the batch size grows, expanding the average metric lead from +6.78% to +10.80%
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models</title>
<link>https://arxiv.org/abs/2506.09684</link>
<guid>https://arxiv.org/abs/2506.09684</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, uncertainty quantification, Markov chains, probabilistic framework, perturbation algorithm <br />
Summary:<br />
This paper presents a novel probabilistic framework for uncertainty quantification in large language models (LLMs). It introduces a dual random walk perspective, treating input-output pairs as Markov chains based on semantic similarity. The proposed framework utilizes an inverse model to assess uncertainty by perturbing the input space conditioned on a given output. A new uncertainty measure, Inv-Entropy, is defined within this framework. The flexibility of this approach allows for customization of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. The paper also introduces GAAP, a perturbation algorithm leveraging genetic algorithms to enhance input diversity. Furthermore, a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), is introduced to directly assess uncertainty levels. Extensive experiments demonstrate the superiority of the proposed Inv-Entropy method over existing semantic uncertainty quantification techniques. The code for reproducing the results is available on GitHub. <br /><br />Summary: <div>
arXiv:2506.09684v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic foundation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComfyUI-R1: Exploring Reasoning Models for Workflow Generation</title>
<link>https://arxiv.org/abs/2506.09790</link>
<guid>https://arxiv.org/abs/2506.09790</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, workflow generation, reasoning model, ComfyUI, long chain-of-thought reasoning

Summary: 
AI-generated content has progressed from monolithic models to modular workflows on platforms like ComfyUI, allowing for customization in creative pipelines. However, creating effective workflows requires expertise in orchestrating specialized components, posing a challenge for users. In response, ComfyUI-R1, a large reasoning model for automated workflow generation, has been introduced. Trained on a curated dataset of 4K workflows, the model utilizes long chain-of-thought reasoning data for node selection, workflow planning, and code-level representation. Through a two-stage framework involving CoT fine-tuning and reinforcement learning, ComfyUI-R1 achieves high validity rates and F1 scores, outperforming existing methods. The emphasis on reasoning processes and the transformation of workflows into code demonstrate the model's ability to synthesize intricate workflows with diverse nodes, showcasing the potential of long CoT reasoning in AI art creation. 

<br /><br />Summary: <div>
arXiv:2506.09790v1 Announce Type: new 
Abstract: AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?</title>
<link>https://arxiv.org/abs/2506.09796</link>
<guid>https://arxiv.org/abs/2506.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: test development, large language models, human-likeness, educational assessments, psychometric plausibility <br />
Summary: <br />
- Study evaluates human-likeness of responses from 18 LLMs with multiple-choice test items in reading, U.S. history, and economics. <br />
- LLMs show excessive confidence but can exhibit more human-like response distributions with temperature scaling calibration. <br />
- Reading comprehension items show better correlation with LLM responses compared to other subjects. <br />
- Overall, LLMs should not be used for piloting educational assessments in a zero-shot setting. <br />
- Methodology combines classical test theory and item response theory to assess psychometric plausibility of LLM responses. <div>
arXiv:2506.09796v1 Announce Type: new 
Abstract: Knowing how test takers answer items in educational assessments is essential for test development, to evaluate item quality, and to improve test validity. However, this process usually requires extensive pilot studies with human participants. If large language models (LLMs) exhibit human-like response behavior to test items, this could open up the possibility of using them as pilot participants to accelerate test development. In this paper, we evaluate the human-likeness or psychometric plausibility of responses from 18 instruction-tuned LLMs with two publicly available datasets of multiple-choice test items across three subjects: reading, U.S. history, and economics. Our methodology builds on two theoretical frameworks from psychometrics which are commonly used in educational assessment, classical test theory and item response theory. The results show that while larger models are excessively confident, their response distributions can be more human-like when calibrated with temperature scaling. In addition, we find that LLMs tend to correlate better with humans in reading comprehension items compared to other subjects. However, the correlations are not very strong overall, indicating that LLMs should not be used for piloting educational assessments in a zero-shot setting.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRT: Code-integrated Reasoning within Thinking</title>
<link>https://arxiv.org/abs/2506.09820</link>
<guid>https://arxiv.org/abs/2506.09820</guid>
<content:encoded><![CDATA[
<div> code interpreter, large reasoning models, mathematical reasoning, hint-engineering, post-training <br />
<br />
Summary: <br />
The paper introduces CoRT, a post-training framework that enables large reasoning models (LRMs) to effectively utilize external knowledge from Code Interpreters (CIs) for complex mathematical operations. By addressing data scarcity through Hint-Engineering, strategically inserting hints to optimize LRM-CI interaction, the framework synthesizes code-integrated reasoning data. Experimentation on LRMs of varying parameters sizes shows significant performance improvements on challenging mathematical reasoning datasets. The models achieve absolute improvements of 4% and 8% on DeepSeek-R1-Distill-Qwen models, using fewer tokens compared to natural language models. The CoRT framework offers a promising approach to enhancing LRMs' efficiency and accuracy in handling complex mathematical operations. Source code and models are available on GitHub. <br /> <div>
arXiv:2506.09820v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\% and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection</title>
<link>https://arxiv.org/abs/2506.09827</link>
<guid>https://arxiv.org/abs/2506.09827</guid>
<content:encoded><![CDATA[
<div> EmoNet-Voice, speech emotion detection, AI systems, benchmark, SER models<br />
<br />
Summary: <br />
The paper introduces EmoNet-Voice as a benchmark for evaluating AI systems in speech emotion detection. It comprises EmoNet-Voice Big, a pre-training dataset with 4,500+ hours of speech in 11 voices, 40 emotions, and 4 languages, and EmoNet-Voice Bench, a benchmark dataset with human annotations. EmoNet-Voice assesses SER models on a diverse range of 40 emotion categories and intensities. Synthetic audio snippets were created to simulate emotional scenes, validated by psychology experts for accuracy. This approach addresses limitations of existing datasets by including a broad spectrum of emotions, even sensitive ones. The Empathic Insight Voice models demonstrate high performance in speech emotion recognition, aligning closely with human expert assessments. Findings reveal that high-arousal emotions like anger are easier to detect compared to low-arousal states like concentration. <div>
arXiv:2506.09827v1 Announce Type: new 
Abstract: The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation</title>
<link>https://arxiv.org/abs/2506.09833</link>
<guid>https://arxiv.org/abs/2506.09833</guid>
<content:encoded><![CDATA[
<div> Keywords: rehabilitation assessment, synthetic data generation, pose augmentation, graph convolutional network, movement quality

Summary: 
Error-Guided Pose Augmentation (EGPA) is introduced as a method to address challenges in effective rehabilitation assessment. By simulating biomechanical errors observed in rehabilitation, EGPA generates synthetic skeleton data to improve performance in detecting movement mistakes. When combined with an attention-based graph convolutional network, EGPA enhances accuracy and interpretability across various evaluation metrics. Experimental results show significant reductions in mean absolute error and improvements in error classification accuracy. The attention visualizations demonstrate that the model learns to focus on clinically significant joints and movement phases. EGPA offers a promising approach to enhance automated movement quality assessment in clinical and home-based rehabilitation settings. <br /><br />Summary: <div>
arXiv:2506.09833v1 Announce Type: new 
Abstract: Effective rehabilitation assessment is essential for monitoring patient progress, particularly in home-based settings. Existing systems often face challenges such as data imbalance and difficulty detecting subtle movement errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method that generates synthetic skeleton data by simulating clinically relevant movement mistakes. Unlike standard augmentation techniques, EGPA targets biomechanical errors observed in rehabilitation. Combined with an attention-based graph convolutional network, EGPA improves performance across multiple evaluation metrics. Experiments demonstrate reductions in mean absolute error of up to 27.6 percent and gains in error classification accuracy of 45.8 percent. Attention visualizations show that the model learns to focus on clinically significant joints and movement phases, enhancing both accuracy and interpretability. EGPA offers a promising approach for improving automated movement quality assessment in both clinical and home-based rehabilitation contexts.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset of News Articles with Provenance Metadata for Media Relevance Assessment</title>
<link>https://arxiv.org/abs/2506.09847</link>
<guid>https://arxiv.org/abs/2506.09847</guid>
<content:encoded><![CDATA[
arXiv:2506.09847v1 Announce Type: new 
Abstract: Out-of-context and misattributed imagery is the leading form of media manipulation in today's misinformation and disinformation landscape. The existing methods attempting to detect this practice often only consider whether the semantics of the imagery corresponds to the text narrative, missing manipulation so long as the depicted objects or scenes somewhat correspond to the narrative at hand. To tackle this, we introduce News Media Provenance Dataset, a dataset of news articles with provenance-tagged images. We formulate two tasks on this dataset, location of origin relevance (LOR) and date and time of origin relevance (DTOR), and present baseline results on six large language models (LLMs). We identify that, while the zero-shot performance on LOR is promising, the performance on DTOR hinders, leaving room for specialized architectures and future work.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2506.09853</link>
<guid>https://arxiv.org/abs/2506.09853</guid>
<content:encoded><![CDATA[
arXiv:2506.09853v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2506.09886</link>
<guid>https://arxiv.org/abs/2506.09886</guid>
<content:encoded><![CDATA[
arXiv:2506.09886v1 Announce Type: new 
Abstract: We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hidden-state distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emergence of Abstract Thought in Large Language Models Beyond Any Language</title>
<link>https://arxiv.org/abs/2506.09890</link>
<guid>https://arxiv.org/abs/2506.09890</guid>
<content:encoded><![CDATA[
arXiv:2506.09890v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may "think" in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space-a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model's ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons-those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants</title>
<link>https://arxiv.org/abs/2506.09902</link>
<guid>https://arxiv.org/abs/2506.09902</guid>
<content:encoded><![CDATA[
arXiv:2506.09902v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced conversational AI assistants. However, systematically evaluating how well these assistants apply personalization--adapting to individual user preferences while completing tasks--remains challenging. Existing personalization benchmarks focus on chit-chat, non-conversational tasks, or narrow domains, failing to capture the complexities of personalized task-oriented assistance. To address this, we introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants. Our benchmark features diverse user profiles equipped with rich preferences and interaction histories, along with two specialized LLM-based agents: a user agent that engages in realistic task-oriented dialogues with AI assistants, and a judge agent that employs the LLM-as-a-Judge paradigm to assess personalization, response quality, and task success. Through extensive experiments with current LLM assistants across diverse tasks, we reveal significant variability in their personalization capabilities, providing crucial insights for advancing conversational AI systems.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aspect-Based Opinion Summarization with Argumentation Schemes</title>
<link>https://arxiv.org/abs/2506.09917</link>
<guid>https://arxiv.org/abs/2506.09917</guid>
<content:encoded><![CDATA[
arXiv:2506.09917v1 Announce Type: new 
Abstract: Reviews are valuable resources for customers making purchase decisions in online shopping. However, it is impractical for customers to go over the vast number of reviews and manually conclude the prominent opinions, which prompts the need for automated opinion summarization systems. Previous approaches, either extractive or abstractive, face challenges in automatically producing grounded aspect-centric summaries. In this paper, we propose a novel summarization system that not only captures predominant opinions from an aspect perspective with supporting evidence, but also adapts to varying domains without relying on a pre-defined set of aspects. Our proposed framework, ASESUM, summarizes viewpoints relevant to the critical aspects of a product by extracting aspect-centric arguments and measuring their salience and validity. We conduct experiments on a real-world dataset to demonstrate the superiority of our approach in capturing diverse perspectives of the original reviews compared to new and existing methods.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerIF: Verification Engineering for Reinforcement Learning in Instruction Following</title>
<link>https://arxiv.org/abs/2506.09942</link>
<guid>https://arxiv.org/abs/2506.09942</guid>
<content:encoded><![CDATA[
arXiv:2506.09942v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking</title>
<link>https://arxiv.org/abs/2506.09944</link>
<guid>https://arxiv.org/abs/2506.09944</guid>
<content:encoded><![CDATA[
arXiv:2506.09944v1 Announce Type: new 
Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resa: Transparent Reasoning Models via SAEs</title>
<link>https://arxiv.org/abs/2506.09967</link>
<guid>https://arxiv.org/abs/2506.09967</guid>
<content:encoded><![CDATA[
arXiv:2506.09967v1 Announce Type: new 
Abstract: How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \$1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around \$1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text</title>
<link>https://arxiv.org/abs/2506.09975</link>
<guid>https://arxiv.org/abs/2506.09975</guid>
<content:encoded><![CDATA[
arXiv:2506.09975v1 Announce Type: new 
Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting AI-generated text on social media is made even more difficult due to the short text length and informal, idiosyncratic language of the internet. It is nonetheless important to tackle this problem, as social media represents a significant attack vector in online influence campaigns, which may be bolstered through the use of mass-produced AI-generated posts supporting (or opposing) particular policies, decisions, or events. We approach this problem with the mindset and resources of a reasonably sophisticated threat actor, and create a dataset of 505,159 AI-generated social media posts from a combination of open-source, closed-source, and fine-tuned LLMs, covering 11 different controversial topics. We show that while the posts can be detected under typical research assumptions about knowledge of and access to the generating models, under the more realistic assumption that an attacker will not release their fine-tuned model to the public, detectability drops dramatically. This result is confirmed with a human study. Ablation experiments highlight the vulnerability of various detection algorithms to fine-tuned LLMs. This result has implications across all detection domains, since fine-tuning is a generally applicable and realistic LLM use case.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs</title>
<link>https://arxiv.org/abs/2506.09983</link>
<guid>https://arxiv.org/abs/2506.09983</guid>
<content:encoded><![CDATA[
arXiv:2506.09983v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled impressive performance in various tasks. However, standard prompting often struggles to produce structurally valid and accurate outputs, especially in dependency parsing. We propose a novel step-by-step instruction strategy, where universal part-of-speech tagging precedes the prediction of syntactic heads and dependency labels, and a simplified CoNLL-U like output format, our method achieves state-of-the-art accuracy on Universal Dependencies datasets across 17 languages without hallucination or contamination. We further show that multilingual fine-tuning simultaneously improves cross-language generalization performance. Our results highlight the effectiveness of explicit reasoning steps in LLM-based parsing and offer a scalable, format-consistent alternative to bracket-based approaches.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages</title>
<link>https://arxiv.org/abs/2506.09992</link>
<guid>https://arxiv.org/abs/2506.09992</guid>
<content:encoded><![CDATA[
arXiv:2506.09992v1 Announce Type: new 
Abstract: Online toxic language causes real harm, especially in regions with limited moderation tools. In this study, we evaluate how large language models handle toxic comments in Serbian, Croatian, and Bosnian, languages with limited labeled data. We built and manually labeled a dataset of 4,500 YouTube and TikTok comments drawn from videos across diverse categories, including music, politics, sports, modeling, influencer content, discussions of sexism, and general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) were tested in two modes: zero-shot and context-augmented. We measured precision, recall, F1 score, accuracy and false positive rates. Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini in context-augmented mode, reaching an F1 score of 0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the lowest false alarms. We show how adding minimal context can improve toxic language detection in low-resource settings and suggest practical strategies such as improved prompt design and threshold calibration. These results show that prompt design alone can yield meaningful gains in toxicity detection for underserved Balkan language communities.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring</title>
<link>https://arxiv.org/abs/2506.09996</link>
<guid>https://arxiv.org/abs/2506.09996</guid>
<content:encoded><![CDATA[
arXiv:2506.09996v1 Announce Type: new 
Abstract: Though safety alignment has been applied to most large language models (LLMs), LLM service providers generally deploy a subsequent moderation as the external safety guardrail in real-world products. Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. Recent works pay more attention to partial detection where moderators oversee the generation midway and early stop the output if harmfulness is detected, but they directly apply moderators trained with the full detection paradigm to incomplete outputs, introducing a training-inference gap that lowers the performance. In this paper, we explore how to form a data-and-model solution that natively supports partial detection. For the data, we construct FineHarm, a dataset consisting of 29K prompt-response pairs with fine-grained annotations to provide reasonable supervision for token-level training. Then, we propose the streaming content monitor, which is trained with dual supervision of response- and token-level labels and can follow the output stream of LLM to make a timely judgment of harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is comparable to full detection, by only seeing the first 18% of tokens in responses on average. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks</title>
<link>https://arxiv.org/abs/2410.16222</link>
<guid>https://arxiv.org/abs/2410.16222</guid>
<content:encoded><![CDATA[
arXiv:2410.16222v2 Announce Type: cross 
Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. These methods largely succeed in coercing the target output in their original settings, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model checks if a given jailbreak is likely to occur in the distribution of text. For this, we build an N-gram language model on 1T tokens, which, unlike model-based perplexity, allows for an LLM-agnostic, nonparametric, and inherently interpretable evaluation. We adapt popular attacks to this threat model, and, for the first time, benchmark these attacks on equal footing with it. After an extensive comparison, we find attack success rates against safety-tuned modern models to be lower than previously presented and that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent bigrams, either selecting the ones absent from real-world text or rare ones, e.g., specific to Reddit or code datasets.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation</title>
<link>https://arxiv.org/abs/2506.09081</link>
<guid>https://arxiv.org/abs/2506.09081</guid>
<content:encoded><![CDATA[
arXiv:2506.09081v1 Announce Type: cross 
Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers</title>
<link>https://arxiv.org/abs/2506.09099</link>
<guid>https://arxiv.org/abs/2506.09099</guid>
<content:encoded><![CDATA[
arXiv:2506.09099v1 Announce Type: cross 
Abstract: The relationship between memorization and generalization in large language models (LLMs) remains an open area of research, with growing evidence that the two are deeply intertwined. In this work, we investigate this relationship by pre-training a series of capacity-limited Transformer models from scratch on two synthetic character-level tasks designed to separately probe generalization (via arithmetic extrapolation) and memorization (via factual recall). We observe a consistent trade-off: small models extrapolate to unseen arithmetic cases but fail to memorize facts, while larger models memorize but fail to extrapolate. An intermediate-capacity model exhibits a similar shift toward memorization. When trained on both tasks jointly, no model (regardless of size) succeeds at extrapolation. These findings suggest that pre-training may intrinsically favor one learning mode over the other. By isolating these dynamics in a controlled setting, our study offers insight into how model capacity shapes learning behavior and offers broader implications for the design and deployment of small language models.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SensorLM: Learning the Language of Wearable Sensors</title>
<link>https://arxiv.org/abs/2506.09108</link>
<guid>https://arxiv.org/abs/2506.09108</guid>
<content:encoded><![CDATA[
arXiv:2506.09108v1 Announce Type: cross 
Abstract: We present SensorLM, a family of sensor-language foundation models that enable wearable sensor data understanding with natural language. Despite its pervasive nature, aligning and interpreting sensor data with language remains challenging due to the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data. We introduce a hierarchical caption generation pipeline designed to capture statistical, structural, and semantic information from sensor data. This approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and recovers them as specific variants within a generic architecture. Extensive experiments on real-world tasks in human activity analysis and healthcare verify the superior performance of SensorLM over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval. SensorLM also demonstrates intriguing capabilities including scaling behaviors, label efficiency, sensor captioning, and zero-shot generalization to unseen tasks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation</title>
<link>https://arxiv.org/abs/2506.09109</link>
<guid>https://arxiv.org/abs/2506.09109</guid>
<content:encoded><![CDATA[
arXiv:2506.09109v1 Announce Type: cross 
Abstract: As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRe, a novel evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRe surpasses all baselines by 28% F1 points. Additionally, we construct two datasets for culturally universal concept, one comprising of T2I-generated outputs and another retrieved from naturally occurring data. CAIRe achieves Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Text Generation with Dynamic Contextual Perturbation</title>
<link>https://arxiv.org/abs/2506.09148</link>
<guid>https://arxiv.org/abs/2506.09148</guid>
<content:encoded><![CDATA[
arXiv:2506.09148v1 Announce Type: cross 
Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose vulnerabilities by introducing subtle perturbations to input text, often leading to misclassification while maintaining human readability. Existing methods typically focus on word-level or local text segment alterations, overlooking the broader context, which results in detectable or semantically inconsistent perturbations. We propose a novel adversarial text attack scheme named Dynamic Contextual Perturbation (DCP). DCP dynamically generates context-aware perturbations across sentences, paragraphs, and documents, ensuring semantic fidelity and fluency. Leveraging the capabilities of pre-trained language models, DCP iteratively refines perturbations through an adversarial objective function that balances the dual objectives of inducing model misclassification and preserving the naturalness of the text. This comprehensive approach allows DCP to produce more sophisticated and effective adversarial examples that better mimic natural language patterns. Our experimental results, conducted on various NLP models and datasets, demonstrate the efficacy of DCP in challenging the robustness of state-of-the-art NLP systems. By integrating dynamic contextual analysis, DCP significantly enhances the subtlety and impact of adversarial attacks. This study highlights the critical role of context in adversarial attacks and lays the groundwork for creating more robust NLP systems capable of withstanding sophisticated adversarial strategies.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search</title>
<link>https://arxiv.org/abs/2506.09171</link>
<guid>https://arxiv.org/abs/2506.09171</guid>
<content:encoded><![CDATA[
arXiv:2506.09171v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research</title>
<link>https://arxiv.org/abs/2506.09206</link>
<guid>https://arxiv.org/abs/2506.09206</guid>
<content:encoded><![CDATA[
arXiv:2506.09206v1 Announce Type: cross 
Abstract: The scarcity of large-scale classroom speech data has hindered the development of AI-driven speech models for education. Public classroom datasets remain limited, and the lack of a dedicated classroom noise corpus prevents the use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom noise using game engines, a framework that extends to other domains. Using this methodology, we present SimClass, a dataset that includes both a synthesized classroom noise corpus and a simulated classroom speech dataset. The speech data is generated by pairing a public children's speech corpus with YouTube lecture videos to approximate real classroom interactions in clean conditions. Our experiments on clean and noisy speech demonstrate that SimClass closely approximates real classroom speech, making it a valuable resource for developing robust speech recognition and enhancement models.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkQE: Query Expansion via an Evolving Thinking Process</title>
<link>https://arxiv.org/abs/2506.09260</link>
<guid>https://arxiv.org/abs/2506.09260</guid>
<content:encoded><![CDATA[
arXiv:2506.09260v1 Announce Type: cross 
Abstract: Effective query expansion for web search benefits from promoting both exploration and result diversity to capture multiple interpretations and facets of a query. While recent LLM-based methods have improved retrieval performance and demonstrate strong domain generalization without additional training, they often generate narrowly focused expansions that overlook these desiderata. We propose ThinkQE, a test-time query expansion framework addressing this limitation through two key components: a thinking-based expansion process that encourages deeper and comprehensive semantic exploration, and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback from the corpus. Experiments on diverse web search benchmarks (DL19, DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches, including training-intensive dense retrievers and rerankers.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench</title>
<link>https://arxiv.org/abs/2506.09289</link>
<guid>https://arxiv.org/abs/2506.09289</guid>
<content:encoded><![CDATA[
arXiv:2506.09289v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Guided Ligand-Binding Protein Design</title>
<link>https://arxiv.org/abs/2506.09332</link>
<guid>https://arxiv.org/abs/2506.09332</guid>
<content:encoded><![CDATA[
arXiv:2506.09332v1 Announce Type: cross 
Abstract: Can AI protein models follow human language instructions and design proteins with desired functions (e.g. binding to a ligand)? Designing proteins that bind to a given ligand is crucial in a wide range of applications in biology and chemistry. Most prior AI models are trained on protein-ligand complex data, which is scarce due to the high cost and time requirements of laboratory experiments. In contrast, there is a substantial body of human-curated text descriptions about protein-ligand interactions and ligand formula. In this paper, we propose InstructPro, a family of protein generative models that follow natural language instructions to design ligand-binding proteins. Given a textual description of the desired function and a ligand formula in SMILES, InstructPro generates protein sequences that are functionally consistent with the specified instructions. We develop the model architecture, training strategy, and a large-scale dataset, InstructProBench, to support both training and evaluation. InstructProBench consists of 9,592,829 triples of (function description, ligand formula, protein sequence). We train two model variants: InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion parameters). Both variants consistently outperform strong baselines, including ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking success rate (81.52% at moderate confidence) and the lowest average root mean square deviation (RMSD) compared to ground truth structures (4.026{\AA}). InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating InstructPro's ability to generate ligand-binding proteins that align with the functional specifications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
<link>https://arxiv.org/abs/2506.09344</link>
<guid>https://arxiv.org/abs/2506.09344</guid>
<content:encoded><![CDATA[
arXiv:2506.09344v1 Announce Type: cross 
Abstract: We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy</title>
<link>https://arxiv.org/abs/2506.09420</link>
<guid>https://arxiv.org/abs/2506.09420</guid>
<content:encoded><![CDATA[
arXiv:2506.09420v1 Announce Type: cross 
Abstract: Recent improvements in large language models (LLMs) have led many researchers to focus on building fully autonomous AI agents. This position paper questions whether this approach is the right path forward, as these autonomous systems still have problems with reliability, transparency, and understanding the actual requirements of human. We suggest a different approach: LLM-based Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing them. By keeping human involved to provide guidance, answer questions, and maintain control, these systems can be more trustworthy and adaptable. Looking at examples from healthcare, finance, and software development, we show how human-AI teamwork can handle complex tasks better than AI working alone. We also discuss the challenges of building these collaborative systems and offer practical solutions. This paper argues that progress in AI should not be measured by how independent systems become, but by how well they can work with humans. The most promising future for AI is not in systems that take over human roles, but in those that enhance human capabilities through meaningful partnership.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary</title>
<link>https://arxiv.org/abs/2506.09448</link>
<guid>https://arxiv.org/abs/2506.09448</guid>
<content:encoded><![CDATA[
arXiv:2506.09448v1 Announce Type: cross 
Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models (OWSM), are trained on massive datasets to achieve accurate automatic speech recognition. However, even SFMs struggle to accurately recognize rare and unseen words. While contextual biasing (CB) is a promising approach to improve recognition of such words, most CB methods are trained from scratch, resulting in lower performance than SFMs due to the lack of pre-trained knowledge. This paper integrates an existing CB method with OWSM v3.1 while freezing its pre-trained parameters. By leveraging the knowledge embedded in SFMs, the proposed method enables effective CB while preserving the advantages of SFMs, even with a small dataset. Experimental results show that the proposed method improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9 point improvement in the overall WER while reducing the real-time factor by 7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean set.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform</title>
<link>https://arxiv.org/abs/2506.09452</link>
<guid>https://arxiv.org/abs/2506.09452</guid>
<content:encoded><![CDATA[
arXiv:2506.09452v1 Announce Type: cross 
Abstract: The high cost of ownership of AI compute infrastructure and challenges of robust serving of large language models (LLMs) has led to a surge in managed Model-as-a-service deployments. Even when enterprises choose on-premises deployments, the compute infrastructure is typically shared across many teams in order to maximize the return on investment. In both scenarios the deployed models operate only on plaintext data, and so enterprise data owners must allow their data to appear in plaintext on a shared or multi-tenant compute infrastructure. This results in data owners with private or sensitive data being hesitant or restricted in what data they use with these types of deployments. In this work we introduce the Stained Glass Transform, a learned, stochastic, and sequence dependent transformation of the word embeddings of an LLM which information theoretically provides privacy to the input of the LLM while preserving the utility of model. We theoretically connect a particular class of Stained Glass Transforms to the theory of mutual information of Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based on mutual information, and verify the privacy and utility of instances of transformed embeddings through token level metrics of privacy and standard LLM performance benchmarks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks</title>
<link>https://arxiv.org/abs/2506.09521</link>
<guid>https://arxiv.org/abs/2506.09521</guid>
<content:encoded><![CDATA[
arXiv:2506.09521v1 Announce Type: cross 
Abstract: Speaker anonymization systems hide the identity of speakers while preserving other information such as linguistic content and emotions. To evaluate their privacy benefits, attacks in the form of automatic speaker verification (ASV) systems are employed. In this study, we assess the impact of intra-speaker linguistic content similarity in the attacker training and evaluation datasets, by adapting BERT, a language model, as an ASV system. On the VoicePrivacy Attacker Challenge datasets, our method achieves a mean equal error rate (EER) of 35%, with certain speakers attaining EERs as low as 2%, based solely on the textual content of their utterances. Our explainability study reveals that the system decisions are linked to semantically similar keywords within utterances, stemming from how LibriSpeech is curated. Our study suggests reworking the VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge the reliance on global EER for privacy evaluations.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs</title>
<link>https://arxiv.org/abs/2506.09522</link>
<guid>https://arxiv.org/abs/2506.09522</guid>
<content:encoded><![CDATA[
arXiv:2506.09522v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across various multimodal tasks by integrating visual perception with language understanding. However, conventional decoding strategies of LVLMs often fail to successfully utilize visual information, leading to visually ungrounded responses. While various approaches have been proposed to address this limitation, they typically require additional training, multi-step inference procedures, or external model dependencies. This paper introduces ReVisiT, a simple yet effective decoding method that references vision tokens to guide the text generation process in LVLMs. Our approach leverages the semantic information embedded within vision tokens by projecting them into the text token distribution space, and dynamically selecting the most relevant vision token at each decoding step through constrained divergence minimization. This selected vision token is then used to refine the output distribution to better incorporate visual semantics. Experiments on three LVLM hallucination benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances visual grounding with minimal computational overhead. Moreover, our method achieves competitive or superior results relative to state-of-the-art baselines while reducing computational costs for up to $2\times$.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
arXiv:2506.09532v1 Announce Type: cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Red-Teaming of Policy-Adherent Agents</title>
<link>https://arxiv.org/abs/2506.09600</link>
<guid>https://arxiv.org/abs/2506.09600</guid>
<content:encoded><![CDATA[
arXiv:2506.09600v1 Announce Type: cross 
Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent Factored Generation: Unleashing the Diversity in Your Language Model</title>
<link>https://arxiv.org/abs/2506.09659</link>
<guid>https://arxiv.org/abs/2506.09659</guid>
<content:encoded><![CDATA[
arXiv:2506.09659v1 Announce Type: cross 
Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response. This is problematic because it leads to poor exploration on reasoning problems and to unengaging, repetitive conversational agents. To address this we propose Intent Factored Generation (IFG), factorising the sampling process into two stages. First, we sample a semantically dense intent, e.g., a summary or keywords. Second, we sample the final response conditioning on both the original prompt and the intent from the first stage. This allows us to use a higher temperature during the intent step to promote conceptual diversity, and a lower temperature during the final generation to ensure the outputs are coherent and self-consistent. Additionally, we find that prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step is beneficial for reasoning tasks. We demonstrate our method's effectiveness across a diverse set of tasks. We show this method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. For instruction-tuning, we combine IFG with Direct Preference Optimisation to increase conversational diversity without sacrificing reward. Finally, we achieve higher diversity while maintaining the quality of generations on a general language modelling task, using a new dataset of reader comments and news articles that we collect and open-source. In summary, we present a simple method of increasing the sample diversity of LLMs while maintaining performance. This method can be implemented by changing the prompt and varying the temperature during generation, making it easy to integrate into many algorithms for gains across various applications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adding simple structure at inference improves Vision-Language Compositionality</title>
<link>https://arxiv.org/abs/2506.09691</link>
<guid>https://arxiv.org/abs/2506.09691</guid>
<content:encoded><![CDATA[
arXiv:2506.09691v1 Announce Type: cross 
Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for image-text retrieval tasks. However, those models struggle with compositionality, showing a bag-of-words-like behavior that limits their retrieval performance. Many different training approaches have been proposed to improve the vision-language compositionality capabilities of those models. In comparison, inference-time techniques have received little attention. In this paper, we propose to add simple structure at inference, where, given an image and a caption: i) we divide the image into different smaller crops, ii) we extract text segments, capturing objects, attributes and relations, iii) using a VLM, we find the image crops that better align with text segments obtaining matches, and iv) we compute the final image-text similarity aggregating the individual similarities of the matches. Based on various popular dual encoder VLMs, we evaluate our approach in controlled and natural datasets for VL compositionality. We find that our approach consistently improves the performance of evaluated VLMs without any training, which shows the potential of inference-time techniques. The results are especially good for attribute-object binding as shown in the controlled dataset. As a result of an extensive analysis: i) we show that processing image crops is actually essential for the observed gains in performance, and ii) we identify specific areas to further improve inference-time approaches.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements</title>
<link>https://arxiv.org/abs/2506.09707</link>
<guid>https://arxiv.org/abs/2506.09707</guid>
<content:encoded><![CDATA[
arXiv:2506.09707v1 Announce Type: cross 
Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements -- identifying their start and stop times -- directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3) -- are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 313 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularizing Learnable Feature Extraction for Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2506.09804</link>
<guid>https://arxiv.org/abs/2506.09804</guid>
<content:encoded><![CDATA[
arXiv:2506.09804v1 Announce Type: cross 
Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature extraction pipelines for automatic speech recognition (ASR) systems since they can be directly trained to fit the acoustic model. However, their performance often falls short compared to classical methods, which we show is largely due to their increased susceptibility to overfitting. This work therefore investigates regularization methods for training ASR models with learnable feature extraction front-ends. First, we examine audio perturbation methods and show that larger relative improvements can be obtained for learnable features. Additionally, we identify two limitations in the standard use of SpecAugment for these front-ends and propose masking in the short time Fourier transform (STFT)-domain as a simple but effective modification to address these challenges. Finally, integrating both regularization approaches effectively closes the performance gap between traditional and learnable features.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets</title>
<link>https://arxiv.org/abs/2506.09851</link>
<guid>https://arxiv.org/abs/2506.09851</guid>
<content:encoded><![CDATA[
arXiv:2506.09851v1 Announce Type: cross 
Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to Bangladeshi Taka (BDT), plays a pivotal role in global financial markets, influencing trade, investments, and economic stability. This study leverages historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo Finance, to develop advanced machine learning models for accurate forecasting. A Long Short-Term Memory (LSTM) neural network is employed, achieving an exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and a test loss of 0.8523, significantly outperforming traditional methods like ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is applied for directional prediction, with backtesting on a $10,000 initial capital revealing a 40.82% profitable trade rate, though resulting in a net loss of $20,653.25 over 49 trades. The study analyzes historical trends, showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates normalized daily returns to capture volatility. These findings highlight the potential of deep learning in forex forecasting, offering traders and policymakers robust tools to mitigate risks. Future work could integrate sentiment analysis and real-time economic indicators to further enhance model adaptability in volatile markets.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos</title>
<link>https://arxiv.org/abs/2506.09953</link>
<guid>https://arxiv.org/abs/2506.09953</guid>
<content:encoded><![CDATA[
arXiv:2506.09953v1 Announce Type: cross 
Abstract: In outside knowledge visual question answering (OK-VQA), the model must identify relevant visual information within an image and incorporate external knowledge to accurately respond to a question. Extending this task to a visually grounded dialogue setting based on videos, a conversational model must both recognize pertinent visual details over time and answer questions where the required information is not necessarily present in the visual information. Moreover, the context of the overall conversation must be considered for the subsequent dialogue. To explore this task, we introduce a dataset comprised of $2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$ interleaved dialogue turns. While the dialogue context is visually grounded in specific video segments, the questions further require external knowledge that is not visually present. Thus, the model not only has to identify relevant video parts but also leverage external knowledge to converse within the dialogue. We further provide several baselines evaluated on our dataset and show future challenges associated with this task. The dataset is made publicly available here: https://github.com/c-patsch/OKCV.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling</title>
<link>https://arxiv.org/abs/2506.09998</link>
<guid>https://arxiv.org/abs/2506.09998</guid>
<content:encoded><![CDATA[
arXiv:2506.09998v1 Announce Type: cross 
Abstract: Large language models (LLMs) can often accurately describe probability distributions using natural language, yet they still struggle to generate faithful samples from them. This mismatch limits their use in tasks requiring reliable stochasticity, such as Monte Carlo methods, agent-based simulations, and randomized decision-making. We investigate this gap between knowledge and sampling in the context of Bernoulli distributions. We introduce Verbalized Rejection Sampling (VRS), a natural-language adaptation of classical rejection sampling that prompts the LLM to reason about and accept or reject proposed samples. Despite relying on the same Bernoulli mechanism internally, VRS substantially reduces sampling bias across models. We provide theoretical analysis showing that, under mild assumptions, VRS improves over direct sampling, with gains attributable to both the algorithm and prompt design. More broadly, our results show how classical probabilistic tools can be verbalized and embedded into LLM workflows to improve reliability, without requiring access to model internals or heavy prompt engineering.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes</title>
<link>https://arxiv.org/abs/2305.14725</link>
<guid>https://arxiv.org/abs/2305.14725</guid>
<content:encoded><![CDATA[
arXiv:2305.14725v2 Announce Type: replace 
Abstract: We propose attribute-aware multimodal entity linking, where the input consists of a mention described with a text paragraph and images, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also accompanied by a text description, visual images, and a collection of attributes that present the meta-information of the entity in a structured format. To facilitate this research endeavor, we construct AMELI, encompassing a new multimodal entity linking benchmark dataset that contains 16,735 mentions described in text and associated with 30,472 images, and a multimodal knowledge base that covers 34,690 entities along with 177,873 entity images and 798,216 attributes. To establish baseline performance on AMELI, we experiment with several state-of-the-art architectures for multimodal entity linking and further propose a new approach that incorporates attributes of entities into disambiguation. Experimental results and extensive qualitative analysis demonstrate that extracting and understanding the attributes of mentions from their text descriptions and visual images play a vital role in multimodal entity linking. To the best of our knowledge, we are the first to integrate attributes in the multimodal entity linking task. The programs, model checkpoints, and the dataset are publicly available at https://github.com/VT-NLP/Ameli.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing</title>
<link>https://arxiv.org/abs/2402.16733</link>
<guid>https://arxiv.org/abs/2402.16733</guid>
<content:encoded><![CDATA[
arXiv:2402.16733v3 Announce Type: replace 
Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring with 48.9K samples in total. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 2.3K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. DREsS will enable further research to provide a more accurate and practical AES system for EFL writing education.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation</title>
<link>https://arxiv.org/abs/2404.01129</link>
<guid>https://arxiv.org/abs/2404.01129</guid>
<content:encoded><![CDATA[
arXiv:2404.01129v4 Announce Type: replace 
Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention, yet remains challenging due to the complexity of assessing response appropriateness. Traditional evaluation metrics, typically trained with true positive and randomly selected negative responses, tend to assign higher scores to responses that share greater content similarity with contexts. However, adversarial negative responses, despite possessing high lexical overlap with contexts, can be semantically incongruous. Consequently, existing metrics struggle to effectively evaluate such responses, resulting in low correlations with human judgments. While recent studies have demonstrated the effectiveness of Large Language Models (LLMs) for open-domain dialogue evaluation, they still face challenges in handling adversarial negative examples. We propose a novel evaluation framework that integrates Abstract Meaning Representation (AMR) enhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly incorporate AMR graph information through a gating mechanism for enhanced semantic representation learning, while both SLM predictions and AMR knowledge are integrated into LLM prompts for robust evaluation. Extensive experiments on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to state-of-the-art baselines. Our comprehensive ablation studies reveal that AMR graph information contributes substantially more to performance improvements. Our framework achieves strong correlations with human judgments across multiple datasets, establishing a new benchmark for dialogue evaluation. Our code and data are publicly available.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR</title>
<link>https://arxiv.org/abs/2405.14259</link>
<guid>https://arxiv.org/abs/2405.14259</guid>
<content:encoded><![CDATA[
arXiv:2405.14259v4 Announce Type: replace 
Abstract: We propose "Generative Fusion Decoding" (GFD), a novel shallow fusion framework designed to integrate large language models (LLMs) into cross-modal text recognition systems for automatic speech recognition (ASR) and optical character recognition (OCR). We derive the necessary formulations to enable GFD to operate across mismatched token spaces of different models by calculating likelihood at the byte level, thereby enabling seamless fusion and synchronous progression during the decoding process. GFD is plug-and-play by design, making it readily compatible with various auto-regressive models without the need for any re-training. GFD proves effective for general ASR and OCR tasks through intermediate and frequent interactions with LLMs, surpassing cascaded methods in English and Mandarin benchmarks. In addition, GFD transfers in-context learning abilities of LLMs and allows for adaptive ASR in instruction-aware and long-context settings, yielding significant WER reductions of up to 17.7\%.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Resist Alignment: Evidence From Data Compression</title>
<link>https://arxiv.org/abs/2406.06144</link>
<guid>https://arxiv.org/abs/2406.06144</guid>
<content:encoded><![CDATA[
arXiv:2406.06144v4 Announce Type: replace 
Abstract: Large language models (LLMs) may exhibit unintended or undesirable behaviors. Recent works have concentrated on aligning LLMs to mitigate harmful outputs. Despite these efforts, some anomalies indicate that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Does alignment fine-tuning yield have robust effects on models, or are its impacts merely superficial? In this work, we make the first exploration of this phenomenon from both theoretical and empirical perspectives. Empirically, we demonstrate the $\mathbf{elasticity}$ of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Leveraging compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude. We validate the presence of elasticity through experiments on models of varying types and scales. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. Furthermore, we further reveal that elasticity positively correlates with the increased model size and the expansion of pre-training data. Our findings underscore the need to address the inherent elasticity of LLMs to mitigate their resistance to alignment. The model weight and code are available at pku-lm-resist-alignment.github.io.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard Language Ideology in AI-Generated Language</title>
<link>https://arxiv.org/abs/2406.08726</link>
<guid>https://arxiv.org/abs/2406.08726</guid>
<content:encoded><![CDATA[
arXiv:2406.08726v2 Announce Type: replace 
Abstract: Standard language ideology is reflected and reinforced in language generated by large language models (LLMs). We present a faceted taxonomy of open problems that illustrate how standard language ideology manifests in AI-generated language, alongside implications for minoritized language communities and society more broadly. We introduce the concept of standard AI-generated language ideology, a process through which LLMs position "standard" languages--particularly Standard American English (SAE)--as the linguistic default, reinforcing the perception that SAE is the most "appropriate" language. We then discuss ongoing tensions around what constitutes desirable system behavior, as well as advantages and drawbacks of generative AI tools attempting, or refusing, to imitate different English language varieties. Rather than prescribing narrow technical fixes, we offer three recommendations for researchers, practitioners, and funders that focus on shifting structural conditions and supporting more emancipatory outcomes for diverse language communities.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing</title>
<link>https://arxiv.org/abs/2406.14230</link>
<guid>https://arxiv.org/abs/2406.14230</guid>
<content:encoded><![CDATA[
arXiv:2406.14230v5 Announce Type: replace 
Abstract: Warning: Contains harmful model outputs. Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges. Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA's evaluation results are more consistent with models' performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaLMQA: Exploring culturally specific long-form question answering across 23 languages</title>
<link>https://arxiv.org/abs/2406.17761</link>
<guid>https://arxiv.org/abs/2406.17761</guid>
<content:encoded><![CDATA[
arXiv:2406.17761v3 Announce Type: replace 
Abstract: Despite rising global usage of large language models (LLMs), their ability to generate long-form answers to culturally specific questions remains unexplored in many languages. To fill this gap, we perform the first study of textual multilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally specific questions across 23 different languages. We define culturally specific questions as those that refer to concepts unique to one or a few cultures, or have different answers depending on the cultural or regional context. We obtain these questions by crawling naturally-occurring questions from community web forums in high-resource languages, and by hiring native speakers to write questions in under-resourced, rarely-studied languages such as Fijian and Kirundi. Our data collection methodologies are translation-free, enabling the collection of culturally unique questions like "Kuber iki umwami wa mbere w'uburundi yitwa Ntare?" (Kirundi; English translation: "Why was the first king of Burundi called Ntare (Lion)?"). We evaluate factuality, relevance and surface-level quality of LLM-generated long-form answers, finding that (1) for many languages, even the best models make critical surface-level errors (e.g., answering in the wrong language, repetition), especially for low-resource languages; and (2) answers to culturally specific questions contain more factual errors than answers to culturally agnostic questions -- questions that have consistent meaning and answer across many cultures. We release CaLMQA to facilitate future research in cultural and multilingual long-form QA.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses</title>
<link>https://arxiv.org/abs/2407.13329</link>
<guid>https://arxiv.org/abs/2407.13329</guid>
<content:encoded><![CDATA[
arXiv:2407.13329v3 Announce Type: replace 
Abstract: Understanding the motivations underlying scholarly citations is essential to evaluate research impact and pro-mote transparent scholarly communication. This study introduces CiteFusion, an ensemble framework designed to address the multi-class Citation Intent Classification task on two benchmark datasets: SciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the multi-class task into class-specific binary sub-tasks, leveraging complementary pairs of SciBERT and XLNet models, independently tuned, for each citation intent. The outputs of these base models are aggregated through a feedforward neural network meta-classifier to reconstruct the original classification task. To enhance interpretability, SHAP (SHapley Additive exPlanations) is employed to analyze token-level contributions, and interactions among base models, providing transparency into the classification dynamics of CiteFusion, and insights about the kind of misclassifications of the ensem-ble. In addition, this work investigates the semantic role of structural context by incorporating section titles, as framing devices, into input sentences, assessing their positive impact on classification accuracy. CiteFusion ul-timately demonstrates robust performance in imbalanced and data-scarce scenarios: experimental results show that CiteFusion achieves state-of-the-art performance, with Macro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to ensure interoperability and reusability, citation intents from both datasets sche-mas are mapped to Citation Typing Ontology (CiTO) object properties, highlighting some overlaps. Finally, we describe and release a web-based application that classifies citation intents leveraging the CiteFusion models developed on SciCite.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMREC: LLM Based Multi-Modal Recommender System</title>
<link>https://arxiv.org/abs/2408.04211</link>
<guid>https://arxiv.org/abs/2408.04211</guid>
<content:encoded><![CDATA[
arXiv:2408.04211v2 Announce Type: replace 
Abstract: The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. This surge in content presents unique challenges for designing effective recommender systems. Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences. This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model. Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information. This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogProber: Disentangling confidence from contamination in LLM responses</title>
<link>https://arxiv.org/abs/2408.14352</link>
<guid>https://arxiv.org/abs/2408.14352</guid>
<content:encoded><![CDATA[
arXiv:2408.14352v2 Announce Type: replace 
Abstract: In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical.In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic</title>
<link>https://arxiv.org/abs/2408.16326</link>
<guid>https://arxiv.org/abs/2408.16326</guid>
<content:encoded><![CDATA[
arXiv:2408.16326v3 Announce Type: replace 
Abstract: Self-critic has become a crucial mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts for intuitive instance-level feedback, which resembles System-1 processes and limits the reasoning capabilities. Moreover, there is a lack of in-depth investigations into the relationship between LLM's ability to criticize and its task-solving performance. To address these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability. Through a step-wise CoT reasoning paradigm and the automatic construction of distant-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement, thereby improving their reasoning abilities. Experiments on GSM8K and MATH demonstrate that our enhanced model significantly boosts task-solving performance by filtering out invalid solutions or iterative refinement. Furthermore, we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models</title>
<link>https://arxiv.org/abs/2409.00598</link>
<guid>https://arxiv.org/abs/2409.00598</guid>
<content:encoded><![CDATA[
arXiv:2409.00598v2 Announce Type: replace 
Abstract: Safety-aligned large language models (LLMs) sometimes falsely refuse pseudo-harmful prompts, like "how to kill a mosquito," which are actually harmless. Frequent false refusals not only frustrate users but also provoke a public backlash against the very values alignment seeks to protect. In this paper, we propose the first method to auto-generate diverse, content-controlled, and model-dependent pseudo-harmful prompts. Using this method, we construct an evaluation dataset called PHTest, which is ten times larger than existing datasets, covers more false refusal patterns, and separately labels controversial prompts. We evaluate 20 LLMs on PHTest, uncovering new insights due to its scale and labeling. Our findings reveal a trade-off between minimizing false refusals and improving safety against jailbreak attacks. Moreover, we show that many jailbreak defenses significantly increase the false refusal rates, thereby undermining usability. Our method and dataset can help developers evaluate and fine-tune safer and more usable LLMs. Our code and dataset are available at https://github.com/umd-huang-lab/FalseRefusal
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: Multiple Observers Spotting AI Content</title>
<link>https://arxiv.org/abs/2409.07615</link>
<guid>https://arxiv.org/abs/2409.07615</guid>
<content:encoded><![CDATA[
arXiv:2409.07615v3 Announce Type: replace 
Abstract: The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities, has made it easier for all to produce harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a binary classification problem. Early approaches evaluate an input document with a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. More recent systems instead consider two LLMs and compare their probability distributions over the document to further discriminate when perplexity alone cannot. However, using a fixed pair of models can induce brittleness in performance. We extend these approaches to the ensembling of several LLMs and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, conducted with various generator LLMs, indicate that this approach effectively leverages the strengths of each model, resulting in robust detection performance across multiple domains. Our code and data are available at https://github.com/BaggerOfWords/MOSAIC .
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining word embeddings with perfect fidelity: Case study in research impact prediction</title>
<link>https://arxiv.org/abs/2409.15912</link>
<guid>https://arxiv.org/abs/2409.15912</guid>
<content:encoded><![CDATA[
arXiv:2409.15912v2 Announce Type: replace 
Abstract: Best performing approaches for scholarly document quality prediction are based on embedding models, which do not allow direct explanation of classifiers as distinct words no longer correspond to the input features for model training. Although model-agnostic explanation methods such as Local interpretable model-agnostic explanations (LIME) can be applied, these produce results with questionable correspondence to the ML model. We introduce a new feature importance method, Self-model Rated Entities (SMER), for logistic regression-based classification models trained on word embeddings. We show that SMER has theoretically perfect fidelity with the explained model, as its prediction corresponds exactly to the average of predictions for individual words in the text. SMER allows us to reliably determine which words or entities positively contribute to predicting impactful articles. Quantitative and qualitative evaluation is performed through five diverse experiments conducted on 50.000 research papers from the CORD-19 corpus. Through an AOPC curve analysis, we experimentally demonstrate that SMER produces better explanations than LIME for logistic regression.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment</title>
<link>https://arxiv.org/abs/2410.08193</link>
<guid>https://arxiv.org/abs/2410.08193</guid>
<content:encoded><![CDATA[
arXiv:2410.08193v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining. Our project page is available at: https://genarm.github.io.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Multilingual Language Models Remember Facts?</title>
<link>https://arxiv.org/abs/2410.14387</link>
<guid>https://arxiv.org/abs/2410.14387</guid>
<content:encoded><![CDATA[
arXiv:2410.14387v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has only focused on English monolingual models. The question of how these mechanisms generalize to non-English languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of three multilingual LLMs. First, we show that previously identified recall mechanisms in English largely apply to multilingual contexts, with nuances based on language and architecture. Next, through patching intermediate representations, we localize the role of language during recall, finding that subject enrichment is language-independent, while object extraction is language-dependent. Additionally, we discover that the last token representation acts as a Function Vector (FV), encoding both the language of the query and the content to be extracted from the subject. Furthermore, in decoder-only LLMs, FVs compose these two pieces of information in two separate stages. These insights reveal unique mechanisms in multilingual LLMs for recalling information, highlighting the need for new methodologies -- such as knowledge evaluation, fact editing, and knowledge acquisition -- that are specifically tailored for multilingual LLMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2410.17131</link>
<guid>https://arxiv.org/abs/2410.17131</guid>
<content:encoded><![CDATA[
arXiv:2410.17131v2 Announce Type: replace 
Abstract: The key to effective alignment lies in high-quality preference data. Recent research has focused on automated alignment, which involves developing alignment systems with minimal human intervention. However, prior research has predominantly focused on developing data generation methods, while insufficient attention has been paid to quality control mechanisms, which often produce inaccurate and unhelpful data, leading to unpredictable benefits during iterative optimization. In this paper, we present Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference data, eliminating manual annotation requirements. $SSO$ employs a specialized optimization objective to build a data generator from the policy model itself, which is used to produce accurate and on-policy data. We demonstrate $SSO$'s effectiveness through comprehensive experiments on two series of models: Llama 3 and Qwen 2. Our evaluation across diverse benchmarks shows that $SSO$ consistently outperforms baselines in human preference alignment and reward optimization. Further analysis validates $SSO$ as a scalable framework for preference optimization, benefiting the advancement in automated alignment techniques.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code-Switching Curriculum Learning for Multilingual Transfer in LLMs</title>
<link>https://arxiv.org/abs/2411.02460</link>
<guid>https://arxiv.org/abs/2411.02460</guid>
<content:encoded><![CDATA[
arXiv:2411.02460v2 Announce Type: replace 
Abstract: Large language models (LLMs) now exhibit near human-level performance in various tasks, but their performance drops drastically after a handful of high-resource languages due to the imbalance in pre-training data. Inspired by the human process of second language acquisition, particularly code-switching$\unicode{x2014}$the practice of language alternation in a conversation$\unicode{x2014}$we propose code-switching curriculum learning (CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of human language learning by progressively training models with a curriculum consisting of 1) token-level code-switching, 2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as our underlying model, we demonstrate the efficacy of the CSCL in improving language transfer to Korean, achieving significant performance gains compared to monolingual continual pre-training methods. Ablation studies reveal that both token- and sentence-level code-switching significantly enhance cross-lingual transfer and that curriculum learning amplifies these effects. We also extend our findings into various languages, including Japanese (high-resource) and Indonesian (low-resource), and using two additional models (Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious correlations between language resources and safety alignment, presenting a robust, efficient framework for more equitable language transfer in LLMs. We observe that CSCL is effective for low-resource settings where high-quality, monolingual corpora for language transfer are hardly available.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization</title>
<link>https://arxiv.org/abs/2411.12768</link>
<guid>https://arxiv.org/abs/2411.12768</guid>
<content:encoded><![CDATA[
arXiv:2411.12768v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are vulnerable to backdoor attacks that manipulate outputs via hidden triggers. Existing defense methods--designed for vision/text classification tasks--fail for text generation. We propose Internal Consistency Regularization (CROW), a defense leveraging the observation that backdoored models exhibit unstable layer-wise hidden representations when triggered, while clean models show smooth transitions. CROW enforces consistency across layers via adversarial perturbations and regularization during finetuning, neutralizing backdoors without requiring clean reference models or trigger knowledge--only a small clean dataset. Experiments across Llama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's effectiveness: it achieves significant reductions in attack success rates across diverse backdoor strategies (sentiment steering, targeted refusal, code injection) while preserving generative performance. CROW's architecture-agnostic design enables practical deployment.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning</title>
<link>https://arxiv.org/abs/2411.17304</link>
<guid>https://arxiv.org/abs/2411.17304</guid>
<content:encoded><![CDATA[
arXiv:2411.17304v2 Announce Type: replace 
Abstract: This paper introduces a novel method, referred to as "hashing", which involves masking potentially bias-inducing words in large language models (LLMs) with hash-like meaningless identifiers to reduce cognitive biases and reliance on external knowledge. The method was tested across three sets of experiments involving a total of 490 prompts. Statistical analysis using chi-square tests showed significant improvements in all tested scenarios, which covered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first experiment, hashing decreased the fallacy rate in a modified version of the "Linda" problem aimed at evaluating susceptibility to cognitive biases. In the second experiment, it improved LLM results on the frequent itemset extraction task. In the third experiment, we found hashing is also effective when the Linda problem is presented in a tabular format rather than text, indicating that the technique works across various input representations. Overall, the method was shown to improve bias reduction and incorporation of external knowledge. Despite bias reduction, hallucination rates were inconsistently reduced across types of LLM models. These findings suggest that masking bias-inducing terms can improve LLM performance, although its effectiveness is model- and task-dependent.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrofitting Large Language Models with Dynamic Tokenization</title>
<link>https://arxiv.org/abs/2411.18553</link>
<guid>https://arxiv.org/abs/2411.18553</guid>
<content:encoded><![CDATA[
arXiv:2411.18553v3 Announce Type: replace 
Abstract: Current language models (LMs) use a fixed, static subword tokenizer. This default choice typically results in degraded efficiency and language capabilities, especially in languages other than English. To address this issue, we challenge the static design and propose retrofitting LMs with dynamic tokenization: a way to dynamically decide on token boundaries based on the input text via a subword-merging algorithm inspired by byte-pair encoding. We merge frequent subword sequences in a batch, then apply a pre-trained embedding-prediction hypernetwork to compute the token embeddings on-the-fly. For encoder-style models (e.g., XLM-R), this on average reduces token sequence lengths by >20% across 14 languages while degrading performance by less than 2%. The same method applied to pre-filling and scoring in decoder-style models (e.g., Mistral-7B) results in minimal performance degradation at up to 17% reduction in sequence length. Overall, we find that dynamic tokenization can mitigate the limitations of static tokenization by substantially improving inference speed and promoting fairness across languages, enabling more equitable and adaptable LMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steps are all you need: Rethinking STEM Education with Prompt Engineering</title>
<link>https://arxiv.org/abs/2412.05023</link>
<guid>https://arxiv.org/abs/2412.05023</guid>
<content:encoded><![CDATA[
arXiv:2412.05023v3 Announce Type: replace 
Abstract: Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination. By utilizing a Mixture of Experts (MoE) Model, along with analogical prompting, we are able to show improved model performance when compared to the baseline on standard LLMs. We also survey the limits of these prompting techniques and the effects they have on model performance. Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation</title>
<link>https://arxiv.org/abs/2412.05342</link>
<guid>https://arxiv.org/abs/2412.05342</guid>
<content:encoded><![CDATA[
arXiv:2412.05342v5 Announce Type: replace 
Abstract: Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering</title>
<link>https://arxiv.org/abs/2412.05453</link>
<guid>https://arxiv.org/abs/2412.05453</guid>
<content:encoded><![CDATA[
arXiv:2412.05453v3 Announce Type: replace 
Abstract: This study explores the effectiveness of using knowledge graphs generated by large language models to decompose high school-level physics questions into sub-questions. We introduce a pipeline aimed at enhancing model response quality for Question Answering tasks. By employing LLMs to construct knowledge graphs that capture the internal logic of the questions, these graphs then guide the generation of subquestions. We hypothesize that this method yields sub-questions that are more logically consistent with the original questions compared to traditional decomposition techniques. Our results show that sub-questions derived from knowledge graphs exhibit significantly improved fidelity to the original question's logic. This approach not only enhances the learning experience by providing clearer and more contextually appropriate sub-questions but also highlights the potential of LLMs to transform educational methodologies. The findings indicate a promising direction for applying AI to improve the quality and effectiveness of educational content.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement</title>
<link>https://arxiv.org/abs/2412.06845</link>
<guid>https://arxiv.org/abs/2412.06845</guid>
<content:encoded><![CDATA[
arXiv:2412.06845v5 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin Reasoning model. Moreover, we develop our vision language model based on our Moxin model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Irony Detection, Reasoning and Understanding in Zero-shot Learning</title>
<link>https://arxiv.org/abs/2501.16884</link>
<guid>https://arxiv.org/abs/2501.16884</guid>
<content:encoded><![CDATA[
arXiv:2501.16884v2 Announce Type: replace 
Abstract: The generalisation of irony detection faces significant challenges, leading to substantial performance deviations when detection models are applied to diverse real-world scenarios. In this study, we find that irony-focused prompts, as generated from our IDADP framework for LLMs, can not only overcome dataset-specific limitations but also generate coherent, human-readable reasoning, transforming ironic text into its intended meaning. Based on our findings and in-depth analysis, we identify several promising directions for future research aimed at enhancing LLMs' zero-shot capabilities in irony detection, reasoning, and comprehension. These include advancing contextual awareness in irony detection, exploring hybrid symbolic-neural methods, and integrating multimodal data, among others.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2502.16033</link>
<guid>https://arxiv.org/abs/2502.16033</guid>
<content:encoded><![CDATA[
arXiv:2502.16033v3 Announce Type: replace 
Abstract: Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting pairwise inconsistencies but struggle with inconsistencies confined to single elements in complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation</title>
<link>https://arxiv.org/abs/2502.19830</link>
<guid>https://arxiv.org/abs/2502.19830</guid>
<content:encoded><![CDATA[
arXiv:2502.19830v2 Announce Type: replace 
Abstract: Self-consistency improves reasoning by aggregating diverse stochastic samples, yet the dynamics behind its efficacy remain underexplored. We reframe self-consistency as a dynamic distributional alignment problem, revealing that decoding temperature not only governs sampling randomness but also actively shapes the latent answer distribution. Given that high temperatures require prohibitively large sample sizes to stabilize, while low temperatures risk amplifying biases, we propose a confidence-driven mechanism that dynamically calibrates temperature: sharpening the sampling distribution under uncertainty to align with high-probability modes, and promoting exploration when confidence is high. Experiments on mathematical reasoning tasks show this approach outperforms fixed-diversity baselines under limited samples, improving both average and best-case performance across varying initial temperatures without additional data or modules. This establishes self-consistency as a synchronization challenge between sampling dynamics and evolving answer distributions.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification</title>
<link>https://arxiv.org/abs/2503.01940</link>
<guid>https://arxiv.org/abs/2503.01940</guid>
<content:encoded><![CDATA[
arXiv:2503.01940v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in tool learning. In real-world scenarios, user queries are often ambiguous and incomplete, requiring effective clarification. However, existing interactive clarification approaches face two critical limitations: reliance on manually constructed datasets, which inherently constrains training data scale and diversity, and lack of error correction mechanisms during multi-turn clarification, leading to error accumulation that compromises both accuracy and efficiency. We present AskToAct, which addresses these challenges by exploiting the structural mapping between queries and their tool invocation solutions. Our key insight is that tool parameters naturally represent explicit user intents. By systematically removing key parameters from queries while retaining them as ground truth, we enable automated construction of high-quality training data. We further enhance model robustness through error-correction pairs and selective masking, enabling dynamic error detection during clarification interactions. Comprehensive experiments demonstrate that AskToAct significantly outperforms existing approaches, achieving above 57% accuracy in recovering critical unspecified intents and enhancing clarification efficiency by an average of 10.46% while maintaining high accuracy in tool invocation. Our framework exhibits robust performance across different model architectures and successfully generalizes to entirely unseen APIs without additional training, achieving performance comparable to GPT-4o with substantially fewer computational resources.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization</title>
<link>https://arxiv.org/abs/2503.02450</link>
<guid>https://arxiv.org/abs/2503.02450</guid>
<content:encoded><![CDATA[
arXiv:2503.02450v3 Announce Type: replace 
Abstract: Personalizing Large Language Models (LLMs) has become a critical step in facilitating their widespread application to enhance individual life experiences. In pursuit of personalization, distilling key preference information from an individual's historical data as instructional preference context to customize LLM generation has emerged as a promising direction. However, these methods face a fundamental limitation by overlooking the inter-user comparative analysis, which is essential for identifying the inter-user differences that truly shape preferences. To address this limitation, we propose Difference-aware Personalization Learning (DPL), a novel approach that emphasizes extracting inter-user differences to enhance LLM personalization. DPL strategically selects representative users for comparison and establishes a structured standard to extract meaningful, task-relevant differences for customizing LLM generation. Extensive experiments on real-world datasets demonstrate that DPL significantly enhances LLM personalization. We release our code at https://github.com/SnowCharmQ/DPL.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering</title>
<link>https://arxiv.org/abs/2503.11314</link>
<guid>https://arxiv.org/abs/2503.11314</guid>
<content:encoded><![CDATA[
arXiv:2503.11314v2 Announce Type: replace 
Abstract: Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLoRE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLoRE in both in-domain and cross-domain scenarios.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering</title>
<link>https://arxiv.org/abs/2503.18491</link>
<guid>https://arxiv.org/abs/2503.18491</guid>
<content:encoded><![CDATA[
arXiv:2503.18491v3 Announce Type: replace 
Abstract: Visual Question Answering (VQA) requires reasoning across visual and textual modalities, yet Large Vision-Language Models (LVLMs) often lack integrated commonsense knowledge, limiting their robustness in real-world scenarios. To address this, we introduce MAGIC-VQA, a novel framework that enhances VQA by systematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs a three-stage process: (1) Explicit Knowledge Integration from external sources, (2) By-Type Post-Processing for contextual refinement, and (3) Implicit Knowledge Augmentation using a Graph Neural Network (GNN) for structured reasoning. While GNNs bring greater depth to structured inference, they enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key gap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating the need for extensive pre-training or complex prompt tuning. Our framework achieves state-of-the-art performance on benchmark datasets, significantly improving commonsense reasoning in VQA.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style over Substance: Distilled Language Models Reason Via Stylistic Replication</title>
<link>https://arxiv.org/abs/2504.01738</link>
<guid>https://arxiv.org/abs/2504.01738</guid>
<content:encoded><![CDATA[
arXiv:2504.01738v2 Announce Type: replace 
Abstract: Specialized reasoning language models (RLMs) have demonstrated that scaling test-time computation through detailed reasoning traces significantly enhances performance. Although these traces effectively facilitate knowledge distillation into smaller, instruction-tuned models, the precise nature of transferred reasoning remains unclear. In this study, we investigate to what extent distilled models internalize replicated stylistic patterns during reasoning. To this end, we systematically analyze reasoning traces, identifying structural and lexical patterns that characterize successful reasoning. We then introduce two new datasets -- a dataset of emergent reasoning traces and a synthetic dataset explicitly constructed to replicate these stylistic patterns -- to precisely examine their influence on distilled models' reasoning capabilities. We find that models trained on the synthetic traces achieve comparable performance, indicating that distilled reasoning abilities rely significantly on surface-level patterns. Surprisingly, we observe an increase in performance even when the synthetic traces are altered to lead to the wrong answer. Our findings highlight how stylistic patterns can be leveraged to efficiently enhance LM reasoning across diverse model families.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title>
<link>https://arxiv.org/abs/2504.02132</link>
<guid>https://arxiv.org/abs/2504.02132</guid>
<content:encoded><![CDATA[
arXiv:2504.02132v2 Announce Type: replace 
Abstract: Multi-modal retrieval augmented generation (M-RAG) is instrumental for inhibiting hallucinations in large multi-modal models (LMMs) through the use of a factual knowledge base (KB). However, M-RAG introduces new attack vectors for adversaries that aim to disrupt the system by injecting malicious entries into the KB. In this paper, we present the first poisoning attack against M-RAG targeting visual document retrieval applications where the KB contains images of document pages. We propose two attacks, each of which require injecting only a single adversarial image into the KB. Firstly, we propose a universal attack that, for any potential user query, influences the response to cause a denial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted attack against one or a group of user queries, with the goal of spreading targeted misinformation. For both attacks, we use a multi-objective gradient-based adversarial approach to craft the injected image while optimizing for both retrieval and generation. We evaluate our attacks against several visual document retrieval datasets, a diverse set of state-of-the-art retrievers (embedding models) and generators (LMMs), demonstrating the attack effectiveness in both the universal and targeted settings. We additionally present results including commonly used defenses, various attack hyper-parameter settings, ablations, and attack transferability.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessment of Evolving Large Language Models in Upper Secondary Mathematics</title>
<link>https://arxiv.org/abs/2504.12347</link>
<guid>https://arxiv.org/abs/2504.12347</guid>
<content:encoded><![CDATA[
arXiv:2504.12347v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown increasing promise in educational settings, yet their mathematical reasoning has been considered evolving. This study evaluates the mathematical capabilities of various LLMs using the Finnish matriculation examination, a high-stakes digital test for upper secondary education. Initial tests yielded moderate performance corresponding to mid-range grades, but later evaluations demonstrated substantial improvements as the language models evolved. Remarkably, some models achieved near-perfect or perfect scores, matching top student performance and qualifying for university admission. Our findings highlight the rapid advances in the mathematical proficiency of LLMs and illustrate their potential as underlying tools to support learning and teaching in a variety of ways.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment</title>
<link>https://arxiv.org/abs/2504.12663</link>
<guid>https://arxiv.org/abs/2504.12663</guid>
<content:encoded><![CDATA[
arXiv:2504.12663v2 Announce Type: replace 
Abstract: Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convert Language Model into a Value-based Strategic Planner</title>
<link>https://arxiv.org/abs/2505.06987</link>
<guid>https://arxiv.org/abs/2505.06987</guid>
<content:encoded><![CDATA[
arXiv:2505.06987v2 Announce Type: replace 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective</title>
<link>https://arxiv.org/abs/2505.07859</link>
<guid>https://arxiv.org/abs/2505.07859</guid>
<content:encoded><![CDATA[
arXiv:2505.07859v2 Announce Type: replace 
Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware (we assume a price of 36ct/hour for a Nvidia 4090 GPU).
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecIF: Improving Instruction-Following through Meta-Decomposition</title>
<link>https://arxiv.org/abs/2505.13990</link>
<guid>https://arxiv.org/abs/2505.13990</guid>
<content:encoded><![CDATA[
arXiv:2505.13990v2 Announce Type: replace 
Abstract: Instruction-following has emerged as a crucial capability for large language models (LLMs). However, existing approaches often rely on pre-existing documents or external resources to synthesize instruction-following data, which limits their flexibility and generalizability. In this paper, we introduce DecIF, a fully autonomous, meta-decomposition guided framework that generates diverse and high-quality instruction-following data using only LLMs. DecIF is grounded in the principle of decomposition. For instruction generation, we guide LLMs to iteratively produce various types of meta-information, which are then combined with response constraints to form well-structured and semantically rich instructions. We further utilize LLMs to detect and resolve potential inconsistencies within the generated instructions. Regarding response generation, we decompose each instruction into atomic-level evaluation criteria, enabling rigorous validation and the elimination of inaccurate instruction-response pairs. Extensive experiments across a wide range of scenarios and settings demonstrate DecIF's superior performance on instruction-following tasks. Further analysis highlights its strong flexibility, scalability, and generalizability in automatically synthesizing high-quality instruction data.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFEBench: Evaluating Length Instruction Following in Large Language Models</title>
<link>https://arxiv.org/abs/2505.16234</link>
<guid>https://arxiv.org/abs/2505.16234</guid>
<content:encoded><![CDATA[
arXiv:2505.16234v2 Announce Type: replace 
Abstract: While large language models (LLMs) can solve PhD-level reasoning problems over long context inputs, they still struggle with a seemingly simpler task: following explicit length instructions-e.g., write a 10,000-word novel. Additionally, models often generate far too short outputs, terminate prematurely, or even refuse the request. Existing benchmarks focus primarily on evaluating generations quality, but often overlook whether the generations meet length constraints. To this end, we introduce Length Instruction Following Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to follow length instructions across diverse tasks and a wide range of specified lengths. LIFEBench consists of 10,800 instances across 4 task categories in both English and Chinese, covering length constraints ranging from 16 to 8192 words. We evaluate 26 widely-used LLMs and find that most models reasonably follow short-length instructions but deteriorate sharply beyond a certain threshold. Surprisingly, almost all models fail to reach the vendor-claimed maximum output lengths in practice, as further confirmed by our evaluations extending up to 32K words. Even long-context LLMs, despite their extended input-output windows, counterintuitively fail to improve length-instructions following. Notably, Reasoning LLMs outperform even specialized long-text generation models, achieving state-of-the-art length following. Overall, LIFEBench uncovers fundamental limitations in current LLMs' length instructions following ability, offering critical insights for future progress.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Forbidden Topics in Language Models</title>
<link>https://arxiv.org/abs/2505.17441</link>
<guid>https://arxiv.org/abs/2505.17441</guid>
<content:encoded><![CDATA[
arXiv:2505.17441v3 Announce Type: replace 
Abstract: Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses token prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawler to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis</title>
<link>https://arxiv.org/abs/2505.24593</link>
<guid>https://arxiv.org/abs/2505.24593</guid>
<content:encoded><![CDATA[
arXiv:2505.24593v2 Announce Type: replace 
Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards</title>
<link>https://arxiv.org/abs/2506.00103</link>
<guid>https://arxiv.org/abs/2506.00103</guid>
<content:encoded><![CDATA[
arXiv:2506.00103v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has enabled large language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks with objective ground-truth answers, such as mathematics and code generation. However, a significant gap remains for non-verifiable tasks, like creative writing and open-ended dialogue, where quality assessment is inherently subjective and lacks definitive references. Existing approaches for these domains often rely on scalar reward models trained with human preferences, which suffer from limited generalization and are prone to reward hacking, such as over-explanation and length bias. In this work, we propose a unified RLVR-based training paradigm that bridges the gap between non-verifiable tasks and verifiable rewards. We introduce a writing-principle-based pairwise Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy Optimization (BRPO) algorithm. The pairwise writing GenRM leverages self-principled critique to transform subjective assessments into reliable, verifiable rewards, while BRPO enables dynamic, reference-free pairwise comparison by leveraging a bootstrapped response as temporary reference from within group rollouts during RL training. Our approach empowers LLMs to develop robust writing capabilities without supervised fine-tuning, as demonstrated by Writing-Zero, which shows consistent improvement and strong resistance to reward hacking compared to scalar reward baselines. Furthermore, our method achieves competitive results on both in-house and open-source writing benchmarks. Our findings suggest the potential to unify rule-based, reference-based, and reference-free reward modeling under the RLVR framework, thus paving the way for a comprehensive and scalable RL training paradigm applicable across all language tasks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech</title>
<link>https://arxiv.org/abs/2506.00628</link>
<guid>https://arxiv.org/abs/2506.00628</guid>
<content:encoded><![CDATA[
arXiv:2506.00628v2 Announce Type: replace 
Abstract: Prior research indicates that LID model performance significantly declines on accented speech; however, the specific causes, extent, and characterization of these errors remain under-explored. (i) We identify a common failure mode on accented speech whereby LID systems often misclassify L2 accented speech as the speaker's native language or a related language. (ii) We present evidence suggesting that state-of-the-art models are invariant to permutations of short spans of speech, implying they classify on the basis of short phonotactic features indicative of accent rather than language. Our analysis reveals a simple method to enhance model robustness to accents through input chunking. (iii) We present an approach that integrates sequence-level information into our model without relying on monolingual ASR systems; this reduces accent-language confusion and significantly enhances performance on accented speech while maintaining comparable results on standard LID.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StochasTok: Improving Fine-Grained Subword Understanding in LLMs</title>
<link>https://arxiv.org/abs/2506.01687</link>
<guid>https://arxiv.org/abs/2506.01687</guid>
<content:encoded><![CDATA[
arXiv:2506.01687v2 Announce Type: replace 
Abstract: Subword-level understanding is integral to numerous tasks, including understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming, and wordplay. Despite this, current large language models (LLMs) still often struggle with seemingly simple subword-level tasks like How many 'r's in 'strawberry'?. A key factor behind these failures is tokenization which obscures the fine-grained structure of words. Current alternatives, such as character-level and dropout tokenization methods, significantly increase computational costs and provide inconsistent improvements. In this paper we revisit tokenization and introduce StochasTok, a simple, efficient stochastic tokenization scheme that randomly splits tokens during training, allowing LLMs to 'see' their internal structure. Our experiments show that pretraining with StochasTok substantially improves LLMs' downstream performance across multiple subword-level language games, including character counting, substring identification, and math tasks. Furthermore, StochasTok's simplicity allows seamless integration at any stage of the training pipeline; and we demonstrate that post-training with StochasTok can instill improved subword understanding into existing pretrained models, thus avoiding costly pretraining from scratch. These dramatic improvements achieved with a minimal change suggest StochasTok holds exciting potential when applied to larger, more capable models. Code open-sourced at: https://github.com/anyasims/stochastok.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.02404</link>
<guid>https://arxiv.org/abs/2506.02404</guid>
<content:encoded><![CDATA[
arXiv:2506.02404v2 Announce Type: replace 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced Auditory Experience</title>
<link>https://arxiv.org/abs/2402.03710</link>
<guid>https://arxiv.org/abs/2402.03710</guid>
<content:encoded><![CDATA[
arXiv:2402.03710v2 Announce Type: replace-cross 
Abstract: In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces "Listen, Chat, and Remix" (LCR), a novel multimodal sound remixer that controls each sound source in a mixture based on user-provided text instructions. LCR distinguishes itself with a user-friendly text interface and its unique ability to remix multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for remixing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles filtered components back to the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse remixing tasks including extraction, removal, and volume control of single or multiple sources. Our experiments demonstrate significant improvements in signal quality across all remixing tasks and robust performance in zero-shot scenarios with varying numbers and types of sound sources. An audio demo is available at: https://listenchatremix.github.io/demo.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Shapley interactions to understand how models use structure</title>
<link>https://arxiv.org/abs/2403.13106</link>
<guid>https://arxiv.org/abs/2403.13106</guid>
<content:encoded><![CDATA[
arXiv:2403.13106v2 Announce Type: replace-cross 
Abstract: Language is an intricately structured system, and a key goal of NLP interpretability is to provide methodological insights for understanding how language models represent this structure internally. In this paper, we use Shapley Taylor interaction indices (STII) in order to examine how language and speech models internally relate and structure their inputs. Pairwise Shapley interactions measure how much two inputs work together to influence model outputs beyond if we linearly added their independent influences, providing a view into how models encode structural interactions between inputs. We relate the interaction patterns in models to three underlying linguistic structures: syntactic structure, non-compositional semantics, and phonetic coarticulation. We find that autoregressive text models encode interactions that correlate with the syntactic proximity of inputs, and that both autoregressive and masked models encode nonlinear interactions in idiomatic phrases with non-compositional semantics. Our speech results show that inputs are more entangled for pairs where a neighboring consonant is likely to influence a vowel or approximant, showing that models encode the phonetic interaction needed for extracting discrete phonemic representations.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative Evolutionary Multitasking</title>
<link>https://arxiv.org/abs/2406.14917</link>
<guid>https://arxiv.org/abs/2406.14917</guid>
<content:encoded><![CDATA[
arXiv:2406.14917v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm (LLM2TEA), the first agentic AI designer within a generative evolutionary multitasking (GEM) framework that promotes the crossover and synergy of designs from multiple domains, leading to innovative solutions that transcend individual disciplines. Of particular interest is the discovery of objects that are not only innovative but also conform to the physical specifications of the real world in science and engineering. LLM2TEA comprises a large language model to initialize a population of genotypes (defined by text prompts) describing the objects of interest, a text-to-3D generative model to produce phenotypes from these prompts, a classifier to interpret the semantic representations of the objects, and a physics simulation model to assess their physical properties. We propose several novel LLM-based multitask evolutionary operators to guide the search toward the discovery of high-performing practical objects. Experimental results in conceptual design optimization validate the effectiveness of LLM2TEA, revealing from 97\% to 174\% improvement in the diversity of innovative objects compared to the present text-to-3D generative model baseline. In addition, more than 73\% of the generated designs have better physical performance than the top 1\% percentile of the designs generated in the baseline. Moreover, LLM2TEA generates designs that are not only aesthetically creative but also functional in real-world applications. Several of these designs have been successfully 3D-printed, emphasizing the proposed approach's capacity to transform AI-generated outputs into tangible physical objects. The designs produced by LLM2TEA meets practical requirements while showcasing creative and innovative features, underscoring its potential applications in complex design optimization and discovery.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding</title>
<link>https://arxiv.org/abs/2406.15481</link>
<guid>https://arxiv.org/abs/2406.15481</guid>
<content:encoded><![CDATA[
arXiv:2406.15481v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) have advanced rapidly, concerns regarding their safety have become prominent. In this paper, we discover that code-switching in red-teaming queries can effectively elicit undesirable behaviors of LLMs, which are common practices in natural language. We introduce a simple yet effective framework, CSRT, to synthesize codeswitching red-teaming queries and investigate the safety and multilingual understanding of LLMs comprehensively. Through extensive experiments with ten state-of-the-art LLMs and code-switching queries combining up to 10 languages, we demonstrate that the CSRT significantly outperforms existing multilingual red-teaming techniques, achieving 46.7% more attacks than standard attacks in English and being effective in conventional safety domains. We also examine the multilingual ability of those LLMs to generate and understand codeswitching texts. Additionally, we validate the extensibility of the CSRT by generating codeswitching attack prompts with monolingual data. We finally conduct detailed ablation studies exploring code-switching and propound unintended correlation between resource availability of languages and safety alignment in existing multilingual LLMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Remarkable Robustness of LLMs: Stages of Inference?</title>
<link>https://arxiv.org/abs/2406.19384</link>
<guid>https://arxiv.org/abs/2406.19384</guid>
<content:encoded><![CDATA[
arXiv:2406.19384v2 Announce Type: replace-cross 
Abstract: We investigate the robustness of Large Language Models (LLMs) to structural interventions by deleting and swapping adjacent layers during inference. Surprisingly, models retain 72-95% of their original top-1 prediction accuracy without any fine-tuning. We find that performance degradation is not uniform across layers: interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers. This pattern of localized sensitivity motivates our hypothesis of four stages of inference, observed across diverse model families and sizes: (1) detokenization, where local context is integrated to lift raw token embeddings into higher-level representations; (2) feature engineering, where task- and entity-specific features are iteratively refined; (3) prediction ensembling, where hidden states are aggregated into plausible next-token predictions; and (4) residual sharpening, where irrelevant features are suppressed to finalize the output distribution. Synthesizing behavioral and mechanistic evidence, we provide a framework for interpreting depth-dependent computations in LLMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AcTracer: Active Testing of Large Language Model via Multi-Stage Sampling</title>
<link>https://arxiv.org/abs/2408.03573</link>
<guid>https://arxiv.org/abs/2408.03573</guid>
<content:encoded><![CDATA[
arXiv:2408.03573v2 Announce Type: replace-cross 
Abstract: Performance evaluation plays a crucial role in the development life cycle of large language models (LLMs). It estimates the model's capability, elucidates behavior characteristics, and facilitates the identification of potential issues and limitations, thereby guiding further improvement. Given that LLMs' diverse task-handling abilities stem from large volumes of training data, a comprehensive evaluation also necessitates abundant, well-annotated, and representative test data to assess LLM performance across various downstream tasks. However, the demand for high-quality test data often entails substantial time, computational resources, and manual efforts, sometimes causing the evaluation to be inefficient or impractical. To address these challenges, researchers propose active testing, which estimates the overall performance by selecting a subset of test data. Nevertheless, the existing active testing methods tend to be inefficient, even inapplicable, given the unique new challenges of LLMs (e.g., diverse task types, increased model complexity, and unavailability of training data). To mitigate such limitations and expedite the development cycle of LLMs, in this work, we introduce AcTracer, an active testing framework tailored for LLMs that strategically selects a small subset of test data to achieve a more accurate performance estimation for LLMs. AcTracer utilizes both internal and external information from LLMs to guide the test sampling process, reducing variance through a multi-stage pool-based active selection. Our experiment results demonstrate that AcTracer achieves state-of-the-art performance compared to existing methods across various tasks.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMMA: Efficient Visual Alignment in Multi-Modal LLMs</title>
<link>https://arxiv.org/abs/2410.02080</link>
<guid>https://arxiv.org/abs/2410.02080</guid>
<content:encoded><![CDATA[
arXiv:2410.02080v2 Announce Type: replace-cross 
Abstract: Multi-modal Large Language Models (MLLMs) have recently exhibited impressive general-purpose capabilities by leveraging vision foundation models to encode the core concepts of images into representations. These are then combined with instructions and processed by the language model to generate high-quality responses. Despite significant progress in enhancing the language component, challenges persist in optimally fusing visual encodings within the language model for task-specific adaptability. Recent research has focused on improving this fusion through modality adaptation modules but at the cost of significantly increased model complexity and training data needs. In this paper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight cross-modality module designed to efficiently fuse visual and textual encodings, generating instruction-aware visual representations for the language model. Our key contributions include: (1) an efficient early fusion mechanism that integrates vision and language representations with minimal added parameters (less than 0.2% increase in model size), (2) an in-depth interpretability analysis that sheds light on the internal mechanisms of the proposed method; (3) comprehensive experiments that demonstrate notable improvements on both specialized and general benchmarks for MLLMs. Empirical results show that EMMA boosts performance across multiple tasks by up to 9.3% while significantly improving robustness against hallucinations. Our code is available at https://github.com/SaraGhazanfari/EMMA
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment</title>
<link>https://arxiv.org/abs/2410.02197</link>
<guid>https://arxiv.org/abs/2410.02197</guid>
<content:encoded><![CDATA[
arXiv:2410.02197v3 Announce Type: replace-cross 
Abstract: Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. In this paper, we introduce preference embedding, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback (RLHF). Experimental results show that our General Preference embedding Model (GPM) consistently outperforms the BT reward model on the RewardBench benchmark and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0, following the language model post-training with GPO and our general preference model, reveal performance improvements over BT models. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning</title>
<link>https://arxiv.org/abs/2411.12977</link>
<guid>https://arxiv.org/abs/2411.12977</guid>
<content:encoded><![CDATA[
arXiv:2411.12977v4 Announce Type: replace-cross 
Abstract: Embodied agents powered by large language models (LLMs), such as Voyager, promise open-ended competence in worlds such as Minecraft. However, when powered by open-weight LLMs they still falter on elementary tasks after domain-specific fine-tuning. We propose MindForge, a generative-agent framework for cultural lifelong learning through explicit perspective taking. We introduce three key innovations: (1) a structured theory of mind representation linking percepts, beliefs, desires, and actions; (2) natural inter-agent communication; and (3) a multi-component memory system. Following the cultural learning framework, we test MindForge in both instructive and collaborative settings within Minecraft. In an instructive setting with GPT-4, MindForge agents powered by open-weight LLMs significantly outperform their Voyager counterparts in basic tasks yielding $3\times$ more tech-tree milestones and collecting $2.3\times$ more unique items than the Voyager baseline. Furthermore, in fully \textit{collaborative} settings, we find that the performance of two underachieving agents improves with more communication rounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate sophisticated behaviors, including expert-novice knowledge transfer, collaborative problem solving, and adaptation to out-of-distribution tasks through accumulated cultural experiences.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey</title>
<link>https://arxiv.org/abs/2412.20367</link>
<guid>https://arxiv.org/abs/2412.20367</guid>
<content:encoded><![CDATA[
arXiv:2412.20367v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing large language models (LLMs) in code generation and optimization. This survey systematically reviews RL-driven techniques across the code development lifecycle, from compiler-level optimizations and resource allocation strategies to end-to-end code synthesis frameworks. We first examine classical and modern RL algorithms -- spanning policy gradients, actor-critic methods, human-feedback alignment, and preference-based optimization -- and their adaptations to the unique challenges of code generation, such as sparse and delayed rewards. Next, we analyze key benchmarks, datasets, and evaluation metrics that drive progress in RL-augmented Code LLMs. Finally, we identify open problems, including the need for richer feedback sources, support for low-level and domain-specific languages, and methods to reduce computational overhead. By consolidating current insights and outlining future directions, this work aims to guide researchers and practitioners in leveraging RL to produce more robust, efficient, and human-aligned code generation systems.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICONS: Influence Consensus for Vision-Language Data Selection</title>
<link>https://arxiv.org/abs/2501.00654</link>
<guid>https://arxiv.org/abs/2501.00654</guid>
<content:encoded><![CDATA[
arXiv:2501.00654v3 Announce Type: replace-cross 
Abstract: Training vision-language models via instruction tuning often relies on large mixtures of data spanning diverse tasks and domains. However, these mixtures frequently include redundant information, increasing computational costs without proportional performance gains, necessitating more effective data selection strategies. Existing methods typically rely on task-agnostic heuristics to estimate data importance or focus on optimizing single tasks in isolation, limiting their effectiveness in multitask settings. In this work, we introduce ICONS, a gradient-based Influence CONsensus approach for vision-language data Selection. Our method leverages first-order training dynamics to estimate the influence of individual training examples on validation performance and aggregates these estimates across tasks via majority voting over task-specific influences. This cross-task consensus identifies data points that are consistently valuable across tasks, enabling us to prioritize examples that drive overall performance. The voting-based design further mitigates issues such as score calibration and outlier sensitivity, resulting in robust and scalable data selection for diverse multitask mixtures. With only 20% of the data from LLaVA-665K and Cambrian-7M, our selected subsets retain 98.6% and 98.8% of the performance achieved with full datasets, and can even surpass full data training at a 60% selection ratio on LLaVA-665K. Our approach also generalizes to unseen tasks and architectures, demonstrating strong transfer. We release two compact, high-utility subsets, LLaVA-ICONS-133K and Cambrian-ICONS-1.4M, preserving impactful training examples for efficient and scalable vision-language model development.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Language Models: A Blueprint</title>
<link>https://arxiv.org/abs/2501.11223</link>
<guid>https://arxiv.org/abs/2501.11223</guid>
<content:encoded><![CDATA[
arXiv:2501.11223v4 Announce Type: replace-cross 
Abstract: Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending LLMs with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining reinforcement learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), supervision schemes (Outcome-Based and Process-Based Supervision), and other related concepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent tools). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we discuss scalable RLM cloud deployments and we outline how RLMs can integrate with a broader LLM ecosystem. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between "rich AI" and "poor AI" by lowering barriers to RLM design and experimentation.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy AI: Safety, Bias, and Privacy -- A Survey</title>
<link>https://arxiv.org/abs/2502.10450</link>
<guid>https://arxiv.org/abs/2502.10450</guid>
<content:encoded><![CDATA[
arXiv:2502.10450v2 Announce Type: replace-cross 
Abstract: The capabilities of artificial intelligence systems have been advancing to a great extent, but these systems still struggle with failure modes, vulnerabilities, and biases. In this paper, we study the current state of the field, and present promising insights and perspectives regarding concerns that challenge the trustworthiness of AI models. In particular, this paper investigates the issues regarding three thrusts: safety, privacy, and bias, which hurt models' trustworthiness. For safety, we discuss safety alignment in the context of large language models, preventing them from generating toxic or harmful content. For bias, we focus on spurious biases that can mislead a network. Lastly, for privacy, we cover membership inference attacks in deep neural networks. The discussions addressed in this paper reflect our own experiments and observations.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Diverse Human Preference Learning through Principal Component Analysis</title>
<link>https://arxiv.org/abs/2502.13131</link>
<guid>https://arxiv.org/abs/2502.13131</guid>
<content:encoded><![CDATA[
arXiv:2502.13131v2 Announce Type: replace-cross 
Abstract: Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment. Our code is available at https://github.com/amandaluof/DRMs.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AAD-LLM: Neural Attention-Driven Auditory Scene Understanding</title>
<link>https://arxiv.org/abs/2502.16794</link>
<guid>https://arxiv.org/abs/2502.16794</guid>
<content:encoded><![CDATA[
arXiv:2502.16794v3 Announce Type: replace-cross 
Abstract: Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.19409</link>
<guid>https://arxiv.org/abs/2502.19409</guid>
<content:encoded><![CDATA[
arXiv:2502.19409v2 Announce Type: replace-cross 
Abstract: Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task -- achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis</title>
<link>https://arxiv.org/abs/2502.20383</link>
<guid>https://arxiv.org/abs/2502.20383</guid>
<content:encoded><![CDATA[
arXiv:2502.20383v2 Announce Type: replace-cross 
Abstract: Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chem42: a Family of chemical Language Models for Target-aware Ligand Generation</title>
<link>https://arxiv.org/abs/2503.16563</link>
<guid>https://arxiv.org/abs/2503.16563</guid>
<content:encoded><![CDATA[
arXiv:2503.16563v2 Announce Type: replace-cross 
Abstract: Revolutionizing drug discovery demands more than just understanding molecular interactions - it requires generative models that can design novel ligands tailored to specific biological targets. While chemical Language Models (cLMs) have made strides in learning molecular properties, most fail to incorporate target-specific insights, restricting their ability to drive de-novo ligand generation. Chem42, a cutting-edge family of generative chemical Language Models, is designed to bridge this gap. By integrating atomic-level interactions with multimodal inputs from Prot42, a complementary protein Language Model, Chem42 achieves a sophisticated cross-modal representation of molecular structures, interactions, and binding patterns. This innovative framework enables the creation of structurally valid, synthetically accessible ligands with enhanced target specificity. Evaluations across diverse protein targets confirm that Chem42 surpasses existing approaches in chemical validity, target-aware design, and predicted binding affinity. By reducing the search space of viable drug candidates, Chem42 could accelerate the drug discovery pipeline, offering a powerful generative AI tool for precision medicine. Our Chem42 models set a new benchmark in molecule property prediction, conditional molecule generation, and target-aware ligand design. The models are publicly available at huggingface.co/inceptionai.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction</title>
<link>https://arxiv.org/abs/2504.15629</link>
<guid>https://arxiv.org/abs/2504.15629</guid>
<content:encoded><![CDATA[
arXiv:2504.15629v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection</title>
<link>https://arxiv.org/abs/2504.17834</link>
<guid>https://arxiv.org/abs/2504.17834</guid>
<content:encoded><![CDATA[
arXiv:2504.17834v3 Announce Type: replace-cross 
Abstract: Spoilers in movie reviews are important on platforms like IMDb and Rotten Tomatoes, offering benefits and drawbacks. They can guide some viewers' choices but also affect those who prefer no plot details in advance, making effective spoiler detection essential. Existing spoiler detection methods mainly analyze review text, often overlooking the impact of movie genres and user bias, limiting their effectiveness. To address this, we analyze movie review data, finding genre-specific variations in spoiler rates and identifying that certain users are more likely to post spoilers. Based on these findings, we introduce a new spoiler detection framework called GUSD (The code is available at https://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler Detection), which incorporates genre-specific data and user behavior bias. User bias is calculated through dynamic graph modeling of review history. Additionally, the R2GFormer module combines RetGAT (Retentive Graph Attention Network) for graph information and GenreFormer for genre-specific aggregation. The GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to specialized experts based on genre. Extensive testing on benchmark datasets shows that GUSD achieves state-of-the-art results. This approach advances spoiler detection by addressing genre and user-specific patterns, enhancing user experience on movie review platforms.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation</title>
<link>https://arxiv.org/abs/2505.23885</link>
<guid>https://arxiv.org/abs/2505.23885</guid>
<content:encoded><![CDATA[
arXiv:2505.23885v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models</title>
<link>https://arxiv.org/abs/2506.02204</link>
<guid>https://arxiv.org/abs/2506.02204</guid>
<content:encoded><![CDATA[
<div> methodology, automated comparison, language models, performance-aware contextual embeddings, BehaviorBox <br />
Summary:
BehaviorBox is a methodology for automated comparison of language models using performance-aware contextual embeddings. It extracts fine-grained features of text where one model outperforms another, such as specific word groups or contexts. The method identifies differences in ease of generation between models, providing insights into where and why one model succeeds over another. BehaviorBox is applied to compare models of different sizes, families, and training methods, revealing meaningful performance differences that are not captured by traditional measures like corpus-level perplexity. This approach enhances language model evaluation by highlighting specific contexts and features that showcase the strengths and weaknesses of different models. <div>
arXiv:2506.02204v2 Announce Type: replace 
Abstract: Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'" and "exclamation marks after emotional statements", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextAtari: 100K Frames Game Playing with Language Agents</title>
<link>https://arxiv.org/abs/2506.04098</link>
<guid>https://arxiv.org/abs/2506.04098</guid>
<content:encoded><![CDATA[
<div> benchmark, language agents, long-horizon, decision-making tasks, Atari games 

Summary:<br /><br />TextAtari is introduced as a benchmark for assessing language agents on extended decision-making tasks, with up to 100,000 steps. It involves translating visual representations of Atari games into text descriptions to test the agents' capabilities at the intersection of sequential decision-making and natural language processing. The benchmark comprises nearly 100 diverse tasks of varying complexity and planning horizons, represented in text form via an unsupervised learning framework, AtariARI. Three large language models are evaluated across different agent frameworks to analyze the impact of prior knowledge on performance in long-horizon challenges. Through various scenarios, the study investigates the influence of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. The findings indicate significant performance disparities between language agents and human players in tasks requiring extensive planning, highlighting difficulties in sequential reasoning, state tracking, and strategic planning over tens of thousands of steps. TextAtari offers standardized evaluation procedures, baseline implementations, and a platform for further research in integrating language models with planning algorithms. <div>
arXiv:2506.04098v2 Announce Type: replace 
Abstract: We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning. Our code is available at https://github.com/Lww007/Text-Atari-Agents.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservative Bias in Large Language Models: Measuring Relation Predictions</title>
<link>https://arxiv.org/abs/2506.08120</link>
<guid>https://arxiv.org/abs/2506.08120</guid>
<content:encoded><![CDATA[
<div> bias, relation extraction, language models, information loss, conservative bias

Summary: 
This article discusses the conservative bias exhibited by Large Language Models (LLMs) in relation extraction tasks. LLMs often default to a No_Relation label when unable to identify an appropriate option, leading to significant information loss. The concept of Hobson's choice is introduced to describe scenarios where models choose safe but uninformative labels over hallucinated ones. The study systematically evaluates this trade-off across various prompts, datasets, and relation types. It is found that conservative bias occurs twice as frequently as hallucination. Semantic similarity analysis using SBERT and LLM prompts helps quantify this effect by comparing conservative bias behaviors in constrained prompts with labels from semi-constrained and open-ended prompts. This research sheds light on the impact of conservative bias on the performance of LLMs in relation extraction tasks. <div>
arXiv:2506.08120v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation extraction tasks, frequently defaulting to No_Relation label when an appropriate option is unavailable. While this behavior helps prevent incorrect relation assignments, our analysis reveals that it also leads to significant information loss when reasoning is not explicitly included in the output. We systematically evaluate this trade-off across multiple prompts, datasets, and relation types, introducing the concept of Hobson's choice to capture scenarios where models opt for safe but uninformative labels over hallucinated ones. Our findings suggest that conservative bias occurs twice as often as hallucination. To quantify this effect, we use SBERT and LLM prompts to capture the semantic similarity between conservative bias behaviors in constrained prompts and labels generated from semi-constrained and open-ended prompts.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA</title>
<link>https://arxiv.org/abs/2506.08123</link>
<guid>https://arxiv.org/abs/2506.08123</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, alignment, reward decomposition, interpretability, AI systems

Summary: 
QA-LIGN introduces an automatic symbolic reward decomposition approach for aligning large language models with explicit principles like helpfulness, honesty, and harmlessness. Instead of collapsing diverse feedback into a single scalar reward, QA-LIGN preserves the structure of each principle within the reward mechanism by deriving separate reward components for each principle based on principle-specific evaluation questions. This approach offers greater transparency and adaptability in the alignment process, making it a drop-in replacement for black-box reward models. Experimental results demonstrate that QA-LIGN achieves performance on par with or better than a DPO baseline when aligning large language models with constitutional principles. The approach improves interpretability and controllability in alignment without sacrificing end-task performance.<br /><br />Summary: <div>
arXiv:2506.08123v1 Announce Type: new 
Abstract: Alignment of large language models with explicit principles (such as helpfulness, honesty, and harmlessness) is crucial for ensuring safe and reliable AI systems. However, standard reward-based alignment methods typically collapse diverse feedback into a single scalar reward, entangling multiple objectives into one opaque training signal, which hinders interpretability. In this work, we introduce QA-LIGN, an automatic symbolic reward decomposition approach that preserves the structure of each constitutional principle within the reward mechanism. Instead of training a black-box reward model that outputs a monolithic score, QA-LIGN formulates principle-specific evaluation questions and derives separate reward components for each principle, making it a drop-in reward model replacement. Experiments aligning an uncensored large language model with a set of constitutional principles demonstrate that QA-LIGN offers greater transparency and adaptability in the alignment process. At the same time, our approach achieves performance on par with or better than a DPO baseline. Overall, these results represent a step toward more interpretable and controllable alignment of language models, achieved without sacrificing end-task performance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments</title>
<link>https://arxiv.org/abs/2506.08136</link>
<guid>https://arxiv.org/abs/2506.08136</guid>
<content:encoded><![CDATA[
<div> benchmark, autonomous agents, economic tasks, web environments, large language models 

Summary: 
EconWebArena introduces a benchmark for assessing autonomous agents on intricate economic tasks in realistic web environments. The benchmark consists of 360 tasks sourced from 82 authoritative websites in areas like macroeconomics and finance. Tasks require agents to navigate websites, interpret content, and extract precise data through multi-step workflows. The benchmark is created through generating candidate tasks with large language models, followed by human curation. Emphasizing fidelity to data sources, EconWebArena focuses on web-based economic reasoning. Evaluating state-of-the-art models as web agents reveals performance gaps and challenges in grounding, navigation, and multimodal understanding. Ablation studies assess the impact of visual grounding, plan-based reasoning, and interaction design. EconWebArena serves as a robust testbed for economic web intelligence. 

Summary: <div>
arXiv:2506.08136v1 Announce Type: new 
Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on complex, multimodal economic tasks in realistic web environments. The benchmark comprises 360 curated tasks from 82 authoritative websites spanning domains such as macroeconomics, labor, finance, trade, and public policy. Each task challenges agents to navigate live websites, interpret structured and visual content, interact with real interfaces, and extract precise, time-sensitive data through multi-step workflows. We construct the benchmark by prompting multiple large language models (LLMs) to generate candidate tasks, followed by rigorous human curation to ensure clarity, feasibility, and source reliability. Unlike prior work, EconWebArena emphasizes fidelity to authoritative data sources and the need for grounded web-based economic reasoning. We evaluate a diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure cases, and conduct ablation studies to assess the impact of visual grounding, plan-based reasoning, and interaction design. Our results reveal substantial performance gaps and highlight persistent challenges in grounding, navigation, and multimodal understanding, positioning EconWebArena as a rigorous testbed for economic web intelligence.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models</title>
<link>https://arxiv.org/abs/2506.08147</link>
<guid>https://arxiv.org/abs/2506.08147</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, social media, multilingual dataset, attention layers, transformer-based models

Summary:
- The study focuses on hate speech detection on social media platforms in English, Urdu, and Spanish languages, using a trilingual dataset of tweets.
- The dataset consists of 10,193 tweets with balanced labels of Hateful and Not-Hateful, collected through keyword filtering.
- Attention layers are utilized as a precursor to transformer-based models and large language models to enhance feature extraction for multilingual hate speech detection.
- State-of-the-art models like GPT-3.5 Turbo and Qwen 2.5 72B, along with traditional machine learning models, were benchmarked on the dataset, showing significant improvement in detection accuracy.
- The approach integrates attention layers with the mentioned models, achieving strong performance with high macro F1 scores in English, Spanish, Urdu, and the joint multilingual model. 

<br /><br />Summary: <div>
arXiv:2506.08147v1 Announce Type: new 
Abstract: Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding</title>
<link>https://arxiv.org/abs/2506.08158</link>
<guid>https://arxiv.org/abs/2506.08158</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Knowledge Graph Embedding, Efficiency, Task-driven, Tokens, Transfer learning

Summary:
Continual Knowledge Graph Embedding (CKGE) aims to update knowledge graphs while retaining existing information. The new method, ETT-CKGE, uses task-driven tokens to improve efficiency and effectiveness by capturing task-specific signals and eliminating the need for manual scoring or traversal. These tokens enable seamless transfer of knowledge between snapshots through simple matrix operations, reducing training time and memory usage significantly. ETT-CKGE outperforms existing CKGE methods on predictive performance while enhancing scalability and training efficiency. The code for ETT-CKGE is available on GitHub for further exploration and application.<br /><br />Summary: <div>
arXiv:2506.08158v1 Announce Type: new 
Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge while preserving past information. However, existing methods struggle with efficiency and scalability due to two key limitations: (1) suboptimal knowledge preservation between snapshots caused by manually designed node/relation importance scores that ignore graph dependencies relevant to the downstream task, and (2) computationally expensive graph traversal for node/relation importance calculation, leading to slow training and high memory overhead. To address these limitations, we introduce ETT-CKGE (Efficient, Task-driven, Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE method that leverages efficient task-driven tokens for efficient and effective knowledge transfer between snapshots. Our method introduces a set of learnable tokens that directly capture task-relevant signals, eliminating the need for explicit node scoring or traversal. These tokens serve as consistent and reusable guidance across snapshots, enabling efficient token-masked embedding alignment between snapshots. Importantly, knowledge transfer is achieved through simple matrix operations, significantly reducing training time and memory usage. Extensive experiments across six benchmark datasets demonstrate that ETT-CKGE consistently achieves superior or competitive predictive performance, while substantially improving training efficiency and scalability compared to state-of-the-art CKGE methods. The code is available at: https://github.com/lijingzhu1/ETT-CKGE/tree/main
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction</title>
<link>https://arxiv.org/abs/2506.08172</link>
<guid>https://arxiv.org/abs/2506.08172</guid>
<content:encoded><![CDATA[
<div> Keywords: automated story writing, AI-generated microfictions, evaluation protocol, literary theory, aesthetic quality

Summary: 
Automated story writing has advanced with large language models capable of generating coherent short fiction texts. However, evaluating AI-generated microfictions for literary merit, especially aesthetic qualities, remains a challenge. This paper introduces GrAImes, an evaluation protocol grounded in literary theory, focused on thematic coherence, textual clarity, interpretive depth, and aesthetic quality of AI-generated microfictions. The protocol was validated using feedback from literature experts and literary enthusiasts, providing an objective framework for assessing the literary value of automated microfictions. This protocol fills a gap in the field by offering a structured approach to evaluate the quality of AI-generated narratives, considering various aspects of the text beyond just linguistic coherence.<br /><br />Summary: <div>
arXiv:2506.08172v1 Announce Type: new 
Abstract: Automated story writing has been a subject of study for over 60 years. Large language models can generate narratively consistent and linguistically coherent short fiction texts. Despite these advancements, rigorous assessment of such outputs for literary merit - especially concerning aesthetic qualities - has received scant attention. In this paper, we address the challenge of evaluating AI-generated microfictions and argue that this task requires consideration of literary criteria across various aspects of the text, such as thematic coherence, textual clarity, interpretive depth, and aesthetic quality. To facilitate this, we present GrAImes: an evaluation protocol grounded in literary theory, specifically drawing from a literary perspective, to offer an objective framework for assessing AI-generated microfiction. Furthermore, we report the results of our validation of the evaluation protocol, as answered by both literature experts and literary enthusiasts. This protocol will serve as a foundation for evaluating automatically generated microfictions and assessing their literary value.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding</title>
<link>https://arxiv.org/abs/2506.08174</link>
<guid>https://arxiv.org/abs/2506.08174</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-BT, back-translation, terminology standardization, multilingual consistency, semantic alignment

Summary: LLM-BT is a framework leveraging large language models to automate terminology verification and standardization in rapidly evolving fields like AI and quantum computing. It achieves high term consistency across models, with over 90% exact or semantic matches. The novel workflow integrates serial and parallel back-translation routes, ensuring strong cross-lingual robustness. Back-translation is viewed as dynamic semantic embedding, providing transparent path-based embeddings shaped by model evolution. LLM-BT transforms back-translation into an active engine for multilingual terminology standardization, facilitating human-AI collaboration where machines ensure semantic fidelity and humans guide cultural interpretation. This infrastructure supports comprehensive terminology governance in scientific and technological domains globally. 

<br /><br />Summary: LLM-BT proposes a framework for automated terminology standardization in fast-evolving technical fields, achieving high consistency and robustness through cross-lingual semantic alignment. The innovative workflow enables efficient back-translation paths and dynamic semantic embeddings, fostering human-AI collaboration for effective multilingual terminology governance. <div>
arXiv:2506.08174v1 Announce Type: new 
Abstract: The rapid growth of English technical terms challenges traditional expert-driven standardization, especially in fast-evolving fields like AI and quantum computing. Manual methods struggle to ensure multilingual consistency. We propose \textbf{LLM-BT}, a back-translation framework powered by large language models (LLMs) to automate terminology verification and standardization via cross-lingual semantic alignment. Our contributions are: \textbf{(1) Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate language $\rightarrow$ English back-translation, LLM-BT achieves high term consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize'' pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw $\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese $\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%). \textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as dynamic semantic embedding, revealing latent meaning trajectories. Unlike static embeddings, LLM-BT provides transparent path-based embeddings shaped by model evolution. LLM-BT transforms back-translation into an active engine for multilingual terminology standardization, enabling human--AI collaboration: machines ensure semantic fidelity, humans guide cultural interpretation. This infrastructure supports terminology governance across scientific and technological fields worldwide.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length</title>
<link>https://arxiv.org/abs/2506.08184</link>
<guid>https://arxiv.org/abs/2506.08184</guid>
<content:encoded><![CDATA[
<div> interference, retrieval, language models, working memory, information manipulation 
Summary: 
- Information retrieval in Large Language Models (LLMs) is not just about lookup but involves generation capabilities. 
- Longer contexts are assumed to enhance retrieval, but intra-context interference is a significant issue that affects accuracy. 
- The study adapts the proactive interference (PI) paradigm from cognitive science to investigate how earlier information can disrupt recall in LLMs. 
- PI-LLM evaluation shows a log-linear decline in retrieval accuracy towards zero as interference accumulates, leading to errors in retrieving previously overwritten values. 
- Prompt engineering to mitigate interference is only partially successful, indicating a working memory bottleneck in LLMs' ability to disentangle and manipulate information. 
<br /><br />Summary: <div>
arXiv:2506.08184v1 Announce Type: new 
Abstract: Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing</title>
<link>https://arxiv.org/abs/2506.08221</link>
<guid>https://arxiv.org/abs/2506.08221</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, feedback, writing process data, student writing, digital writing tool 

Summary:
Large language models (LLMs) like Gemini are being utilized in student writing support, but their feedback often lacks information on how the text was written. This study investigates the use of writing process data, collected through keystroke logging and periodic snapshots, to enhance LLM feedback by capturing how learners think and revise during writing. A digital writing tool was developed to track students' typing and essay evolution. Twenty students used this tool to write timed essays, with evaluations conducted based on LLM feedback incorporating writing traces and post-task surveys on feedback usefulness. Results show that learners preferred process-aware LLM feedback, finding it more aligned with their thinking. Certain edits, such as adding new content or reorganizing paragraphs, were linked to higher scores in coherence and elaboration. The study suggests that integrating writing process awareness into LLMs can improve feedback quality, making it more meaningful, personal, and supportive. 

<br /><br />Summary: 
- LLM feedback often lacks insight into writing process
- Writing process data enhances LLM feedback quality
- Digital tool captures students' thinking and revisions 
- Learners prefer process-aware LLM feedback
- Certain edits correlate with higher coherence and elaboration scores <div>
arXiv:2506.08221v1 Announce Type: new 
Abstract: Large language models(LLMs) like Gemini are becoming common tools for supporting student writing. But most of their feedback is based only on the final essay missing important context about how that text was written. In this paper, we explore whether using writing process data, collected through keystroke logging and periodic snapshots, can help LLMs give feedback that better reflects how learners think and revise while writing. We built a digital writing tool that captures both what students type and how their essays evolve over time. Twenty students used this tool to write timed essays, which were then evaluated in two ways: (i) LLM generated feedback using both the final essay and the full writing trace, and (ii) After the task, students completed surveys about how useful and relatable they found the feedback. Early results show that learners preferred the process-aware LLM feedback, finding it more in tune with their own thinking. We also found that certain types of edits, like adding new content or reorganizing paragraphs, aligned closely with higher scores in areas like coherence and elaboration. Our findings suggest that making LLMs more aware of the writing process can lead to feedback that feels more meaningful, personal, and supportive.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.08234</link>
<guid>https://arxiv.org/abs/2506.08234</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, compound AI systems, optimization, natural language feedback, non-differentiable systems

Summary: 
Recent advancements in large language models (LLMs) and AI systems have paved the way for complex compound AI systems capable of sophisticated tasks. Optimizing these systems involves not only individual components but also their interactions. Traditional methods such as supervised fine-tuning and reinforcement learning are foundational, while natural language feedback offers promising approaches for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, covering numerical and language-based techniques. The formalization of compound AI system optimization, classification of existing methods, and identification of open research challenges and future directions are discussed. The surveyed papers are available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2506.08234v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning</title>
<link>https://arxiv.org/abs/2506.08235</link>
<guid>https://arxiv.org/abs/2506.08235</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, scientific claim-evidence extraction, scientific comprehension, GPT-4

Summary:
CLAIM-BENCH is introduced as a benchmark for evaluating large language models' (LLMs) abilities in scientific claim-evidence extraction and validation. The study compares three approaches across six diverse LLMs, highlighting model-specific strengths and weaknesses. Results indicate limitations in LLMs' processing of complex scientific content, with closed-source models like GPT-4 outperforming open-source counterparts in precision and recall. Strategic prompting approaches improve LLMs' accuracy in linking evidence with claims, albeit with increased computational costs. The study sets a new standard for evaluating scientific comprehension in LLMs, providing insights for building systems capable of deeper reasoning across research papers. <br /><br />Summary: <div>
arXiv:2506.08235v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Generation of Inference Making Questions for Reading Comprehension Assessments</title>
<link>https://arxiv.org/abs/2506.08260</link>
<guid>https://arxiv.org/abs/2506.08260</guid>
<content:encoded><![CDATA[
<div> Inference types, reading comprehension, diagnostic questions, automatic item generation, GPT-4o <br />
<br />
Summary: In this study, the authors delve into the complexities of inference making in reading comprehension (RC) and the importance of diagnostic questions for educational practices. They introduce a taxonomy of inference types for RC and analyze the distribution of items in a diagnostic RC item bank. Experiments using GPT-4o to generate bridging-inference RC items show promising results, with high-quality questions suitable for grade 3-12 contexts. However, only 42.6% of generated questions accurately matched the targeted inference type. The study highlights the potential of combining automatic item generation with human judgment to create scalable and high-quality diagnostic RC assessments. <div>
arXiv:2506.08260v1 Announce Type: new 
Abstract: Inference making is an essential but complex skill in reading comprehension (RC). Some inferences require resolving references across sentences, and some rely on using prior knowledge to fill in the detail that is not explicitly written in the text. Diagnostic RC questions can help educators provide more effective and targeted reading instruction and interventions for school-age students. We introduce a taxonomy of inference types for RC and use it to analyze the distribution of items within a diagnostic RC item bank. Next, we present experiments using GPT-4o to generate bridging-inference RC items for given reading passages via few-shot prompting, comparing conditions with and without chain-of-thought prompts. Generated items were evaluated on three aspects: overall item quality, appropriate inference type, and LLM reasoning, achieving high inter-rater agreements above 0.90. Our results show that GPT-4o produced 93.8% good-quality questions suitable for operational use in grade 3-12 contexts; however, only 42.6% of the generated questions accurately matched the targeted inference type. We conclude that combining automatic item generation with human judgment offers a promising path toward scalable, high-quality diagnostic RC assessments.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability</title>
<link>https://arxiv.org/abs/2506.08300</link>
<guid>https://arxiv.org/abs/2506.08300</guid>
<content:encoded><![CDATA[
<div> keywords: Large language models, training data, public domain books, Harvard Library, dataset extraction <br />
Summary: 
The article discusses the importance of high-quality datasets for training large language models (LLMs) and the need for sustainable data stewardship practices. It introduces Institutional Books 1.0, a dataset of historic texts extracted from public domain books digitized by Harvard Library in collaboration with the Google Books project. The dataset contains over 250 billion tokens from 1,075,899 volumes in 250 languages. The release includes OCR-extracted text and metadata for 983,004 volumes in the public domain. The report details the project's goals, methods, and analyses to enhance accessibility and usability for both humans and machines. <br /><br />Summary: <div>
arXiv:2506.08300v1 Announce Type: new 
Abstract: Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency</title>
<link>https://arxiv.org/abs/2506.08343</link>
<guid>https://arxiv.org/abs/2506.08343</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, self-reflection, multimodal reasoning, efficiency, utility

Summary: NoWait is a new approach that aims to improve efficiency in reasoning models by removing explicit self-reflection cues like "Wait" and "Hmm". The study evaluates the impact of explicit self-reflection on advanced reasoning and introduces a method to suppress these cues during inference. Results from experiments on various reasoning tasks demonstrate that NoWait significantly reduces the length of the chain-of-thought trajectory without sacrificing model utility. This approach offers a simple and effective solution for enhancing efficiency in multimodal reasoning processes, particularly in scenarios where overthinking and redundancy are common pitfalls. The findings suggest that disabling explicit self-reflection cues can lead to more streamlined and effective reasoning outcomes, highlighting the importance of considering the role of self-reflection in optimizing complex reasoning processes. <br /><br />Summary: <div>
arXiv:2506.08343v1 Announce Type: new 
Abstract: Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving</title>
<link>https://arxiv.org/abs/2506.08349</link>
<guid>https://arxiv.org/abs/2506.08349</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, medical, language models, cognitive levels  
Summary:  
- A new evaluation framework for assessing Large Language Models (LLMs) in the medical domain is proposed, inspired by Bloom's Taxonomy. 
- The framework includes tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. 
- State-of-the-art general and medical LLMs from six prominent families were systematically evaluated using this framework. 
- Performance of the evaluated models shows a significant decline as cognitive complexity increases, with model size playing a crucial role at higher cognitive levels. 
- The study emphasizes the need to improve LLMs' medical capabilities at higher cognitive levels and offers insights for developing LLMs suitable for real-world medical applications. 

<br /><br />Summary: <div>
arXiv:2506.08349v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning</title>
<link>https://arxiv.org/abs/2506.08354</link>
<guid>https://arxiv.org/abs/2506.08354</guid>
<content:encoded><![CDATA[
<div> semantic modeling, text embedding, linguistic theory, implicit meaning, NLP

Summary: 
The paper argues for a shift in the text embedding research community towards prioritizing implicit semantics as a central modeling goal. While current models excel at capturing surface-level meaning, they struggle with tasks that require interpretive reasoning, speaker stance, and social meaning. The authors emphasize that meaning is often implicit and influenced by pragmatics, speaker intent, and sociocultural context, highlighting the need for more linguistically grounded training data and benchmarks that evaluate deeper semantic understanding. A pilot study demonstrates that even state-of-the-art models perform only marginally better than simplistic baselines on implicit semantics tasks. The paper calls for embedding research to explicitly prioritize implicit meaning as a core modeling objective to better align embeddings with the complexities of real-world language. <div>
arXiv:2506.08354v1 Announce Type: new 
Abstract: This position paper argues that the text embedding research community should move beyond surface meaning and embrace implicit semantics as a central modeling goal. Text embedding models have become foundational in modern NLP, powering a wide range of applications and drawing increasing research attention. Yet, much of this progress remains narrowly focused on surface-level semantics. In contrast, linguistic theory emphasizes that meaning is often implicit, shaped by pragmatics, speaker intent, and sociocultural context. Current embedding models are typically trained on data that lacks such depth and evaluated on benchmarks that reward the capture of surface meaning. As a result, they struggle with tasks requiring interpretive reasoning, speaker stance, or social meaning. Our pilot study highlights this gap, showing that even state-of-the-art models perform only marginally better than simplistic baselines on implicit semantics tasks. To address this, we call for a paradigm shift: embedding research should prioritize more diverse and linguistically grounded training data, design benchmarks that evaluate deeper semantic understanding, and explicitly frame implicit meaning as a core modeling objective, better aligning embeddings with real-world language complexity.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEAL: Disentangling Transformer Head Activations for LLM Steering</title>
<link>https://arxiv.org/abs/2506.08359</link>
<guid>https://arxiv.org/abs/2506.08359</guid>
<content:encoded><![CDATA[
<div> transformers, language models, attention heads, inference-time steering, behavioral relevance

Summary:<br />
- The study focuses on inference-time steering of large language models (LLMs) by identifying behavior-relevant attention heads in transformers.
- A causal-attribution framework is proposed using vector-quantized autoencoders (VQ-AE) to separate behavior-relevant and behavior-irrelevant subspaces in attention heads.
- The relevance of each head is assessed based on discriminative capacity using a binary classification metric, guiding selection and weighting.
- Experiments on seven LLMs demonstrate improved accuracy in inference-time interventions, particularly in truthfulness-steering tasks.
- Selected heads show strong generalization in cross-domain truthfulness-steering scenarios.<br /><br /> <div>
arXiv:2506.08359v1 Announce Type: new 
Abstract: Inference-time steering aims to alter the response characteristics of large language models (LLMs) without modifying their underlying parameters. A critical step in this process is the identification of internal modules within LLMs that are associated with the target behavior. However, current approaches to module selection often depend on superficial cues or ad-hoc heuristics, which can result in suboptimal or unintended outcomes. In this work, we propose a principled causal-attribution framework for identifying behavior-relevant attention heads in transformers. For each head, we train a vector-quantized autoencoder (VQ-AE) on its attention activations, partitioning the latent space into behavior-relevant and behavior-irrelevant subspaces, each quantized with a shared learnable codebook. We assess the behavioral relevance of each head by quantifying the separability of VQ-AE encodings for behavior-aligned versus behavior-violating responses using a binary classification metric. This yields a behavioral relevance score that reflects each head discriminative capacity with respect to the target behavior, guiding both selection and importance weighting. Experiments on seven LLMs from two model families and five behavioral steering datasets demonstrate that our method enables more accurate inference-time interventions, achieving superior performance on the truthfulness-steering task. Furthermore, the heads selected by our approach exhibit strong zero-shot generalization in cross-domain truthfulness-steering scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs</title>
<link>https://arxiv.org/abs/2506.08364</link>
<guid>https://arxiv.org/abs/2506.08364</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Causal-Chain RAG, Directed Acyclic Graph, structured inference

Summary:<br /><br />Large Language Models (LLMs) struggle to understand cause and effect relationships in specialized domains. The Causal-Chain RAG (CC-RAG) approach integrates triple extraction and graph chaining to enable structured multi-hop inference. It constructs a Directed Acyclic Graph (DAG) of triples to guide answer generation. Experiments in Bitcoin price fluctuations and Gaucher disease domains show CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity. Both LLM-as-a-Judge and human evaluations prefer CC-RAG. Explicitly modeling causal structure allows LLMs to generate more accurate and interpretable responses in specialized domains where flat retrieval falls short. <br /><br />Summary: <div>
arXiv:2506.08364v1 Announce Type: new 
Abstract: Understanding cause and effect relationships remains a formidable challenge for Large Language Models (LLMs), particularly in specialized domains where reasoning requires more than surface-level correlations. Retrieval-Augmented Generation (RAG) improves factual accuracy, but standard RAG pipelines treat evidence as flat context, lacking the structure required to model true causal dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that integrates zero-shot triple extraction and theme-aware graph chaining into the RAG pipeline, enabling structured multi-hop inference. Given a domain specific corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of  triples and uses forward/backward chaining to guide structured answer generation. Experiments on two real-world domains: Bitcoin price fluctuations and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity. Both LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results demonstrate that explicitly modeling causal structure enables LLMs to generate more accurate and interpretable responses, especially in specialized domains where flat retrieval fails.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding</title>
<link>https://arxiv.org/abs/2506.08371</link>
<guid>https://arxiv.org/abs/2506.08371</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Posterior Salience Attenuation, Positional Contrastive Decoding, long-context benchmarks, attention score degradation

Summary: 
Large Language Models (LLMs) face performance degradation within long contexts, and current solutions are costly and underexplored. The Posterior Salience Attenuation (PSA) phenomenon shows a correlation between salience ratio and performance degradation. Despite this attenuation, gold tokens remain important in decoding. Positional Contrastive Decoding (PCD) is a training-free approach that contrasts long-aware attention with local-aware attention to focus on gains from training. PCD effectively alleviates attention score degradation in long-term decay simulation. Experimental results demonstrate that PCD achieves state-of-the-art performance on long-context benchmarks.<br /><br />Summary: <div>
arXiv:2506.08371v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) support long contexts, they struggle with performance degradation within the context window. Current solutions incur prohibitive training costs, leaving statistical behaviors and cost-effective approaches underexplored. From the decoding perspective, we identify the Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio correlates with long-text performance degradation. Notably, despite the attenuation, gold tokens still occupy high-ranking positions in the decoding space. Motivated by it, we propose the training-free Positional Contrastive Decoding (PCD) that contrasts the logits derived from long-aware attention with those from designed local-aware attention, enabling the model to focus on the gains introduced by large-scale short-to-long training. Through the analysis of long-term decay simulation, we demonstrate that PCD effectively alleviates attention score degradation. Experimental results show that PCD achieves state-of-the-art performance on long-context benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Draft-based Approximate Inference for LLMs</title>
<link>https://arxiv.org/abs/2506.08373</link>
<guid>https://arxiv.org/abs/2506.08373</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, approximate inference, draft models, SpecKV, SpecPC

Summary:
The paper introduces a novel framework for approximate inference in Large Language Models (LLMs) that utilizes draft models to better assess token and key-value pair importance. Two specific methods are proposed within this framework: SpecKV for accurate assessment of key-value pair importance and SpecPC for identifying and discarding unimportant prompt tokens. This approach extends the utility of draft models beyond traditional speculative decoding. The theoretical and empirical analyses demonstrate a strong correlation between the attention patterns of draft and target models. Experimental results on long-context benchmarks show that the proposed methods achieve higher accuracy compared to existing baselines while maintaining improvements in memory usage, latency, and throughput. The code for this work is publicly available on GitHub at https://github.com/furiosa-ai/draft-based-approx-llm.<br /><br />Summary:Keywords: LLM, approximate inference, draft models, SpecKV, SpecPC. Introducing a new framework for approximate inference in Large Language Models using draft models to assess token and key-value pair importance accurately. Two methods, SpecKV and SpecPC, are proposed. Theoretical and empirical analyses show a strong correlation between attention patterns of draft and target models. Experimental results exhibit improved accuracy, memory usage, latency, and throughput compared to existing methods. Code available on GitHub. <div>
arXiv:2506.08373v1 Announce Type: new 
Abstract: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2506.08375</link>
<guid>https://arxiv.org/abs/2506.08375</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, complex instruction following, multi-task scenarios, constraints, Segment Policy Optimization algorithm 

Summary: 
The article introduces a new benchmark called EIFBENCH designed to evaluate the capabilities of large language models (LLMs) in complex multi-task scenarios with various constraints. The benchmark aims to provide a more realistic assessment of LLM performance in real-world operational environments. The Segment Policy Optimization (SegPO) algorithm is proposed to enhance LLMs' ability to accurately fulfill multi-task workflows. Evaluations on EIFBENCH reveal significant performance differences among existing LLMs, highlighting the need for continuous optimization to address the challenges posed by complex instruction following tasks. This research underscores the importance of developing high-capacity LLMs that can effectively understand and execute complex user needs. 

<br /><br />Summary: <div>
arXiv:2506.08375v1 Announce Type: new 
Abstract: With the development and widespread application of large language models (LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands higher capabilities to address complex user needs, often requiring precise workflow execution which involves the accurate understanding of multiple tasks. However, existing benchmarks focusing on single-task environments with limited constraints lack the complexity required to fully reflect real-world scenarios. To bridge this gap, we present the Extremely Complex Instruction Following Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that enable comprehensive assessment across diverse task types concurrently, but also integrates a variety of constraints, replicating complex operational environments. Furthermore, we propose the Segment Policy Optimization (SegPO) algorithm to enhance the LLM's ability to accurately fulfill multi-task workflow. Evaluations on EIFBENCH have unveiled considerable performance discrepancies in existing LLMs when challenged with these extremely complex instructions. This finding underscores the necessity for ongoing optimization to navigate the intricate challenges posed by LLM applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks</title>
<link>https://arxiv.org/abs/2506.08400</link>
<guid>https://arxiv.org/abs/2506.08400</guid>
<content:encoded><![CDATA[
<div> benchmark, evaluation, language models, low-resource languages, performance <br />
<br />
Summary: <br />
Large Language models (LLMs) have shown impressive performance in various tasks, including multimodal settings such as speech, but their evaluation is often limited to high-resource languages like English. A new benchmark, mSTEB, has been introduced to assess LLMs in language identification, text classification, question answering, and translation tasks across speech and text modalities, filling a gap for low-resource languages. Leading LLMs like Gemini 2.0 Flash and GPT-4o (Audio) were evaluated, highlighting a performance disparity between high-resource and low-resource languages, particularly in Africa and Americas/Oceania. The study underscores the need for increased focus and investment to address the under-representation of these languages in LLMs. <br /> <div>
arXiv:2506.08400v1 Announce Type: new 
Abstract: Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration</title>
<link>https://arxiv.org/abs/2506.08403</link>
<guid>https://arxiv.org/abs/2506.08403</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, large language models, multi-agent systems, cognitive translation studies, TACTIC framework

Summary:
The article discusses the challenges in fully realizing the translation potential of large language models (LLMs) despite their remarkable progress in translation quality. To address this, a cognitively informed multi-agent framework called TACTIC is proposed, which mirrors key cognitive processes observed in human translation behavior. The framework includes agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. Experimental results on various language pairs show that TACTIC consistently outperforms existing models, surpassing GPT-4.1 by +0.6 XCOMET and +1.18 COMETKIWI-23. When compared to DeepSeek-R1, TACTIC achieves even higher improvements, with +0.84 XCOMET and +2.99 COMETKIWI-23. The code for TACTIC is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2506.08403v1 Announce Type: new 
Abstract: Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens</title>
<link>https://arxiv.org/abs/2506.08410</link>
<guid>https://arxiv.org/abs/2506.08410</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, meta-cognition, self-awareness, AutoMeco, MIRA <br />
<br />
Summary: 
This research focuses on evaluating the meta-cognitive abilities of Large Language Models (LLMs) and proposes a framework called AutoMeco for benchmarking existing evaluation lenses. The study explores self-awareness of step errors in LLMs, essential for their reliability. While previous research mainly looked at cognitive error detection, this study delves into meta-cognition, including self-evaluation measures like perplexity. The Automated Meta-cognition Evaluation framework, AutoMeco, and the Markovian Intrinsic Reward Adjustment strategy, MIRA, are introduced to enhance meta-cognition evaluation. Experimental results with mathematical reasoning datasets and LLMs demonstrate the effectiveness of AutoMeco compared to traditional verification methods and highlight the potential for improving LLM meta-cognition using MIRA. This research sheds light on the importance of meta-cognition in LLMs and provides novel methodologies for evaluating and enhancing their self-awareness. <br />  <div>
arXiv:2506.08410v1 Announce Type: new 
Abstract: Previous research has primarily focused on the cognitive error detection capabilities of Large Language Models (LLMs), often prompting them to analyze mistakes in reasoning chains. However, few studies have examined the meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors), which are crucial for their reliability. While studies on LLM self-evaluation present some measures, such as perplexity, which can reflect the answer correctness and be viewed as the lens of meta-cognition, they lack step-level analysis and adaptation. This paper studies the evaluation of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking the existing lenses. Furthermore, a training-free Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost current meta-cognition lenses. Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco by comparing it with Best-of-N verification. Moreover, the meta-cognition ability of LLMs can be better evaluated using MIRA.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know-MRI: A Knowledge Mechanisms Revealer&amp;Interpreter for Large Language Models</title>
<link>https://arxiv.org/abs/2506.08427</link>
<guid>https://arxiv.org/abs/2506.08427</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, interpretation methods, knowledge mechanisms, open-source tool, NLP

Summary:
The article discusses the importance of enhancing the interpretability of large language models (LLMs) and the emergence of various interpretation methods to unravel their internal knowledge mechanisms. The lack of standardized tools to support different input data formats and interpretation outputs limits the practical applications of these methods. To address this issue, the authors introduce an open-source tool called Knowledge Mechanisms Revealer&amp;Interpreter (Know-MRI). This tool incorporates an extensible core module that automatically matches input data with interpretation methods and consolidates the interpreting outputs. Users can freely choose interpretation methods based on the inputs, allowing for a comprehensive diagnosis of the model's internal knowledge mechanisms from multiple perspectives. The code for Know-MRI is available on GitHub, and a demonstration video is provided for reference. Overall, Know-MRI aims to support systematic analysis of knowledge mechanisms within LLMs in a more user-friendly and flexible manner. 

<br /><br />Summary: The article presents the development of Know-MRI, an open-source tool designed to systematically analyze the internal knowledge mechanisms of large language models. By automatically matching different input data with interpretation methods and consolidating interpreting outputs, Know-MRI enables users to comprehensively diagnose model knowledge mechanisms from multiple perspectives. The tool addresses the lack of standardized support for various input formats and interpretation outputs, providing a more user-friendly and flexible approach to analyzing LLMs. The code for Know-MRI is available on GitHub, with a demonstration video highlighting its capabilities. <div>
arXiv:2506.08427v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance, there is a growing urgency to enhance the interpretability of their internal knowledge mechanisms. Consequently, many interpretation methods have emerged, aiming to unravel the knowledge mechanisms of LLMs from various perspectives. However, current interpretation methods differ in input data formats and interpreting outputs. The tools integrating these methods are only capable of supporting tasks with specific inputs, significantly constraining their practical applications. To address these challenges, we present an open-source Knowledge Mechanisms Revealer&amp;Interpreter (Know-MRI) designed to analyze the knowledge mechanisms within LLMs systematically. Specifically, we have developed an extensible core module that can automatically match different input data with interpretation methods and consolidate the interpreting outputs. It enables users to freely choose appropriate interpretation methods based on the inputs, making it easier to comprehensively diagnose the model's internal knowledge mechanisms from multiple perspectives. Our code is available at https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on https://youtu.be/NVWZABJ43Bs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models</title>
<link>https://arxiv.org/abs/2506.08430</link>
<guid>https://arxiv.org/abs/2506.08430</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, Irony detection, Multi-agent system, Collaborative framework, Interpretability

Summary:
Large language models (LLMs) are widely used for sarcasm detection, but face challenges in detecting irony due to single-perspective limitations, lack of comprehensive understanding, and interpretability issues. To address these challenges, the Collaborative Agent Framework for Irony (CAF-I) is introduced. CAF-I utilizes specialized agents for Context, Semantics, and Rhetoric, conducting multidimensional analysis and interactive optimization. A Decision Agent consolidates these perspectives, with a Refinement Evaluator Agent providing feedback for optimization. Experimental results on benchmark datasets demonstrate CAF-I's state-of-the-art zero-shot performance, achieving an average Macro-F1 of 76.31, a significant improvement over prior baselines. By simulating human-like multi-perspective analysis, CAF-I enhances detection accuracy and interpretability, showcasing its effectiveness in irony detection. 

<br /><br />Summary: <div>
arXiv:2506.08430v1 Announce Type: new 
Abstract: Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-resource domain adaptation while minimizing energy and hardware resource consumption</title>
<link>https://arxiv.org/abs/2506.08433</link>
<guid>https://arxiv.org/abs/2506.08433</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Domain Adaptation, Numerical Precision, Data Parallelization, Low-resource Environments

Summary:
Training Large Language Models (LLMs) incurs high costs in terms of energy, hardware, and data, often leading to biases rooted in dominant cultures. Domain adaptation has shown promise in aligning models with diverse contexts but faces computational barriers, especially for those with limited resources. This study evaluates the impact of numerical precision and data parallelization on training speed, model accuracy, and energy consumption to enable domain adaptation in low-resource settings. The findings are relevant for scenarios prioritizing energy efficiency, accessibility, or constrained hardware availability.<br /><br />Summary: <div>
arXiv:2506.08433v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware, and annotated data, often resulting in a positionality rooted in predominant cultures and values (Santy et al., 2023). Domain adaptation has emerged as a promising strategy to better align models with diverse cultural and value contexts (Hershcovich et al., 2022), but its computational cost remains a significant barrier, particularly for research groups lacking access to large-scale infrastructure. In this paper, we evaluate how the use of different numerical precisions and data parallelization strategies impacts both training speed (as a proxy to energy and hardware consumption) and model accuracy, with the goal of facilitating domain adaptation in low-resource environments. Our findings are relevant to any setting where energy efficiency, accessibility, or limited hardware availability are key concerns.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Olica: Efficient Structured Pruning of Large Language Models without Retraining</title>
<link>https://arxiv.org/abs/2506.08436</link>
<guid>https://arxiv.org/abs/2506.08436</guid>
<content:encoded><![CDATA[
<div> Pruning, Large Language Models, Orthogonal decomposition, Linear Calibration, Principal Component Analysis<br />
Summary: 
The proposed pruning framework for Large Language Models (LLMs), Olica, eliminates the need for retraining by utilizing principal component analysis (PCA) to compress LLMs without sacrificing accuracy or disrupting their original structure. By treating the matrix products in the multi-head attention (MHA) layer as unified entities, important information is extracted, reducing the complexity of PCA and making retraining unnecessary. A fast decomposition method is devised to further optimize the process. Additionally, a linear calibration method is introduced to mitigate error accumulation during pruning of the feed-forward network (FFN) layer. By leveraging singular value decomposition (SVD), low-rank matrices are obtained to reconstruct the residual errors without requiring retraining. Extensive experiments demonstrate that Olica is efficient in terms of data usage, GPU memory, and running time, while achieving superior performance across multiple benchmarks.<br /><br />Summary: <div>
arXiv:2506.08436v1 Announce Type: new 
Abstract: Most existing structured pruning methods for Large Language Models (LLMs) require substantial computational and data resources for retraining to reestablish the corrupted correlations, making them prohibitively expensive. To address this, we propose a pruning framework for LLMs called Orthogonal decomposition and Linear Calibration (Olica), which eliminates the need for retraining. A key observation is that the multi-head attention (MHA) layer depends on two types of matrix products. By treating these matrix products as unified entities and applying principal component analysis (PCA), we extract the most important information to compress LLMs without sacrificing accuracy or disrupting their original structure. Consequently, retraining becomes unnecessary. A fast decomposition method is devised, reducing the complexity of PCA by a factor of the square of the number of attention heads. Additionally, to mitigate error accumulation problem caused by pruning the feed-forward network (FFN) layer, we introduce a linear calibration method to reconstruct the residual errors of pruned layers using low-rank matrices. By leveraging singular value decomposition (SVD) on the solution of the least-squares problem, these matrices are obtained without requiring retraining. Extensive experiments show that the proposed Olica is efficient in terms of data usage, GPU memory, and running time, while delivering superior performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning</title>
<link>https://arxiv.org/abs/2506.08477</link>
<guid>https://arxiv.org/abs/2506.08477</guid>
<content:encoded><![CDATA[
<div> Keywords: harmful meme detection, resource efficiency, flexibility, explainability, U-CoT+

Summary:
The article introduces U-CoT+, a new framework for detecting harmful memes that addresses challenges related to resource efficiency, flexibility, and explainability. Unlike current approaches, U-CoT+ utilizes a meme-to-text pipeline to convert visual memes into textual descriptions, enabling more efficient and detailed analysis. By decoupling meme interpretation from classification, the framework allows for harmful meme detection using large language models (LLMs) and incorporating human-crafted guidelines for interpretability. This design offers flexibility in adapting to different detection criteria and regions while ensuring explainability in the decision-making process. Extensive experiments on multiple benchmark datasets validate the effectiveness of U-CoT+, demonstrating its potential for low-resource harmful meme detection with small-scale LLMs.

<br /><br />Summary: 
- U-CoT+ framework for harmful meme detection emphasizes resource efficiency, flexibility, and explainability. 
- Meme-to-text pipeline converts visual memes into textual descriptions for detailed analysis.
- Decoupling meme interpretation from classification enables efficient detection using large language models. 
- Incorporation of human-crafted guidelines enhances interpretability and adaptability to diverse criteria and regions. 
- Extensive experiments validate the framework's effectiveness for low-resource harmful meme detection with small-scale LLMs. <div>
arXiv:2506.08477v1 Announce Type: new 
Abstract: Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$</title>
<link>https://arxiv.org/abs/2506.08479</link>
<guid>https://arxiv.org/abs/2506.08479</guid>
<content:encoded><![CDATA[
<div> Adaptive-$k$ retrieval, Open-domain question answering, Context limitations, Long-context language models, Retrieval-augmented generation <br />
Summary: <br />
Adaptive-$k$ retrieval is introduced as a method to dynamically select the number of passages for open-domain question answering models. It addresses the issue of context limitations faced by long-context language models (LCLMs) and retrieval-augmented generation (RAG) models. Existing methods like Self-RAG and Self-Route struggle with aggregation question answering where the optimal context size is unknown and variable. Adaptive-$k$ retrieval adapts the context size based on similarity scores between the query and candidate passages. It does not require extra inference or model fine-tuning. The method outperforms fixed-$k$ baselines on factoid and aggregation question answering benchmarks using up to 10 times fewer tokens. Across various LCLMs and embedding models, Adaptive-$k$ retrieval improves accuracy by dynamically adjusting context size, leading to more efficient and accurate question answering. <div>
arXiv:2506.08479v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs) both address context limitations of LLMs in open-domain question answering (QA). However, optimal external context to retrieve remains an open problem: fixing the retrieval size risks either wasting tokens or omitting key evidence. Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM prompting and perform well on factoid QA, but struggle with aggregation QA, where the optimal context size is both unknown and variable. We present Adaptive-$k$ retrieval, a simple and effective single-pass method that adaptively selects the number of passages based on the distribution of the similarity scores between the query and the candidate passages. It does not require model fine-tuning, extra LLM inferences or changes to existing retriever-reader pipelines. On both factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens than full-context input, yet still retrieves 70% of relevant passages. It improves accuracy across five LCLMs and two embedding models, highlighting that dynamically adjusting context size leads to more efficient and accurate QA.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2506.08480</link>
<guid>https://arxiv.org/abs/2506.08480</guid>
<content:encoded><![CDATA[
<div> reliable evaluation, text-to-image models, image-text alignment, evaluation framework, metrics <br />
Summary: 

This study focuses on evaluating text-to-image models' ability to generate images that align with textual prompts. It identifies two key aspects that a dependable evaluation framework should address and demonstrates that current mainstream evaluation methods do not fully meet these requirements across various metrics and models. The research highlights the importance of considering factors beyond just human assessments in evaluating image-text alignment. By proposing recommendations for enhancing evaluation processes, this work aims to improve the overall assessment of text-to-image generation models. <div>
arXiv:2506.08480v1 Announce Type: new 
Abstract: Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models</title>
<link>https://arxiv.org/abs/2506.08487</link>
<guid>https://arxiv.org/abs/2506.08487</guid>
<content:encoded><![CDATA[
<div> evaluation, Small Language Models, fairness, social bias, compression<br />
Summary:<br />
The article presents a large-scale audit of Small Language Models (SLMs) tuned for efficiency and fairness. The evaluation includes models across various architectures and sizes from the Qwen, LLaMA, Gemma, and Phi families using the BBQ benchmark. The study reveals that competence and fairness can coexist, with Phi models demonstrating high F1 scores and minimal bias. However, social bias varies significantly across architectures, with Qwen models showing vacuous neutrality and LLaMA models exhibiting stereotypical bias. Additionally, the study highlights the trade-offs introduced by compression techniques, such as quantization, on performance and bias. These insights offer practical guidance for the ethical deployment of SLMs in applications requiring fairness and efficiency, particularly in small enterprises and resource-constrained environments.<br /> <div>
arXiv:2506.08487v1 Announce Type: new 
Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EtiCor++: Towards Understanding Etiquettical Bias in LLMs</title>
<link>https://arxiv.org/abs/2506.08488</link>
<guid>https://arxiv.org/abs/2506.08488</guid>
<content:encoded><![CDATA[
<div> Keywords: cultural sensitivity, LLMs, etiquettes, bias, corpus <br />
Summary: <br />
Researchers have been examining the cultural sensitivity of Large Language Models (LLMs) with a focus on etiquettes, which are integral to regional cultures. The EtiCor++ corpus has been introduced to evaluate LLMs' understanding and bias towards etiquettes worldwide. Various tasks have been outlined to assess LLMs' knowledge of etiquettes in different regions, along with metrics to measure bias. Through extensive experimentation, it was discovered that LLMs exhibit inherent bias towards specific regions. This resource paper emphasizes the importance of evaluating LLMs for their awareness and adherence to etiquettes across various cultural contexts. <div>
arXiv:2506.08488v1 Announce Type: new 
Abstract: In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework</title>
<link>https://arxiv.org/abs/2506.08490</link>
<guid>https://arxiv.org/abs/2506.08490</guid>
<content:encoded><![CDATA[
<div> Keywords: Intent detection, Generalized Intent Discovery, prototype-prompting framework, domain adaptation, consistency constraint 

Summary: 
Intent detection is crucial in identifying user intents from natural language inputs, with supervised methods facing challenges in handling out-of-domain intents. Generalized Intent Discovery (GID) addresses this issue by utilizing unlabeled OOD data to discover new intents without additional annotation. The proposed consistency-driven prototype-prompting framework integrates old and new knowledge by transferring old knowledge from external sources and incorporating a hierarchical consistency constraint for learning new knowledge from target domains. Extensive experiments demonstrate that the method surpasses baseline approaches, achieving state-of-the-art results and highlighting its effectiveness and generalization. The availability of the source code further enhances the accessibility and reproducibility of the proposed framework. 

<br /><br />Summary: <div>
arXiv:2506.08490v1 Announce Type: new 
Abstract: Intent detection aims to identify user intents from natural language inputs, where supervised methods rely heavily on labeled in-domain (IND) data and struggle with out-of-domain (OOD) intents, limiting their practical applicability. Generalized Intent Discovery (GID) addresses this by leveraging unlabeled OOD data to discover new intents without additional annotation. However, existing methods focus solely on clustering unsupervised data while neglecting domain adaptation. Therefore, we propose a consistency-driven prototype-prompting framework for GID from the perspective of integrating old and new knowledge, which includes a prototype-prompting framework for transferring old knowledge from external sources, and a hierarchical consistency constraint for learning new knowledge from target domains. We conducted extensive experiments and the results show that our method significantly outperforms all baseline methods, achieving state-of-the-art results, which strongly demonstrates the effectiveness and generalization of our methods. Our source code is publicly available at https://github.com/smileix/cpp.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs</title>
<link>https://arxiv.org/abs/2506.08500</link>
<guid>https://arxiv.org/abs/2506.08500</guid>
<content:encoded><![CDATA[
<div> Retrieval Augmented Generation (RAG); language models; knowledge conflicts; benchmark; expert annotations <br />
<br />Summary:
In the context of enhancing large language models with relevant information, the issue of conflicting sources arises. This study introduces a taxonomy of knowledge conflict types in Retrieval Augmented Generation (RAG) and proposes desired model behavior for each type. The researchers create CONFLICTS, a benchmark with expert annotations of conflict types in a realistic RAG setting, allowing for tracking model progress. Experiments conducted on this benchmark reveal that language models struggle to resolve conflicts between sources effectively. Prompting models to reason about potential conflicts in retrieved documents improves response quality but shows scope for improvement in future research. This work highlights the importance of addressing knowledge conflicts in enhancing model performance and encourages further exploration in this area. <br /> <div>
arXiv:2506.08500v1 Announce Type: new 
Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations</title>
<link>https://arxiv.org/abs/2506.08504</link>
<guid>https://arxiv.org/abs/2506.08504</guid>
<content:encoded><![CDATA[
<div> Keywords: Discourse parsing, CoMuMDR, Code-mixed, Multi-modal, Multi-domain

Summary:
CoMuMDR is a new corpus for discourse parsing in conversations, featuring code-mixed content in Hindi and English with audio and text data. It includes annotations for nine discourse relations and presents challenges for state-of-the-art models. Current discourse parsing datasets are limited to single-domain English dialogues, making CoMuMDR a valuable resource for NLU applications. Despite experimenting with various baseline models, the performance is poor, indicating the need for more advanced models to handle multi-domain, code-mixed corpora effectively. This study highlights the importance of developing better models to improve discourse parsing in realistic conversational settings.<br /><br />Summary: <div>
arXiv:2506.08504v1 Announce Type: new 
Abstract: Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Post-Training Refinement of Latent Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.08552</link>
<guid>https://arxiv.org/abs/2506.08552</guid>
<content:encoded><![CDATA[
<div> framework, latent reasoning, post-training, contrastive reasoning feedback, residual embedding refinement<br />
<br />
Summary: 
This paper introduces a new post-training framework for improving reasoning in Large Language Models. The framework focuses on refining internal reasoning processes in the model's latent space without producing explicit outputs. It addresses the challenge of updating reasoning embeddings post-training by using two novel strategies: contrastive reasoning feedback and residual embedding refinement. The contrastive reasoning feedback compares reasoning embeddings against strong and weak baselines to determine effective update directions, while residual embedding refinement stabilizes updates by integrating current and historical gradients. Experimental results on five reasoning benchmarks show the effectiveness of the proposed framework, including a 5% increase in accuracy on MathQA without additional training. <div>
arXiv:2506.08552v1 Announce Type: new 
Abstract: Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA without additional training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?</title>
<link>https://arxiv.org/abs/2506.08564</link>
<guid>https://arxiv.org/abs/2506.08564</guid>
<content:encoded><![CDATA[
<div> linguistic relationships, machine learning, speech embeddings, language variation, language diversity
Summary: 
This study explores linguistic relationships globally using machine learning and speech embeddings. The XLS-R model is employed to analyze 106 world languages based on speech recordings. Linear discriminant analysis is used to cluster language embeddings and compare them with genealogical, lexical, and geographical distances. The study finds that embedding-based distances closely align with traditional measures, capturing global and local typological patterns. Challenges in visualizing relationships highlight the dynamic nature of language change. The findings suggest scalable analyses of language variation through speech embeddings, offering new insights into language relationships. Methodological considerations, such as corpus size and latent space dimensionality, are addressed to study low-resource languages and bridge linguistic variation levels. Future work aims to extend these methods to underrepresented languages and integrate sociolinguistic variation for a more comprehensive understanding of language diversity. 
<br /><br />Summary: <div>
arXiv:2506.08564v1 Announce Type: new 
Abstract: Investigating linguistic relationships on a global scale requires analyzing diverse features such as syntax, phonology and prosody, which evolve at varying rates influenced by internal diversification, language contact, and sociolinguistic factors. Recent advances in machine learning (ML) offer complementary alternatives to traditional historical and typological approaches. Instead of relying on expert labor in analyzing specific linguistic features, these new methods enable the exploration of linguistic variation through embeddings derived directly from speech, opening new avenues for large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised language identification model voxlingua107-xls-r-300m-wav2vec, to analyze relationships between 106 world languages based on speech recordings. Using linear discriminant analysis (LDA), language embeddings are clustered and compared with genealogical, lexical, and geographical distances. The results demonstrate that embedding-based distances align closely with traditional measures, effectively capturing both global and local typological patterns. Challenges in visualizing relationships, particularly with hierarchical clustering and network-based methods, highlight the dynamic nature of language change.
  The findings show potential for scalable analyses of language variation based on speech embeddings, providing new perspectives on relationships among languages. By addressing methodological considerations such as corpus size and latent space dimensionality, this approach opens avenues for studying low-resource languages and bridging macro- and micro-level linguistic variation. Future work aims to extend these methods to underrepresented languages and integrate sociolinguistic variation for a more comprehensive understanding of linguistic diversity.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling</title>
<link>https://arxiv.org/abs/2506.08584</link>
<guid>https://arxiv.org/abs/2506.08584</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Large language models, Mental health support, Counseling scenarios, Safety concerns <br />
Summary: 
- Large language models (LLMs) are being considered for use in mental health support, but their performance in counseling scenarios needs evaluation.
- CounselBench is a benchmark developed with input from mental health professionals to assess LLMs in single-turn counseling.
- CounselBench-EVAL contains expert evaluations of responses from LLMs and human therapists, highlighting model superiority in quality but safety concerns raised by experts.
- LLM judges tend to overrate model responses and miss safety issues flagged by human experts.
- CounselBench-Adv, an adversarial dataset, reveals consistent failure patterns in LLMs in response to expert-authored counseling questions, providing insights for improvement in high-stakes mental health settings. <br /> <div>
arXiv:2506.08584v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly proposed for use in mental health support, yet their behavior in realistic counseling scenarios remains largely untested. We introduce CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test LLMs in single-turn counseling. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human therapists to real patient questions. Each response is rated along six clinically grounded dimensions, with written rationales and span-level annotations. We find that LLMs often outperform online human therapists in perceived quality, but experts frequently flag their outputs for safety concerns such as unauthorized medical advice. Follow-up experiments show that LLM judges consistently overrate model responses and overlook safety issues identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling questions designed to trigger specific model issues. Evaluation across 2,880 responses from eight LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking and improving LLM behavior in high-stakes mental health settings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings</title>
<link>https://arxiv.org/abs/2506.08592</link>
<guid>https://arxiv.org/abs/2506.08592</guid>
<content:encoded><![CDATA[
<div> Keywords: text encoders, fine-grained entities, dense retrieval, Chinese dataset, data generation strategies 

Summary: 
This study addresses the limitation of text encoders in recognizing fine-grained entities or events, leading to unsuccessful dense retrieval. The researchers introduce the CapRetrieval dataset in Chinese to evaluate this issue, where queries involve entities or events in different forms. The zero-shot evaluation reveals that encoders struggle with fine-grained matching, regardless of training sources or model sizes. To improve performance, the researchers propose data generation strategies and fine-tune the encoders, achieving the best results on CapRetrieval. Additionally, the study identifies a granularity dilemma, wherein embeddings find it challenging to express fine-grained salience while aligning with overall semantics. The dataset, code, and models from this study are publicly available for further research and experimentation at https://github.com/lxucs/CapRetrieval. 

Summary:<br /><br />Keywords: text encoders, fine-grained entities, dense retrieval, Chinese dataset, data generation strategies<br />This study investigates the limitations of text encoders in recognizing fine-grained entities or events, leading to difficulties in dense retrieval. The researchers introduce the CapRetrieval dataset in Chinese and observe that encoders struggle with fine-grained matching, regardless of their training sources or model sizes. To address this issue, they propose data generation strategies and fine-tuning methods, which enhance encoder performance on CapRetrieval. Moreover, the study highlights a granularity dilemma in embeddings, where expressing fine-grained salience while aligning with overall semantics presents a challenge. The dataset, code, and models produced in this work are available for public use at https://github.com/lxucs/CapRetrieval. <div>
arXiv:2506.08592v1 Announce Type: new 
Abstract: This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models</title>
<link>https://arxiv.org/abs/2506.08593</link>
<guid>https://arxiv.org/abs/2506.08593</guid>
<content:encoded><![CDATA[
<div> MBTI, hate speech detection, personality traits, large language models, annotation <br />
Summary: <br />
The study explores the impact of MBTI personality traits on hate speech detection and classification. It finds that MBTI dimensions significantly influence labeling behavior, both in human annotation and large language models (LLMs). Prompting LLMs with MBTI personas reveals notable persona-driven variation in outputs, including inconsistencies with ground truth, disagreements among personas, and biases at the logit level. The findings emphasize the importance of carefully defining persona prompts in LLM-based annotation workflows to ensure fairness and alignment with human values. This study sheds light on the role of personality traits in hate speech detection and raises concerns about the potential biases introduced by using certain personas in LLM-based classification tasks. <div>
arXiv:2506.08593v1 Announce Type: new 
Abstract: Hate speech detection is a socially sensitive and inherently subjective task, with judgments often varying based on personal traits. While prior work has examined how socio-demographic factors influence annotation, the impact of personality traits on Large Language Models (LLMs) remains largely unexplored. In this paper, we present the first comprehensive study on the role of persona prompts in hate speech classification, focusing on MBTI-based traits. A human annotation survey confirms that MBTI dimensions significantly affect labeling behavior. Extending this to LLMs, we prompt four open-source models with MBTI personas and evaluate their outputs across three hate speech datasets. Our analysis uncovers substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases. These findings highlight the need to carefully define persona prompts in LLM-based annotation workflows, with implications for fairness and alignment with human values.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval</title>
<link>https://arxiv.org/abs/2506.08625</link>
<guid>https://arxiv.org/abs/2506.08625</guid>
<content:encoded><![CDATA[
<div> Keywords: RAISE, scientific reasoning, retrieval-augmented framework, problem decomposition, logical query generation

Summary:
RAISE is a novel framework designed to enhance scientific reasoning by retrieving relevant documents from a dataset. It operates in three main steps: problem decomposition, logical query generation, and logical retrieval. Compared to other baselines, RAISE consistently outperforms in scientific reasoning benchmarks. The framework not only retrieves documents related to domain knowledge but also focuses on logically relevant content. This approach improves the overall process of scientific reasoning by providing access to up-to-date findings and domain-specific terminologies. RAISE's success lies in its ability to retrieve information that is not just similar to the query but also logically relevant, making it a valuable tool for enhancing reasoning processes in scientific research. 

Summary: <div>
arXiv:2506.08625v1 Announce Type: new 
Abstract: Scientific reasoning requires not only long-chain reasoning processes, but also knowledge of domain-specific terminologies and adaptation to updated findings. To deal with these challenges for scientific reasoning, we introduce RAISE, a step-by-step retrieval-augmented framework which retrieves logically relevant documents from in-the-wild corpus. RAISE is divided into three steps: problem decomposition, logical query generation, and logical retrieval. We observe that RAISE consistently outperforms other baselines on scientific reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves documents that are not only similar in terms of the domain knowledge, but also documents logically more relevant.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models</title>
<link>https://arxiv.org/abs/2506.08643</link>
<guid>https://arxiv.org/abs/2506.08643</guid>
<content:encoded><![CDATA[
<div> framework, large language models, decoding, optimization, metaheuristic<br />
<br />
Summary: The article introduces MEMETRON, a framework for optimizing the decoding of large language models (LLMs) using metaheuristic algorithms, GENETRON and ANNETRON. This framework allows for task-agnostic decoding by treating it as a discrete black-box optimization problem. By leveraging reward models and contextual operations within the LLM, MEMETRON can efficiently discover high-reward responses without model retraining or gradient access. The framework is modular and can be applied across diverse tasks with only a reward function and prompt templates. In testing on the human preference alignment task, MEMETRON outperformed standard decoding and reranking methods, showcasing its potential to improve alignment without the need for model retraining. <br /><br />Summary: <div>
arXiv:2506.08643v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used for both open-ended and structured tasks, yet their inference-time behavior is still largely dictated by heuristic decoding strategies such as greedy search, sampling, or reranking. These methods provide limited control and do not explicitly optimize for task-specific objectives. We introduce MEMETRON, a task-agnostic framework that formulates LLM decoding as a discrete black-box optimization problem. MEMETRON leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the response space, guided by reward models and contextual operations performed by the LLM itself. This approach enables efficient discovery of high-reward responses without requiring model retraining or gradient access. The framework is modular and generalizes across diverse tasks, requiring only a reward function and lightweight prompt templates. We evaluate our framework on the critical human preference alignment task and demonstrate that it significantly outperforms standard decoding and reranking methods, highlighting its potential to improve alignment without model retraining.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2506.08646</link>
<guid>https://arxiv.org/abs/2506.08646</guid>
<content:encoded><![CDATA[
<div> data synthesis, table understanding, LLM, weakness-guided, TableDreamer

Summary:
The paper introduces TableDreamer, a data synthesis framework tailored for table instruction tuning. It addresses limitations in generating table instruction tuning data faced by existing LLM-based methods. TableDreamer synthesizes diverse tables and instructions as seed data and iteratively explores the input space guided by identified weaknesses to improve data efficiency. Experiments on 10 tabular benchmarks show that TableDreamer significantly boosts the accuracy of the target LLM and outperforms state-of-the-art baselines using less training data. Overall, TableDreamer offers a progressive and weakness-guided approach to generating high-quality data for table understanding tasks, leading to improved performance of LLM models. The code and data for TableDreamer are available on GitHub for further exploration and use. 

<br /><br />Summary: <div>
arXiv:2506.08646v1 Announce Type: new 
Abstract: Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Summarization for Generative Relation Extraction in the Microbiome Domain</title>
<link>https://arxiv.org/abs/2506.08647</link>
<guid>https://arxiv.org/abs/2506.08647</guid>
<content:encoded><![CDATA[
<div> Keywords: generative relation extraction, intestinal microbiome, large language models, summarization, low-resource domain 

Summary: 
The study focuses on developing a generative relation extraction pipeline tailored for the investigation of interactions within the intestinal microbiome, a challenging and under-resourced biomedical field. The method employs large language models (LLMs) for context refinement before extracting relations through instruction-tuned generation. Initial findings on a specialized corpus indicate that the integration of summarization enhances the performance of generative relation extraction by reducing noise and providing model guidance. However, it is noted that BERT-based relation extraction approaches still outperform generative models. This work serves as an ongoing demonstration of the potential for generative methods to support research in specialized areas within low-resource environments. 

<br /><br />Summary: <div>
arXiv:2506.08647v1 Announce Type: new 
Abstract: We explore a generative relation extraction (RE) pipeline tailored to the study of interactions in the intestinal microbiome, a complex and low-resource biomedical domain. Our method leverages summarization with large language models (LLMs) to refine context before extracting relations via instruction-tuned generation. Preliminary results on a dedicated corpus show that summarization improves generative RE performance by reducing noise and guiding the model. However, BERT-based RE approaches still outperform generative models. This ongoing work demonstrates the potential of generative methods to support the study of specialized domains in low-resources setting.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling</title>
<link>https://arxiv.org/abs/2506.08672</link>
<guid>https://arxiv.org/abs/2506.08672</guid>
<content:encoded><![CDATA[
<div> Reinforced Rule-based Reasoning, RuleReasoner, rule-based reasoning, large reasoning models, reinforcement learning, robust generalization, dynamic sampling, domain augmentation, online learning, computational efficiency

Summary:<br />
RuleReasoner, a method for rule-based reasoning, effectively learns rule formats, types, and complexity in diverse tasks and domains. It uses reinforcement learning and domain-aware dynamic sampling to improve reasoning capabilities without needing pre-hoc human-engineered mix-training recipes. RuleReasoner outperforms large reasoning models in in-distribution and out-of-distribution benchmarks, with higher computational efficiency. Empirical evaluations show a significant performance increase of 4.1% on in-distribution tasks and 10.4% on out-of-distribution tasks over OpenAI-o1. <div>
arXiv:2506.08672v1 Announce Type: new 
Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1% average points on eight ID tasks and $\Delta$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brevity is the soul of sustainability: Characterizing LLM response lengths</title>
<link>https://arxiv.org/abs/2506.08686</link>
<guid>https://arxiv.org/abs/2506.08686</guid>
<content:encoded><![CDATA[
<div> benchmark, decoder-only LLMs, information categories, prompt engineering, energy optimization

Summary:
Decoder-only Large Language Models (LLMs) consume significant energy during inference processes. This study benchmarks 12 LLMs on 5 datasets, revealing that they often produce unnecessarily long responses. A quality assessment identifies six information categories in LLM responses, showing inclusion of redundant or additional information. To address long responses, prompt-engineering strategies are explored, achieving 25-60% energy optimization by reducing response length while maintaining quality. These strategies target length reduction and control information content effectively. The study highlights the importance of output compression in optimizing energy consumption in LLM inference. 

<br /><br />Summary: <div>
arXiv:2506.08686v1 Announce Type: new 
Abstract: A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\% by reducing the response length while preserving the quality of LLM responses.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts</title>
<link>https://arxiv.org/abs/2506.08700</link>
<guid>https://arxiv.org/abs/2506.08700</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific fact-checking, ClimateViz, benchmark, scientific charts, multimodal language models

Summary: 
ClimateViz introduces a benchmark for scientific fact-checking using expert-curated scientific charts, containing 49,862 claims linked to 2,896 visualizations labeled as support, refute, or not enough information. Each example includes structured knowledge graph explanations. Current multimodal language models struggle with chart-based reasoning, with top systems reaching only 76.2 to 77.8 percent accuracy in label-only settings, below human performance. Explanation-augmented outputs show some improvement in certain models' performance. The dataset and code have been released alongside the paper. <div>
arXiv:2506.08700v1 Announce Type: new 
Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking scientific charts, which are key for presenting quantitative evidence and statistical reasoning. We introduce ClimateViz, the first large-scale benchmark for scientific fact-checking using expert-curated scientific charts. ClimateViz contains 49,862 claims linked to 2,896 visualizations, each labeled as support, refute, or not enough information. To improve interpretability, each example includes structured knowledge graph explanations covering trends, comparisons, and causal relations. We evaluate state-of-the-art multimodal language models, including both proprietary and open-source systems, in zero-shot and few-shot settings. Results show that current models struggle with chart-based reasoning: even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to 77.8 percent accuracy in label-only settings, far below human performance (89.3 and 92.7 percent). Explanation-augmented outputs improve performance in some models. We released our dataset and code alongside the paper.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization</title>
<link>https://arxiv.org/abs/2506.08712</link>
<guid>https://arxiv.org/abs/2506.08712</guid>
<content:encoded><![CDATA[
<div> ConfPO, preference learning, Large Language Models, Direct Alignment Algorithms, optimization <br />
Summary: <br />
ConfPO is a method for preference learning in Large Language Models that focuses on optimizing preference-critical tokens based on the training policy's confidence. It improves alignment quality and mitigates overoptimization compared to uniform Direct Alignment Algorithms. ConfPO does not require auxiliary models or additional computational overhead, making it simple and lightweight. Experimental results on challenging benchmarks show that ConfPO outperforms uniform DAAs across various LLMs, delivering better alignment without compromising scalability or reliability. <div>
arXiv:2506.08712v1 Announce Type: new 
Abstract: We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure</title>
<link>https://arxiv.org/abs/2506.08713</link>
<guid>https://arxiv.org/abs/2506.08713</guid>
<content:encoded><![CDATA[
<div> Keywords: Compliance detection, Natural Language Inference, Assurance cases, Multi-hop reasoning, GDPR requirements

Summary:
Compliance detection for complex systems often involves verifying assurance cases using a claim-argument-evidence framework. Challenges include the complexity of legal and technical texts, the need for model explanations, and limited access to assurance case data. To address these challenges, a new approach called EXCLAIM is proposed, utilizing Natural Language Inference (NLI) for explainable and traceable compliance detection. The claim-argument-evidence structure of assurance cases is formulated as a multi-hop inference, with assurance cases generated using large language models (LLMs). Metrics are introduced to measure coverage and structural consistency. A case study demonstrates the effectiveness of the generated assurance case from GDPR requirements in a multi-hop inference task, showcasing the potential of NLI-based approaches in automating the regulatory compliance process. 

<br /><br />Summary: <div>
arXiv:2506.08713v1 Announce Type: new 
Abstract: Ensuring complex systems meet regulations typically requires checking the validity of assurance cases through a claim-argument-evidence framework. Some challenges in this process include the complicated nature of legal and technical texts, the need for model explanations, and limited access to assurance case data. We propose a compliance detection approach based on Natural Language Inference (NLI): EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the claim-argument-evidence structure of an assurance case as a multi-hop inference for explainable and traceable compliance detection. We address the limited number of assurance cases by generating them using large language models (LLMs). We introduce metrics that measure the coverage and structural consistency. We demonstrate the effectiveness of the generated assurance case from GDPR requirements in a multi-hop inference task as a case study. Our results highlight the potential of NLI-based approaches in automating the regulatory compliance process.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2506.08717</link>
<guid>https://arxiv.org/abs/2506.08717</guid>
<content:encoded><![CDATA[
arXiv:2506.08717v1 Announce Type: new 
Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved LLM Agents for Financial Document Question Answering</title>
<link>https://arxiv.org/abs/2506.08726</link>
<guid>https://arxiv.org/abs/2506.08726</guid>
<content:encoded><![CDATA[
arXiv:2506.08726v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Societal AI Research Has Become Less Interdisciplinary</title>
<link>https://arxiv.org/abs/2506.08738</link>
<guid>https://arxiv.org/abs/2506.08738</guid>
<content:encoded><![CDATA[
arXiv:2506.08738v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is often championed as a key pathway for fostering such engagement. Yet it remains unclear whether interdisciplinary research teams are actually leading this shift in practice. This study analyzes over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to examine how ethical values and societal concerns are integrated into technical AI research. We develop a classifier to identify societal content and measure the extent to which research papers express these considerations. We find a striking shift: while interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output. These teams are increasingly integrating societal concerns into their papers and tackling a wide range of domains - from fairness and safety to healthcare and misinformation. These findings challenge common assumptions about the drivers of societal AI and raise important questions. First, what are the implications for emerging understandings of AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams? Second, for scholars in the social sciences and humanities: in a technical field increasingly responsive to societal demands, what distinctive perspectives can we still offer to help shape the future of AI?
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Secure and Private Language Models for Nuclear Power Plants</title>
<link>https://arxiv.org/abs/2506.08746</link>
<guid>https://arxiv.org/abs/2506.08746</guid>
<content:encoded><![CDATA[
arXiv:2506.08746v1 Announce Type: new 
Abstract: This paper introduces a domain-specific Large Language Model for nuclear applications, built from the publicly accessible Essential CANDU textbook. Drawing on a compact Transformer-based architecture, the model is trained on a single GPU to protect the sensitive data inherent in nuclear operations. Despite relying on a relatively small dataset, it shows encouraging signs of capturing specialized nuclear vocabulary, though the generated text sometimes lacks syntactic coherence. By focusing exclusively on nuclear content, this approach demonstrates the feasibility of in-house LLM solutions that align with rigorous cybersecurity and data confidentiality standards. Early successes in text generation underscore the model's utility for specialized tasks, while also revealing the need for richer corpora, more sophisticated preprocessing, and instruction fine-tuning to enhance domain accuracy. Future directions include extending the dataset to cover diverse nuclear subtopics, refining tokenization to reduce noise, and systematically evaluating the model's readiness for real-world applications in nuclear domain.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data</title>
<link>https://arxiv.org/abs/2506.08750</link>
<guid>https://arxiv.org/abs/2506.08750</guid>
<content:encoded><![CDATA[
arXiv:2506.08750v1 Announce Type: new 
Abstract: The nuclear industry possesses a wealth of valuable information locked away in unstructured text data. This data, however, is not readily usable for advanced Large Language Model (LLM) applications that require clean, structured question-answer pairs for tasks like model training, fine-tuning, and evaluation. This paper explores how synthetic data generation can bridge this gap, enabling the development of robust LLMs for the nuclear domain. We discuss the challenges of data scarcity and privacy concerns inherent in the nuclear industry and how synthetic data provides a solution by transforming existing text data into usable Q&amp;A pairs. This approach leverages LLMs to analyze text, extract key information, generate relevant questions, and evaluate the quality of the resulting synthetic dataset. By unlocking the potential of LLMs in the nuclear industry, synthetic data can pave the way for improved information retrieval, enhanced knowledge sharing, and more informed decision-making in this critical sector.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factors affecting the in-context learning abilities of LLMs for dialogue state tracking</title>
<link>https://arxiv.org/abs/2506.08753</link>
<guid>https://arxiv.org/abs/2506.08753</guid>
<content:encoded><![CDATA[
arXiv:2506.08753v1 Announce Type: new 
Abstract: This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL</title>
<link>https://arxiv.org/abs/2506.08757</link>
<guid>https://arxiv.org/abs/2506.08757</guid>
<content:encoded><![CDATA[
arXiv:2506.08757v1 Announce Type: new 
Abstract: Retrieving operational data from nuclear power plants requires exceptional accuracy and transparency due to the criticality of the decisions it supports. Traditionally, natural language to SQL (NL-to-SQL) approaches have been explored for querying such data. While NL-to-SQL promises ease of use, it poses significant risks: end-users cannot easily validate generated SQL queries, and legacy nuclear plant databases -- often complex and poorly structured -- complicate query generation due to decades of incremental modifications. These challenges increase the likelihood of inaccuracies and reduce trust in the approach. In this work, we propose an alternative paradigm: leveraging function-calling large language models (LLMs) to address these challenges. Instead of directly generating SQL queries, we define a set of pre-approved, purpose-specific functions representing common use cases. Queries are processed by invoking these functions, which encapsulate validated SQL logic. This hybrid approach mitigates the risks associated with direct NL-to-SQL translations by ensuring that SQL queries are reviewed and optimized by experts before deployment. While this strategy introduces the upfront cost of developing and maintaining the function library, we demonstrate how NL-to-SQL tools can assist in the initial generation of function code, allowing experts to focus on validation rather than creation. Our study includes a performance comparison between direct NL-to-SQL generation and the proposed function-based approach, highlighting improvements in accuracy and maintainability. This work underscores the importance of balancing user accessibility with operational safety and provides a novel, actionable framework for robust data retrieval in critical systems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP</title>
<link>https://arxiv.org/abs/2506.08768</link>
<guid>https://arxiv.org/abs/2506.08768</guid>
<content:encoded><![CDATA[
arXiv:2506.08768v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at https://anonymous.4open.science/r/AraReasoner41299
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation</title>
<link>https://arxiv.org/abs/2506.08827</link>
<guid>https://arxiv.org/abs/2506.08827</guid>
<content:encoded><![CDATA[
arXiv:2506.08827v1 Announce Type: new 
Abstract: The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing STT for Low-Resource Real-World Speech</title>
<link>https://arxiv.org/abs/2506.08836</link>
<guid>https://arxiv.org/abs/2506.08836</guid>
<content:encoded><![CDATA[
arXiv:2506.08836v1 Announce Type: new 
Abstract: Swiss German is a low-resource language represented by diverse dialects that differ significantly from Standard German and from each other, lacking a standardized written form. As a result, transcribing Swiss German involves translating into Standard German. Existing datasets have been collected in controlled environments, yielding effective speech-to-text (STT) models, but these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour annotated speech corpus featuring real-world long-audio recordings from 39 Swiss German radio and TV stations. It captures spontaneous speech across all major Swiss dialects recorded in various realistic environments and overcomes the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, achieving notable enhancements over previous zero-shot performance metrics. Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for developing effective and robust STT systems for Swiss German and other low-resource languages in real-world contexts.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)</title>
<link>https://arxiv.org/abs/2506.08885</link>
<guid>https://arxiv.org/abs/2506.08885</guid>
<content:encoded><![CDATA[
arXiv:2506.08885v1 Announce Type: new 
Abstract: Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantBert: An Open Source Language Model for Plant Science</title>
<link>https://arxiv.org/abs/2506.08897</link>
<guid>https://arxiv.org/abs/2506.08897</guid>
<content:encoded><![CDATA[
arXiv:2506.08897v1 Announce Type: new 
Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis</title>
<link>https://arxiv.org/abs/2506.08899</link>
<guid>https://arxiv.org/abs/2506.08899</guid>
<content:encoded><![CDATA[
arXiv:2506.08899v1 Announce Type: new 
Abstract: We present a novel approach to the automated semantic analysis of legal texts using large language models (LLMs), targeting their transformation into formal representations in Defeasible Deontic Logic (DDL). We propose a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. Our methodology is evaluated across various LLM configurations, including prompt engineering strategies, fine-tuned models, and multi-stage pipelines, focusing on legal norms from the Australian Telecommunications Consumer Protections Code. Empirical results demonstrate promising alignment between machine-generated and expert-crafted formalizations, showing that LLMs - particularly when prompted effectively - can significantly contribute to scalable legal informatics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialect Normalization using Large Language Models and Morphological Rules</title>
<link>https://arxiv.org/abs/2506.08907</link>
<guid>https://arxiv.org/abs/2506.08907</guid>
<content:encoded><![CDATA[
arXiv:2506.08907v1 Announce Type: new 
Abstract: Natural language understanding systems struggle with low-resource languages, including many dialects of high-resource ones. Dialect-to-standard normalization attempts to tackle this issue by transforming dialectal text so that it can be used by standard-language tools downstream. In this study, we tackle this task by introducing a new normalization method that combines rule-based linguistically informed transformations and large language models (LLMs) with targeted few-shot prompting, without requiring any parallel data. We implement our method for Greek dialects and apply it on a dataset of regional proverbs, evaluating the outputs using human annotators. We then use this dataset to conduct downstream experiments, finding that previous results regarding these proverbs relied solely on superficial linguistic information, including orthographic artifacts, while new observations can still be made through the remaining semantics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropMEND: Hypernetworks for Knowledge Propagation in LLMs</title>
<link>https://arxiv.org/abs/2506.08920</link>
<guid>https://arxiv.org/abs/2506.08920</guid>
<content:encoded><![CDATA[
arXiv:2506.08920v1 Announce Type: new 
Abstract: Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can A Gamer Train A Mathematical Reasoning Model?</title>
<link>https://arxiv.org/abs/2506.08935</link>
<guid>https://arxiv.org/abs/2506.08935</guid>
<content:encoded><![CDATA[
arXiv:2506.08935v1 Announce Type: new 
Abstract: While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.08938</link>
<guid>https://arxiv.org/abs/2506.08938</guid>
<content:encoded><![CDATA[
arXiv:2506.08938v1 Announce Type: new 
Abstract: Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM`s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model`s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model`s parametric knowledge, which undermines the model`s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model`s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https:// github.com/DeepLearnXMU/Faithful-RAG
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions</title>
<link>https://arxiv.org/abs/2506.08952</link>
<guid>https://arxiv.org/abs/2506.08952</guid>
<content:encoded><![CDATA[
arXiv:2506.08952v1 Announce Type: new 
Abstract: Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers</title>
<link>https://arxiv.org/abs/2506.08966</link>
<guid>https://arxiv.org/abs/2506.08966</guid>
<content:encoded><![CDATA[
arXiv:2506.08966v1 Announce Type: new 
Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing work showed limited success in probing numeric values from models' representations, indicating that these errors can be attributed to the inherent unreliability of distributionally learned embeddings in representing exact quantities. However, we observe that previous probing methods are inadequate for the emergent structure of learned number embeddings with sinusoidal patterns.
  In response, we propose a novel probing technique that decodes numeric values from input embeddings with near-perfect accuracy across a range of open-source LMs. This proves that after the sole pre-training, LMs represent numbers with remarkable precision. Finally, we find that the embeddings' preciseness judged by our probe's accuracy explains a large portion of LM's errors in elementary arithmetic, and show that aligning the embeddings with the pattern discovered by our probe can mitigate these errors.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System</title>
<link>https://arxiv.org/abs/2506.08972</link>
<guid>https://arxiv.org/abs/2506.08972</guid>
<content:encoded><![CDATA[
arXiv:2506.08972v1 Announce Type: new 
Abstract: Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks -- such as shot-chain execution tasks and single-screen grounding tasks -- while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at https://ui-nexus.github.io.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents</title>
<link>https://arxiv.org/abs/2506.08981</link>
<guid>https://arxiv.org/abs/2506.08981</guid>
<content:encoded><![CDATA[
arXiv:2506.08981v1 Announce Type: new 
Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers, who produced speech in their native language (L1), second language (L2), and imitated L2 (fake foreign accent). The new corpus enables research into language variability from phonetic and technological points of view. Accordingly, we include two preliminary case studies to demonstrate both perspectives. The first case study explores the impact of L2 and imitated L2 on the performance of an automatic speaker verification system, while the second illustrates the articulatory patterns of one speaker in L1, L2, and a fake accent.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder</title>
<link>https://arxiv.org/abs/2506.08986</link>
<guid>https://arxiv.org/abs/2506.08986</guid>
<content:encoded><![CDATA[
arXiv:2506.08986v1 Announce Type: new 
Abstract: Early detection is crucial for timely intervention aimed at preventing and slowing the progression of neurocognitive disorder (NCD), a common and significant health problem among the aging population. Recent evidence has suggested that language-related functional magnetic resonance imaging (fMRI) may be a promising approach for detecting cognitive decline and early NCD. In this paper, we proposed a novel, naturalistic language-related fMRI task for this purpose. We examined the effectiveness of this task among 97 non-demented Chinese older adults from Hong Kong. The results showed that machine-learning classification models based on fMRI features extracted from the task and demographics (age, gender, and education year) achieved an average area under the curve of 0.86 when classifying participants' cognitive status (labeled as NORMAL vs DECLINE based on their scores on a standard neurcognitive test). Feature localization revealed that the fMRI features most frequently selected by the data-driven approach came primarily from brain regions associated with language processing, such as the superior temporal gyrus, middle temporal gyrus, and right cerebellum. The study demonstrated the potential of the naturalistic language-related fMRI task for early detection of aging-related cognitive decline and NCD.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Employing self-supervised learning models for cross-linguistic child speech maturity classification</title>
<link>https://arxiv.org/abs/2506.08999</link>
<guid>https://arxiv.org/abs/2506.08999</guid>
<content:encoded><![CDATA[
arXiv:2506.08999v1 Announce Type: new 
Abstract: Speech technology systems struggle with many downstream tasks for child speech due to small training corpora and the difficulties that child speech pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer models to address a fundamental classification task: identifying child vocalizations. Unlike previous corpora, our dataset captures maximally ecologically-valid child vocalizations across an unprecedented sample, comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu, Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004 labeled vocalizations, magnitudes larger than previous work. Models were trained to distinguish between cry, laughter, mature (consonant+vowel), and immature speech (just consonant or vowel). Models trained on the dataset outperform state-of-the-art models trained on previous datasets, achieved classification accuracy comparable to humans, and were robust across rural and urban settings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner</title>
<link>https://arxiv.org/abs/2506.09003</link>
<guid>https://arxiv.org/abs/2506.09003</guid>
<content:encoded><![CDATA[
arXiv:2506.09003v1 Announce Type: new 
Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags</title>
<link>https://arxiv.org/abs/2506.09009</link>
<guid>https://arxiv.org/abs/2506.09009</guid>
<content:encoded><![CDATA[
arXiv:2506.09009v1 Announce Type: new 
Abstract: The present study extends recent work on Universal Dependencies annotations for second-language (L2) Korean by introducing a semi-automated framework that identifies morphosyntactic constructions from XPOS sequences and aligns those constructions with corresponding UPOS categories. We also broaden the existing L2-Korean corpus by annotating 2,998 new sentences from argumentative essays. To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean morphosyntactic analysis models on datasets both with and without these alignments, using two NLP toolkits. Our results indicate that the aligned dataset not only improves consistency across annotation layers but also enhances morphosyntactic tagging and dependency-parsing accuracy, particularly in cases of limited annotated data.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason Across Parallel Samples for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.09014</link>
<guid>https://arxiv.org/abs/2506.09014</guid>
<content:encoded><![CDATA[
arXiv:2506.09014v1 Announce Type: new 
Abstract: Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features</title>
<link>https://arxiv.org/abs/2506.09021</link>
<guid>https://arxiv.org/abs/2506.09021</guid>
<content:encoded><![CDATA[
arXiv:2506.09021v1 Announce Type: new 
Abstract: This study examines the lexical and syntactic interventions of human and LLM proofreading aimed at improving overall intelligibility in identical second language writings, and evaluates the consistency of outcomes across three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and LLM proofreading enhance bigram lexical features, which may contribute to better coherence and contextual connectedness between adjacent words. However, LLM proofreading exhibits a more generative approach, extensively reworking vocabulary and sentence structures, such as employing more diverse and sophisticated vocabulary and incorporating a greater number of adjective modifiers in noun phrases. The proofreading outcomes are highly consistent in major lexical and syntactic features across the three models.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09033</link>
<guid>https://arxiv.org/abs/2506.09033</guid>
<content:encoded><![CDATA[
arXiv:2506.09033v1 Announce Type: new 
Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs</title>
<link>https://arxiv.org/abs/2506.09047</link>
<guid>https://arxiv.org/abs/2506.09047</guid>
<content:encoded><![CDATA[
arXiv:2506.09047v1 Announce Type: new 
Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task on text (e.g., counting words in a text). We investigate this accuracy gap by identifying and comparing the \textit{circuits} - the task-specific computational sub-graphs - in different modalities. We show that while circuits are largely disjoint between modalities, they implement relatively similar functionalities: the differences lie primarily in processing modality-specific data positions (an image or a text sequence). Zooming in on the image data representations, we observe they become aligned with the higher-performing analogous textual representations only towards later layers, too late in processing to effectively influence subsequent positions. To overcome this, we patch the representations of visual data tokens from later layers back into earlier layers. In experiments with multiple tasks and models, this simple intervention closes a third of the performance gap between the modalities, on average. Our analysis sheds light on the multi-modal performance gap in VLMs and suggests a training-free approach for reducing it.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining</title>
<link>https://arxiv.org/abs/2506.08022</link>
<guid>https://arxiv.org/abs/2506.08022</guid>
<content:encoded><![CDATA[
arXiv:2506.08022v1 Announce Type: cross 
Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval</title>
<link>https://arxiv.org/abs/2506.08074</link>
<guid>https://arxiv.org/abs/2506.08074</guid>
<content:encoded><![CDATA[
arXiv:2506.08074v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in external evidence, yet it still falters when answers must be pieced together across semantically distant documents. We close this gap with the Hierarchical Lexical Graph (HLG), a three-tier index that (i) traces every atomic proposition to its source, (ii) clusters propositions into latent topics, and (iii) links entities and relations to expose cross-document paths. On top of HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG, which performs fine-grained entity-aware beam search over propositions for high-precision factoid questions, and TopicGraphRAG, which selects coarse topics before expanding along entity links to supply broad yet relevant context for exploratory queries. Additionally, existing benchmarks lack the complexity required to rigorously evaluate multi-hop summarization systems, often focusing on single-document queries or limited datasets. To address this, we introduce a synthetic dataset generation pipeline that curates realistic, multi-document question-answer pairs, enabling robust evaluation of multi-hop retrieval systems. Extensive experiments across five datasets demonstrate that our methods outperform naive chunk-based RAG achieving an average relative improvement of 23.1% in retrieval recall and correctness. Open-source Python library is available at https://github.com/awslabs/graphrag-toolkit.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08125</link>
<guid>https://arxiv.org/abs/2506.08125</guid>
<content:encoded><![CDATA[
arXiv:2506.08125v1 Announce Type: cross 
Abstract: Large language models have demonstrated impressive reasoning capabilities, yet they often suffer from inefficiencies due to unnecessarily verbose or redundant outputs. While many works have explored reinforcement learning (RL) to enhance reasoning abilities, most primarily focus on improving accuracy, with limited attention to reasoning efficiency. Some existing approaches introduce direct length-based rewards to encourage brevity, but this often leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL framework that advances length-based reward design to boost efficient reasoning. Bingo incorporates two key mechanisms: a significance-aware length reward, which gradually guides the model to reduce only insignificant tokens, and a dynamic length reward, which initially encourages elaborate reasoning for hard questions but decays over time to improve overall efficiency. Experiments across multiple reasoning benchmarks show that Bingo improves both accuracy and efficiency. It outperforms the vanilla reward and several other length-based reward baselines in RL, achieving a favorable trade-off between accuracy and efficiency. These results underscore the potential of training LLMs explicitly for efficient reasoning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists</title>
<link>https://arxiv.org/abs/2506.08140</link>
<guid>https://arxiv.org/abs/2506.08140</guid>
<content:encoded><![CDATA[
arXiv:2506.08140v1 Announce Type: cross 
Abstract: Despite long-standing efforts in accelerating scientific discovery with AI, building AI co-scientists remains challenging due to limited high-quality data for training and evaluation. To tackle this data scarcity issue, we present AutoSDT, an automatic pipeline that collects high-quality coding tasks in real-world data-driven discovery workflows. AutoSDT leverages the coding capabilities and parametric knowledge of LLMs to search for diverse sources, select ecologically valid tasks, and synthesize accurate task instructions and code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404 coding tasks for data-driven discovery that covers four scientific disciplines and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the only automatically collected and the largest open dataset for data-driven scientific discovery. Expert feedback on a subset of 256 tasks shows the effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid, and 92.2% of the synthesized programs are functionally correct. Trained on AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show substantial improvement on two challenging data-driven discovery benchmarks, ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches the same level of performance as GPT-4o on ScienceAgentBench with a success rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it lifts the hypothesis matching score to 8.1, bringing a 17.4% relative improvement and closing the gap between open-weight models and GPT-4o.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors</title>
<link>https://arxiv.org/abs/2506.08188</link>
<guid>https://arxiv.org/abs/2506.08188</guid>
<content:encoded><![CDATA[
arXiv:2506.08188v1 Announce Type: cross 
Abstract: In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2506.08210</link>
<guid>https://arxiv.org/abs/2506.08210</guid>
<content:encoded><![CDATA[
arXiv:2506.08210v1 Announce Type: cross 
Abstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADAR: Benchmarking Language Models on Imperfect Tabular Data</title>
<link>https://arxiv.org/abs/2506.08249</link>
<guid>https://arxiv.org/abs/2506.08249</guid>
<content:encoded><![CDATA[
arXiv:2506.08249v1 Announce Type: cross 
Abstract: Language models (LMs) are increasingly being deployed to perform autonomous data analyses. However, their data awareness -- the ability to recognize, reason over, and appropriately handle data artifacts such as missing values, outliers, and logical inconsistencies -- remains underexplored. These artifacts are especially common in real-world tabular data and, if mishandled, can significantly compromise the validity of analytical conclusions. To address this gap, we present RADAR, a benchmark for systematically evaluating data-aware reasoning on tabular data. We develop a framework to simulate data artifacts via programmatic perturbations to enable targeted evaluation of model behavior. RADAR comprises 2980 table query pairs, grounded in real-world data spanning 9 domains and 5 data artifact types. In addition to evaluating artifact handling, RADAR systematically varies table size to study how reasoning performance holds when increasing table size. Our evaluation reveals that, despite decent performance on tables without data artifacts, frontier models degrade significantly when data artifacts are introduced, exposing critical gaps in their capacity for robust, data-aware analysis. Designed to be flexible and extensible, RADAR supports diverse perturbation types and controllable table sizes, offering a valuable resource for advancing tabular reasoning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints</title>
<link>https://arxiv.org/abs/2506.08266</link>
<guid>https://arxiv.org/abs/2506.08266</guid>
<content:encoded><![CDATA[
arXiv:2506.08266v1 Announce Type: cross 
Abstract: Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain</title>
<link>https://arxiv.org/abs/2506.08277</link>
<guid>https://arxiv.org/abs/2506.08277</guid>
<content:encoded><![CDATA[
arXiv:2506.08277v1 Announce Type: cross 
Abstract: Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [https://github.com/subbareddy248/mllm_videos].
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>