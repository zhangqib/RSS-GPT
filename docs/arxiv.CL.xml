<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant</title>
<link>https://arxiv.org/abs/2409.11055</link>
<guid>https://arxiv.org/abs/2409.11055</guid>
<content:encoded><![CDATA[
<div> evaluation, quantization methods, model performance, task difficulty, instruction-following, hallucination detection, AWQ, GPTQ, FP16, FP8, 4-bit quantization, 70B-scale models, task weaknesses

Summary:
Our evaluation of instruction-tuned models ranging from 1B to 405B parameters and applying four quantization methods across 13 datasets revealed that quantized models generally outperform smaller FP16 baselines but struggle with instruction-following and hallucination detection. FP8 emerged as the most robust option, with AWQ outperforming GPTQ in weight-only quantization. Smaller models experienced severe accuracy drops at 4-bit quantization, while larger models maintained stable performance. Notably, task difficulty did not always correlate with the largest accuracy losses, indicating that quantization magnifies a model's weaknesses. An LLM-based judge highlighted significant performance declines in Coding and STEM tasks, with occasional improvements in reasoning tasks. Overall, quantization offers promising cost-effective deployment options for language models but requires careful consideration of model size, task requirements, and quantization methods. <br><br>Summary: <div>
arXiv:2409.11055v5 Announce Type: replace 
Abstract: Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.00979</link>
<guid>https://arxiv.org/abs/2505.00979</guid>
<content:encoded><![CDATA[
<div> framework, data generation, synthetic, knowledge, language models
Summary:
- Large Language Models (LLMs) are successful but data-inefficient, especially with limited, specialized data.
- Synthetic-on-Graph (SoG) framework aims to improve data diversity and coherence by incorporating cross-document knowledge associations.
- SoG constructs a context graph from the original corpus to enhance synthetic data quality.
- Integration of Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic methods further enhances reasoning and discriminative power.
- Experiments show SoG outperforms state-of-the-art methods in a multi-hop document Q&A dataset and performs comparably in reading comprehension tasks, showing better generalization capabilities for LLMs in limited data domains.
<br><br>Summary: <div>
arXiv:2505.00979v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&amp;A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RM-R1: Reward Modeling as Reasoning</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
<div> reward modeling, deep thinking, interpretable reasoning, reasoning capabilities, generative reward models<br>
<br>
Summary:<br>
Reward modeling is crucial for aligning large language models with human preferences. By integrating reasoning capabilities into reward modeling, a new class of generative reward models called Reasoning Reward Models (ReasRMs) is introduced. These models formulate reward modeling as a reasoning task, stimulating deep thinking and conducting interpretable reasoning before assigning scores or judgments. The training pipeline involves distillation of high-quality reasoning chains and reinforcement learning with verifiable rewards. Empirical results show that ReasRMs, specifically RM-R1, outperform existing models on benchmark tasks by up to 4.9%. The release of six REASRM models, along with code and data, aims to facilitate future research in this area.<br> 
<br>Summary: <div>
arXiv:2505.02387v3 Announce Type: replace 
Abstract: Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math/code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection</title>
<link>https://arxiv.org/abs/2505.05327</link>
<guid>https://arxiv.org/abs/2505.05327</guid>
<content:encoded><![CDATA[
<div> method, data selection, language models, performance improvement, training costs

Summary:
The paper introduces Refined Contribution Measurement with In-Context Learning (RICo), a novel gradient-free method for data selection in instruction tuning for large language models. RICo accurately quantifies the contribution of individual data samples to task-level and global-level model performance, leading to better instruction tuning. A lightweight selection paradigm based on RICo scores allows for scalable data selection with linear inference complexity. Experiments across multiple benchmarks demonstrate RICo's effectiveness, with models trained on RICo-selected data outperforming full datasets and other selection methods. The analysis of high-contribution samples selected by RICo reveals diverse tasks and appropriate difficulty levels, showing the method's capability to select data beyond just the hardest ones. <div>
arXiv:2505.05327v2 Announce Type: replace 
Abstract: Data selection for instruction tuning is crucial for improving the performance of large language models (LLMs) while reducing training costs. In this paper, we propose Refined Contribution Measurement with In-Context Learning (RICo), a novel gradient-free method that quantifies the fine-grained contribution of individual samples to both task-level and global-level model performance. RICo enables more accurate identification of high-contribution data, leading to better instruction tuning. We further introduce a lightweight selection paradigm trained on RICo scores, enabling scalable data selection with a strictly linear inference complexity. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of RICo. Remarkably, on LLaMA3.1-8B, models trained on 15% of RICo-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by RICo, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multi-Agent Systems, Swarm Intelligence, Decentralized Coordination, Emergent Collective Behavior

Summary:<br><br>
The article introduces SwarmBench, a new benchmark to evaluate the swarm intelligence capabilities of Large Language Models (LLMs) acting as decentralized agents in Multi-Agent Systems (MAS). The benchmark includes five coordination tasks within a 2D grid environment, where agents rely on local sensory information and communication. Results from zero-shot evaluations of leading LLMs show significant performance variations across tasks, with struggles in long-range planning and adaptive strategy formation. The study highlights the importance of assessing LLMs under swarm-like constraints for understanding their utility in decentralized intelligent systems. SwarmBench is released as an open toolkit to support reproducible research in LLM-based MAS coordination and the study of emergent collective behavior under severe decentralization. The code repository is available at https://github.com/x66ccff/swarmbench. <div>
arXiv:2505.04364v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict swarm-like constraints-limited local perception and communication-remains largely unexplored. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid environment, forcing agents to rely solely on local sensory input ($k\times k$ view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Zero-shot evaluations of leading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent performance variations. While some rudimentary coordination is observed, our results indicate that current LLMs significantly struggle with robust long-range planning and adaptive strategy formation under the uncertainty inherent in these decentralized scenarios. Assessing LLMs under such swarm-like constraints is crucial for understanding their utility in future decentralized intelligent systems. We release SwarmBench as an open, extensible toolkit-built on a customizable physical system-providing environments, prompts, evaluation scripts, and comprehensive datasets. This aims to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of emergent collective behavior under severe informational decentralization. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism</title>
<link>https://arxiv.org/abs/2505.11533</link>
<guid>https://arxiv.org/abs/2505.11533</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, implicit user intentions, data synthesis, proactive mining, tourism<br />
<br />
Summary:
SynPT is a new approach that addresses limitations in current methods for mining implicit user intentions in the tourism domain using Large Language Models (LLMs). By creating a data synthesis method driven by LLMs, SynPT aims to generate a high-quality training dataset containing explicit reasoning for proactive intention mining. By simulating dialogues based on seed data from Chinese tourism websites, SynPT-Dialog is constructed to fine-tune LLMs to better understand and guide user needs. The approach focuses on adapting to the tourism domain, balancing detail levels in inquiries, reducing contextual redundancy, and considering tourists' emotions and values. Experimental evaluations show the superiority of SynPT compared to existing methods, with insights on key hyperparameters and case studies demonstrating practical applicability, including potential adaptation to English-language scenarios. The code and data are publicly available. <br /><br />Summary: <div>
arXiv:2505.11533v1 Announce Type: new 
Abstract: In the tourism domain, Large Language Models (LLMs) often struggle to mine implicit user intentions from tourists' ambiguous inquiries and lack the capacity to proactively guide users toward clarifying their needs. A critical bottleneck is the scarcity of high-quality training datasets that facilitate proactive questioning and implicit intention mining. While recent advances leverage LLM-driven data synthesis to generate such datasets and transfer specialized knowledge to downstream models, existing approaches suffer from several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed distributions of detail levels in initial inquiries, (3) contextual redundancy in the implicit intention mining module, and (4) lack of explicit thinking about tourists' emotions and intention values. Therefore, we propose SynPT (A Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User Intentions in the Tourism), which constructs an LLM-driven user agent and assistant agent to simulate dialogues based on seed data collected from Chinese tourism websites. This approach addresses the aforementioned limitations and generates SynPT-Dialog, a training dataset containing explicit reasoning. The dataset is utilized to fine-tune a general LLM, enabling it to proactively mine implicit user intentions. Experimental evaluations, conducted from both human and LLM perspectives, demonstrate the superiority of SynPT compared to existing methods. Furthermore, we analyze key hyperparameters and present case studies to illustrate the practical applicability of our method, including discussions on its adaptability to English-language scenarios. All code and data are publicly available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification</title>
<link>https://arxiv.org/abs/2505.11550</link>
<guid>https://arxiv.org/abs/2505.11550</guid>
<content:encoded><![CDATA[
<div> Language Models, AI-generated text, Detection, Model identification, Neural architectures
Summary: 
- Large Language Models (LLMs) are powerful in generating text resembling human writing, but may be misused for fake news and spam.
- Accurate detection of AI-generated text and identifying the model responsible are crucial for responsible use.
- This work addressed two tasks from the Defactify workshop on AI-Generated Text Detection.
- Task A involved distinguishing between human and AI-generated text, where an optimized neural architecture ranked fifth with an F1 score of 0.994.
- Task B focused on attributing text to its originating language model, with a simpler neural architecture ranking fifth with an F1 score of 0.627.<br /><br />Summary: <div>
arXiv:2505.11550v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across a wide range of styles and genres. However, such capabilities are prone to potential misuse, such as fake news generation, spam email creation, and misuse in academic assignments. As a result, accurate detection of AI-generated text and identification of the model that generated it are crucial for maintaining the responsible use of LLMs. In this work, we addressed two sub-tasks put forward by the Defactify workshop under AI-Generated Text Detection shared task at the Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A involved distinguishing between human-authored or AI-generated text, while Task B focused on attributing text to its originating language model. For each task, we proposed two neural architectures: an optimized model and a simpler variant. For Task A, the optimized neural architecture achieved fifth place with $F1$ score of 0.994, and for Task B, the simpler neural architecture also ranked fifth place with $F1$ score of 0.627.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks</title>
<link>https://arxiv.org/abs/2505.11556</link>
<guid>https://arxiv.org/abs/2505.11556</guid>
<content:encoded><![CDATA[
<div> Hidden Profile paradigm, multi-agent systems, large language models, collective reasoning, distributed knowledge. 
Summary: 
This paper introduces the Hidden Profile paradigm as a benchmark for evaluating multi-agent systems built on large language models (LLMs). The paradigm involves distributing critical information unevenly among agents to assess collective reasoning. Experiments with GPT-4.1 and other LLMs show that multi-agent systems struggle to match single-agent accuracy with complete information. Despite comparable performance to human groups, subtle behavioral differences, like sensitivity to social desirability, emerge. The study also reveals a trade-off between cooperation and contradiction in multi-agent systems, with cooperative agents tending to over-coordinate and increased contradiction hindering group convergence. This work provides a reproducible framework for assessing multi-agent LLM systems, with implications for artificial collective intelligence and human-AI interaction. 
<br /><br />Summary: <div>
arXiv:2505.11556v1 Announce Type: new 
Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but also risk replicating collective reasoning failures observed in human groups. Yet, no theory-grounded benchmark exists to systematically evaluate such failures. In this paper, we introduce the Hidden Profile paradigm from social psychology as a diagnostic testbed for multi-agent LLM systems. By distributing critical information asymmetrically across agents, the paradigm reveals how inter-agent dynamics support or hinder collective reasoning. We first formalize the paradigm for multi-agent decision-making under distributed knowledge and instantiate it as a benchmark with nine tasks spanning diverse scenarios, including adaptations from prior human studies. We then conduct experiments with GPT-4.1 and five other leading LLMs, including reasoning-enhanced variants, showing that multi-agent systems across all models fail to match the accuracy of single agents given complete information. While agents' collective performance is broadly comparable to that of human groups, nuanced behavioral differences emerge, such as increased sensitivity to social desirability. Finally, we demonstrate the paradigm's diagnostic utility by exploring a cooperation-contradiction trade-off in multi-agent LLM systems. We find that while cooperative agents are prone to over-coordination in collective settings, increased contradiction impairs group convergence. This work contributes a reproducible framework for evaluating multi-agent LLM systems and motivates future research on artificial collective intelligence and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models</title>
<link>https://arxiv.org/abs/2505.11604</link>
<guid>https://arxiv.org/abs/2505.11604</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, PowerPoint, slide editing, Talk-to-Your-Slides, TSBench<br />
Summary:<br />
Existing research has primarily focused on generating slides using large language models (LLMs) for PowerPoint, neglecting the editing aspect. This study introduces Talk-to-Your-Slides, an LLM-powered system that directly edits slides in active PowerPoint sessions through COM communication. The system utilizes a two-level approach involving high-level processing by an LLM agent and low-level execution through Python scripts, enabling flexible and contextually-aware editing. A dataset called TSBench, consisting of 379 editing instructions and slide variations, is created for evaluation purposes. Experimental results demonstrate the superior performance of Talk-to-Your-Slides over baseline methods in terms of execution success rate, instruction fidelity, and editing efficiency. The code and benchmark are publicly available for further research. <br /><br />Summary: <div>
arXiv:2505.11604v1 Announce Type: new 
Abstract: Existing research on large language models (LLMs) for PowerPoint predominantly focuses on slide generation, overlooking the common yet tedious task of editing existing slides. We introduce Talk-to-Your-Slides, an LLM-powered agent that directly edits slides within active PowerPoint sessions through COM communication. Our system employs a two-level approach: (1) high-level processing where an LLM agent interprets instructions and formulates editing plans, and (2) low-level execution where Python scripts directly manipulate PowerPoint objects. Unlike previous methods relying on predefined operations, our approach enables more flexible and contextually-aware editing. To facilitate evaluation, we present TSBench, a human-annotated dataset of 379 diverse editing instructions with corresponding slide variations. Experimental results demonstrate that Talk-to-Your-Slides significantly outperforms baseline methods in execution success rate, instruction fidelity, and editing efficiency. Our code and benchmark are available at https://anonymous.4open.science/r/talk-to-your-slides/
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models</title>
<link>https://arxiv.org/abs/2505.11613</link>
<guid>https://arxiv.org/abs/2505.11613</guid>
<content:encoded><![CDATA[
<div> benchmark, clinical guidelines, Large Language Models, MedGUIDE, structured protocols

Summary:
- Clinical guidelines are essential for safe and accurate diagnostic decision-making in evidence-based medical practice.
- The MedGUIDE benchmark evaluates Large Language Models (LLMs) on their ability to make guideline-consistent clinical decisions.
- MedGUIDE consists of 55 curated NCCN decision trees across 17 cancer types and uses LLM-generated clinical scenarios for multiple-choice diagnostic questions.
- A two-stage quality selection process is used to identify high-quality samples for evaluation.
- Results show that even domain-specific LLMs often struggle with tasks requiring adherence to structured guidelines. Continued pretraining or including guidelines in context did not significantly improve performance. The importance of MedGUIDE in assessing the ability of LLMs to operate safely within real-world clinical settings is highlighted. 

<br /><br />Summary: <div>
arXiv:2505.11613v1 Announce Type: new 
Abstract: Clinical guidelines, typically structured as decision trees, are central to evidence-based medical practice and critical for ensuring safe and accurate diagnostic decision-making. However, it remains unclear whether Large Language Models (LLMs) can reliably follow such structured protocols. In this work, we introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to make guideline-consistent clinical decisions. MedGUIDE is constructed from 55 curated NCCN decision trees across 17 cancer types and uses clinical scenarios generated by LLMs to create a large pool of multiple-choice diagnostic questions. We apply a two-stage quality selection process, combining expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25 LLMs spanning general-purpose, open-source, and medically specialized models, and find that even domain-specific LLMs often underperform on tasks requiring structured guideline adherence. We also test whether performance can be improved via in-context guideline inclusion or continued pretraining. Our findings underscore the importance of MedGUIDE in assessing whether LLMs can operate safely within the procedural frameworks expected in real-world clinical settings.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations</title>
<link>https://arxiv.org/abs/2505.11615</link>
<guid>https://arxiv.org/abs/2505.11615</guid>
<content:encoded><![CDATA[
<div> behavioral methods, latent representations, steering vectors, large language models, neural activations
Summary:<br />
- Large language models' behavior can be changed by editing residual streams with steering vectors.
- Steering vectors modify internal neural activations without retraining the model.
- A systematic approach is proposed to identify steering vectors by aligning latent representations with neural counterparts.
- The approach focuses on extracting risk preferences from LLMs and steering their risk-related outputs using aligned representations.
- Results show that steering vectors effectively modulate LLM outputs to achieve targeted behavior.<br />Summary: <div>
arXiv:2505.11615v1 Announce Type: new 
Abstract: Changing the behavior of large language models (LLMs) can be as straightforward as editing the Transformer's residual streams using appropriately constructed "steering vectors." These modifications to internal neural activations, a form of representation engineering, offer an effective and targeted means of influencing model behavior without retraining or fine-tuning the model. But how can such steering vectors be systematically identified? We propose a principled approach for uncovering steering vectors by aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts. To evaluate this approach, we focus on extracting latent risk preferences from LLMs and steering their risk-related outputs using the aligned representations as steering vectors. We show that the resulting steering vectors successfully and reliably modulate LLM outputs in line with the targeted behavior.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering</title>
<link>https://arxiv.org/abs/2505.11626</link>
<guid>https://arxiv.org/abs/2505.11626</guid>
<content:encoded><![CDATA[
<div> metrics, RAG, question answering, evaluation, large language model<br />
Summary:<br />
THELMA is a framework designed for holistic evaluation of RAG (Retrieval Augmented Generation) based question answering applications. It consists of six interdependent metrics that allow developers and application owners to assess and enhance end-to-end RAG QA pipelines without the need for labeled sources or reference responses. The framework aims to assist in the monitoring and improvement of RAG QA applications by providing specific metrics for evaluation. The findings presented in the article highlight the dynamic interplay of THELMA metrics, enabling the identification of areas within RAG components that require enhancement. These insights can help developers optimize their QA applications by focusing on the specific components that need improvement. <div>
arXiv:2505.11626v1 Announce Type: new 
Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation</title>
<link>https://arxiv.org/abs/2505.11628</link>
<guid>https://arxiv.org/abs/2505.11628</guid>
<content:encoded><![CDATA[
<div> fine-tuning, expert demonstrations, Critique-Guided Distillation, entropy-based analysis, benchmark tasks 
Summary: 
Critique-Guided Distillation (CGD) addresses the imitation problem in supervised fine-tuning by integrating explanatory critiques and refined responses into the learning process. The framework trains a student model to understand both what to imitate and why by mapping prompts, teacher critiques, and student responses to refined teacher responses. By reducing refinement uncertainty and resembling a Bayesian posterior update, CGD achieves significant gains on math and language tasks while mitigating format drift issues seen in previous techniques. Extensive empirical evaluation shows improvements in math (AMC23 +17.5%) and language understanding tasks (MMLU-Pro +6.3%), highlighting the effectiveness of the approach. <br /><br /> <div>
arXiv:2505.11628v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from the imitation problem, where the model learns to reproduce the correct responses without \emph{understanding} the underlying rationale. To address this limitation, we propose \textsc{Critique-Guided Distillation (CGD)}, a novel multi-stage framework that integrates teacher model generated \emph{explanatory critiques} and \emph{refined responses} into the SFT process. A student model is then trained to map the triplet of prompt, teacher critique, and its own initial response to the corresponding refined teacher response, thereby learning both \emph{what} to imitate and \emph{why}. Using entropy-based analysis, we show that \textsc{CGD} reduces refinement uncertainty and can be interpreted as a Bayesian posterior update. We perform extensive empirical evaluation of \textsc{CGD}, on variety of benchmark tasks, and demonstrate significant gains on both math (AMC23 +17.5%) and language understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format drift issues observed in previous critique fine-tuning (CFT) techniques.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2</title>
<link>https://arxiv.org/abs/2505.11643</link>
<guid>https://arxiv.org/abs/2505.11643</guid>
<content:encoded><![CDATA[
<div> curriculum, reasoning transparency, sample-efficiency, language models, Cognivolve

Summary: 
- The study introduces a curriculum-based training approach for small language models, enhancing reasoning transparency and efficiency.
- Cognivolve, a 124 M-parameter GPT-2 model, is trained on a four-stage syllabus progressing from lexical matching to multi-step symbolic inference.
- Without task-specific fine-tuning, Cognivolve achieves target accuracy in half the optimization steps compared to a single-phase baseline.
- The curriculum activates more gradient-salient reasoning heads, shifting them towards deeper layers for higher-entropy attention and balanced contextual understanding.
- Applying the curriculum out of order or with optimizer resets does not replicate the performance gains, indicating the importance of progression over extra compute resources.
- Challenges remain in final-answer success and detection of verbal-knowledge heads, suggesting potential directions for mixed-stage fine-tuning and probe expansion. <div>
arXiv:2505.11643v1 Announce Type: new 
Abstract: We demonstrate that a developmentally ordered curriculum markedly improves reasoning transparency and sample-efficiency in small language models (SLMs). Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage syllabus that ascends from lexical matching to multi-step symbolic inference and then evaluate it without any task-specific fine-tuning. Cognivolve reaches target accuracy in half the optimization steps of a single-phase baseline, activates an order-of-magnitude more gradient-salient reasoning heads, and shifts those heads toward deeper layers, yielding higher-entropy attention that balances local and long-range context. The same curriculum applied out of order or with optimizer resets fails to reproduce these gains, confirming that progression--not extra compute--drives the effect. We also identify open challenges: final-answer success still lags a conventional run by about 30%, and our saliency probe under-detects verbal-knowledge heads in the hardest stage, suggesting directions for mixed-stage fine-tuning and probe expansion.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks</title>
<link>https://arxiv.org/abs/2505.11665</link>
<guid>https://arxiv.org/abs/2505.11665</guid>
<content:encoded><![CDATA[
<div> fluent multilingual prompting techniques, LLMs, NLP tasks, language families, resource levels <br />
Summary: <br />
- Researchers have developed fluent multilingual prompting techniques to enhance Large Language Models (LLMs) performance in diverse Natural Language Processing (NLP) tasks.
- Multilingual prompt engineering allows LLMs to excel in various linguistic settings without the need for extensive re-training or fine-tuning.
- The survey categorizes different multilingual prompting techniques based on the NLP tasks they address across a wide range of datasets spanning approximately 250 languages.
- Insights are derived across language families and resource levels, analyzing the distribution of NLP tasks by language resource types and prompting methods across different language families.
- The review covers 36 research papers detailing 39 prompting techniques applied to 30 multilingual NLP tasks, showcasing the rapid growth of interest in multilingual prompt engineering in recent years. <br /> <div>
arXiv:2505.11665v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ambiguity Resolution in Text-to-Structured Data Mapping</title>
<link>https://arxiv.org/abs/2505.11679</link>
<guid>https://arxiv.org/abs/2505.11679</guid>
<content:encoded><![CDATA[
<div> Ambiguity, natural language, text-to-structured data mapping, large language models, ReACT framework<br />
Summary:<br />
The study addresses ambiguity in natural language that hinders accurate mapping of text to structured data by large language models (LLMs). Existing methods for handling ambiguity involve trial and error or supervised fine-tuning. The proposed approach focuses on characterizing representation differences of ambiguous text in the latent space to detect ambiguity before mapping to structured data. The research emphasizes the relationship between ambiguous questions and their interpretations, highlighting why LLMs may ignore multiple interpretations. Rather than using dense embedding vectors for distance calculation, a new distance measurement is devised based on the observation that ambiguity stems from missing concepts in the LLM's latent space. This measurement, computed through the path kernel, relies on the integral of gradient values for each concept from a sparse autoencoder (SAE) under each state. The study aims to improve LLM performance on ambiguous agentic tool calling by predicting missing concepts. <div>
arXiv:2505.11679v1 Announce Type: new 
Abstract: Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods of ambiguity handling either exploit ReACT framework to produce the correct mapping through trial and error, or supervised fine tuning to guide models to produce a biased mapping to improve certain tasks. In this paper, we adopt a different approach that characterizes the representation difference of ambiguous text in the latent space and leverage the difference to identify ambiguity before mapping them to structured data. To detect ambiguity of a sentence, we focused on the relationship between ambiguous questions and their interpretations and what cause the LLM ignore multiple interpretations. Different to the distance calculated by dense embedding vectors, we utilize the observation that ambiguity is caused by concept missing in latent space of LLM to design a new distance measurement, computed through the path kernel by the integral of gradient values for each concepts from sparse-autoencoder (SAE) under each state. We identify patterns to distinguish ambiguous questions with this measurement. Based on our observation, We propose a new framework to improve the performance of LLMs on ambiguous agentic tool calling through missing concepts prediction.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation</title>
<link>https://arxiv.org/abs/2505.11683</link>
<guid>https://arxiv.org/abs/2505.11683</guid>
<content:encoded><![CDATA[
<div> Entity disambiguation, Dual Encoder, loss function, similarity metric, label verbalization format
<br />
Summary:
VerbalizED is a new model for entity disambiguation (ED) that utilizes Dual Encoders, focusing on key design decisions such as loss function, similarity metric, label verbalization format, and negative sampling strategy. The model includes contextual label verbalizations and efficient hard negative sampling, improving disambiguation accuracy. An iterative prediction variant aims to enhance challenging data point disambiguation. Extensive experiments on AIDA-Yago demonstrate the effectiveness of the approach, achieving a new State-of-the-Art system on the ZELDA benchmark. The study provides insights into influential design choices that contribute to the model's success. <div>
arXiv:2505.11683v1 Announce Type: new 
Abstract: Entity disambiguation (ED) is the task of linking mentions in text to corresponding entries in a knowledge base. Dual Encoders address this by embedding mentions and label candidates in a shared embedding space and applying a similarity metric to predict the correct label. In this work, we focus on evaluating key design decisions for Dual Encoder-based ED, such as its loss function, similarity metric, label verbalization format, and negative sampling strategy. We present the resulting model VerbalizED, a document-level Dual Encoder model that includes contextual label verbalizations and efficient hard negative sampling. Additionally, we explore an iterative prediction variant that aims to improve the disambiguation of challenging data points. Comprehensive experiments on AIDA-Yago validate the effectiveness of our approach, offering insights into impactful design choices that result in a new State-of-the-Art system on the ZELDA benchmark.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2505.11690</link>
<guid>https://arxiv.org/abs/2505.11690</guid>
<content:encoded><![CDATA[
<div> data scarcity, linguistic complexity, limited computational resources, acoustic variability, ethical concerns

Summary:<br />
This study addresses the challenges facing the development of Automatic Speech Recognition (ASR) systems for low-resource African languages. The key barriers include data scarcity, linguistic complexity, limited computational resources, acoustic variability, and ethical concerns. To overcome these challenges, the study suggests strategies such as community-driven data collection, self-supervised and multilingual learning, lightweight model architectures, and techniques for privacy preservation. Pilot projects in African languages demonstrate the feasibility and impact of tailored solutions like morpheme-based modeling and domain-specific ASR applications in healthcare and education. Interdisciplinary collaboration and sustained investment are crucial to tackle the linguistic and infrastructural obstacles in Africa. The study provides a roadmap for creating ethical, efficient, and inclusive ASR systems that enhance digital accessibility, promote socioeconomic participation, and preserve linguistic diversity in African languages. 

<br /><br /> <div>
arXiv:2505.11690v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) technologies have transformed human-computer interaction; however, low-resource languages in Africa remain significantly underrepresented in both research and practical applications. This study investigates the major challenges hindering the development of ASR systems for these languages, which include data scarcity, linguistic complexity, limited computational resources, acoustic variability, and ethical concerns surrounding bias and privacy. The primary goal is to critically analyze these barriers and identify practical, inclusive strategies to advance ASR technologies within the African context. Recent advances and case studies emphasize promising strategies such as community-driven data collection, self-supervised and multilingual learning, lightweight model architectures, and techniques that prioritize privacy. Evidence from pilot projects involving various African languages showcases the feasibility and impact of customized solutions, which encompass morpheme-based modeling and domain-specific ASR applications in sectors like healthcare and education. The findings highlight the importance of interdisciplinary collaboration and sustained investment to tackle the distinct linguistic and infrastructural challenges faced by the continent. This study offers a progressive roadmap for creating ethical, efficient, and inclusive ASR systems that not only safeguard linguistic diversity but also improve digital accessibility and promote socioeconomic participation for speakers of African languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Bracketing Encodings for Dependency Parsing as Tagging</title>
<link>https://arxiv.org/abs/2505.11693</link>
<guid>https://arxiv.org/abs/2505.11693</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence labeling, dependency parsing, hierarchical bracketing, projective encoding, treebanks

Summary: 
Hierarchical bracketing is utilized for a family of encodings in sequence labeling dependency parsing, with proven optimality over existing methods. The established 4-bit projective encoding, although belonging to this family, is shown to be suboptimal in terms of label efficiency for tree encoding. A more optimal hierarchical bracketing scheme has been derived, significantly reducing the number of labels required to encode projective trees to just 12, compared to 16 in the 4-bit encoding. This optimal hierarchical bracketing extends its support to include non-projective structures in a compact manner. These newly proposed encodings demonstrate competitive accuracy across various treebanks, showcasing their effectiveness in improving the efficiency and performance of sequence labeling dependency parsing. 

<br /><br />Summary: <div>
arXiv:2505.11693v1 Announce Type: new 
Abstract: We present a family of encodings for sequence labeling dependency parsing, based on the concept of hierarchical bracketing. We prove that the existing 4-bit projective encoding belongs to this family, but it is suboptimal in the number of labels used to encode a tree. We derive an optimal hierarchical bracketing, which minimizes the number of symbols used and encodes projective trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also extend optimal hierarchical bracketing to support arbitrary non-projectivity in a more compact way than previous encodings. Our new encodings yield competitive accuracy on a diverse set of treebanks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures</title>
<link>https://arxiv.org/abs/2505.11726</link>
<guid>https://arxiv.org/abs/2505.11726</guid>
<content:encoded><![CDATA[
<div> phrase grounding, multimodal reference resolution, dialogue, coreference resolution, predicate-argument structure analysis

Summary: 

- The paper discusses the importance of integrating textual and multimodal reference resolution for understanding semantic relations between mentions and real-world objects in dialogue.
- A framework is presented that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity.
- Experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, improves performance in multimodal reference resolution.
- The model with coreference resolution performs better in pronoun phrase grounding compared to existing models MDETR and GLIP.
- Qualitative analysis reveals that incorporating textual reference relations increases confidence scores between mentions, including pronouns and predicates, and objects, reducing ambiguities in visually grounded dialogues. 

<br /><br />Summary: <div>
arXiv:2505.11726v1 Announce Type: new 
Abstract: Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports</title>
<link>https://arxiv.org/abs/2505.11733</link>
<guid>https://arxiv.org/abs/2505.11733</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical diagnosis, Diagnostic reasoning, Dataset, Clinical reasoning

Summary:<br /><br />
Doctors and patients use Large Language Models (LLMs) for diagnosing clinical cases, but current benchmarks only assess final answer accuracy. To address this limitation, a new dataset called MedCaseReasoning evaluates LLMs' alignment with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic cases with detailed reasoning statements from medical reports. State-of-the-art LLMs show shortcomings in diagnoses and reasoning. Fine-tuning LLMs on MedCaseReasoning improves diagnostic accuracy and clinical reasoning recall. The top-performing open-source model, DeepSeek-R1, achieves 48% 10-shot diagnostic accuracy and mentions 64% of clinician reasoning statements. This dataset, code, and models are available for use, highlighting the importance of accurate clinical reasoning in medical diagnosis.Visit https://github.com/kevinwu23/Stanford-MedCaseReasoning for more information. 

<br /><br /> <div>
arXiv:2505.11733v1 Announce Type: new 
Abstract: Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training</title>
<link>https://arxiv.org/abs/2505.11739</link>
<guid>https://arxiv.org/abs/2505.11739</guid>
<content:encoded><![CDATA[
<div> attention tuning, large language models, ZeroTuning, token-level, model performance

Summary: 
The paper introduces ZeroTuning, a method that enhances the performance of large language models (LLMs) by adjusting the attention given to the semantically empty initial token. Theoretical analysis reveals that tuning the initial token's attention influences the attention distribution over subsequent tokens. ZeroTuning proves more effective in improving LLM performance compared to tuning other task-specific tokens, with earlier layers showing a greater impact. Different attention heads exhibit distinct preferences in attending to the initial token. Empirical results demonstrate the efficacy of ZeroTuning across various tasks and LLM models, achieving significant performance gains. The approach remains robust under various conditions like limited resources, few-shot settings, and prompt variations. ZeroTuning sheds light on an overlooked control point in LLMs, offering insights into both inference-time tuning and model interpretability. 

<br /><br />Summary: <div>
arXiv:2505.11739v1 Announce Type: new 
Abstract: Recently, training-free methods for improving large language models (LLMs) have attracted growing interest, with token-level attention tuning emerging as a promising and interpretable direction. However, existing methods typically rely on auxiliary mechanisms to identify important or irrelevant task-specific tokens, introducing potential bias and limiting applicability. In this paper, we uncover a surprising and elegant alternative: the semantically empty initial token is a powerful and underexplored control point for optimizing model behavior. Through theoretical analysis, we show that tuning the initial token's attention sharpens or flattens the attention distribution over subsequent tokens, and its role as an attention sink amplifies this effect. Empirically, we find that: (1) tuning its attention improves LLM performance more effectively than tuning other task-specific tokens; (2) the effect follows a consistent trend across layers, with earlier layers having greater impact, but varies across attention heads, with different heads showing distinct preferences in how they attend to this token. Based on these findings, we propose ZeroTuning, a training-free approach that improves LLM performance by applying head-specific attention adjustments to this special token. Despite tuning only one token, ZeroTuning achieves higher performance on text classification, multiple-choice, and multi-turn conversation tasks across models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its multi-turn score from 7.804 to 7.966. The method is also robust to limited resources, few-shot settings, long contexts, quantization, decoding strategies, and prompt variations. Our work sheds light on a previously overlooked control point in LLMs, offering new insights into both inference-time tuning and model interpretability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Masking Improves Transformer-Based Text Classification</title>
<link>https://arxiv.org/abs/2505.11746</link>
<guid>https://arxiv.org/abs/2505.11746</guid>
<content:encoded><![CDATA[
<div> masking, token, regularization, transformer-based models, text classification <br />
<br />
Summary: 
The article introduces token masking regularization, a method that randomly replaces input tokens with a special [MASK] token during training to enhance transformer-based models in text classification tasks. The proposed approach introduces stochastic perturbations that encourage the model to capture deeper inter-token dependencies by implicitly averaging gradients. Experimental results across different models show consistent improvements in language identification and sentiment analysis tasks. Optimal masking rates are identified for specific tasks, with a recommended default value of p = 0.1. The benefits of the approach are attributed to reducing overfitting through input perturbation and acting as implicit ensembling through gradient-level smoothing. <div>
arXiv:2505.11746v1 Announce Type: new 
Abstract: While transformer-based models achieve strong performance on text classification, we explore whether masking input tokens can further enhance their effectiveness. We propose token masking regularization, a simple yet theoretically motivated method that randomly replaces input tokens with a special [MASK] token at probability p. This introduces stochastic perturbations during training, leading to implicit gradient averaging that encourages the model to capture deeper inter-token dependencies. Experiments on language identification and sentiment analysis -- across diverse models (mBERT, Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard regularization techniques. We identify task-specific optimal masking rates, with p = 0.1 as a strong general default. We attribute the gains to two key effects: (1) input perturbation reduces overfitting, and (2) gradient-level smoothing acts as implicit ensembling.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation</title>
<link>https://arxiv.org/abs/2505.11754</link>
<guid>https://arxiv.org/abs/2505.11754</guid>
<content:encoded><![CDATA[
<div> Encoder-decoder, Flan T5, multi-hop reasoning, LM attention weights, GitHub repository <br />
Summary: <br />
1) Encoder-decoder models like those in the Flan T5 family outperform causal decoder-only LMs in multi-hop question answering tasks, despite their smaller size. <br />
2) Altering the order of gold documents reveals performance trends, with optimal results when the order aligns with the reasoning chain sequence. <br />
3) Enhancing causal decoder-only models with bi-directional attention by modifying the causal mask improves their performance. <br />
4) LM attention weights peak at higher values when the answer is correct, allowing for heuristic performance improvements. <br />
5) The study provides insights into the behavior of LMs in multi-hop question answering tasks and offers a publicly available code repository for further research. <br /> <div>
arXiv:2505.11754v1 Announce Type: new 
Abstract: Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Semantics With Large Language Models</title>
<link>https://arxiv.org/abs/2505.11764</link>
<guid>https://arxiv.org/abs/2505.11764</guid>
<content:encoded><![CDATA[
<div> semantic primes, Natural Semantic Metalanguage, explications, large language models, automatic evaluation

Summary:
- The Natural Semantic Metalanguage (NSM) theory is based on semantic primes, basic word meanings found in all languages.
- Any word can be paraphrased using these primes to reveal a universally translatable meaning.
- NSM explications, or paraphrases, have potential applications in various natural language processing (NLP) tasks.
- This study explores using large language models (LLMs) to generate NSM explications, a traditionally manual process.
- The introduced models, 1B and 8B, outperform GPT-4o in producing accurate and cross-translatable explications, advancing universal semantic representation with LLMs and enabling new applications in semantic analysis and translation.

<br /><br />Summary: <div>
arXiv:2505.11764v1 Announce Type: new 
Abstract: The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospex: Language Agent Meets Offline Reinforcement Learning Critic</title>
<link>https://arxiv.org/abs/2505.11807</link>
<guid>https://arxiv.org/abs/2505.11807</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrospex, Reinforcement Learning, past experiences, agent framework <br />
Summary:
The paper introduces Retrospex, a new agent framework based on Large Language Models (LLMs) that leverages past experiences for improved performance. Unlike existing frameworks, Retrospex does not directly incorporate past experiences into the LLM's context but combines action likelihood with values estimated by a Reinforcement Learning (RL) Critic trained through a retrospective process. Additionally, Retrospex utilizes a dynamic action rescoring mechanism to enhance the importance of experience-based values for tasks requiring more interaction with the environment. Evaluation in ScienceWorld, ALFWorld, and Webshop environments showcases Retrospex's superiority over contemporary baselines in terms of performance and efficiency. The framework exhibits enhanced knowledge and reasoning capabilities, making it a valuable tool for creating powerful agents in various applications. <br /><br />Summary: <div>
arXiv:2505.11807v1 Announce Type: new 
Abstract: Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model</title>
<link>https://arxiv.org/abs/2505.11810</link>
<guid>https://arxiv.org/abs/2505.11810</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, Classical Chinese, AI Taiyan, domain-specific, information processing

Summary: 
AI Taiyan is a large language model developed specifically for understanding and generating Classical Chinese texts. With 1.8 billion parameters, it outperforms both general-purpose models and traditional domain-specific models in tasks such as punctuation, identification of allusions, and translation. The model showcases high accuracy levels, close to or surpassing human baselines, showcasing its effectiveness in processing Classical Chinese information. The research provides insights into efficiently constructing specialized domain-specific language models and discusses applications in ancient text collation, dictionary editing, and language research through case studies.<br /><br />Summary: <div>
arXiv:2505.11810v1 Announce Type: new 
Abstract: General-purpose large language models demonstrate notable capabilities in language comprehension and generation, achieving results that are comparable to, or even surpass, human performance in many language information processing tasks. Nevertheless, when general models are applied to some specific domains, e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and fine-tuning open-source foundational models similarly struggles to adequately incorporate domain-specific knowledge. To address this challenge, this study developed a large language model, AI Taiyan, specifically designed for understanding and generating Classical Chinese. Experiments show that with a reasonable model design, data processing, foundational training, and fine-tuning, satisfactory results can be achieved with only 1.8 billion parameters. In key tasks related to Classical Chinese information processing such as punctuation, identification of allusions, explanation of word meanings, and translation between ancient and modern Chinese, this model exhibits a clear advantage over both general-purpose large models and domain-specific traditional models, achieving levels close to or surpassing human baselines. This research provides a reference for the efficient construction of specialized domain-specific large language models. Furthermore, the paper discusses the application of this model in fields such as the collation of ancient texts, dictionary editing, and language research, combined with case studies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2505.11811</link>
<guid>https://arxiv.org/abs/2505.11811</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-hop question answering, large language models, Bi-levEL muLti-agEnt reasoning, question types, BELLE framework

Summary: 
The paper analyzes multi-hop question answering benchmarks and identifies four question types. It evaluates different methods for multi-hop QA and introduces the Bi-levEL muLti-agEnt reasoning (BELLE) framework, which matches question types with specific methods. BELLE consists of multiple agents debating to create a comprehensive plan using different ''operators''. The model incorporates fast and slow debaters to ensure rational changes in viewpoints. Experimental results show that BELLE outperforms strong baselines across various datasets. It proves to be more cost-effective in complex multi-hop QA scenarios compared to single models. The study highlights the importance of aligning question types with appropriate methods for effective multi-hop question answering. 

<br /><br />Summary: <div>
arXiv:2505.11811v1 Announce Type: new 
Abstract: Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ''operator'' by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ''operators'' to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Model Learning for Language Model</title>
<link>https://arxiv.org/abs/2505.11820</link>
<guid>https://arxiv.org/abs/2505.11820</guid>
<content:encoded><![CDATA[
<div> Chain-of-Model, causal relationship, scalability, model training, inference flexibility
<br />
Summary:
The paper introduces a novel learning paradigm called Chain-of-Model (CoM) that incorporates the causal relationship into hidden states, leading to improved scalability in model training and flexibility in deployment. The concept of Chain-of-Representation (CoR) is introduced, forming hidden states as a combination of multiple sub-representations. The model built on the CoM framework can scale up by increasing chains based on previous models and offer multiple sub-models for elastic inference. Chain-of-Language-Model (CoLM) integrates CoM into each layer of the Transformer architecture. CoLM-Air, a variation of CoLM, introduces a KV sharing mechanism for added extensibility. Experimental results show that the CoLM family achieves performance similar to the standard Transformer while enabling progressive scaling and multiple model sizes for elastic inference. This framework provides a new approach to building language models. 
<br /><br />Summary: <div>
arXiv:2505.11820v1 Announce Type: new 
Abstract: In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11827</link>
<guid>https://arxiv.org/abs/2505.11827</guid>
<content:encoded><![CDATA[
<div> Keywords: long chain-of-thought compression, large language models, reasoning efficiency, automated chunking, collaborative reasoning

Summary:<br />
The study focuses on compressing long chain-of-thought from large language models to enhance reasoning efficiency. It examines the importance of different thoughts in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. A metric is proposed to measure the effectiveness and efficiency of various thoughts. The Long$\otimes$Short framework is introduced, where two LLMs collaborate to solve problems effectively and efficiently. Fine-tuning for long-thought and short-thought reasoning styles is done using cold-start data. Synergizing-oriented multi-turn reinforcement learning is applied to promote model self-evolution and collaboration between the two LLMs. Experimental results demonstrate that the approach achieves comparable performance while significantly reducing token length across various benchmarks. The data and code for the study are available on GitHub. 

Summary: <div>
arXiv:2505.11827v1 Announce Type: new 
Abstract: Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at https://github.com/yasNing/Long-otimes-Short/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2505.11829</link>
<guid>https://arxiv.org/abs/2505.11829</guid>
<content:encoded><![CDATA[
<div> Keywords: deviant language, sexism detection, metaphor detection, sarcasm detection, ClaD

Summary:
ClaD is introduced as a novel training paradigm for detecting deviant and nuanced language in online social discourse. It aims to distill small target classes from diverse backgrounds efficiently. ClaD incorporates a loss function based on Mahalanobis distance and an interpretable decision algorithm for class separation. It outperforms competitive baselines in detecting sexism, metaphor, and sarcasm, even with smaller language models and fewer parameters. The results indicate that ClaD is effective for pragmatic language understanding tasks requiring extraction of small target classes from heterogeneous backgrounds. <div>
arXiv:2505.11829v1 Announce Type: new 
Abstract: Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of online social discourse. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose \textbf{Cla}ss \textbf{D}istillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks -- sexism, metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models (LLMs). These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Collaborative Defense for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11835</link>
<guid>https://arxiv.org/abs/2505.11835</guid>
<content:encoded><![CDATA[
<div> robustness, security, large language models, multilingual safeguarding, jailbreak

Summary:
Multilingual collaborative defense (MCD) is proposed as a novel learning method to enhance the safeguarding of large language models (LLMs) in multilingual scenarios. MCD optimizes a continuous, soft safety prompt automatically to improve safeguarding performance across multiple languages, maintain strong generalization capabilities, and mitigate language safety misalignment. The effectiveness of MCD is evaluated using manually constructed multilingual jailbreak benchmarks, such as MaliciousInstruct and AdvBench, showing superior performance in safeguarding against multilingual jailbreak attempts. Additionally, MCD exhibits strong language transfer capabilities, especially in underrepresented languages. This approach addresses the urgent need for multilingual safety in LLMs and offers a promising solution to the vulnerabilities posed by translating harmful queries into rare or underrepresented languages. The code for MCD is available at https://github.com/HLiang-Lee/MCD.

<br /><br />Summary: <div>
arXiv:2505.11835v1 Announce Type: new 
Abstract: The robustness and security of large language models (LLMs) has become a prominent research area. One notable vulnerability is the ability to bypass LLM safeguards by translating harmful queries into rare or underrepresented languages, a simple yet effective method of "jailbreaking" these models. Despite the growing concern, there has been limited research addressing the safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to enhance multilingual safety. In this work, we investigate the correlation between various attack features across different languages and propose Multilingual Collaborative Defense (MCD), a novel learning method that optimizes a continuous, soft safety prompt automatically to facilitate multilingual safeguarding of LLMs. The MCD approach offers three advantages: First, it effectively improves safeguarding performance across multiple languages. Second, MCD maintains strong generalization capabilities while minimizing false refusal rates. Third, MCD mitigates the language safety misalignment caused by imbalances in LLM training corpora. To evaluate the effectiveness of MCD, we manually construct multilingual versions of commonly used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess various safeguarding methods. Additionally, we introduce these datasets in underrepresented (zero-shot) languages to verify the language transferability of MCD. The results demonstrate that MCD outperforms existing approaches in safeguarding against multilingual jailbreak attempts while also exhibiting strong language transfer capabilities. Our code is available at https://github.com/HLiang-Lee/MCD.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research</title>
<link>https://arxiv.org/abs/2505.11855</link>
<guid>https://arxiv.org/abs/2505.11855</guid>
<content:encoded><![CDATA[
<div> verifiers, scientific manuscripts, AI Co-Scientists, large language models, dataset

Summary:
SPOT dataset created for academic verification of scientific manuscripts includes errors from 83 papers. State-of-the-art LLMs show limited recall and precision in identifying errors. Confidence estimates are low, and models struggle to rediscover errors consistently. Qualitative analysis indicates errors resemble student-level misconceptions. Current LLM capabilities fall short of reliable AI-assisted academic verification.
<br /><br />Summary: <div>
arXiv:2505.11855v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization</title>
<link>https://arxiv.org/abs/2505.11876</link>
<guid>https://arxiv.org/abs/2505.11876</guid>
<content:encoded><![CDATA[
<div> noise, model editing, transformers, knowledge update, LLMs

Summary:
NAMET is a noise-aware model editing technique proposed to address embedding collisions among knowledge items in large language models (LLMs). It introduces noise during memory extraction, outperforming existing methods in massive editing scenarios. The one-line modification to MEMIT enhances editing reliability at scale, making it more effective in context-rich settings. Extensive experiments across six LLMs and three datasets showcase NAMET's superiority when editing thousands of facts. This simple yet effective approach improves the efficiency of updating knowledge in LLMs, making it a valuable tool for researchers and practitioners working with large language models. NAMET proves to be essential in overcoming the limitations of existing model editing techniques, offering a reliable solution for managing large-scale knowledge updates in the context of language models. 

Summary: <br />
NAMET is a noise-aware model editing technique that addresses embedding collisions in large language models. It outperforms existing methods in massive editing scenarios by introducing noise during memory extraction. The one-line modification to MEMIT enhances editing reliability at scale, making it more effective in context-rich settings. Extensive experiments across six LLMs and three datasets demonstrate NAMET's superiority when editing thousands of facts. This approach improves efficiency in updating knowledge in LLMs, making it a valuable tool for researchers and practitioners working with large language models. <div>
arXiv:2505.11876v1 Announce Type: new 
Abstract: Model editing techniques are essential for efficiently updating knowledge in large language models (LLMs). However, the effectiveness of existing approaches degrades in massive editing scenarios, particularly when evaluated with practical metrics or in context-rich settings. We attribute these failures to embedding collisions among knowledge items, which undermine editing reliability at scale. To address this, we propose NAMET (Noise-aware Model Editing in Transformers), a simple yet effective method that introduces noise during memory extraction via a one-line modification to MEMIT. Extensive experiments across six LLMs and three datasets demonstrate that NAMET consistently outperforms existing methods when editing thousands of facts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation</title>
<link>https://arxiv.org/abs/2505.11887</link>
<guid>https://arxiv.org/abs/2505.11887</guid>
<content:encoded><![CDATA[
<div> evaluation, medical language models, AutoMedEval, question-answering proficiency, hierarchical training method

Summary:
AutoMedEval is a new automatic evaluation model designed to assess the question-answering proficiency of medical language models. It addresses the limitations of traditional metrics by focusing on medical terminology. The model, with 13B parameters, uses a hierarchical training method and iterative knowledge introspection to mimic professional medical assessment capabilities. It aims to reduce the reliance on costly and potentially inaccurate human evaluations. AutoMedEval outperforms other baselines in correlation with human judgments, making it a valuable tool for evaluating the quality of responses generated by medical language models. <div>
arXiv:2505.11887v1 Announce Type: new 
Abstract: With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents</title>
<link>https://arxiv.org/abs/2505.11891</link>
<guid>https://arxiv.org/abs/2505.11891</guid>
<content:encoded><![CDATA[
<div> Keywords: VLM-based mobile agents, benchmark, multi-path evaluation, noisy environment, proactive interaction

Summary:
Mobile-Bench-v2 is a new benchmark designed to evaluate VLM-based mobile agents' capabilities in interacting with smartphone GUIs and completing tasks. It addresses limitations of existing benchmarks by including a common task split with multi-path evaluation to assess agents' ability to obtain step rewards, a noisy split with pop-ups and ads apps for a real noisy environment, and an ambiguous instruction split with preset Q&amp;A interactions to test proactive interaction capabilities. Evaluation is conducted using various mobile agents, including AppAgent-v1, Mobile-Agent-v2, UI-Tars, and OS-Atlas. The benchmark aims to provide a realistic and comprehensive assessment of mobile agents' performance in dynamic environments and their ability to handle noise and engage in proactive interactions. The code and data for Mobile-Bench-v2 are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.

<br /><br />Summary: <div>
arXiv:2505.11891v1 Announce Type: new 
Abstract: VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline benchmarks evaluate the agents through single-path trajectories, which stands in contrast to the inherently multi-solution characteristics of GUI tasks. Additionally, both types of benchmarks fail to assess whether mobile agents can handle noise or engage in proactive interactions due to a lack of noisy apps or overly full instructions during the evaluation process. To address these limitations, we use a slot-based instruction generation method to construct a more realistic and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a common task split, with offline multi-path evaluation to assess the agent's ability to obtain step rewards during task execution. It contains a noisy split based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to formulate a real noisy environment. Furthermore, an ambiguous instruction split with preset Q\&amp;A interactions is released to evaluate the agent's proactive interaction capabilities. We conduct evaluations on these splits using the single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2, as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving</title>
<link>https://arxiv.org/abs/2505.11893</link>
<guid>https://arxiv.org/abs/2505.11893</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Adaptive Planning, Large Language Models, Natural Language Processing, Multi-step Planning
Summary:<br /><br />The paper introduces a Reinforcement Learning enhanced Adaptive Planning framework (RLAP) to improve the performance of large language models (LLMs) on multi-step natural language processing (NLP) tasks. They model NLP tasks as Markov decision processes and use an Actor model trained through reinforcement learning to estimate Q-values for sequences of states and actions. This allows for considering linguistic features during planning and interaction with the LLM to determine the optimal order of subtasks for each task instance. RLAP is applied to various NLP tasks and tested on multiple datasets, demonstrating its effectiveness and robustness. The framework improves task-solving outcomes by leveraging the linguistic features of task instances and guiding LLMs in a more structured manner through adaptive planning. <div>
arXiv:2505.11893v1 Announce Type: new 
Abstract: Multi-step planning has been widely employed to enhance the performance of large language models (LLMs) on downstream natural language processing (NLP) tasks, which decomposes the original task into multiple subtasks and guide LLMs to solve them sequentially without additional training. When addressing task instances, existing methods either preset the order of steps or attempt multiple paths at each step. However, these methods overlook instances' linguistic features and rely on the intrinsic planning capabilities of LLMs to evaluate intermediate feedback and then select subtasks, resulting in suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this paper we propose a Reinforcement Learning enhanced Adaptive Planning framework (RLAP). In our framework, we model an NLP task as a Markov decision process (MDP) and employ an LLM directly into the environment. In particular, a lightweight Actor model is trained to estimate Q-values for natural language sequences consisting of states and actions through reinforcement learning. Therefore, during sequential planning, the linguistic features of each sequence in the MDP can be taken into account, and the Actor model interacts with the LLM to determine the optimal order of subtasks for each task instance. We apply RLAP on three different types of NLP tasks and conduct extensive experiments on multiple datasets to verify RLAP's effectiveness and robustness.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data</title>
<link>https://arxiv.org/abs/2505.11900</link>
<guid>https://arxiv.org/abs/2505.11900</guid>
<content:encoded><![CDATA[
<div> Keywords: question answering, mixed sources, personal information, ReQAP, PerQA

Summary: 
ReQAP introduces a new method for question answering over heterogeneous data sources, such as text and tables, with a focus on personal information stored on user devices. The method creates an executable operator tree that enables seamless integration of structured and unstructured data sources to provide users with convenient access to their personal data while keeping it securely on their devices. The PerQA benchmark is released along with this method, containing persona-based data and questions that cover a wide range of realistic user needs. This benchmark aims to challenge existing question answering systems to handle complex queries and provide traceable answers. <div>
arXiv:2505.11900v1 Announce Type: new 
Abstract: Question answering over mixed sources, like text and tables, has been advanced by verbalizing all contents and encoding it with a language model. A prominent case of such heterogeneous data is personal information: user devices log vast amounts of data every day, such as calendar entries, workout statistics, shopping records, streaming history, and more. Information needs range from simple look-ups to queries of analytical nature. The challenge is to provide humans with convenient access with small footprint, so that all personal data stays on the user devices. We present ReQAP, a novel method that creates an executable operator tree for a given question, via recursive decomposition. Operators are designed to enable seamless integration of structured and unstructured sources, and the execution of the operator tree yields a traceable answer. We further release the PerQA benchmark, with persona-based data and questions, covering a diverse spectrum of realistic user needs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELITE: Embedding-Less retrieval with Iterative Text Exploration</title>
<link>https://arxiv.org/abs/2505.11908</link>
<guid>https://arxiv.org/abs/2505.11908</guid>
<content:encoded><![CDATA[
<div> Framework, Retrieval, Language Models, Long-context QA, Logical Inferencing <br />
Summary: <br />
This paper introduces a new approach to Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) that addresses the issue of limited long-term context retention. The proposed method does not rely on embeddings for retrieval but instead utilizes the logical inferencing capability of LLMs. By iteratively refining the search space guided by an importance measure, the approach enhances retrieval accuracy without the need for explicit graph construction. Experiments on long-context QA benchmarks such as NovelQA and Marathon demonstrate that the method outperforms existing baselines while significantly reducing storage and runtime requirements. The framework proves effective in leveraging LLMs for retrieval tasks, providing a more nuanced and accurate approach to accessing external information for document-level or multi-turn tasks. <br /> <div>
arXiv:2505.11908v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved impressive progress in natural language processing, but their limited ability to retain long-term context constrains performance on document-level or multi-turn tasks. Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant information from an external corpus. However, existing RAG systems often rely on embedding-based retrieval trained on corpus-level semantic similarity, which can lead to retrieving content that is semantically similar in form but misaligned with the question's true intent. Furthermore, recent RAG variants construct graph- or hierarchy-based structures to improve retrieval accuracy, resulting in significant computation and storage overhead. In this paper, we propose an embedding-free retrieval framework. Our method leverages the logical inferencing ability of LLMs in retrieval using iterative search space refinement guided by our novel importance measure and extend our retrieval results with logically related information without explicit graph construction. Experiments on long-context QA benchmarks, including NovelQA and Marathon, show that our approach outperforms strong baselines while reducing storage and runtime by over an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning</title>
<link>https://arxiv.org/abs/2505.11922</link>
<guid>https://arxiv.org/abs/2505.11922</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, instruction-following, fine-tuning, MISO, transformer-based

Summary:
Large language models (LLMs) are powerful in handling natural language tasks but struggle with complex instructions. Post-training LLMs using supervised fine-tuning (SFT) is a common approach to enhance instruction following. Existing methods focus on synthesizing complex instruction-output pairs for SFT, but may overlook crucial sub-contexts. This work introduces MISO, an extension to decoder-only transformer-based LLMs, that transforms structured input instructions into multiple parallel subcontexts to improve SFT effectiveness. MISO utilizes a mixture-of-contexts paradigm to consider both overall alignment and individual sub-context influence. Applying MISO fine-tuning to complex instruction-following datasets shows its superiority in handling complex instructions and potential for training efficiency. MISO offers a promising solution for improving LLMs in complex instruction-following scenarios. 

<br /><br />Summary: <div>
arXiv:2505.11922v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable capabilities in handling natural language tasks; however, they may struggle to consistently follow complex instructions including those involve multiple constraints. Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to improve their ability to follow instructions. In addressing complex instruction following, existing efforts primarily focus on data-driven methods that synthesize complex instruction-output pairs for SFT. However, insufficient attention allocated to crucial sub-contexts may reduce the effectiveness of SFT. In this work, we propose transforming sequentially structured input instruction into multiple parallel instructions containing subcontexts. To support processing this multi-input, we propose MISO (Multi-Input Single-Output), an extension to currently dominant decoder-only transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that jointly considers the overall instruction-output alignment and the influence of individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning to complex instructionfollowing datasets and evaluate it with standard LLM inference. Empirical results demonstrate the superiority of MISO as a fine-tuning method for LLMs, both in terms of effectiveness in complex instruction-following scenarios and its potential for training efficiency.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts</title>
<link>https://arxiv.org/abs/2505.11924</link>
<guid>https://arxiv.org/abs/2505.11924</guid>
<content:encoded><![CDATA[
<div> Keywords: intrinsic self-correction, language model, hidden states, output distributions, prompt-induced shifts

Summary: 
This article explores the performance gains of intrinsic self-correction in language models, particularly focusing on the impact of prompting on hidden states and output distributions. The study proposes that prompt-induced shifts in language models can be explained by linear representation vectors, leading to a separation of tokens based on concept alignment. By mathematically formulating self-correction and analyzing alignment magnitudes, the research demonstrates a concentration result for output tokens. Experiments conducted on text detoxification with the zephyr-7b-sft model reveal significant differences in prompt-induced shifts when instructed with toxic prompts, indicating an enhanced capability of latent concept recognition in language models. The analysis sheds light on the underlying mechanisms of self-correction and provides insights into how prompting influences a language model's behavior in an explainable manner.

Summary: <div>
arXiv:2505.11924v1 Announce Type: new 
Abstract: We provide an explanation for the performance gains of intrinsic self-correction, a process where a language model iteratively refines its outputs without external feedback. More precisely, we investigate how prompting induces interpretable changes in hidden states and thus affects the output distributions. We hypothesize that each prompt-induced shift lies in a linear span of some linear representation vectors, naturally separating tokens based on individual concept alignment. Building around this idea, we give a mathematical formulation of self-correction and derive a concentration result for output tokens based on alignment magnitudes. Our experiments on text detoxification with zephyr-7b-sft reveal a substantial gap in the inner products of the prompt-induced shifts and the unembeddings of the top-100 most toxic tokens vs. those of the unembeddings of the bottom-100 least toxic tokens, under toxic instructions. This suggests that self-correction prompts enhance a language model's capability of latent concept recognition. Our analysis offers insights into the underlying mechanism of self-correction by characterizing how prompting works explainably. For reproducibility, our code is available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Query Compiler</title>
<link>https://arxiv.org/abs/2505.11932</link>
<guid>https://arxiv.org/abs/2505.11932</guid>
<content:encoded><![CDATA[
<div> Neuro-symbolic framework, Query intent recognition, Retrieval-augmented generation, Compiler design, Backus-Naur Form<br />
Summary:<br />
The paper introduces QCompiler, a neuro-symbolic framework for precise recognition of search intent in Retrieval-Augmented Generation systems. It utilizes a minimal Backus-Naur Form grammar to formalize complex queries, improving query understanding under resource constraints. QCompiler includes components like Query Expression Translator and Lexical Syntax Parser to compile queries into Abstract Syntax Trees, ensuring atomicity of sub-queries for more precise document retrieval and response generation. By minimizing redundancy and maintaining completeness, this approach bridges the gap in addressing complex queries, enhancing the RAG system's performance. <div>
arXiv:2505.11932v1 Announce Type: new 
Abstract: Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing</title>
<link>https://arxiv.org/abs/2505.11935</link>
<guid>https://arxiv.org/abs/2505.11935</guid>
<content:encoded><![CDATA[
<div> benchmark, chart editing, multimodal large language models, evaluation framework, code generation
Summary:
The article introduces ChartEdit, a new benchmark designed for evaluating chart editing tasks using multimodal large language models (MLLMs). The benchmark consists of 1,405 diverse editing instructions applied to 233 real-world charts, with manual annotation and validation for accuracy. Evaluation of 10 mainstream MLLMs shows that while large-scale models can partially match reference images, precise edits according to instructions remain a challenge. The state-of-the-art model achieves a score of only 59.96, indicating limitations in accurate modifications. Small-scale and chart-domain models struggle with both following instructions and generating overall chart images, highlighting the need for further development in this area.
<br /><br />Summary: <div>
arXiv:2505.11935v1 Announce Type: new 
Abstract: Although multimodal large language models (MLLMs) show promise in generating chart rendering code, chart editing presents a greater challenge. This difficulty stems from its nature as a labor-intensive task for humans that also demands MLLMs to integrate chart understanding, complex reasoning, and precise intent interpretation. While many MLLMs claim such editing capabilities, current assessments typically rely on limited case studies rather than robust evaluation methodologies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose ChartEdit, a new high-quality benchmark designed for chart editing tasks. This benchmark comprises $1,405$ diverse editing instructions applied to $233$ real-world charts, with each instruction-chart instance having been manually annotated and validated for accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream MLLMs across two types of experiments, assessing them at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at https://github.com/xxlllz/ChartEdit.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning</title>
<link>https://arxiv.org/abs/2505.11958</link>
<guid>https://arxiv.org/abs/2505.11958</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Hate speech, Hierarchical Prefix learning, Preference Optimization, IntentCONANv2

Summary: 
HiPPrO introduces a novel framework for generating counterspeech online, taking into account multiple attributes simultaneously to produce more nuanced and effective responses. The framework utilizes hierarchical prefix learning and preference optimization in a two-stage process to optimize attribute-specific prefix embedding spaces during counterspeech generation. By incorporating emotion labels and optimizing for intent conformity and relevance through preference optimization, HiPPrO outperforms baseline models in intent conformity and Rouge score metrics. Human evaluations confirm the enhanced relevance and appropriateness of the generated counterspeech, highlighting the potential of multi-attribute conditioning in improving the efficacy of counterspeech generation systems. <div>
arXiv:2505.11958v1 Announce Type: new 
Abstract: Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English</title>
<link>https://arxiv.org/abs/2505.11959</link>
<guid>https://arxiv.org/abs/2505.11959</guid>
<content:encoded><![CDATA[
<div> Keywords: bilingual dataset, emotions, hope speech, Arabic, English

Summary:
This research introduces a bilingual dataset containing entries for Arabic and English, annotated for emotions and hope speech. The dataset addresses the lack of multi-emotion datasets by providing annotations for emotion intensity, complexity, and causes, as well as detailed classifications for hope speech. Fleiss' Kappa was used to ensure annotation reliability, showing high agreement among annotators for both languages. The evaluation metrics from the baseline model validate the quality of the annotations. This dataset serves as a valuable resource for enhancing natural language processing in underrepresented languages and facilitating cross-linguistic analysis of emotions and hope speech. <div>
arXiv:2505.11959v1 Announce Type: new 
Abstract: This research introduces a bilingual dataset comprising 23,456 entries for Arabic and 10,036 entries for English, annotated for emotions and hope speech, addressing the scarcity of multi-emotion (Emotion and hope) datasets. The dataset provides comprehensive annotations capturing emotion intensity, complexity, and causes, alongside detailed classifications and subcategories for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed, revealing 0.75-0.85 agreement among annotators both for Arabic and English language. The evaluation metrics (micro-F1-Score=0.67) obtained from the baseline model (i.e., using a machine learning model) validate that the data annotations are worthy. This dataset offers a valuable resource for advancing natural language processing in underrepresented languages, fostering better cross-linguistic analysis of emotions and hope speech.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation</title>
<link>https://arxiv.org/abs/2505.11965</link>
<guid>https://arxiv.org/abs/2505.11965</guid>
<content:encoded><![CDATA[
<div> hallucinations, question-answering systems, Large Language Models, Mu-SHROOM, Central China Normal University 

Summary:
The Central China Normal University (CCNU) team developed a system for the Mu-SHROOM shared task, focusing on spotting hallucinations in question-answering systems in 14 languages. Their approach utilized multiple Large Language Models (LLMs) with different expertise simultaneously to mark hallucinations, mimicking a crowdsourcing process. Each LLM-based annotator incorporated internal and external knowledge while annotating. Using DeepSeek-V3 LLM, their system ranked first for Hindi data and achieved a Top-5 position in seven other languages. The paper also discusses unsuccessful strategies explored during development and shares insights gained from the shared task participation. <div>
arXiv:2505.11965v1 Announce Type: new 
Abstract: We present the system developed by the Central China Normal University (CCNU) team for the Mu-SHROOM shared task, which focuses on identifying hallucinations in question-answering systems across 14 different languages. Our approach leverages multiple Large Language Models (LLMs) with distinct areas of expertise, employing them in parallel to annotate hallucinations, effectively simulating a crowdsourcing annotation process. Furthermore, each LLM-based annotator integrates both internal and external knowledge related to the input during the annotation process. Using the open-source LLM DeepSeek-V3, our system achieves the top ranking (\#1) for Hindi data and secures a Top-5 position in seven other languages. In this paper, we also discuss unsuccessful approaches explored during our development process and share key insights gained from participating in this shared task.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Annotated Corpus of Arabic Tweets for Hate Speech Analysis</title>
<link>https://arxiv.org/abs/2505.11969</link>
<guid>https://arxiv.org/abs/2505.11969</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech, Arabic language, dataset, annotation, transformer model

Summary: 
A new dataset of 10000 Arabic tweets containing hate speech content has been introduced, annotated for offensive content and categorized into different targets such as religion, gender, politics, ethnicity, and origin. Inter-annotator agreement was high at 0.86 for offensive content and 0.71 for multiple hate speech targets. Various annotators participated in the data annotation task. The performance evaluation of different transformers-based models showed that AraBERTv2 achieved a micro-F1 score of 0.7865 and an accuracy of 0.786. This dataset and model can be helpful in identifying and combating hate speech in Arabic content on social media platforms. 

Summary: <div>
arXiv:2505.11969v1 Announce Type: new 
Abstract: Identifying hate speech content in the Arabic language is challenging due to the rich quality of dialectal variations. This study introduces a multilabel hate speech dataset in the Arabic language. We have collected 10000 Arabic tweets and annotated each tweet, whether it contains offensive content or not. If a text contains offensive content, we further classify it into different hate speech targets such as religion, gender, politics, ethnicity, origin, and others. A text can contain either single or multiple targets. Multiple annotators are involved in the data annotation task. We calculated the inter-annotator agreement, which was reported to be 0.86 for offensive content and 0.71 for multiple hate speech targets. Finally, we evaluated the data annotation task by employing a different transformers-based model in which AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of 0.786.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.11995</link>
<guid>https://arxiv.org/abs/2505.11995</guid>
<content:encoded><![CDATA[
<div> knowledge utilization, retrieval-augmented generation, LLM, knowledge streaming, knowledge activation probability entropy

Summary:
The paper explores how large language models (LLMs) integrate internal and external knowledge in retrieval-augmented generation (RAG). Analysis reveals four stages in knowledge utilization within LLM layers: refinement, elicitation, expression, and contestation, guided by passage relevance. A new method, knowledge activation probability entropy (KAPE), identifies neurons linked to internal or external knowledge, enabling targeted shifts in knowledge source reliance. Multi-head attention and multi-layer perceptron layers play complementary roles in knowledge formation. These insights enhance interpretability and reliability in RAG, leading to more robust generative solutions in knowledge-intensive domains. <br /><br /> <div>
arXiv:2505.11995v1 Announce Type: new 
Abstract: Considering the inherent limitations of parametric knowledge in large language models (LLMs), retrieval-augmented generation (RAG) is widely employed to expand their knowledge scope. Since RAG has shown promise in knowledge-intensive tasks like open-domain question answering, its broader application to complex tasks and intelligent assistants has further advanced its utility. Despite this progress, the underlying knowledge utilization mechanisms of LLM-based RAG remain underexplored. In this paper, we present a systematic investigation of the intrinsic mechanisms by which LLMs integrate internal (parametric) and external (retrieved) knowledge in RAG scenarios. Specially, we employ knowledge stream analysis at the macroscopic level, and investigate the function of individual modules at the microscopic level. Drawing on knowledge streaming analyses, we decompose the knowledge utilization process into four distinct stages within LLM layers: knowledge refinement, knowledge elicitation, knowledge expression, and knowledge contestation. We further demonstrate that the relevance of passages guides the streaming of knowledge through these stages. At the module level, we introduce a new method, knowledge activation probability entropy (KAPE) for neuron identification associated with either internal or external knowledge. By selectively deactivating these neurons, we achieve targeted shifts in the LLM's reliance on one knowledge source over the other. Moreover, we discern complementary roles for multi-head attention and multi-layer perceptron layers during knowledge formation. These insights offer a foundation for improving interpretability and reliability in retrieval-augmented LLMs, paving the way for more robust and transparent generative solutions in knowledge-intensive domains.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method</title>
<link>https://arxiv.org/abs/2505.12028</link>
<guid>https://arxiv.org/abs/2505.12028</guid>
<content:encoded><![CDATA[
<div> Keywords: Argument mining, Large Language Models, Fine-grained relation types, Argument structure, Automated essay grading

Summary:<br /><br />Argument mining research has gained attention with the advancement of Large Language Models (LLMs), but current argument relations are limited in capturing complex structures. This study introduces 14 fine-grained relation types to better understand argument components in real-world scenarios. Experiments were conducted on argument component detection, relation prediction, and automated essay grading tasks. The impact of writing quality on detection and prediction, as well as the connection between discourse relations and argumentative features, were also explored. The findings emphasize the need for detailed argumentative annotations for assessing writing quality and advocate for multi-dimensional argument analysis. <div>
arXiv:2505.12028v1 Announce Type: new 
Abstract: Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities</title>
<link>https://arxiv.org/abs/2505.12043</link>
<guid>https://arxiv.org/abs/2505.12043</guid>
<content:encoded><![CDATA[
<div> CPT, domain-specific applications, MoL, cross-entropy loss, KL divergence <br />
Summary: 
- The article discusses the limitations of current language models in domain-specific tasks due to data bias and corpus mixture ratios.
- It proposes a new framework called Mixture of Losses (MoL) to address these issues, using cross-entropy loss for domain data and KL divergence for general corpora.
- The dual-loss architecture aims to preserve universal language skills while enhancing domain expertise without forgetting previous knowledge.
- Empirical validation shows that a 1:1 domain-to-general corpus ratio optimally balances training and overfitting.
- Experiments demonstrate significant performance gains compared to traditional approaches, with the model achieving higher accuracy on the Math-500 benchmark and a remarkable improvement on the AIME25 subset. <br /><br />Summary: <div>
arXiv:2505.12043v1 Announce Type: new 
Abstract: Although LLMs perform well in general tasks, domain-specific applications suffer from hallucinations and accuracy limitations. CPT approaches encounter two key issues: (1) domain-biased data degrades general language skills, and (2) improper corpus-mixture ratios limit effective adaptation. To address these, we propose a novel framework, Mixture of Losses (MoL), which decouples optimization objectives for domain-specific and general corpora. Specifically, cross-entropy (CE) loss is applied to domain data to ensure knowledge acquisition, while Kullback-Leibler (KL) divergence aligns general-corpus training with the base model's foundational capabilities. This dual-loss architecture preserves universal skills while enhancing domain expertise, avoiding catastrophic forgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio optimally balances training and overfitting without the need for extensive tuning or resource-intensive experiments. Furthermore, our experiments demonstrate significant performance gains compared to traditional CPT approaches, which often suffer from degradation in general language capabilities; our model achieves 27.9% higher accuracy on the Math-500 benchmark in the non-think reasoning mode, and an impressive 83.3% improvement on the challenging AIME25 subset in the think mode, underscoring the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABoN: Adaptive Best-of-N Alignment</title>
<link>https://arxiv.org/abs/2505.12050</link>
<guid>https://arxiv.org/abs/2505.12050</guid>
<content:encoded><![CDATA[
<div> Best-of-N sampling, reward models, test-time alignment, language models, prompt-adaptive strategy
Summary: 
This study introduces a prompt-adaptive strategy for Best-of-N alignment in language models (LMs) paired with reward models (RMs). The aim is to efficiently allocate compute resources during inference, particularly when dealing with varying prompt difficulty. The proposed two-stage algorithm first estimates reward distributions for prompts with a small exploration budget, then adaptively allocates remaining resources based on these estimates. Results on the AlpacaEval dataset demonstrate that the adaptive strategy consistently outperforms uniform allocation methods with the same inference budget across 12 LM/RM pairs and 50 prompt batches. The method also remains competitive when compared to uniform allocations with larger budgets, showing potential for improved performance as batch sizes increase. <br /><br />Summary: <div>
arXiv:2505.12050v1 Announce Type: new 
Abstract: Recent advances in test-time alignment methods, such as Best-of-N sampling, offer a simple and effective way to steer language models (LMs) toward preferred behaviors using reward models (RM). However, these approaches can be computationally expensive, especially when applied uniformly across prompts without accounting for differences in alignment difficulty. In this work, we propose a prompt-adaptive strategy for Best-of-N alignment that allocates inference-time compute more efficiently. Motivated by latency concerns, we develop a two-stage algorithm: an initial exploratory phase estimates the reward distribution for each prompt using a small exploration budget, and a second stage adaptively allocates the remaining budget using these estimates. Our method is simple, practical, and compatible with any LM/RM combination. Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different batches of prompts show that our adaptive strategy consistently outperforms the uniform allocation with the same inference budget. Moreover, our experiments show that our adaptive strategy remains competitive against uniform allocations with 20% larger inference budgets and even improves in performance as the batch size grows.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenderBench: Evaluation Suite for Gender Biases in LLMs</title>
<link>https://arxiv.org/abs/2505.12054</link>
<guid>https://arxiv.org/abs/2505.12054</guid>
<content:encoded><![CDATA[
<div> Evaluation, GenderBench, LLMs, gender biases, harmful behaviors <br />
Summary:
GenderBench is introduced as a tool to assess gender biases in Large Language Models (LLMs) through 14 probes that measure 19 harmful gender-related behaviors. It is shared as an open-source library to enhance benchmarking reproducibility and reliability. The study evaluates 12 LLMs and identifies consistent issues in their performance. LLMs struggle with stereotypical reasoning, lack equitable gender representation in generated texts, and exhibit discriminatory behavior in crucial scenarios like hiring. The evaluation underscores the necessity for continued research and development to address these issues. The insights provided by GenderBench contribute to the understanding of LLM behavior and highlight the need for improvements in addressing gender biases and promoting equity in language model outputs. The findings emphasize the importance of ethical considerations and accountability in the development and deployment of artificial intelligence technologies that impact societal perceptions and interactions. <br /><br />Summary: <div>
arXiv:2505.12054v1 Announce Type: new 
Abstract: We present GenderBench -- a comprehensive evaluation suite designed to measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19 gender-related harmful behaviors exhibited by LLMs. We release GenderBench as an open-source and extensible library to improve the reproducibility and robustness of benchmarking across the field. We also publish our evaluation of 12 LLMs. Our measurements reveal consistent patterns in their behavior. We show that LLMs struggle with stereotypical reasoning, equitable gender representation in generated texts, and occasionally also with discriminatory behavior in high-stakes scenarios, such as hiring.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement</title>
<link>https://arxiv.org/abs/2505.12060</link>
<guid>https://arxiv.org/abs/2505.12060</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Jailbreak Attacks, Safety Gap, SAGE, Defense Strategy

Summary: 

Large Language Models (LLMs) have shown impressive capabilities but are vulnerable to jailbreak attacks due to a safety gap in their response generation. A new defense strategy called SAGE (Self-Aware Guard Enhancement) addresses this issue by aligning LLMs' safety discrimination with generation abilities. SAGE consists of a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against jailbreak attempts. Extensive experiments across various LLMs demonstrate SAGE's effectiveness with a 99% defense success rate. Mechanistic interpretability analysis reveals the underlying detection-generation discrepancy. The research contributes to developing LLMs with coherent safety awareness and generation behavior. The code and datasets are publicly available for further research and development.

<br /><br />Summary: <div>
arXiv:2505.12060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs' strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE's effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at https://github.com/NJUNLP/SAGE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach</title>
<link>https://arxiv.org/abs/2505.12071</link>
<guid>https://arxiv.org/abs/2505.12071</guid>
<content:encoded><![CDATA[
<div> cognitive-computational perspective, morphological productivity, discriminative lexicon model, Finnish nominal inflection, Thomas Mann <br />
Summary: This study examines morphological productivity from two perspectives: a cognitive-computational approach and a diachronic analysis focusing on writer Thomas Mann. Using the discriminative lexicon model, the study explores the systematicities between form and meaning in Finnish nominal inflection, Malay derivation, and English compounding. The model associates affix-like sublexical units with word embeddings to trace differences in productivity. Analyzing Thomas Mann's output and intake, the study finds a low rate of novel derived word production, with more novel words in his input than output. Mann's likelihood of producing novel derived words decreases with the distance of word embeddings from centroids. The study discusses the challenges of using speaker-specific embeddings for low-frequency and novel words. <br /> <div>
arXiv:2505.12071v1 Announce Type: new 
Abstract: In this study, we approach morphological productivity from two perspectives: a cognitive-computational perspective, and a diachronic perspective zooming in on an actual speaker, Thomas Mann. For developing the first perspective, we make use of a cognitive computational model of the mental lexicon, the discriminative lexicon model. For computational mappings between form and meaning to be productive, in the sense that novel, previously unencountered words, can be understood and produced, there must be systematicities between the form space and the semantic space. If the relation between form and meaning would be truly arbitrary, a model could memorize form and meaning pairings, but there is no way in which the model would be able to generalize to novel test data. For Finnish nominal inflection, Malay derivation, and English compounding, we explore, using the Discriminative Lexicon Model as a computational tool, to trace differences in the degree to which inflectional and word formation patterns are productive. We show that the DLM tends to associate affix-like sublexical units with the centroids of the embeddings of the words with a given affix. For developing the second perspective, we study how the intake and output of one prolific writer, Thomas Mann, changes over time. We show by means of an examination of what Thomas Mann is likely to have read, and what he wrote, that the rate at which Mann produces novel derived words is extremely low. There are far more novel words in his input than in his output. We show that Thomas Mann is less likely to produce a novel derived word with a given suffix the greater the average distance is of the embeddings of all derived words to the corresponding centroid, and discuss the challenges of using speaker-specific embeddings for low-frequency and novel words.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do different prompting methods yield a common task representation in language models?</title>
<link>https://arxiv.org/abs/2505.12075</link>
<guid>https://arxiv.org/abs/2505.12075</guid>
<content:encoded><![CDATA[
<div> vector, language models, in-context learning, task representation, instruction prompts

Summary:
- The study compares the use of demonstrations and instruction prompts to prompt language models for in-context learning tasks.
- New findings suggest that different task presentations elicit different mechanisms in language models.
- Function vectors are proposed as a way to extract task representations for instruction prompts, leading to improved zero-shot task accuracy.
- The study reveals that demonstration- and instruction-based function vectors rely on distinct model components.
- The results challenge the idea of a common task representation and highlight the need for further exploration of language model task inference mechanisms. 

<br /><br />Summary: <div>
arXiv:2505.12075v1 Announce Type: new 
Abstract: Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through function vectors, recently proposed as a mechanism to extract few-shot ICL task representations. We generalize function vectors to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task presentations do not induce a common task representation but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining textual instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging in Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.12082</link>
<guid>https://arxiv.org/abs/2505.12082</guid>
<content:encoded><![CDATA[
arXiv:2505.12082v1 Announce Type: new 
Abstract: Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Author Obfuscation with Large Language Models</title>
<link>https://arxiv.org/abs/2505.12090</link>
<guid>https://arxiv.org/abs/2505.12090</guid>
<content:encoded><![CDATA[
arXiv:2505.12090v1 Announce Type: new 
Abstract: In this paper, we investigate the efficacy of large language models (LLMs) in obfuscating authorship by paraphrasing and altering writing styles. Rather than adopting a holistic approach that evaluates performance across the entire dataset, we focus on user-wise performance to analyze how obfuscation effectiveness varies across individual authors. While LLMs are generally effective, we observe a bimodal distribution of efficacy, with performance varying significantly across users. To address this, we propose a personalized prompting method that outperforms standard prompting techniques and partially mitigates the bimodality issue.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Fairness in LLMs Through Testing-Time Adversaries</title>
<link>https://arxiv.org/abs/2505.12100</link>
<guid>https://arxiv.org/abs/2505.12100</guid>
<content:encoded><![CDATA[
arXiv:2505.12100v1 Announce Type: new 
Abstract: Large Language Models (LLMs) push the bound-aries in natural language processing and generative AI, driving progress across various aspects of modern society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e., predictions) poses a significant and open challenge, hindering their application in tasks involving ethical sensitivity and responsible decision-making. In this work, we propose a straightforward, user-friendly and practical method to mitigate such biases, enhancing the reliability and trustworthiness of LLMs. Our method creates multiple variations of a given sentence by modifying specific attributes and evaluates the corresponding prediction behavior compared to the original, unaltered, prediction/sentence. The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias. Unlike previous approaches, our method relies solely on forward passes (i.e., testing-time adversaries), eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution. Through extensive experiments on the popular Llama family, we demonstrate the effectiveness of our method in improving various fairness metrics, focusing on the reduction of disparities in how the model treats individuals from different racial groups. Specifically, using standard metrics, we improve the fairness in Llama3 in up to 27 percentage points. Overall, our approach significantly enhances fairness, equity, and reliability in LLM-generated results without parameter tuning or training data modifications, confirming its effectiveness in practical scenarios. We believe our work establishes an important step toward enabling the use of LLMs in tasks that require ethical considerations and responsible decision-making.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2505.12116</link>
<guid>https://arxiv.org/abs/2505.12116</guid>
<content:encoded><![CDATA[
arXiv:2505.12116v1 Announce Type: new 
Abstract: Content moderation research has recently made significant advances, but still fails to serve the majority of the world's languages due to the lack of resources, leaving millions of vulnerable users to online hostility. This work presents a large-scale human-annotated multi-task benchmark dataset for abusive language detection in Tigrinya social media with joint annotations for three tasks: abusiveness, sentiment, and topic classification. The dataset comprises 13,717 YouTube comments annotated by nine native speakers, collected from 7,373 videos with a total of over 1.2 billion views across 51 channels. We developed an iterative term clustering approach for effective data selection. Recognizing that around 64% of Tigrinya social media content uses Romanized transliterations rather than native Ge'ez script, our dataset accommodates both writing systems to reflect actual language use. We establish strong baselines across the tasks in the benchmark, while leaving significant challenges for future contributions. Our experiments reveal that small, specialized multi-task models outperform the current frontier models in the low-resource setting, achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the resources publicly available to promote research on online safety.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Gap: How Socioeconomic Status Affects Language Technology Interactions</title>
<link>https://arxiv.org/abs/2505.12158</link>
<guid>https://arxiv.org/abs/2505.12158</guid>
<content:encoded><![CDATA[
arXiv:2505.12158v1 Announce Type: new 
Abstract: Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse</title>
<link>https://arxiv.org/abs/2505.12160</link>
<guid>https://arxiv.org/abs/2505.12160</guid>
<content:encoded><![CDATA[
arXiv:2505.12160v1 Announce Type: new 
Abstract: Social media platforms like X (formerly Twitter) play a crucial role in shaping public discourse and societal norms. This study examines the term Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM) tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such as happiness, fear, anger, sadness, disgust, and surprise. By applying this model to large-scale X data, the study uncovers emotional nuances in Turkish discourse, contributing to computational social science by advancing sentiment analysis in underrepresented languages and enhancing our understanding of global digital discourse and the unique linguistic challenges of Turkish. The findings underscore the transformative potential of localized NLP tools, with our ERM model offering practical applications for real-time sentiment analysis in Turkish-language contexts. By addressing critical areas, including marketing, public relations, and crisis management, these models facilitate improved decision-making through timely and accurate sentiment tracking. This highlights the significance of advancing research that accounts for regional and linguistic nuances.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth Neurons</title>
<link>https://arxiv.org/abs/2505.12182</link>
<guid>https://arxiv.org/abs/2505.12182</guid>
<content:encoded><![CDATA[
arXiv:2505.12182v1 Announce Type: new 
Abstract: Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases</title>
<link>https://arxiv.org/abs/2505.12183</link>
<guid>https://arxiv.org/abs/2505.12183</guid>
<content:encoded><![CDATA[
arXiv:2505.12183v1 Announce Type: new 
Abstract: The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled</title>
<link>https://arxiv.org/abs/2505.12196</link>
<guid>https://arxiv.org/abs/2505.12196</guid>
<content:encoded><![CDATA[
arXiv:2505.12196v1 Announce Type: new 
Abstract: The impressive linguistic abilities of large language models (LLMs) have recommended them as models of human sentence processing, with some conjecturing a positive 'quality-power' relationship (Wilcox et al., 2023), in which language models' (LMs') fit to psychometric data continues to improve as their ability to predict words in context increases. This is important because it suggests that elements of LLM architecture, such as veridical attention to context and a unique objective of predicting upcoming words, reflect the architecture of the human sentence processing faculty, and that any inadequacies in predicting human reading time and brain imaging data may be attributed to insufficient model complexity, which recedes as larger models become available. Recent studies (Oh and Schuler, 2023) have shown this scaling inverts after a point, as LMs become excessively large and accurate, when word prediction probability (as information-theoretic surprisal) is used as a predictor. Other studies propose the use of entire vectors from differently sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting doubt on the value of surprisal as a predictor, but do not control for the larger number of predictors in vectors from larger LMs. This study evaluates LLM scaling using entire LLM vectors, while controlling for the larger number of predictors in vectors from larger LLMs. Results show that inverse scaling obtains, suggesting that inadequacies in predicting human reading time and brain imaging data may be due to substantial misalignment between LLMs and human sentence processing, which worsens as larger models are used.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Reliable is Multilingual LLM-as-a-Judge?</title>
<link>https://arxiv.org/abs/2505.12201</link>
<guid>https://arxiv.org/abs/2505.12201</guid>
<content:encoded><![CDATA[
arXiv:2505.12201v1 Announce Type: new 
Abstract: LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced large language models assess generation results in alignment with human instructions. While these models serve as a promising alternative to human annotators, their reliability in multilingual evaluation remains uncertain. To bridge this gap, we conduct a comprehensive analysis of multilingual LLM-as-a-Judge. Specifically, we evaluate five models from different model families across five diverse tasks involving 25 languages. Our findings reveal that LLMs struggle to achieve consistent judgment results across languages, with an average Fleiss' Kappa of approximately 0.3, and some models performing even worse. To investigate the cause of inconsistency, we analyze various influencing factors. We observe that consistency varies significantly across languages, with particularly poor performance in low-resource languages. Additionally, we find that neither training on multilingual data nor increasing model scale directly improves judgment consistency. These findings suggest that LLMs are not yet reliable for evaluating multilingual predictions. We finally propose an ensemble strategy which improves the consistency of the multilingual judge in real-world applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning</title>
<link>https://arxiv.org/abs/2505.12212</link>
<guid>https://arxiv.org/abs/2505.12212</guid>
<content:encoded><![CDATA[
arXiv:2505.12212v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment</title>
<link>https://arxiv.org/abs/2505.12215</link>
<guid>https://arxiv.org/abs/2505.12215</guid>
<content:encoded><![CDATA[
arXiv:2505.12215v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance in a variety of natural language processing (NLP) tasks. However, when applied to long-context scenarios, they face two challenges, i.e., low computational efficiency and much redundant information. This paper introduces GMSA, a context compression framework based on the encoder-decoder architecture, which addresses these challenges by reducing input sequence length and redundant information. Structurally, GMSA has two key components: Group Merging and Layer Semantic Alignment (LSA). Group merging is used to effectively and efficiently extract summary vectors from the original context. Layer semantic alignment, on the other hand, aligns the high-level summary vectors with the low-level primary input semantics, thus bridging the semantic gap between different layers. In the training process, GMSA first learns soft tokens that contain complete semantics through autoencoder training. To furtherly adapt GMSA to downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract knowledge from the soft tokens for downstream tasks. We train GMSA by randomly sampling the compression rate for each sample in the dataset. Under this condition, GMSA not only significantly outperforms the traditional compression paradigm in context restoration but also achieves stable and significantly faster convergence with only a few encoder layers. In downstream question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in end-to-end inference while outperforming both the original input prompts and various state-of-the-art (SOTA) methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2505.12216</link>
<guid>https://arxiv.org/abs/2505.12216</guid>
<content:encoded><![CDATA[
arXiv:2505.12216v1 Announce Type: new 
Abstract: Existing pruning methods for large language models (LLMs) focus on achieving high compression rates while maintaining model performance. Although these methods have demonstrated satisfactory performance in handling a single user's compression request, their processing time increases linearly with the number of requests, making them inefficient for real-world scenarios with multiple simultaneous requests. To address this limitation, we propose a Univeral Model for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that learns to map arbitrary requests to their optimal pruning strategy. The challenge in training StratNet lies in the high computational cost of evaluating pruning strategies and the non-differentiable nature of the pruning process, which hinders gradient backpropagation for StratNet updates. To overcome these challenges, we leverage a Gaussian process to approximate the evaluation process. Since the gradient of the Gaussian process is computable, we can use it to approximate the gradient of the non-differentiable pruning process, thereby enabling StratNet updates. Experimental results show that UniCuCo is 28 times faster than baselines in processing 64 requests, while maintaining comparable accuracy to baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers</title>
<link>https://arxiv.org/abs/2505.12218</link>
<guid>https://arxiv.org/abs/2505.12218</guid>
<content:encoded><![CDATA[
arXiv:2505.12218v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, have prompted academic concerns about their impact on academic writing. Existing studies have primarily examined LLM usage in academic writing through quantitative approaches, such as word frequency statistics and probability-based analyses. However, few have systematically examined the potential impact of LLMs on the linguistic characteristics of academic writing. To address this gap, we conducted a large-scale analysis across 823,798 abstracts published in last decade from arXiv dataset. Through the linguistic analysis of features such as the frequency of LLM-preferred words, lexical complexity, syntactic complexity, cohesion, readability and sentiment, the results indicate a significant increase in the proportion of LLM-preferred words in abstracts, revealing the widespread influence of LLMs on academic writing. Additionally, we observed an increase in lexical complexity and sentiment in the abstracts, but a decrease in syntactic complexity, suggesting that LLMs introduce more new vocabulary and simplify sentence structure. However, the significant decrease in cohesion and readability indicates that abstracts have fewer connecting words and are becoming more difficult to read. Moreover, our analysis reveals that scholars with weaker English proficiency were more likely to use the LLMs for academic writing, and focused on improving the overall logic and fluency of the abstracts. Finally, at discipline level, we found that scholars in Computer Science showed more pronounced changes in writing style, while the changes in Mathematics were minimal.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training</title>
<link>https://arxiv.org/abs/2505.12236</link>
<guid>https://arxiv.org/abs/2505.12236</guid>
<content:encoded><![CDATA[
arXiv:2505.12236v1 Announce Type: new 
Abstract: Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE through in-context learning (ICL), their general-purpose training objectives often result in suboptimal performance for task-specific relation extraction. To overcome these challenges, we propose TKRE (Two-Stage Knowledge-Guided Pre-training for Relation Extraction), a novel framework that synergistically integrates LLMs with traditional relation extraction models, bridging generative and discriminative learning paradigms. TKRE introduces two key innovations: (1) leveraging LLMs to generate explanation-driven knowledge and schema-constrained synthetic data, addressing the issue of data scarcity; and (2) a two-stage pre-training strategy combining Masked Span Language Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational reasoning and generalization. Together, these components enable TKRE to effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in FSRE and underscoring its potential for broader application in low-resource scenarios. \footnote{The code and data are released on https://github.com/UESTC-GQJ/TKRE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs</title>
<link>https://arxiv.org/abs/2505.12238</link>
<guid>https://arxiv.org/abs/2505.12238</guid>
<content:encoded><![CDATA[
arXiv:2505.12238v1 Announce Type: new 
Abstract: The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce</title>
<link>https://arxiv.org/abs/2505.12244</link>
<guid>https://arxiv.org/abs/2505.12244</guid>
<content:encoded><![CDATA[
arXiv:2505.12244v1 Announce Type: new 
Abstract: Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce, showing that some distributions are significantly harder to elicit than others. Specifically, for any target next-token distribution over the vocabulary, we attempt to find a prompt that induces the LM to output a distribution as close as possible to the target, using either soft or hard gradient-based prompt tuning. We find that (1) in general, distributions with very low or very high entropy are easier to approximate than those with moderate entropy; (2) among distributions with the same entropy, those containing ''outlier tokens'' are easier to approximate; (3) target distributions generated by LMs -- even LMs with different tokenizers -- are easier to approximate than randomly chosen targets. These results offer insights into the expressiveness of LMs and the challenges of using them as probability distribution proposers.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Documents Are What You Need for Extracting Instruction Tuning Data</title>
<link>https://arxiv.org/abs/2505.12250</link>
<guid>https://arxiv.org/abs/2505.12250</guid>
<content:encoded><![CDATA[
arXiv:2505.12250v1 Announce Type: new 
Abstract: Instruction tuning improves the performance of large language models (LLMs), but it heavily relies on high-quality training data. Recently, LLMs have been used to synthesize instruction data using seed question-answer (QA) pairs. However, these synthesized instructions often lack diversity and tend to be similar to the input seeds, limiting their applicability in real-world scenarios. To address this, we propose extracting instruction tuning data from web corpora that contain rich and diverse knowledge. A naive solution is to retrieve domain-specific documents and extract all QA pairs from them, but this faces two key challenges: (1) extracting all QA pairs using LLMs is prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to the downstream tasks, potentially degrading model performance. To tackle these issues, we introduce EQUAL, an effective and scalable data extraction framework that iteratively alternates between document selection and high-quality QA pair extraction to enhance instruction tuning. EQUAL first clusters the document corpus based on embeddings derived from contrastive learning, then uses a multi-armed bandit strategy to efficiently identify clusters that are likely to contain valuable QA pairs. This iterative approach significantly reduces computational cost while boosting model performance. Experiments on AutoMathText and StackOverflow across four downstream tasks show that EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5 percent on LLaMA-3.1-8B and Mistral-7B
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches</title>
<link>https://arxiv.org/abs/2505.12259</link>
<guid>https://arxiv.org/abs/2505.12259</guid>
<content:encoded><![CDATA[
arXiv:2505.12259v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has outpaced the development of effective evaluation methods. Traditional benchmarks rely on task-specific metrics and static datasets, which often suffer from fairness issues, limited scalability, and contamination risks. In this paper, we introduce Teach2Eval, an indirect evaluation framework inspired by the Feynman Technique. Instead of directly testing LLMs on predefined tasks, our method evaluates a model's multiple abilities to teach weaker student models to perform tasks effectively. By converting open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, Teach2Eval enables scalable, automated, and multi-dimensional assessment. Our approach not only avoids data leakage and memorization but also captures a broad range of cognitive abilities that are orthogonal to current benchmarks. Experimental results across 26 leading LLMs show strong alignment with existing human and model-based dynamic rankings, while offering additional interpretability for training guidance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation</title>
<link>https://arxiv.org/abs/2505.12265</link>
<guid>https://arxiv.org/abs/2505.12265</guid>
<content:encoded><![CDATA[
arXiv:2505.12265v1 Announce Type: new 
Abstract: Hallucination, the generation of factually incorrect information, remains a significant challenge for large language models (LLMs), especially in open-domain long-form generation. Existing approaches for detecting hallucination in long-form tasks either focus on limited domains or rely heavily on external fact-checking tools, which may not always be available.
  In this work, we systematically investigate reference-free hallucination detection in open-domain long-form responses. Our findings reveal that internal states (e.g., model's output probability and entropy) alone are insufficient for reliably (i.e., better than random guessing) distinguishing between factual and hallucinated content. To enhance detection, we explore various existing approaches, including prompting-based methods, probing, and fine-tuning, with fine-tuning proving the most effective. To further improve the accuracy, we introduce a new paradigm, named RATE-FT, that augments fine-tuning with an auxiliary task for the model to jointly learn with the main task of hallucination detection. With extensive experiments and analysis using a variety of model families & datasets, we demonstrate the effectiveness and generalizability of our method, e.g., +3% over general fine-tuning methods on LongFact.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks</title>
<link>https://arxiv.org/abs/2505.12268</link>
<guid>https://arxiv.org/abs/2505.12268</guid>
<content:encoded><![CDATA[
arXiv:2505.12268v1 Announce Type: new 
Abstract: Understanding which neural components drive specific capabilities in mid-sized language models ($\leq$10B parameters) remains a key challenge. We introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems. Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network. We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance. While grammar and arithmetic share many "weak" heads, arithmetic and word problems share more consistently critical "strong" heads. Importantly, we find that each task maintains dedicated "super-heads" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark</title>
<link>https://arxiv.org/abs/2505.12273</link>
<guid>https://arxiv.org/abs/2505.12273</guid>
<content:encoded><![CDATA[
arXiv:2505.12273v1 Announce Type: new 
Abstract: Evaluating machine translation (MT) for low-resource languages poses a persistent challenge, primarily due to the limited availability of high quality reference translations. This issue is further exacerbated in languages with multiple dialects, where linguistic diversity and data scarcity hinder robust evaluation. Large Language Models (LLMs) present a promising solution through reference-free evaluation techniques; however, their effectiveness diminishes in the absence of dialect-specific context and tailored guidance. In this work, we propose a comprehensive framework that enhances LLM-based MT evaluation using a dialect guided approach. We extend the ONUBAD dataset by incorporating Sylheti-English sentence pairs, corresponding machine translations, and Direct Assessment (DA) scores annotated by native speakers. To address the vocabulary gap, we augment the tokenizer vocabulary with dialect-specific terms. We further introduce a regression head to enable scalar score prediction and design a dialect-guided (DG) prompting strategy. Our evaluation across multiple LLMs shows that the proposed pipeline consistently outperforms existing methods, achieving the highest gain of +0.1083 in Spearman correlation, along with improvements across other evaluation settings. The dataset and the code are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models</title>
<link>https://arxiv.org/abs/2505.12287</link>
<guid>https://arxiv.org/abs/2505.12287</guid>
<content:encoded><![CDATA[
arXiv:2505.12287v1 Announce Type: new 
Abstract: Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhance Mobile Agents Thinking Process Via Iterative Preference Learning</title>
<link>https://arxiv.org/abs/2505.12299</link>
<guid>https://arxiv.org/abs/2505.12299</guid>
<content:encoded><![CDATA[
arXiv:2505.12299v1 Announce Type: new 
Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to improve the reasoning performance of VLM-based mobile agents in GUI tasks. However, the scarcity of diverse CoaT trajectories limits the expressiveness and generalization ability of such agents. While self-training is commonly employed to address data scarcity, existing approaches either overlook the correctness of intermediate reasoning steps or depend on expensive process-level annotations to construct process reward models (PRM). To address the above problems, we propose an Iterative Preference Learning (IPL) that constructs a CoaT-tree through interative sampling, scores leaf nodes using rule-based reward, and backpropagates feedback to derive Thinking-level Direct Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up supervised fine-tuning, we further introduce a three-stage instruction evolution, which leverages GPT-4o to generate diverse Q\&amp;A pairs based on real mobile UI screenshots, enhancing both generality and layout understanding. Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our agent MobileIPL outperforms strong baselines, including continual pretraining models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance across three standard Mobile GUI-Agents benchmarks and shows strong generalization to out-of-domain scenarios.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2505.12300</link>
<guid>https://arxiv.org/abs/2505.12300</guid>
<content:encoded><![CDATA[
arXiv:2505.12300v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) on a mixture of diverse datasets poses challenges due to data imbalance and heterogeneity. Existing methods often address these issues across datasets (globally) but overlook the imbalance and heterogeneity within individual datasets (locally), which limits their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a novel method that enables LLMs to autonomously adjust data allocation during fine-tuning both across datasets (globally) and within each individual dataset (locally). HBO employs a bilevel optimization strategy with two types of actors: a Global Actor, which balances data sampling across different subsets of the training mixture, and several Local Actors, which optimizes data usage within each subset based on difficulty levels. These actors are guided by reward functions derived from the LLM's training state, which measure learning progress and relative performance improvement. We evaluate HBO on three LLM backbones across nine diverse tasks in multilingual and multitask setups. Results show that HBO consistently outperforms existing baselines, achieving significant accuracy gains. Our in-depth analysis further demonstrates that both the global actor and local actors of HBO effectively adjust data usage during fine-tuning. HBO provides a comprehensive solution to the challenges of data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection</title>
<link>https://arxiv.org/abs/2505.12306</link>
<guid>https://arxiv.org/abs/2505.12306</guid>
<content:encoded><![CDATA[
arXiv:2505.12306v1 Announce Type: new 
Abstract: Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's "Did You Know..." entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpertSteer: Intervening in LLMs through Expert Knowledge</title>
<link>https://arxiv.org/abs/2505.12313</link>
<guid>https://arxiv.org/abs/2505.12313</guid>
<content:encoded><![CDATA[
arXiv:2505.12313v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across various tasks, yet guiding them to follow desired behaviours during inference remains a significant challenge. Activation steering offers a promising method to control the generation process of LLMs by modifying their internal activations. However, existing methods commonly intervene in the model's behaviour using steering vectors generated by the model itself, which constrains their effectiveness to that specific model and excludes the possibility of leveraging powerful external expert models for steering. To address these limitations, we propose ExpertSteer, a novel approach that leverages arbitrary specialized expert models to generate steering vectors, enabling intervention in any LLMs. ExpertSteer transfers the knowledge from an expert model to a target LLM through a cohesive four-step process: first aligning representation dimensions with auto-encoders to enable cross-model transfer, then identifying intervention layer pairs based on mutual information analysis, next generating steering vectors from the expert model using Recursive Feature Machines, and finally applying these vectors on the identified layers during inference to selectively guide the target LLM without updating model parameters. We conduct comprehensive experiments using three LLMs on 15 popular benchmarks across four distinct domains. Experiments demonstrate that ExpertSteer significantly outperforms established baselines across diverse tasks at minimal cost.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning</title>
<link>https://arxiv.org/abs/2505.12328</link>
<guid>https://arxiv.org/abs/2505.12328</guid>
<content:encoded><![CDATA[
arXiv:2505.12328v1 Announce Type: new 
Abstract: We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which evaluates large language models on producing fine-grained, controllable, and interpretable reasoning processes. Systems must extract all problem conditions, decompose a chain of thought into statement-evidence pairs, and verify the logical validity of each pair. Leveraging only the off-the-shelf Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that first enumerates all conditions and then guides the model to label, cite, and adjudicate every reasoning step. A lightweight post-processor based on regular expressions normalises spans and enforces the official JSON schema. Without fine-tuning, external retrieval, or ensembling, our method ranks 5th overall, achieving macro F1 scores on par with substantially more complex and resource-consuming pipelines. We conclude by analysing the strengths and limitations of our approach and outlining directions for future research in structural reasoning with LLMs. Our code is available at https://github.com/asdfo123/LLMSR-asdfo123.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2505.12345</link>
<guid>https://arxiv.org/abs/2505.12345</guid>
<content:encoded><![CDATA[
arXiv:2505.12345v1 Announce Type: new 
Abstract: Model editing aims to enhance the accuracy and reliability of large language models (LLMs) by efficiently adjusting their internal parameters. Currently, most LLM editing datasets are confined to narrow knowledge domains and cover a limited range of editing evaluation. They often overlook the broad scope of editing demands and the diversity of ripple effects resulting from edits. In this context, we introduce UniEdit, a unified benchmark for LLM editing grounded in open-domain knowledge. First, we construct editing samples by selecting entities from 25 common domains across five major categories, utilizing the extensive triple knowledge available in open-domain knowledge graphs to ensure comprehensive coverage of the knowledge domains. To address the issues of generality and locality in editing, we design an Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we employ proprietary LLMs to convert the sampled knowledge subgraphs into natural language text, guaranteeing grammatical accuracy and syntactical diversity. Extensive statistical analysis confirms the scale, comprehensiveness, and diversity of our UniEdit benchmark. We conduct comprehensive experiments across multiple LLMs and editors, analyzing their performance to highlight strengths and weaknesses in editing across open knowledge domains and various evaluation criteria, thereby offering valuable insights for future research endeavors.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</title>
<link>https://arxiv.org/abs/2505.12349</link>
<guid>https://arxiv.org/abs/2505.12349</guid>
<content:encoded><![CDATA[
arXiv:2505.12349v1 Announce Type: new 
Abstract: Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the "wisdom of the crowd", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement</title>
<link>https://arxiv.org/abs/2505.12368</link>
<guid>https://arxiv.org/abs/2505.12368</guid>
<content:encoded><![CDATA[
arXiv:2505.12368v1 Announce Type: new 
Abstract: Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling</title>
<link>https://arxiv.org/abs/2505.12381</link>
<guid>https://arxiv.org/abs/2505.12381</guid>
<content:encoded><![CDATA[
arXiv:2505.12381v1 Announce Type: new 
Abstract: Current research on bias in language models (LMs) predominantly focuses on data quality, with significantly less attention paid to model architecture and temporal influences of data. Even more critically, few studies systematically investigate the origins of bias. We propose a methodology grounded in comparative behavioral theory to interpret the complex interaction between training data and model architecture in bias propagation during language modeling. Building on recent work that relates transformers to n-gram LMs, we evaluate how data, model design choices, and temporal dynamics affect bias propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to context window size in bias propagation, while transformers demonstrate architectural robustness; (2) the temporal provenance of training data significantly affects bias; and (3) different model architectures respond differentially to controlled bias injection, with certain biases (e.g. sexual orientation) being disproportionately amplified. As language models become ubiquitous, our findings highlight the need for a holistic approach -- tracing bias to its origins across both data and model dimensions, not just symptoms, to mitigate harm.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Sample-specific Language Model Optimization at Test-time</title>
<link>https://arxiv.org/abs/2505.12392</link>
<guid>https://arxiv.org/abs/2505.12392</guid>
<content:encoded><![CDATA[
arXiv:2505.12392v1 Announce Type: new 
Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traversal Verification for Speculative Tree Decoding</title>
<link>https://arxiv.org/abs/2505.12398</link>
<guid>https://arxiv.org/abs/2505.12398</guid>
<content:encoded><![CDATA[
arXiv:2505.12398v1 Announce Type: new 
Abstract: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT</title>
<link>https://arxiv.org/abs/2505.12405</link>
<guid>https://arxiv.org/abs/2505.12405</guid>
<content:encoded><![CDATA[
arXiv:2505.12405v1 Announce Type: new 
Abstract: Generative AI paraphrased text can be used for copyright infringement and the AI paraphrased content can deprive substantial revenue from original content creators. Despite this recent surge of malicious use of generative AI, there are few academic publications that research this threat. In this article, we demonstrate the ability of pattern-based similarity detection for AI paraphrased news recognition. We propose an algorithmic scheme, which is not limited to detect whether an article is an AI paraphrase, but, more importantly, to identify that the source of infringement is the ChatGPT. The proposed method is tested with a benchmark dataset specifically created for this task that incorporates real articles from BBC, incorporating a total of 2,224 articles across five different news categories, as well as 2,224 paraphrased articles created with ChatGPT. Results show that our pattern similarity-based method, that makes no use of deep learning, can detect ChatGPT assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1 score.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table-R1: Region-based Reinforcement Learning for Table Understanding</title>
<link>https://arxiv.org/abs/2505.12415</link>
<guid>https://arxiv.org/abs/2505.12415</guid>
<content:encoded><![CDATA[
arXiv:2505.12415v1 Announce Type: new 
Abstract: Tables present unique challenges for language models due to their structured row-column interactions, necessitating specialized approaches for effective comprehension. While large language models (LLMs) have demonstrated potential in table reasoning through prompting and techniques like chain-of-thought (CoT) and program-of-thought (PoT), optimizing their performance for table question answering remains underexplored. In this paper, we introduce region-based Table-R1, a novel reinforcement learning approach that enhances LLM table understanding by integrating region evidence into reasoning steps. Our method employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in identifying relevant table regions before generating answers, incorporating textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group Relative Policy Optimization (TARPO) introduces a mixed reward system to dynamically balance region accuracy and answer correctness, with decaying region rewards and consistency penalties to align reasoning steps. Experiments show that Table-R1 achieves an average performance improvement of 14.36 points across multiple base models on three benchmark datasets, even outperforming baseline models with ten times the parameters, while TARPO reduces response token consumption by 67.5% compared to GRPO, significantly advancing LLM capabilities in efficient tabular reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSC: Extending Context Window of Large Language Models via Phase Shift Calibration</title>
<link>https://arxiv.org/abs/2505.12423</link>
<guid>https://arxiv.org/abs/2505.12423</guid>
<content:encoded><![CDATA[
arXiv:2505.12423v1 Announce Type: new 
Abstract: Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks. The code can be found at https://github.com/WNQzhu/PSC.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games</title>
<link>https://arxiv.org/abs/2505.12439</link>
<guid>https://arxiv.org/abs/2505.12439</guid>
<content:encoded><![CDATA[
arXiv:2505.12439v1 Announce Type: new 
Abstract: Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment</title>
<link>https://arxiv.org/abs/2505.12452</link>
<guid>https://arxiv.org/abs/2505.12452</guid>
<content:encoded><![CDATA[
arXiv:2505.12452v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly demonstrate signs of conceptual understanding, yet much of their internal knowledge remains latent, loosely structured, and difficult to access or evaluate. We propose self-questioning as a lightweight and scalable strategy to improve LLMs' understanding, particularly in domains where success depends on fine-grained semantic distinctions. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. The benchmark centers on a pairwise differentiation task: can a model distinguish between closely related but substantively different inventions? We show that prompting LLMs to generate and answer their own questions - targeting the background knowledge required for the task - significantly improves performance. These self-generated questions and answers activate otherwise underutilized internal knowledge. Allowing LLMs to retrieve answers from external scientific texts further enhances performance, suggesting that model knowledge is compressed and lacks the full richness of the training data. We also find that chain-of-thought prompting and self-questioning converge, though self-questioning remains more effective for improving understanding of technical concepts. Notably, we uncover an asymmetry in prompting: smaller models often generate more fundamental, more open-ended, better-aligned questions for mid-sized models than large models with better understanding do, revealing a new strategy for cross-model collaboration. Altogether, our findings establish self-questioning as both a practical mechanism for automatically improving LLM comprehension, especially in domains with sparse and underrepresented knowledge, and a diagnostic probe of how internal and external knowledge are organized.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations</title>
<link>https://arxiv.org/abs/2505.12454</link>
<guid>https://arxiv.org/abs/2505.12454</guid>
<content:encoded><![CDATA[
arXiv:2505.12454v1 Announce Type: new 
Abstract: Distantly supervised named entity recognition (DS-NER) has emerged as a cheap and convenient alternative to traditional human annotation methods, enabling the automatic generation of training data by aligning text with external resources. Despite the many efforts in noise measurement methods, few works focus on the latent noise distribution between different distant annotation methods. In this work, we explore the effectiveness and robustness of DS-NER by two aspects: (1) distant annotation techniques, which encompasses both traditional rule-based methods and the innovative large language model supervision approach, and (2) noise assessment, for which we introduce a novel framework. This framework addresses the challenges by distinctly categorizing them into the unlabeled-entity problem (UEP) and the noisy-entity problem (NEP), subsequently providing specialized solutions for each. Our proposed method achieves significant improvements on eight real-world distant supervision datasets originating from three different data sources and involving four distinct annotation techniques, confirming its superiority over current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization</title>
<link>https://arxiv.org/abs/2505.12474</link>
<guid>https://arxiv.org/abs/2505.12474</guid>
<content:encoded><![CDATA[
arXiv:2505.12474v1 Announce Type: new 
Abstract: In this work, we investigate the performance of LLMs on a new task that requires combining discussion with background knowledge for summarization. This aims to address the limitation of outside observer confusion in existing dialogue summarization systems due to their reliance solely on discussion information. To achieve this, we model the task output as background and opinion summaries and define two standardized summarization patterns. To support assessment, we introduce the first benchmark comprising high-quality samples consistently annotated by human experts and propose a novel hierarchical evaluation framework with fine-grained, interpretable metrics. We evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our findings reveal: (1) LLMs struggle with background summary retrieval, generation, and opinion summary integration. (2) Even top LLMs achieve less than 69% average performance across both patterns. (3) Current LLMs lack adequate self-evaluation and self-correction capabilities for this task.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering</title>
<link>https://arxiv.org/abs/2505.12476</link>
<guid>https://arxiv.org/abs/2505.12476</guid>
<content:encoded><![CDATA[
arXiv:2505.12476v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have demonstrated impressive performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to find answers based on knowledge graphs (KGs) for natural language questions. Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the large KGs, and then generates the answers based on them. However, these methods emphasize the exploration of new optimal reasoning paths in KGs while ignoring the exploitation of historical reasoning paths, which may lead to sub-optimal reasoning paths. Additionally, the complex semantics contained in questions may lead to the retrieval of inaccurate reasoning paths. To address these issues, this paper proposes a novel and training-free framework for KGQA tasks called Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original question into a series of simpler and well-defined sub-questions to handle the complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided by a reward model is introduced to iteratively retrieve weighted reasoning paths as contextual knowledge. Finally, it stacks the weighted reasoning paths according to their weights to generate the final answers. Extensive experiments on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves 8.7\% and 7.0\% performance improvement over the state-of-the-art method on the GrailQA and the WebQSP respectively.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.12495</link>
<guid>https://arxiv.org/abs/2505.12495</guid>
<content:encoded><![CDATA[
arXiv:2505.12495v1 Announce Type: new 
Abstract: The increasing context length of modern language models has created a need for evaluating their ability to retrieve and process information across extensive documents. While existing benchmarks test long-context capabilities, they often lack a structured way to systematically vary question complexity. We introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a framework that (1) extracts QA pairs at multiple complexity levels (2) by leveraging structured representations of financial agreements (3) along three key dimensions -- multi-hop retrieval, set operations, and answer plurality -- enabling fine-grained assessment of model performance across controlled difficulty levels. Using this framework, we construct a dataset of 20,139 QA pairs (the largest number among the long-context benchmarks) and open-source a part of it. We evaluate 13 proprietary and open-source LLMs and observe that even the best-performing models are struggling with set-based comparisons and multi-hop logical inference. Our analysis reveals systematic failure modes tied to semantic misinterpretation and inability to handle implicit relations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection</title>
<link>https://arxiv.org/abs/2505.12507</link>
<guid>https://arxiv.org/abs/2505.12507</guid>
<content:encoded><![CDATA[
arXiv:2505.12507v1 Announce Type: new 
Abstract: The impressive ability of large language models to generate natural text across various tasks has led to critical challenges in authorship authentication. Although numerous detection methods have been developed to differentiate between machine-generated texts (MGT) and human-generated texts (HGT), the explainability of these methods remains a significant gap. Traditional explainability techniques often fall short in capturing the complex word relationships that distinguish HGT from MGT. To address this limitation, we present LM$^2$otifs, a novel explainable framework for MGT detection. Inspired by probabilistic graphical models, we provide a theoretical rationale for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks to achieve both accurate detection and interpretability. The LM$^2$otifs pipeline operates in three key stages: first, it transforms text into graphs based on word co-occurrence to represent lexical dependencies; second, graph neural networks are used for prediction; and third, a post-hoc explainability method extracts interpretable motifs, offering multi-level explanations from individual words to sentence structures. Extensive experiments on multiple benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The empirical evaluation of the extracted explainable motifs confirms their effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis reveals distinct and visible linguistic fingerprints characteristic of MGT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design</title>
<link>https://arxiv.org/abs/2505.12511</link>
<guid>https://arxiv.org/abs/2505.12511</guid>
<content:encoded><![CDATA[
arXiv:2505.12511v1 Announce Type: new 
Abstract: Inverse Protein Folding (IPF) is a critical subtask in the field of protein design, aiming to engineer amino acid sequences capable of folding correctly into a specified three-dimensional (3D) conformation. Although substantial progress has been achieved in recent years, existing methods generally rely on either backbone coordinates or molecular surface features alone, which restricts their ability to fully capture the complex chemical and geometric constraints necessary for precise sequence prediction. To address this limitation, we present DS-ProGen, a dual-structure deep language model for functional protein design, which integrates both backbone geometry and surface-level representations. By incorporating backbone coordinates as well as surface chemical and geometric descriptors into a next-amino-acid prediction paradigm, DS-ProGen is able to generate functionally relevant and structurally stable sequences while satisfying both global and local conformational constraints. On the PRIDE dataset, DS-ProGen attains the current state-of-the-art recovery rate of 61.47%, demonstrating the synergistic advantage of multi-modal structural encoding in protein design. Furthermore, DS-ProGen excels in predicting interactions with a variety of biological partners, including ligands, ions, and RNA, confirming its robust functional retention capabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents</title>
<link>https://arxiv.org/abs/2505.12531</link>
<guid>https://arxiv.org/abs/2505.12531</guid>
<content:encoded><![CDATA[
arXiv:2505.12531v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly power mental-health chatbots, yet the field still lacks a scalable, theory-grounded way to decide which model is most effective to deploy. We present ESC-Judge, the first end-to-end evaluation framework that (i) grounds head-to-head comparisons of emotional-support LLMs in Clara Hill's established Exploration-Insight-Action counseling model, providing a structured and interpretable view of performance, and (ii) fully automates the evaluation pipeline at scale. ESC-Judge operates in three stages: first, it synthesizes realistic help-seeker roles by sampling empirically salient attributes such as stressors, personality, and life history; second, it has two candidate support agents conduct separate sessions with the same role, isolating model-specific strategies; and third, it asks a specialized judge LLM to express pairwise preferences across rubric-anchored skills that span the Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and 86 percent of Action decisions, demonstrating human-level reliability at a fraction of the cost. All code, prompts, synthetic roles, transcripts, and judgment scripts are released to promote transparent progress in emotionally supportive AI.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE</title>
<link>https://arxiv.org/abs/2505.12533</link>
<guid>https://arxiv.org/abs/2505.12533</guid>
<content:encoded><![CDATA[
arXiv:2505.12533v1 Announce Type: new 
Abstract: Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with unseen data, even within similar domains. Notably, higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts. Our results also show that data quality, rather than lexical similarity, is key to robust transfer, and the choice of optimal adaptation strategy depends on the quality of data available: while fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data. However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results. Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation in Conversational Question Answering in the Era of LLM: A Survey</title>
<link>https://arxiv.org/abs/2505.12543</link>
<guid>https://arxiv.org/abs/2505.12543</guid>
<content:encoded><![CDATA[
arXiv:2505.12543v1 Announce Type: new 
Abstract: Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable language systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models</title>
<link>https://arxiv.org/abs/2505.12545</link>
<guid>https://arxiv.org/abs/2505.12545</guid>
<content:encoded><![CDATA[
arXiv:2505.12545v1 Announce Type: new 
Abstract: Predicting crash events is crucial for understanding crash distributions and their contributing factors, thereby enabling the design of proactive traffic safety policy interventions. However, existing methods struggle to interpret the complex interplay among various sources of traffic crash data, including numeric characteristics, textual reports, crash imagery, environmental conditions, and driver behavior records. As a result, they often fail to capture the rich semantic information and intricate interrelationships embedded in these diverse data sources, limiting their ability to identify critical crash risk factors. In this research, we propose TrafficSafe, a framework that adapts LLMs to reframe crash prediction and feature attribution as text-based reasoning. A multi-modal crash dataset including 58,903 real-world reports together with belonged infrastructure, environmental, driver, and vehicle information is collected and textualized into TrafficSafe Event Dataset. By customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves a 42% average improvement in F1-score over baselines. To interpret these predictions and uncover contributing factors, we introduce TrafficSafe Attribution, a sentence-level feature attribution framework enabling conditional risk analysis. Findings show that alcohol-impaired driving is the leading factor in severe crashes, with aggressive and impairment-related behaviors having nearly twice the contribution for severe crashes compared to other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal features during model training, guiding strategic crash data collection for iterative performance improvements. The proposed TrafficSafe offers a transformative leap in traffic safety research, providing a blueprint for translating advanced AI technologies into responsible, actionable, and life-saving outcomes.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting memorized pieces of (copyrighted) books from open-weight language models</title>
<link>https://arxiv.org/abs/2505.12546</link>
<guid>https://arxiv.org/abs/2505.12546</guid>
<content:encoded><![CDATA[
arXiv:2505.12546v1 Announce Type: new 
Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make sweeping, opposing claims about the extent to which large language models (LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial ML and copyright law, we show that these polarized positions dramatically oversimplify the relationship between memorization and copyright. To do so, we leverage a recent probabilistic extraction technique to extract pieces of the Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show that it's possible to extract substantial parts of at least some books from different LLMs. This is evidence that the LLMs have memorized the extracted text; this memorized content is copied inside the model parameters. But the results are complicated: the extent of memorization varies both by model and by book. With our specific experiments, we find that the largest LLMs don't memorize most books -- either in whole or in part. However, we also find that Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost entirely. We discuss why our results have significant implications for copyright cases, though not ones that unambiguously favor either side.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations</title>
<link>https://arxiv.org/abs/2505.12560</link>
<guid>https://arxiv.org/abs/2505.12560</guid>
<content:encoded><![CDATA[
arXiv:2505.12560v1 Announce Type: new 
Abstract: Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large automatically tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains more than 1,800 sentences of pos-tagged parallel text data from over 1,500 languages, representing 133 language families and 111 isolates, dwarfing previously available resources. The accuracy of tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Patent Claim Generation with European Patent Dataset</title>
<link>https://arxiv.org/abs/2505.12568</link>
<guid>https://arxiv.org/abs/2505.12568</guid>
<content:encoded><![CDATA[
arXiv:2505.12568v1 Announce Type: new 
Abstract: Drafting patent claims is time-intensive, costly, and requires professional skill. Therefore, researchers have investigated large language models (LLMs) to assist inventors in writing claims. However, existing work has largely relied on datasets from the United States Patent and Trademark Office (USPTO). To enlarge research scope regarding various jurisdictions, drafting conventions, and legal standards, we introduce EPD, a European patent dataset. EPD presents rich textual data and structured metadata to support multiple patent-related tasks, including claim generation. This dataset enriches the field in three critical aspects: (1) Jurisdictional diversity: Patents from different offices vary in legal and drafting conventions. EPD fills a critical gap by providing a benchmark for European patents to enable more comprehensive evaluation. (2) Quality improvement: EPD offers high-quality granted patents with finalized and legally approved texts, whereas others consist of patent applications that are unexamined or provisional. Experiments show that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets and even GPT-4o in claim quality and cross-domain generalization. (3) Real-world simulation: We propose a difficult subset of EPD to better reflect real-world challenges of claim generation. Results reveal that all tested LLMs perform substantially worse on these challenging samples, which highlights the need for future research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio</title>
<link>https://arxiv.org/abs/2505.12572</link>
<guid>https://arxiv.org/abs/2505.12572</guid>
<content:encoded><![CDATA[
arXiv:2505.12572v1 Announce Type: new 
Abstract: Writing novels with Large Language Models (LLMs) raises a critical question: how much human-authored outline is necessary to generate high-quality million-word novels? While frameworks such as DOME, Plan&amp;Write, and Long Writer have improved stylistic coherence and logical consistency, they primarily target shorter novels (10k--100k words), leaving ultra-long generation largely unexplored. Drawing on insights from recent text compression methods like LLMZip and LLM2Vec, we conduct an information-theoretic analysis that quantifies distortion occurring when LLMs compress and reconstruct ultra-long novels under varying compression-expansion ratios. We introduce a hierarchical two-stage generation pipeline (outline -> detailed outline -> manuscript) and find an optimal outline length that balances information preservation with human effort. Through extensive experimentation with Chinese novels, we establish that a two-stage hierarchical outline approach significantly reduces semantic distortion compared to single-stage methods. Our findings provide empirically-grounded guidance for authors and researchers collaborating with LLMs to create million-word novels.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multilingual Language Models by Aligning Representations through Steering</title>
<link>https://arxiv.org/abs/2505.12584</link>
<guid>https://arxiv.org/abs/2505.12584</guid>
<content:encoded><![CDATA[
arXiv:2505.12584v1 Announce Type: new 
Abstract: In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layer's activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\textsc{sft}) and reinforcement learning from human feedback (\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling</title>
<link>https://arxiv.org/abs/2505.12587</link>
<guid>https://arxiv.org/abs/2505.12587</guid>
<content:encoded><![CDATA[
arXiv:2505.12587v1 Announce Type: new 
Abstract: Code-mixed languages, characterized by frequent within-sentence language transitions, present structural challenges that standard language models fail to address. In this work, we propose CMLFormer, an enhanced multi-layer dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, designed to model the linguistic and semantic dynamics of code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with switching point and translation annotations with multiple new objectives specifically aimed at capturing switching behavior, cross-lingual structure, and code-mixing complexity. Our experiments show that CMLFormer improves F1 score, precision, and accuracy over other approaches on the HASOC-2021 benchmark under select pre-training setups. Attention analyses further show that it can identify and attend to switching points, validating its sensitivity to code-mixed structure. These results demonstrate the effectiveness of CMLFormer's architecture and multi-task pre-training strategy for modeling code-mixed languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptPrism: A Linguistically-Inspired Taxonomy for Prompts</title>
<link>https://arxiv.org/abs/2505.12592</link>
<guid>https://arxiv.org/abs/2505.12592</guid>
<content:encoded><![CDATA[
arXiv:2505.12592v1 Announce Type: new 
Abstract: Prompts are the interface for eliciting the capabilities of large language models (LLMs). Understanding their structure and components is critical for analyzing LLM behavior and optimizing performance. However, the field lacks a comprehensive framework for systematic prompt analysis and understanding. We introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt analysis across three hierarchical levels: functional structure, semantic component, and syntactic pattern. We show the practical utility of PromptPrism by applying it to three applications: (1) a taxonomy-guided prompt refinement approach that automatically improves prompt quality and enhances model performance across a range of tasks; (2) a multi-dimensional dataset profiling method that extracts and aggregates structural, semantic, and syntactic characteristics from prompt datasets, enabling comprehensive analysis of prompt distributions and patterns; (3) a controlled experimental framework for prompt sensitivity analysis by quantifying the impact of semantic reordering and delimiter modifications on LLM performance. Our experimental results validate the effectiveness of our taxonomy across these applications, demonstrating that PromptPrism provides a foundation for refining, profiling, and analyzing prompts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.12594</link>
<guid>https://arxiv.org/abs/2505.12594</guid>
<content:encoded><![CDATA[
arXiv:2505.12594v1 Announce Type: new 
Abstract: Anomaly detection (AD) is essential in areas such as fraud detection, network monitoring, and scientific research. However, the diversity of data modalities and the increasing number of specialized AD libraries pose challenges for non-expert users who lack in-depth library-specific knowledge and advanced programming skills. To tackle this, we present AD-AGENT, an LLM-driven multi-agent framework that turns natural-language instructions into fully executable AD pipelines. AD-AGENT coordinates specialized agents for intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging. Using a shared short-term workspace and a long-term cache, the agents integrate popular AD libraries like PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that AD-AGENT produces reliable scripts and recommends competitive models across libraries. The system is open-sourced to support further research and practical applications in AD.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2505.12616</link>
<guid>https://arxiv.org/abs/2505.12616</guid>
<content:encoded><![CDATA[
arXiv:2505.12616v1 Announce Type: new 
Abstract: This paper presents the Duluth approach to the SemEval-2025 Task 7 on Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a TF-IDF-based retrieval system with experimentation on vector dimensions and tokenization strategies. Our best-performing configuration used word-level tokenization with a vocabulary size of 15,000 features, achieving an average success@10 score of 0.78 on the development set and 0.69 on the test set across ten languages. Our system showed stronger performance on higher-resource languages but still lagged significantly behind the top-ranked system, which achieved 0.96 average success@10. Our findings suggest that though advanced neural architectures are increasingly dominant in multilingual retrieval tasks, properly optimized traditional methods like TF-IDF remain competitive baselines, especially in limited compute resource scenarios.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Attribute: Improving the Performance of LLMs Attribution Systems</title>
<link>https://arxiv.org/abs/2505.12621</link>
<guid>https://arxiv.org/abs/2505.12621</guid>
<content:encoded><![CDATA[
arXiv:2505.12621v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly applied in various science domains, yet their broader adoption remains constrained by a critical challenge: the lack of trustworthy, verifiable outputs. Current LLMs often generate answers without reliable source attribution, or worse, with incorrect attributions, posing a barrier to their use in scientific and high-stakes settings, where traceability and accountability are non-negotiable. To be reliable, attribution systems need high accuracy and retrieve data with short lengths, i.e., attribute to a sentence within a document rather than a whole document. We propose a sentence-level pre-attribution step for Retrieve-Augmented Generation (RAG) systems that classify sentences into three categories: not attributable, attributable to a single quote, and attributable to multiple quotes. By separating sentences before attribution, a proper attribution method can be selected for the type of sentence, or the attribution can be skipped altogether. Our results indicate that classifiers are well-suited for this task. In this work, we propose a pre-attribution step to reduce the computational complexity of attribution, provide a clean version of the HAGRID dataset, and provide an end-to-end attribution system that works out of the box.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model</title>
<link>https://arxiv.org/abs/2505.12625</link>
<guid>https://arxiv.org/abs/2505.12625</guid>
<content:encoded><![CDATA[
arXiv:2505.12625v1 Announce Type: new 
Abstract: DeepSeek recently released R1, a high-performing large language model (LLM) optimized for reasoning tasks. Despite its efficient training pipeline, R1 achieves competitive performance, even surpassing leading reasoning models like OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1 refuses to answer certain prompts related to politically sensitive topics in China. While existing LLMs often implement safeguards to avoid generating harmful or offensive outputs, R1 represents a notable shift - exhibiting censorship-like behavior on politically charged queries. In this paper, we investigate this phenomenon by first introducing a large-scale set of heavily curated prompts that get censored by R1, covering a range of politically sensitive topics, but are not censored by other models. We then conduct a comprehensive analysis of R1's censorship patterns, examining their consistency, triggers, and variations across topics, prompt phrasing, and context. Beyond English-language queries, we explore censorship behavior in other languages. We also investigate the transferability of censorship to models distilled from the R1 language model. Finally, we propose techniques for bypassing or removing this censorship. Our findings reveal possible additional censorship integration likely shaped by design choices during training or alignment, raising concerns about transparency, bias, and governance in language model deployment.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing</title>
<link>https://arxiv.org/abs/2505.12636</link>
<guid>https://arxiv.org/abs/2505.12636</guid>
<content:encoded><![CDATA[
arXiv:2505.12636v1 Announce Type: new 
Abstract: Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of "superficial editing" to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available here.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals</title>
<link>https://arxiv.org/abs/2505.12654</link>
<guid>https://arxiv.org/abs/2505.12654</guid>
<content:encoded><![CDATA[
arXiv:2505.12654v1 Announce Type: new 
Abstract: This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10\% increase in F1-score on turn-taking and a 33\% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering</title>
<link>https://arxiv.org/abs/2505.12662</link>
<guid>https://arxiv.org/abs/2505.12662</guid>
<content:encoded><![CDATA[
arXiv:2505.12662v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have led to impressive progress in natural language generation, yet their tendency to produce hallucinated or unsubstantiated content remains a critical concern. To improve factual reliability, Retrieval-Augmented Generation (RAG) integrates external knowledge during inference. However, existing RAG systems face two major limitations: (1) unreliable adaptive control due to limited external knowledge supervision, and (2) hallucinations caused by inaccurate or irrelevant references. To address these issues, we propose Know3-RAG, a knowledge-aware RAG framework that leverages structured knowledge from knowledge graphs (KGs) to guide three core stages of the RAG process, including retrieval, generation, and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval module that employs KG embedding to assess the confidence of the generated answer and determine retrieval necessity, a knowledge-enhanced reference generation strategy that enriches queries with KG-derived entities to improve generated reference relevance, and a knowledge-driven reference filtering mechanism that ensures semantic alignment and factual accuracy of references. Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG consistently outperforms strong baselines, significantly reducing hallucinations and enhancing answer reliability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shadow-FT: Tuning Instruct via Base</title>
<link>https://arxiv.org/abs/2505.12716</link>
<guid>https://arxiv.org/abs/2505.12716</guid>
<content:encoded><![CDATA[
arXiv:2505.12716v1 Announce Type: new 
Abstract: Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{https://github.com/wutaiqiang/Shadow-FT}{Github}.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving</title>
<link>https://arxiv.org/abs/2505.12717</link>
<guid>https://arxiv.org/abs/2505.12717</guid>
<content:encoded><![CDATA[
arXiv:2505.12717v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework</title>
<link>https://arxiv.org/abs/2505.12718</link>
<guid>https://arxiv.org/abs/2505.12718</guid>
<content:encoded><![CDATA[
arXiv:2505.12718v1 Announce Type: new 
Abstract: Recent advances in Generative Artificial Intelligence (GenAI) have transformed educational content creation, particularly in developing tutor training materials. However, biases embedded in AI-generated content--such as gender, racial, or national stereotypes--raise significant ethical and educational concerns. Despite the growing use of GenAI, systematic methods for detecting and evaluating such biases in educational materials remain limited. This study proposes an automated bias assessment approach that integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework. We applied this method to AI-generated texts used in tutor training lessons. Results show a high alignment between the automated and manually curated word sets, with a Pearson correlation coefficient of r = 0.993, indicating reliable and consistent bias assessment. Our method reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing GenAI-produced educational content.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding</title>
<link>https://arxiv.org/abs/2505.12723</link>
<guid>https://arxiv.org/abs/2505.12723</guid>
<content:encoded><![CDATA[
arXiv:2505.12723v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve remarkable performance in code generation tasks. However, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages. Moreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies. Within OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests. Complementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method. Specifically, GEPO trains the LLM using intermediate representations (IRs) groups. LLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group. This process allows LLMs to capture nuanced aspects of code functionality. By employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. Extensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma</title>
<link>https://arxiv.org/abs/2505.12727</link>
<guid>https://arxiv.org/abs/2505.12727</guid>
<content:encoded><![CDATA[
arXiv:2505.12727v1 Announce Type: new 
Abstract: Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL</title>
<link>https://arxiv.org/abs/2505.12768</link>
<guid>https://arxiv.org/abs/2505.12768</guid>
<content:encoded><![CDATA[
arXiv:2505.12768v1 Announce Type: new 
Abstract: In Text-to-SQL, execution feedback is essential for guiding large language models (LLMs) to reason accurately and generate reliable SQL queries. However, existing methods treat execution feedback solely as a post-hoc signal for correction or selection, failing to integrate it into the generation process. This limitation hinders their ability to address reasoning errors as they occur, ultimately reducing query accuracy and robustness. To address this issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement Learning), a framework for Text-to-SQL that enables models to interact with the database during decoding and dynamically adjust their reasoning based on execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm that interleaves intermediate SQL execution into reasoning paths, facilitating context-sensitive revisions. It achieves this through structured prompts with markup tags and a stepwise rollout strategy that integrates execution feedback into each stage of generation. To supervise policy learning, we develop a composite reward function that includes an exploration reward, explicitly encouraging effective database interaction. Additionally, ReEx-SQL adopts a tree-based decoding strategy to support exploratory reasoning, enabling dynamic expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving 85.2% on Spider-Realistic with leading performance. In addition, its tree-structured decoding improves efficiency and performance over linear decoding, reducing inference time by 51.9% on the BIRD development set.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
arXiv:2505.12781v1 Announce Type: new 
Abstract: Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs</title>
<link>https://arxiv.org/abs/2505.12792</link>
<guid>https://arxiv.org/abs/2505.12792</guid>
<content:encoded><![CDATA[
arXiv:2505.12792v1 Announce Type: new 
Abstract: The rapid evolution of large language models (LLMs) has revolutionized various fields, including the identification and discovery of human values within text data. While traditional NLP models, such as BERT, have been employed for this task, their ability to represent textual data is significantly outperformed by emerging LLMs like GPTs. However, the performance of online LLMs often degrades when handling long contexts required for value identification, which also incurs substantial computational costs. To address these challenges, we propose EAVIT, an efficient and accurate framework for human value identification that combines the strengths of both locally fine-tunable and online black-box LLMs. Our framework employs a value detector - a small, local language model - to generate initial value estimations. These estimations are then used to construct concise input prompts for online LLMs, enabling accurate final value identification. To train the value detector, we introduce explanation-based training and data generation techniques specifically tailored for value identification, alongside sampling strategies to optimize the brevity of LLM input prompts. Our approach effectively reduces the number of input tokens by up to 1/6 compared to directly querying online LLMs, while consistently outperforming traditional NLP methods and other LLM-based strategies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models</title>
<link>https://arxiv.org/abs/2505.12808</link>
<guid>https://arxiv.org/abs/2505.12808</guid>
<content:encoded><![CDATA[
arXiv:2505.12808v1 Announce Type: new 
Abstract: The recent explosion of large language models (LLMs), each with its own general or specialized strengths, makes scalable, reliable benchmarking more urgent than ever. Standard practices nowadays face fundamental trade-offs: closed-ended question-based benchmarks (eg MMLU) struggle with saturation as newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely on costly and slow human judges. Recently, automated methods (eg LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one or a few "authority" models. To tackle these issues, we propose Decentralized Arena (dearena), a fully automated framework leveraging collective intelligence from all LLMs to evaluate each other. It mitigates single-model judge bias by democratic, pairwise evaluation, and remains efficient at scale through two key components: (1) a coarse-to-fine ranking algorithm for fast incremental insertion of new models with sub-quadratic complexity, and (2) an automatic question selection strategy for the construction of new evaluation dimensions. Across extensive experiments across 66 LLMs, dearena attains up to 97% correlation with human judgements, while significantly reducing the cost. Our code and data will be publicly released on https://github.com/maitrix-org/de-arena.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs</title>
<link>https://arxiv.org/abs/2505.12814</link>
<guid>https://arxiv.org/abs/2505.12814</guid>
<content:encoded><![CDATA[
arXiv:2505.12814v1 Announce Type: new 
Abstract: Existing LLM-based role-playing methods often rely on superficial textual descriptions or simplistic metrics, inadequately modeling both intrinsic and extrinsic character dimensions. Additionally, they typically simulate character memory with implicit model knowledge or basic retrieval augment generation without explicit memory alignment, compromising memory consistency. The two issues weaken reliability of role-playing LLMs in several applications, such as trustworthy social simulation. To address these limitations, we propose PsyMem, a novel framework integrating fine-grained psychological attributes and explicit memory control for role-playing. PsyMem supplements textual descriptions with 26 psychological indicators to detailed model character. Additionally, PsyMem implements memory alignment training, explicitly trains the model to align character's response with memory, thereby enabling dynamic memory-controlled responding during inference. By training Qwen2.5-7B-Instruct on our specially designed dataset (including 5,414 characters and 38,962 dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen, outperforms baseline models in role-playing, achieving the best performance in human-likeness and character fidelity.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models</title>
<link>https://arxiv.org/abs/2505.12821</link>
<guid>https://arxiv.org/abs/2505.12821</guid>
<content:encoded><![CDATA[
arXiv:2505.12821v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9\% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering</title>
<link>https://arxiv.org/abs/2505.12831</link>
<guid>https://arxiv.org/abs/2505.12831</guid>
<content:encoded><![CDATA[
arXiv:2505.12831v1 Announce Type: new 
Abstract: Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) method that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs. Our code will be released at https://github.com/zifengcheng/CP.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.12835</link>
<guid>https://arxiv.org/abs/2505.12835</guid>
<content:encoded><![CDATA[
arXiv:2505.12835v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting</title>
<link>https://arxiv.org/abs/2505.12837</link>
<guid>https://arxiv.org/abs/2505.12837</guid>
<content:encoded><![CDATA[
arXiv:2505.12837v1 Announce Type: new 
Abstract: Legal contracts possess an inherent, semantically vital structure (e.g., sections, clauses) that is crucial for human comprehension but whose impact on LLM processing remains under-explored. This paper investigates the effects of explicit input text structure and prompt engineering on the performance of GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the CUAD. We compare model exact-match accuracy across various input formats: well-structured plain-text (human-generated from CUAD), plain-text cleaned of line breaks, extracted plain-text from Azure OCR, plain-text extracted by GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o Vision. To give an indication of the impact of possible prompt engineering, we assess the impact of shifting task instructions to the system prompt and explicitly informing the model about the structured nature of the input. Our findings reveal that GPT-4o demonstrates considerable robustness to variations in input structure, but lacks in overall performance. Conversely, GPT-4.1's performance is markedly sensitive; poorly structured inputs yield suboptimal results (but identical with GPT-4o), while well-structured formats (original CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by ~20 percentage points. Optimizing the system prompt to include task details and an advisory about structured input further elevates GPT-4.1's accuracy by an additional ~10-13 percentage points, with Markdown ultimately achieving the highest performance under these conditions (79 percentage points overall exact-match accuracy). This research empirically demonstrates that while newer models exhibit greater resilience, careful input structuring and strategic prompt design remain critical for optimizing the performance of LLMs, and can significantly affect outcomes in high-stakes legal applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-identification of De-identified Documents with Autoregressive Infilling</title>
<link>https://arxiv.org/abs/2505.12859</link>
<guid>https://arxiv.org/abs/2505.12859</guid>
<content:encoded><![CDATA[
arXiv:2505.12859v1 Announce Type: new 
Abstract: Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEXam: Benchmarking Legal Reasoning on 340 Law Exams</title>
<link>https://arxiv.org/abs/2505.12864</link>
<guid>https://arxiv.org/abs/2505.12864</guid>
<content:encoded><![CDATA[
arXiv:2505.12864v1 Announce Type: new 
Abstract: Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation</title>
<link>https://arxiv.org/abs/2505.12888</link>
<guid>https://arxiv.org/abs/2505.12888</guid>
<content:encoded><![CDATA[
arXiv:2505.12888v1 Announce Type: new 
Abstract: Medication recommendations have become an important task in the healthcare domain, especially in measuring the accuracy and safety of medical dialogue systems (MDS). Different from the recommendation task based on electronic health records (EHRs), dialogue-based medication recommendations require research on the interaction details between patients and doctors, which is crucial but may not exist in EHRs. Recent advancements in large language models (LLM) have extended the medical dialogue domain. These LLMs can interpret patients' intent and provide medical suggestions including medication recommendations, but some challenges are still worth attention. During a multi-turn dialogue, LLMs may ignore the fine-grained medical information or connections across the dialogue turns, which is vital for providing accurate suggestions. Besides, LLMs may generate non-factual responses when there is a lack of domain-specific knowledge, which is more risky in the medical domain. To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted \textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication recommendation. It extracts medical concepts and corresponding states from dialogue to construct an explicitly patient-centric graph, which can describe the neglected but important information. Further, combined with external medical knowledge graphs, GAP can generate abundant queries and prompts, thus retrieving information from multiple sources to reduce the non-factual responses. We evaluate GAP on a dialogue-based medication recommendation dataset and further explore its potential in a more difficult scenario, dynamically diagnostic interviewing. Extensive experiments demonstrate its competitive performance when compared with strong baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Thinking-Language Modeling Gap in Large Language Models</title>
<link>https://arxiv.org/abs/2505.12896</link>
<guid>https://arxiv.org/abs/2505.12896</guid>
<content:encoded><![CDATA[
arXiv:2505.12896v1 Announce Type: new 
Abstract: System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking, modeling human language can easily absorb language biases into LLMs deviated from the chain of thoughts in minds. Furthermore, we show that the biases will mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of the premise. To this end, we propose a new prompt technique termed Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of directly eliciting the chain of thoughts from partial information, LoT instructs LLMs to adjust the order and token used for the expressions of all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyFCG: Fluid Construction Grammar in Python</title>
<link>https://arxiv.org/abs/2505.12920</link>
<guid>https://arxiv.org/abs/2505.12920</guid>
<content:encoded><![CDATA[
arXiv:2505.12920v1 Announce Type: new 
Abstract: We present PyFCG, an open source software library that ports Fluid Construction Grammar (FCG) to the Python programming language. PyFCG enables its users to seamlessly integrate FCG functionality into Python programs, and to use FCG in combination with other libraries within Python's rich ecosystem. Apart from a general description of the library, this paper provides three walkthrough tutorials that demonstrate example usage of PyFCG in typical use cases of FCG: (i) formalising and testing construction grammar analyses, (ii) learning usage-based construction grammars from corpora, and (iii) implementing agent-based experiments on emergent communication.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs</title>
<link>https://arxiv.org/abs/2505.12929</link>
<guid>https://arxiv.org/abs/2505.12929</guid>
<content:encoded><![CDATA[
arXiv:2505.12929v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&amp;K Logic Puzzle reasoning tasks. Our implementation is available at https://github.com/zhyang2226/AR-Lopti.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A3 : an Analytical Low-Rank Approximation Framework for Attention</title>
<link>https://arxiv.org/abs/2505.12942</link>
<guid>https://arxiv.org/abs/2505.12942</guid>
<content:encoded><![CDATA[
arXiv:2505.12942v1 Announce Type: new 
Abstract: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Morphological Tagging for Nguni Languages</title>
<link>https://arxiv.org/abs/2505.12949</link>
<guid>https://arxiv.org/abs/2505.12949</guid>
<content:encoded><![CDATA[
arXiv:2505.12949v1 Announce Type: new 
Abstract: Morphological parsing is the task of decomposing words into morphemes, the smallest units of meaning in a language, and labelling their grammatical roles. It is a particularly challenging task for agglutinative languages, such as the Nguni languages of South Africa, which construct words by concatenating multiple morphemes. A morphological parsing system can be framed as a pipeline with two separate components, a segmenter followed by a tagger. This paper investigates the use of neural methods to build morphological taggers for the four Nguni languages. We compare two classes of approaches: training neural sequence labellers (LSTMs and neural CRFs) from scratch and finetuning pretrained language models. We compare performance across these two categories, as well as to a traditional rule-based morphological parser. Neural taggers comfortably outperform the rule-based baseline and models trained from scratch tend to outperform pretrained models. We also compare parsing results across different upstream segmenters and with varying linguistic input features. Our findings confirm the viability of employing neural taggers based on pre-existing morphological segmenters for the Nguni languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuRE:Generative Query REwriter for Legal Passage Retrieval</title>
<link>https://arxiv.org/abs/2505.12950</link>
<guid>https://arxiv.org/abs/2505.12950</guid>
<content:encoded><![CDATA[
arXiv:2505.12950v1 Announce Type: new 
Abstract: Legal Passage Retrieval (LPR) systems are crucial as they help practitioners save time when drafting legal arguments. However, it remains an underexplored avenue. One primary reason is the significant vocabulary mismatch between the query and the target passage. To address this, we propose a simple yet effective method, the Generative query REwriter (GuRE). We leverage the generative capabilities of Large Language Models (LLMs) by training the LLM for query rewriting. "Rewritten queries" help retrievers to retrieve target passages by mitigating vocabulary mismatch. Experimental results show that GuRE significantly improves performance in a retriever-agnostic manner, outperforming all baseline methods. Further analysis reveals that different training objectives lead to distinct retrieval behaviors, making GuRE more suitable than direct retriever fine-tuning for real-world applications. Codes are avaiable at github.com/daehuikim/GuRE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition</title>
<link>https://arxiv.org/abs/2505.12964</link>
<guid>https://arxiv.org/abs/2505.12964</guid>
<content:encoded><![CDATA[
arXiv:2505.12964v1 Announce Type: new 
Abstract: Recognizing biomedical concepts in the text is vital for ontology refinement, knowledge graph construction, and concept relationship discovery. However, traditional concept recognition methods, relying on explicit mention identification, often fail to capture complex concepts not explicitly stated in the text. To overcome this limitation, we introduce MA-COIR, a framework that reformulates concept recognition as an indexing-recognition task. By assigning semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in ontology entries and enhances recognition efficiency. Using a pretrained BART-based model fine-tuned on small datasets, our approach reduces computational requirements to facilitate adoption by domain experts. Furthermore, we incorporate large language models (LLMs)-generated queries and synthetic data to improve recognition in low-resource settings. Experimental results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of MA-COIR in recognizing both explicit and implicit concepts without the need for mention-level annotations during inference, advancing ontology-driven concept recognition in biomedical domain applications. Our code and constructed data are available at https://github.com/sl-633/macoir-master.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down</title>
<link>https://arxiv.org/abs/2505.12969</link>
<guid>https://arxiv.org/abs/2505.12969</guid>
<content:encoded><![CDATA[
arXiv:2505.12969v1 Announce Type: new 
Abstract: OpenAI's Whisper has achieved significant success in Automatic Speech Recognition. However, it has consistently been found to exhibit hallucination issues, particularly in non-speech segments, which limits its broader application in complex industrial settings.
  In this paper, we introduce a novel method to reduce Whisper's hallucination on non-speech segments without using any pre- or post-possessing techniques. Specifically, we benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder to the hallucination problem by performing a head-wise mask. Our findings reveal that only 3 of the 20 heads account for over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune these three crazy heads using a collection of non-speech data. The results show that our best fine-tuned model, namely Calm-Whisper, achieves over 80% reduction in non-speech hallucination with only less than 0.1% WER degradation on LibriSpeech test-clean and test-other.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Literature Review on Traditional Approaches in Current Natural Language Processing</title>
<link>https://arxiv.org/abs/2505.12970</link>
<guid>https://arxiv.org/abs/2505.12970</guid>
<content:encoded><![CDATA[
arXiv:2505.12970v1 Announce Type: new 
Abstract: The continued rise of neural networks and large language models in the more recent past has altered the natural language processing landscape, enabling new approaches towards typical language tasks and achieving mainstream success. Despite the huge success of large language models, many disadvantages still remain and through this work we assess the state of the art in five application scenarios with a particular focus on the future perspectives and sensible application scenarios of traditional and older approaches and techniques.
  In this paper we survey recent publications in the application scenarios classification, information and relation extraction, text simplification as well as text summarization. After defining our terminology, i.e., which features are characteristic for traditional techniques in our interpretation for the five scenarios, we survey if such traditional approaches are still being used, and if so, in what way they are used. It turns out that all five application scenarios still exhibit traditional models in one way or another, as part of a processing pipeline, as a comparison/baseline to the core model of the respective paper, or as the main model(s) of the paper. For the complete statistics, see https://zenodo.org/records/13683801
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models</title>
<link>https://arxiv.org/abs/2505.12973</link>
<guid>https://arxiv.org/abs/2505.12973</guid>
<content:encoded><![CDATA[
arXiv:2505.12973v1 Announce Type: new 
Abstract: Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Many-to-Many Summarization with Large Language Models</title>
<link>https://arxiv.org/abs/2505.12983</link>
<guid>https://arxiv.org/abs/2505.12983</guid>
<content:encoded><![CDATA[
arXiv:2505.12983v1 Announce Type: new 
Abstract: Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.12996</link>
<guid>https://arxiv.org/abs/2505.12996</guid>
<content:encoded><![CDATA[
arXiv:2505.12996v1 Announce Type: new 
Abstract: In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code</title>
<link>https://arxiv.org/abs/2505.13004</link>
<guid>https://arxiv.org/abs/2505.13004</guid>
<content:encoded><![CDATA[
arXiv:2505.13004v1 Announce Type: new 
Abstract: Existing code generation benchmarks primarily evaluate functional correctness, with limited focus on code efficiency and often restricted to a single language like Python. To address this gap, we introduce EffiBench-X, the first multi-language benchmark designed to measure the efficiency of LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby, and Golang. It comprises competitive programming tasks with human-expert solutions as efficiency baselines. Evaluating state-of-the-art LLMs on EffiBench-X reveals that while models generate functionally correct code, they consistently underperform human experts in efficiency. Even the most efficient LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human efficiency on average, with significant language-specific variations. LLMs show better efficiency in Python, Ruby, and JavaScript than in Java, C++, and Golang. For instance, DeepSeek-R1's Python code is significantly more efficient than its Java code. These results highlight the critical need for research into LLM optimization techniques to improve code efficiency across diverse languages. The dataset and evaluation infrastructure are submitted and available at https://github.com/EffiBench/EffiBench-X.git and https://huggingface.co/datasets/EffiBench/effibench-x.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain</title>
<link>https://arxiv.org/abs/2505.13006</link>
<guid>https://arxiv.org/abs/2505.13006</guid>
<content:encoded><![CDATA[
arXiv:2505.13006v1 Announce Type: new 
Abstract: Airports from the top 20 in terms of annual passengers are highly dynamic environments with thousands of flights daily, and they aim to increase the degree of automation. To contribute to this, we implemented a Conversational AI system that enables staff in an airport to communicate with flight information systems. This system not only answers standard airport queries but also resolves airport terminology, jargon, abbreviations, and dynamic questions involving reasoning. In this paper, we built three different Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally produced hallucinations, which is risky to airport safety. In contrast, SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Moreover, Graph RAG was especially effective for questions that involved reasoning. Based on our observations, we thus recommend SQL RAG and Graph RAG are better for airport environments, due to fewer hallucinations and the ability to handle dynamic questions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Bias or Not to Bias: Detecting bias in News with bias-detector</title>
<link>https://arxiv.org/abs/2505.13010</link>
<guid>https://arxiv.org/abs/2505.13010</guid>
<content:encoded><![CDATA[
arXiv:2505.13010v1 Announce Type: new 
Abstract: Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation</title>
<link>https://arxiv.org/abs/2505.13034</link>
<guid>https://arxiv.org/abs/2505.13034</guid>
<content:encoded><![CDATA[
arXiv:2505.13034v1 Announce Type: new 
Abstract: Topic models are statistical tools that allow their users to gain qualitative and quantitative insights into the contents of textual corpora without the need for close reading. They can be applied in a wide range of settings from discourse analysis, through pretraining data curation, to text filtering. Topic models are typically parameter-rich, complex models, and interpreting these parameters can be challenging for their users. It is typical practice for users to interpret topics based on the top 10 highest ranking terms on a given topic. This list-of-words approach, however, gives users a limited and biased picture of the content of topics. Thoughtful user interface design and visualizations can help users gain a more complete and accurate understanding of topic models' output. While some visualization utilities do exist for topic models, these are typically limited to a certain type of topic model. We introduce topicwizard, a framework for model-agnostic topic model interpretation, that provides intuitive and interactive tools that help users examine the complex semantic relations between documents, words and topics learned by topic models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025</title>
<link>https://arxiv.org/abs/2505.13036</link>
<guid>https://arxiv.org/abs/2505.13036</guid>
<content:encoded><![CDATA[
arXiv:2505.13036v1 Announce Type: new 
Abstract: The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation</title>
<link>https://arxiv.org/abs/2505.13053</link>
<guid>https://arxiv.org/abs/2505.13053</guid>
<content:encoded><![CDATA[
arXiv:2505.13053v1 Announce Type: new 
Abstract: Adapting to the addressee is crucial for successful explanations, yet poses significant challenges for dialogsystems. We adopt the approach of treating explanation generation as a non-stationary decision process, where the optimal strategy varies according to changing beliefs about the explainee and the interaction context. In this paper we address the questions of (1) how to track the interaction context and the relevant listener features in a formally defined computational partner model, and (2) how to utilize this model in the dynamically adjusted, rational decision process that determines the currently best explanation strategy. We propose a Bayesian inference-based approach to continuously update the partner model based on user feedback, and a non-stationary Markov Decision Process to adjust decision-making based on the partner model values. We evaluate an implementation of this framework with five simulated interlocutors, demonstrating its effectiveness in adapting to different partners with constant and even changing feedback behavior. The results show high adaptivity with distinct explanation strategies emerging for different partners, highlighting the potential of our approach to improve explainable AI systems and dialogsystems in general.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset</title>
<link>https://arxiv.org/abs/2505.13069</link>
<guid>https://arxiv.org/abs/2505.13069</guid>
<content:encoded><![CDATA[
arXiv:2505.13069v1 Announce Type: new 
Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide risk assessment in adolescents. This study investigates a multimodal approach for this challenge, integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM. Additionally, handcrafted acoustic features -- including MFCCs, spectral contrast, and pitch-related statistics -- were incorporated. We explored three fusion strategies: early concatenation, modality-specific processing, and weighted attention with mixup regularization. Results show that weighted attention provided the best generalization, achieving 69% accuracy on the development set, though a performance gap between development and test sets highlights generalization challenges. Our findings, strictly tied to the MINI-KID framework, emphasize the importance of refining embedding representations and fusion mechanisms to enhance classification reliability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Sequential Numerical Prediction in Autoregressive Models</title>
<link>https://arxiv.org/abs/2505.13077</link>
<guid>https://arxiv.org/abs/2505.13077</guid>
<content:encoded><![CDATA[
arXiv:2505.13077v1 Announce Type: new 
Abstract: Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Generalization in Language Models Scales with Information Entropy</title>
<link>https://arxiv.org/abs/2505.13089</link>
<guid>https://arxiv.org/abs/2505.13089</guid>
<content:encoded><![CDATA[
arXiv:2505.13089v1 Announce Type: new 
Abstract: Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation</title>
<link>https://arxiv.org/abs/2505.13090</link>
<guid>https://arxiv.org/abs/2505.13090</guid>
<content:encoded><![CDATA[
arXiv:2505.13090v1 Announce Type: new 
Abstract: Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning</title>
<link>https://arxiv.org/abs/2505.13115</link>
<guid>https://arxiv.org/abs/2505.13115</guid>
<content:encoded><![CDATA[
arXiv:2505.13115v1 Announce Type: new 
Abstract: The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA).
  We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModernGBERT: German-only 1B Encoder Model Trained from Scratch</title>
<link>https://arxiv.org/abs/2505.13136</link>
<guid>https://arxiv.org/abs/2505.13136</guid>
<content:encoded><![CDATA[
arXiv:2505.13136v1 Announce Type: new 
Abstract: Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LL\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling a controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance encoder models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Cross-Lingual Inconsistency in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13141</link>
<guid>https://arxiv.org/abs/2505.13141</guid>
<content:encoded><![CDATA[
arXiv:2505.13141v1 Announce Type: new 
Abstract: Large language models (LLMs) are demonstrably capable of cross-lingual transfer, but can produce inconsistent output when prompted with the same queries written in different languages. To understand how language models are able to generalize knowledge from one language to the others, we apply the logit lens to interpret the implicit steps taken by LLMs to solve multilingual multi-choice reasoning questions. We find LLMs predict inconsistently and are less accurate because they rely on subspaces of individual languages, rather than working in a shared semantic space. While larger models are more multilingual, we show their hidden states are more likely to dissociate from the shared representation compared to smaller models, but are nevertheless more capable of retrieving knowledge embedded across different languages. Finally, we demonstrate that knowledge sharing can be modulated by steering the models' latent processing towards the shared semantic space. We find reinforcing utilization of the shared space improves the models' multilingual reasoning performance, as a result of more knowledge transfer from, and better output consistency with English.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text</title>
<link>https://arxiv.org/abs/2505.13147</link>
<guid>https://arxiv.org/abs/2505.13147</guid>
<content:encoded><![CDATA[
arXiv:2505.13147v1 Announce Type: new 
Abstract: Can deception be detected solely from written text? Cues of deceptive communication are inherently subtle, even more so in text-only communication. Yet, prior studies have reported considerable success in automatic deception detection. We hypothesize that such findings are largely driven by artifacts introduced during data collection and do not generalize beyond specific datasets. We revisit this assumption by introducing a belief-based deception framework, which defines deception as a misalignment between an author's claims and true beliefs, irrespective of factual accuracy, allowing deception cues to be studied in isolation. Based on this framework, we construct three corpora, collectively referred to as DeFaBel, including a German-language corpus of deceptive and non-deceptive arguments and a multilingual version in German and English, each collected under varying conditions to account for belief change and enable cross-linguistic analysis. Using these corpora, we evaluate commonly reported linguistic cues of deception. Across all three DeFaBel variants, these cues show negligible, statistically insignificant correlations with deception labels, contrary to prior work that treats such cues as reliable indicators. We further benchmark against other English deception datasets following similar data collection protocols. While some show statistically significant correlations, effect sizes remain low and, critically, the set of predictive cues is inconsistent across datasets. We also evaluate deception detection using feature-based models, pretrained language models, and instruction-tuned large language models. While some models perform well on established deception datasets, they consistently perform near chance on DeFaBel. Our findings challenge the assumption that deception can be reliably inferred from linguistic cues and call for rethinking how deception is studied and modeled in NLP.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice</title>
<link>https://arxiv.org/abs/2505.13156</link>
<guid>https://arxiv.org/abs/2505.13156</guid>
<content:encoded><![CDATA[
arXiv:2505.13156v1 Announce Type: new 
Abstract: Natural medicines, particularly Traditional Chinese Medicine (TCM), are gaining global recognition for their therapeutic potential in addressing human symptoms and diseases. TCM, with its systematic theories and extensive practical experience, provides abundant resources for healthcare. However, the effective application of TCM requires precise syndrome diagnosis, determination of treatment principles, and prescription formulation, which demand decades of clinical expertise. Despite advancements in TCM-based decision systems, machine learning, and deep learning research, limitations in data and single-objective constraints hinder their practical application. In recent years, large language models (LLMs) have demonstrated potential in complex tasks, but lack specialization in TCM and face significant challenges, such as too big model scale to deploy and issues with hallucination. To address these challenges, we introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and specifically designed for TCM, pre-trained and fine-tuned on diverse TCM corpora, including classical texts, expert treatises, clinical records, and knowledge graphs. Tianyi is designed to assimilate interconnected and systematic TCM knowledge through a progressive learning manner. Additionally, we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in TCM examinations, clinical tasks, domain-specific question-answering, and real-world trials. The extensive evaluations demonstrate the significant potential of Tianyi as an AI assistant in TCM clinical practice and research, bridging the gap between TCM knowledge and practical application.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Playing Evaluation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13157</link>
<guid>https://arxiv.org/abs/2505.13157</guid>
<content:encoded><![CDATA[
arXiv:2505.13157v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks</title>
<link>https://arxiv.org/abs/2505.13171</link>
<guid>https://arxiv.org/abs/2505.13171</guid>
<content:encoded><![CDATA[
arXiv:2505.13171v1 Announce Type: new 
Abstract: Large language models are known to memorize parts of their training data, posing risk of copyright violations. To systematically examine this risk, we pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing web-scale data with public domain books used to simulate copyrighted content at controlled frequencies at lengths at least ten times longer than prior work. We thereby identified the offset effect, a phenomenon characterized by two key findings: (1) verbatim memorization is most strongly triggered by short prefixes drawn from the beginning of the context window, with memorization decreasing counterintuitively as prefix length increases; and (2) a sharp decline in verbatim recall when prefix begins offset from the initial tokens of the context window. We attribute this to positional fragility: models rely disproportionately on the earliest tokens in their context window as retrieval anchors, making them sensitive to even slight shifts. We further observe that when the model fails to retrieve memorized content, it often produces degenerated text. Leveraging these findings, we show that shifting sensitive data deeper into the context window suppresses both extractable memorization and degeneration. Our results suggest that positional offset is a critical and previously overlooked axis for evaluating memorization risks, since prior work implicitly assumed uniformity by probing only from the beginning of training sequences.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs</title>
<link>https://arxiv.org/abs/2505.13173</link>
<guid>https://arxiv.org/abs/2505.13173</guid>
<content:encoded><![CDATA[
arXiv:2505.13173v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages -- Sanskrit, Ancient Greek and Latin -- to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question-answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13176</link>
<guid>https://arxiv.org/abs/2505.13176</guid>
<content:encoded><![CDATA[
arXiv:2505.13176v1 Announce Type: new 
Abstract: While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at https://github.com/Chengziha0/ToolSpectrum.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space</title>
<link>https://arxiv.org/abs/2505.13181</link>
<guid>https://arxiv.org/abs/2505.13181</guid>
<content:encoded><![CDATA[
arXiv:2505.13181v1 Announce Type: new 
Abstract: We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification</title>
<link>https://arxiv.org/abs/2505.13204</link>
<guid>https://arxiv.org/abs/2505.13204</guid>
<content:encoded><![CDATA[
arXiv:2505.13204v1 Announce Type: new 
Abstract: Recent works have revealed the great potential of speculative decoding in accelerating the autoregressive generation process of large language models. The success of these methods relies on the alignment between draft candidates and the sampled outputs of the target model. Existing methods mainly achieve draft-target alignment with training-based methods, e.g., EAGLE, Medusa, involving considerable training costs. In this paper, we present a training-free alignment-augmented speculative decoding algorithm. We propose alignment sampling, which leverages output distribution obtained in the prefilling phase to provide more aligned draft candidates. To further benefit from high-quality but non-aligned draft candidates, we also introduce a simple yet effective flexible verification strategy. Through an adaptive probability threshold, our approach can improve generation accuracy while further improving inference efficiency. Experiments on 8 datasets (including question answering, summarization and code completion tasks) show that our approach increases the average generation score by 3.3 points for the LLaMA3 model. Our method achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry</title>
<link>https://arxiv.org/abs/2505.13210</link>
<guid>https://arxiv.org/abs/2505.13210</guid>
<content:encoded><![CDATA[
arXiv:2505.13210v1 Announce Type: new 
Abstract: Classical Chinese poetry is a vital and enduring part of Chinese literature, conveying profound emotional resonance. Existing studies analyze sentiment based on textual meanings, overlooking the unique rhythmic and visual features inherent in poetry,especially since it is often recited and accompanied by Chinese paintings. In this work, we propose a dialect-enhanced multimodal framework for classical Chinese poetry sentiment analysis. We extract sentence-level audio features from the poetry and incorporate audio from multiple dialects,which may retain regional ancient Chinese phonetic features, enriching the phonetic representation. Additionally, we generate sentence-level visual features, and the multimodal features are fused with textual features enhanced by LLM translation through multimodal contrastive representation learning. Our framework outperforms state-of-the-art methods on two public datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro F1. We open-source the code to facilitate research in this area and provide insights for general multimodal Chinese representation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science</title>
<link>https://arxiv.org/abs/2505.13220</link>
<guid>https://arxiv.org/abs/2505.13220</guid>
<content:encoded><![CDATA[
arXiv:2505.13220v1 Announce Type: new 
Abstract: Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and insufficient technological support. While large language models (LLMs) have shown promise across various fields, their application in seed science remains limited due to the scarcity of digital resources, complex gene-trait relationships, and the lack of standardized benchmarks. To address this gap, we introduce SeedBench -- the first multi-task benchmark specifically designed for seed science. Developed in collaboration with domain experts, SeedBench focuses on seed breeding and simulates key aspects of modern breeding processes. We conduct a comprehensive evaluation of 26 leading LLMs, encompassing proprietary, open-source, and domain-specific fine-tuned models. Our findings not only highlight the substantial gaps between the power of LLMs and the real-world seed science problems, but also make a foundational step for research on LLMs for seed design.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models</title>
<link>https://arxiv.org/abs/2505.13244</link>
<guid>https://arxiv.org/abs/2505.13244</guid>
<content:encoded><![CDATA[
arXiv:2505.13244v1 Announce Type: new 
Abstract: With the rapid advancement of global digitalization, users from different countries increasingly rely on social media for information exchange. In this context, multilingual multi-label emotion detection has emerged as a critical research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task: (1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity. To tackle multilingual challenges, we leverage pre-trained multilingual models and focus on two architectures: (1) a fine-tuned BERT-based classification model and (2) an instruction-tuned generative LLM. Additionally, we propose two methods for handling multi-label classification: the base method, which maps an input directly to all its corresponding emotion labels, and the pairwise method, which models the relationship between the input text and each emotion category individually. Experimental results demonstrate the strong generalization ability of our approach in multilingual emotion recognition. In Track A, our method achieved Top 4 performance across 10 languages, ranking 1st in Hindi. In Track B, our approach also secured Top 5 performance in 7 languages, highlighting its simplicity and effectiveness\footnote{Our code is available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger Together: Unleashing the Social Impact of Hate Speech Research</title>
<link>https://arxiv.org/abs/2505.13251</link>
<guid>https://arxiv.org/abs/2505.13251</guid>
<content:encoded><![CDATA[
arXiv:2505.13251v1 Announce Type: new 
Abstract: The advent of the internet has been both a blessing and a curse for once marginalised communities. When used well, the internet can be used to connect and establish communities crossing different intersections; however, it can also be used as a tool to alienate people and communities as well as perpetuate hate, misinformation, and disinformation especially on social media platforms. We propose steering hate speech research and researchers away from pre-existing computational solutions and consider social methods to inform social solutions to address this social problem. In a similar way linguistics research can inform language planning policy, linguists should apply what we know about language and society to mitigate some of the emergent risks and dangers of anti-social behaviour in digital spaces. We argue linguists and NLP researchers can play a principle role in unleashing the social impact potential of linguistics research working alongside communities, advocates, activists, and policymakers to enable equitable digital inclusion and to close the digital divide.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Planning via Coding and Inference Scaling</title>
<link>https://arxiv.org/abs/2505.13252</link>
<guid>https://arxiv.org/abs/2505.13252</guid>
<content:encoded><![CDATA[
arXiv:2505.13252v1 Announce Type: new 
Abstract: Real-life textual planning tasks such as meeting scheduling have posed much challenge to LLMs especially when the complexity is high. While previous work primarily studied auto-regressive generation of plans with closed-source models, we systematically evaluate both closed- and open-source models, including those that scales output length with complexity during inference, in generating programs, which are executed to output the plan. We consider not only standard Python code, but also the code to a constraint satisfaction problem solver. Despite the algorithmic nature of the task, we show that programming often but not always outperforms planning. Our detailed error analysis also indicates a lack of robustness and efficiency in the generated code that hinders generalization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.13254</link>
<guid>https://arxiv.org/abs/2505.13254</guid>
<content:encoded><![CDATA[
arXiv:2505.13254v1 Announce Type: new 
Abstract: Autoregressive decoding, the standard approach for Large Language Model (LLM) inference, remains a significant bottleneck due to its sequential nature. While speculative decoding algorithms mitigate this inefficiency through parallel verification, they fail to exploit the inherent heterogeneity in linguistic complexity, a key factor leading to suboptimal resource allocation. We address this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding framework that dynamically optimizes computational resource allocation based on linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying predictable contexts. (2) A dynamic resource allocation strategy based on data-driven entropy partitioning, enabling adaptive speculative expansion and pruning tailored to local context difficulty. Evaluated on five public benchmarks and four models, HeteroSpec achieves an average speedup of 4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across speedup rates, average acceptance length, and verification cost. Notably, HeteroSpec requires no draft model retraining, incurs minimal overhead, and is orthogonal to other acceleration techniques. It demonstrates enhanced acceleration with stronger draft models, establishing a new paradigm for context-aware LLM inference acceleration.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?</title>
<link>https://arxiv.org/abs/2505.13257</link>
<guid>https://arxiv.org/abs/2505.13257</guid>
<content:encoded><![CDATA[
arXiv:2505.13257v1 Announce Type: new 
Abstract: Preference alignment has become a standard pipeline in finetuning models to follow \emph{generic} human preferences. Majority of work seeks to optimize model to produce responses that would be preferable \emph{on average}, simplifying the diverse and often \emph{contradicting} space of human preferences. While research has increasingly focused on personalized alignment: adapting models to individual user preferences, there is a lack of personalized preference dataset which focus on nuanced individual-level preferences. To address this, we introduce WikiPersona: the first fine-grained personalization using well-documented, famous individuals. Our dataset challenges models to align with these personas through an interpretable process: generating verifiable textual descriptions of a persona's background and preferences in addition to alignment. We systematically evaluate different personalization approaches and find that as few-shot prompting with preferences and fine-tuning fail to simultaneously ensure effectiveness and efficiency, using \textit{inferred personal preferences} as prefixes enables effective personalization, especially in topics where preferences clash while leading to more equitable generalization across unseen personas.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability</title>
<link>https://arxiv.org/abs/2505.13258</link>
<guid>https://arxiv.org/abs/2505.13258</guid>
<content:encoded><![CDATA[
arXiv:2505.13258v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generator's (LLM's) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</title>
<link>https://arxiv.org/abs/2505.13259</link>
<guid>https://arxiv.org/abs/2505.13259</guid>
<content:encoded><![CDATA[
arXiv:2505.13259v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation of perceived prosodic similarity of conversational feedback</title>
<link>https://arxiv.org/abs/2505.13268</link>
<guid>https://arxiv.org/abs/2505.13268</guid>
<content:encoded><![CDATA[
arXiv:2505.13268v1 Announce Type: new 
Abstract: Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of spoken dialogue and is crucial to ensuring common ground in conversational systems. The exact meaning of such feedback is conveyed through both lexical and prosodic form. In this work, we investigate the perceived prosodic similarity of vocal feedback with the same lexical form, and to what extent existing speech representations reflect such similarities. A triadic comparison task with recruited participants is used to measure perceived similarity of feedback responses taken from two different datasets. We find that spectral and self-supervised speech representations encode prosody better than extracted pitch features, especially in the case of feedback from the same speaker. We also find that it is possible to further condense and align the representations to human perception through contrastive learning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13271</link>
<guid>https://arxiv.org/abs/2505.13271</guid>
<content:encoded><![CDATA[
arXiv:2505.13271v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution accuracy, while the 7B model achieves 69.19%. The code will be open sourced at https://github.com/CycloneBoy/csc_sql.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2505.13282</link>
<guid>https://arxiv.org/abs/2505.13282</guid>
<content:encoded><![CDATA[
arXiv:2505.13282v1 Announce Type: new 
Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates' hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13302</link>
<guid>https://arxiv.org/abs/2505.13302</guid>
<content:encoded><![CDATA[
arXiv:2505.13302v1 Announce Type: new 
Abstract: Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its effect on vision-language models (VLMs) remains unclear. We present the first study examining how images influence VLMs' propensity to reshare news content, whether this effect varies across model families, and how persona conditioning and content attributes modulate this behavior. To support this analysis, we introduce two methodological contributions: a jailbreaking-inspired prompting strategy that elicits resharing decisions from VLMs while simulating users with antisocial traits and political alignments; and a multimodal dataset of fact-checked political news from PolitiFact, paired with corresponding images and ground-truth veracity labels. Experiments across model families reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Persona conditioning further modulates this effect: Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the tested models, only Claude-3-Haiku demonstrates robustness to visual misinformation. These findings highlight emerging risks in multimodal model behavior and motivate the development of tailored evaluation frameworks and mitigation strategies for personalized AI systems. Code and dataset are available at: https://github.com/3lis/misinfo_vlm
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.13307</link>
<guid>https://arxiv.org/abs/2505.13307</guid>
<content:encoded><![CDATA[
arXiv:2505.13307v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at https://github.com/LightChen233/reasoning-boundary.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection</title>
<link>https://arxiv.org/abs/2505.13312</link>
<guid>https://arxiv.org/abs/2505.13312</guid>
<content:encoded><![CDATA[
arXiv:2505.13312v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, making unlearning often at the expense of overall performance. To avoid the negative impact of fine-tuning, it would be better to unlearn solely at inference time by safely guarding the model against generating responses related to the forget target, without destroying the fluency of text generation. In this work, we propose Generation-time Unlearning via Adaptive Restriction and Detection (GUARD), a framework that enables dynamic unlearning during LLM generation. Specifically, we first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden token. We then dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching, effectively preventing the model from leaking the forgotten content. Experimental results on copyright content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that GUARD achieves strong forget quality across various tasks while causing almost no degradation to the LLM's general capabilities, striking an excellent trade-off between forgetting and utility.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges</title>
<link>https://arxiv.org/abs/2505.13328</link>
<guid>https://arxiv.org/abs/2505.13328</guid>
<content:encoded><![CDATA[
arXiv:2505.13328v1 Announce Type: new 
Abstract: Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool utilization}: tool awareness, tool selection, tool execution; and 3) \textit{role-consistent response}: response generation and role play. Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs\footnote{We will use tools and APIs alternatively, there are no significant differences between them in this paper.}. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation</title>
<link>https://arxiv.org/abs/2505.13338</link>
<guid>https://arxiv.org/abs/2505.13338</guid>
<content:encoded><![CDATA[
arXiv:2505.13338v1 Announce Type: new 
Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization</title>
<link>https://arxiv.org/abs/2505.13346</link>
<guid>https://arxiv.org/abs/2505.13346</guid>
<content:encoded><![CDATA[
arXiv:2505.13346v1 Announce Type: new 
Abstract: To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks</title>
<link>https://arxiv.org/abs/2505.13348</link>
<guid>https://arxiv.org/abs/2505.13348</guid>
<content:encoded><![CDATA[
arXiv:2505.13348v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</title>
<link>https://arxiv.org/abs/2505.13353</link>
<guid>https://arxiv.org/abs/2505.13353</guid>
<content:encoded><![CDATA[
arXiv:2505.13353v1 Announce Type: new 
Abstract: Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts</title>
<link>https://arxiv.org/abs/2505.13360</link>
<guid>https://arxiv.org/abs/2505.13360</guid>
<content:encoded><![CDATA[
arXiv:2505.13360v1 Announce Type: new 
Abstract: Building LLM-powered software requires developers to communicate their requirements through natural language, but developer prompts are frequently underspecified, failing to fully capture many user-important requirements. In this paper, we present an in-depth analysis of prompt underspecification, showing that while LLMs can often (41.1%) guess unspecified requirements by default, such behavior is less robust: Underspecified prompts are 2x more likely to regress over model or prompt changes, sometimes with accuracy drops by more than 20%. We then demonstrate that simply adding more requirements to a prompt does not reliably improve performance, due to LLMs' limited instruction-following capabilities and competing constraints, and standard prompt optimizers do not offer much help. To address this, we introduce novel requirements-aware prompt optimization mechanisms that can improve performance by 4.8% on average over baselines that naively specify everything in the prompt. Beyond prompt optimization, we envision that effectively managing prompt underspecification requires a broader process, including proactive requirements discovery, evaluation, and monitoring.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinkless: LLM Learns When to Think</title>
<link>https://arxiv.org/abs/2505.13379</link>
<guid>https://arxiv.org/abs/2505.13379</guid>
<content:encoded><![CDATA[
arXiv:2505.13379v1 Announce Type: new 
Abstract: Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens,  for concise responses and  for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3: Robust Rubric-Agnostic Reward Models</title>
<link>https://arxiv.org/abs/2505.13388</link>
<guid>https://arxiv.org/abs/2505.13388</guid>
<content:encoded><![CDATA[
arXiv:2505.13388v1 Announce Type: new 
Abstract: Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR. Judge: Multimodal Reasoner as a Judge</title>
<link>https://arxiv.org/abs/2505.13403</link>
<guid>https://arxiv.org/abs/2505.13403</guid>
<content:encoded><![CDATA[
arXiv:2505.13403v1 Announce Type: new 
Abstract: The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Granary: Speech Recognition and Translation Dataset in 25 European Languages</title>
<link>https://arxiv.org/abs/2505.13404</link>
<guid>https://arxiv.org/abs/2505.13404</guid>
<content:encoded><![CDATA[
arXiv:2505.13404v1 Announce Type: new 
Abstract: Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at https://hf.co/datasets/nvidia/Granary
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptThink: Reasoning Models Can Learn When to Think</title>
<link>https://arxiv.org/abs/2505.13417</link>
<guid>https://arxiv.org/abs/2505.13417</guid>
<content:encoded><![CDATA[
arXiv:2505.13417v1 Announce Type: new 
Abstract: Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness</title>
<link>https://arxiv.org/abs/2505.13418</link>
<guid>https://arxiv.org/abs/2505.13418</guid>
<content:encoded><![CDATA[
arXiv:2505.13418v1 Announce Type: new 
Abstract: Cognitive decline often surfaces in language years before diagnosis. It is frequently non-experts, such as those closest to the patient, who first sense a change and raise concern. As LLMs become integrated into daily communication and used over prolonged periods, it may even be an LLM that notices something is off. But what exactly do they notice--and should be noticing--when making that judgment? This paper investigates how dementia is perceived through language by non-experts. We presented transcribed picture descriptions to non-expert humans and LLMs, asking them to intuitively judge whether each text was produced by someone healthy or with dementia. We introduce an explainable method that uses LLMs to extract high-level, expert-guided features representing these picture descriptions, and use logistic regression to model human and LLM perceptions and compare with clinical diagnoses. Our analysis reveals that human perception of dementia is inconsistent and relies on a narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a richer, more nuanced feature set that aligns more closely with clinical patterns. Still, both groups show a tendency toward false negatives, frequently overlooking dementia cases. Through our interpretable framework and the insights it provides, we hope to help non-experts better recognize the linguistic signs that matter.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOTExT: SMOTE meets Large Language Models</title>
<link>https://arxiv.org/abs/2505.13434</link>
<guid>https://arxiv.org/abs/2505.13434</guid>
<content:encoded><![CDATA[
arXiv:2505.13434v1 Announce Type: new 
Abstract: Data scarcity and class imbalance are persistent challenges in training robust NLP models, especially in specialized domains or low-resource settings. We propose a novel technique, SMOTExT, that adapts the idea of Synthetic Minority Over-sampling (SMOTE) to textual data. Our method generates new synthetic examples by interpolating between BERT-based embeddings of two existing examples and then decoding the resulting latent point into text with xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation framework, we can effectively turn interpolated vectors into coherent text. While this is preliminary work supported by qualitative outputs only, the method shows strong potential for knowledge distillation and data augmentation in few-shot settings. Notably, our approach also shows promise for privacy-preserving machine learning: in early experiments, training models solely on generated data achieved comparable performance to models trained on the original dataset. This suggests a viable path toward safe and effective learning under data protection constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13444</link>
<guid>https://arxiv.org/abs/2505.13444</guid>
<content:encoded><![CDATA[
arXiv:2505.13444v1 Announce Type: new 
Abstract: Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIE: Controlling Language Model Text Generations Using Continuous Signals</title>
<link>https://arxiv.org/abs/2505.13448</link>
<guid>https://arxiv.org/abs/2505.13448</guid>
<content:encoded><![CDATA[
arXiv:2505.13448v1 Announce Type: new 
Abstract: Aligning language models with user intent is becoming increasingly relevant to enhance user experience. This calls for designing methods that can allow users to control the properties of the language that LMs generate. For example, controlling the length of the generation, the complexity of the language that gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate users' control by conditioning LM generations on natural language prompts or discrete control signals, which are often brittle and hard to scale. In this work, we are interested in \textit{continuous} control signals, ones that exist along a spectrum that can't easily be captured in a natural language prompt or via existing techniques in conditional generation. Through a case study in controlling the precise response-length of generations produced by LMs, we demonstrate how after fine-tuning, behaviors of language models can be controlled via continuous signals -- as vectors that are interpolated between a "low" and a "high" token embedding. Our method more reliably exerts response-length control than in-context learning methods or fine-tuning methods that represent the control signal as a discrete signal. Our full open-sourced code and datasets are available at https://github.com/vsamuel2003/CIE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TARGET: Benchmarking Table Retrieval for Generative Tasks</title>
<link>https://arxiv.org/abs/2505.11545</link>
<guid>https://arxiv.org/abs/2505.11545</guid>
<content:encoded><![CDATA[
arXiv:2505.11545v1 Announce Type: cross 
Abstract: The data landscape is rich with structured data, often of high value to organizations, driving important applications in data analysis and machine learning. Recent progress in representation learning and generative models for such data has led to the development of natural language interfaces to structured data, including those leveraging text-to-SQL. Contextualizing interactions, either through conversational interfaces or agentic components, in structured data through retrieval-augmented generation can provide substantial benefits in the form of freshness, accuracy, and comprehensiveness of answers. The key question is: how do we retrieve the right table(s) for the analytical query or task at hand? To this end, we introduce TARGET: a benchmark for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the retrieval performance of different retrievers in isolation, as well as their impact on downstream tasks. We find that dense embedding-based retrievers far outperform a BM25 baseline which is less effective than it is for retrieval over unstructured text. We also surface the sensitivity of retrievers across various metadata (e.g., missing table titles), and demonstrate a stark variation of retrieval performance across datasets and tasks. TARGET is available at https://target-benchmark.github.io.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems</title>
<link>https://arxiv.org/abs/2505.11572</link>
<guid>https://arxiv.org/abs/2505.11572</guid>
<content:encoded><![CDATA[
arXiv:2505.11572v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday applications, yet significant disparities in performance across diverse demographic groups persist. In this work, we introduce the ASR-FAIRBENCH leaderboard which is designed to assess both the accuracy and equity of ASR models in real-time. Leveraging the Meta's Fair-Speech dataset, which captures diverse demographic characteristics, we employ a mixed-effects Poisson regression model to derive an overall fairness score. This score is integrated with traditional metrics like Word Error Rate (WER) to compute the Fairness Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our approach reveals significant performance disparities in SOTA ASR models across demographic groups and offers a benchmark to drive the development of more inclusive ASR technologies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO</title>
<link>https://arxiv.org/abs/2505.11595</link>
<guid>https://arxiv.org/abs/2505.11595</guid>
<content:encoded><![CDATA[
arXiv:2505.11595v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated significant success in enhancing reasoning capabilities in large language models (LLMs). One of the most widely used RL methods is Group Relative Policy Optimization (GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls when all sampled responses in a group are incorrect -- referred to as an \emph{all-negative-sample} group -- as it fails to update the policy, hindering learning progress. The contributions of this paper are two-fold. First, we propose a simple yet effective framework that introduces response diversity within all-negative-sample groups in GRPO using AI feedback. We also provide a theoretical analysis, via a stylized model, showing how this diversification improves learning dynamics. Second, we empirically validate our approach, showing the improved performance across various model sizes (7B, 14B, 32B) in both offline and online learning settings with 10 benchmarks, including base and distilled variants. Our findings highlight that learning from all-negative-sample groups is not only feasible but beneficial, advancing recent insights from \citet{Xiong-2025-Minimalist}.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Vulnerability of Large Language Models to Polysemantic Interventions</title>
<link>https://arxiv.org/abs/2505.11611</link>
<guid>https://arxiv.org/abs/2505.11611</guid>
<content:encoded><![CDATA[
arXiv:2505.11611v1 Announce Type: cross 
Abstract: Polysemanticity -- where individual neurons encode multiple unrelated features -- is a well-known characteristic of large neural networks and remains a central challenge in the interpretability of language models. At the same time, its implications for model safety are also poorly understood. Leveraging recent advances in sparse autoencoders, we investigate the polysemantic structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their vulnerability to targeted, covert interventions at the prompt, feature, token, and neuron levels. Our analysis reveals a consistent polysemantic topology shared across both models. Strikingly, we demonstrate that this structure can be exploited to mount effective interventions on two larger, black-box instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These findings suggest not only the generalizability of the interventions but also point to a stable and transferable polysemantic structure that could potentially persist across architectures and training regimes.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions</title>
<link>https://arxiv.org/abs/2505.11614</link>
<guid>https://arxiv.org/abs/2505.11614</guid>
<content:encoded><![CDATA[
arXiv:2505.11614v1 Announce Type: cross 
Abstract: A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title>
<link>https://arxiv.org/abs/2505.11717</link>
<guid>https://arxiv.org/abs/2505.11717</guid>
<content:encoded><![CDATA[
arXiv:2505.11717v1 Announce Type: cross 
Abstract: Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. Environmental prompt injection attacks manipulate the environment to induce the web agent to perform a specific, attacker-chosen action--referred to as the target action. However, existing attacks suffer from limited effectiveness or stealthiness, or are impractical in real-world settings. In this work, we propose EnvInjection, a new attack that addresses these limitations. Our attack adds a perturbation to the raw pixel values of the rendered webpage, which can be implemented by modifying the webpage's source code. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the target action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple webpage datasets shows that EnvInjection is highly effective and significantly outperforms existing baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models</title>
<link>https://arxiv.org/abs/2505.11731</link>
<guid>https://arxiv.org/abs/2505.11731</guid>
<content:encoded><![CDATA[
arXiv:2505.11731v1 Announce Type: cross 
Abstract: Recent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation have addressed key challenges of reliability and simplicity. However, existing Bayesian methods typically require multiple sampling iterations during inference, creating significant efficiency issues that limit practical deployment. In this paper, we investigate the possibility of eliminating the need for test-time sampling for LLM uncertainty estimation. Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned confidence into a non-Bayesian student LLM by minimizing the divergence between their predictive distributions. Unlike typical calibration methods, our distillation is carried out solely on the training dataset without the need of an additional validation dataset. This simple yet effective approach achieves N-times more efficient uncertainty estimation during testing, where N is the number of samples traditionally required by Bayesian LLMs. Our extensive experiments demonstrate that uncertainty estimation capabilities on training data can successfully generalize to unseen test data through our distillation technique, consistently producing results comparable to (or even better than) state-of-the-art Bayesian LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Uncertainty Estimation for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.11737</link>
<guid>https://arxiv.org/abs/2505.11737</guid>
<content:encoded><![CDATA[
arXiv:2505.11737v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a token-level uncertainty estimation framework to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.11756</link>
<guid>https://arxiv.org/abs/2505.11756</guid>
<content:encoded><![CDATA[
arXiv:2505.11756v1 Announce Type: cross 
Abstract: It is assumed that sparse autoencoders (SAEs) decompose polysemantic activations into interpretable linear directions, as long as the activations are composed of sparse linear combinations of underlying features. However, we find that if an SAE is more narrow than the number of underlying "true features" on which it is trained, and there is correlation between features, the SAE will merge components of correlated features together, thus destroying monosemanticity. In LLM SAEs, these two conditions are almost certainly true. This phenomenon, which we call feature hedging, is caused by SAE reconstruction loss, and is more severe the narrower the SAE. In this work, we introduce the problem of feature hedging and study it both theoretically in toy models and empirically in SAEs trained on LLMs. We suspect that feature hedging may be one of the core reasons that SAEs consistently underperform supervised baselines. Finally, we use our understanding of feature hedging to propose an improved variant of matryoshka SAEs. Our work shows there remain fundamental issues with SAEs, but we are hopeful that that highlighting feature hedging will catalyze future advances that allow SAEs to achieve their full potential of interpreting LLMs at scale.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title>
<link>https://arxiv.org/abs/2505.11770</link>
<guid>https://arxiv.org/abs/2505.11770</guid>
<content:encoded><![CDATA[
arXiv:2505.11770v1 Announce Type: cross 
Abstract: Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VenusX: Unlocking Fine-Grained Functional Understanding of Proteins</title>
<link>https://arxiv.org/abs/2505.11812</link>
<guid>https://arxiv.org/abs/2505.11812</guid>
<content:encoded><![CDATA[
arXiv:2505.11812v1 Announce Type: cross 
Abstract: Deep learning models have driven significant progress in predicting protein function and interactions at the protein level. While these advancements have been invaluable for many biological applications such as enzyme engineering and function annotation, a more detailed perspective is essential for understanding protein functional mechanisms and evaluating the biological knowledge captured by models. To address this demand, we introduce VenusX, the first large-scale benchmark for fine-grained functional annotation and function-based protein pairing at the residue, fragment, and domain levels. VenusX comprises three major task categories across six types of annotations, including residue-level binary classification, fragment-level multi-class classification, and pairwise functional similarity scoring for identifying critical active sites, binding sites, conserved sites, motifs, domains, and epitopes. The benchmark features over 878,000 samples curated from major open-source databases such as InterPro, BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three sequence identity thresholds, our benchmark enables a comprehensive assessment of model performance on both in-distribution and out-of-distribution scenarios. For baseline evaluation, we assess a diverse set of popular and open-source models, including pre-trained protein language models, sequence-structure hybrids, structure-based methods, and alignment-based techniques. Their performance is reported across all benchmark datasets and evaluation settings using multiple metrics, offering a thorough comparison and a strong foundation for future research. Code and data are publicly available at https://github.com/ai4protein/VenusX.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</title>
<link>https://arxiv.org/abs/2505.11842</link>
<guid>https://arxiv.org/abs/2505.11842</guid>
<content:encoded><![CDATA[
arXiv:2505.11842v1 Announce Type: cross 
Abstract: The increasing deployment of Large Vision-Language Models (LVLMs) raises safety concerns under potential malicious inputs. However, existing multimodal safety evaluations primarily focus on model vulnerabilities exposed by static image inputs, ignoring the temporal dynamics of video that may induce distinct safety risks. To bridge this gap, we introduce Video-SafetyBench, the first comprehensive benchmark designed to evaluate the safety of LVLMs under video-text attacks. It comprises 2,264 video-text pairs spanning 48 fine-grained unsafe categories, each pairing a synthesized video with either a harmful query, which contains explicit malice, or a benign query, which appears harmless but triggers harmful behavior when interpreted alongside the video. To generate semantically accurate videos for safety evaluation, we design a controllable pipeline that decomposes video semantics into subject images (what is shown) and motion text (how it moves), which jointly guide the synthesis of query-relevant videos. To effectively evaluate uncertain or borderline harmful outputs, we propose RJScore, a novel LLM-based metric that incorporates the confidence of judge models and human-aligned decision threshold calibration. Extensive experiments show that benign-query video composition achieves average attack success rates of 67.2%, revealing consistent vulnerabilities to video-induced attacks. We believe Video-SafetyBench will catalyze future research into video-based safety evaluation and defense strategies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity</title>
<link>https://arxiv.org/abs/2505.11861</link>
<guid>https://arxiv.org/abs/2505.11861</guid>
<content:encoded><![CDATA[
arXiv:2505.11861v1 Announce Type: cross 
Abstract: Human preference plays a crucial role in the refinement of large language models (LLMs). However, collecting human preference feedback is costly and most existing datasets neglect the correlation between personalization and preferences. To address this issue, we introduce Fair-PP, a synthetic dataset of personalized preferences targeting social equity, derived from real-world social survey data, which includes 28 social groups, 98 equity topics, and 5 personal preference dimensions. Leveraging GPT-4o-mini, we engage in role-playing based on seven representative persona portrayals guided by existing social survey data, yielding a total of 238,623 preference records. Through Fair-PP, we also contribute (i) An automated framework for generating preference data, along with a more fine-grained dataset of personalized preferences; (ii) analysis of the positioning of the existing mainstream LLMs across five major global regions within the personalized preference space; and (iii) a sample reweighting method for personalized preference alignment, enabling alignment with a target persona while maximizing the divergence from other personas. Empirical experiments show our method outperforms the baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.11875</link>
<guid>https://arxiv.org/abs/2505.11875</guid>
<content:encoded><![CDATA[
arXiv:2505.11875v1 Announce Type: cross 
Abstract: The current focus of AI research is shifting from emphasizing model training towards enhancing evaluation quality, a transition that is crucial for driving further advancements in AI systems. Traditional evaluation methods typically rely on reward models assigning scalar preference scores to outputs. Although effective, such approaches lack interpretability, leaving users often uncertain about why a reward model rates a particular response as high or low. The advent of LLM-as-a-Judge provides a more scalable and interpretable method of supervision, offering insights into the decision-making process. Moreover, with the emergence of large reasoning models, which consume more tokens for deeper thinking and answer refinement, scaling test-time computation in the LLM-as-a-Judge paradigm presents an avenue for further boosting performance and providing more interpretability through reasoning traces. In this paper, we introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on reflection-enhanced datasets collected via rejection-sampling and subsequently trained using Reinforcement Learning (RL) with verifiable rewards. At inference time, we apply Simple Test-Time Scaling (STTS) strategies for additional performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$ surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally, we present three key findings: (1) Existing LLM-as-a-Judge does not inherently exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced datasets continues to demonstrate similarly weak scaling behavior. (3) Significant scaling trend emerges primarily during the RL phase, suggesting that effective STTS capability is acquired predominantly through RL training.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to Analytical Software Engineering Design Paradigm</title>
<link>https://arxiv.org/abs/2505.11979</link>
<guid>https://arxiv.org/abs/2505.11979</guid>
<content:encoded><![CDATA[
arXiv:2505.11979v1 Announce Type: cross 
Abstract: As modern software systems expand in scale and complexity, the challenges associated with their modeling and formulation grow increasingly intricate. Traditional approaches often fall short in effectively addressing these complexities, particularly in tasks such as design pattern detection for maintenance and assessment, as well as code refactoring for optimization and long-term sustainability. This growing inadequacy underscores the need for a paradigm shift in how such challenges are approached and resolved. This paper presents Analytical Software Engineering (ASE), a novel design paradigm aimed at balancing abstraction, tool accessibility, compatibility, and scalability. ASE enables effective modeling and resolution of complex software engineering problems. The paradigm is evaluated through two frameworks Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR), both developed in accordance with ASE principles. BSS offers a compact, language-agnostic representation of codebases to facilitate precise design pattern detection. ODR unifies artifact and solution representations to optimize code refactoring via heuristic algorithms while eliminating iterative computational overhead. By providing a structured approach to software design challenges, ASE lays the groundwork for future research in encoding and analyzing complex software metrics.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</title>
<link>https://arxiv.org/abs/2505.12039</link>
<guid>https://arxiv.org/abs/2505.12039</guid>
<content:encoded><![CDATA[
arXiv:2505.12039v1 Announce Type: cross 
Abstract: The Science of Science (SoS) explores the mechanisms underlying scientific discovery, and offers valuable insights for enhancing scientific efficiency and fostering innovation. Traditional approaches often rely on simplistic assumptions and basic statistical tools, such as linear regression and rule-based simulations, which struggle to capture the complexity and scale of modern research ecosystems. The advent of artificial intelligence (AI) presents a transformative opportunity for the next generation of SoS, enabling the automation of large-scale pattern discovery and uncovering insights previously unattainable. This paper offers a forward-looking perspective on the integration of Science of Science with AI for automated research pattern discovery and highlights key open challenges that could greatly benefit from AI. We outline the advantages of AI over traditional methods, discuss potential limitations, and propose pathways to overcome them. Additionally, we present a preliminary multi-agent system as an illustrative example to simulate research societies, showcasing AI's ability to replicate real-world research patterns and accelerate progress in Science of Science research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation &amp; Smoke-Tests for Continuous LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.12058</link>
<guid>https://arxiv.org/abs/2505.12058</guid>
<content:encoded><![CDATA[
arXiv:2505.12058v1 Announce Type: cross 
Abstract: Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents</title>
<link>https://arxiv.org/abs/2505.12065</link>
<guid>https://arxiv.org/abs/2505.12065</guid>
<content:encoded><![CDATA[
arXiv:2505.12065v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without compromising generation quality. SearchAgent-X is available at https://github.com/tiannuo-yang/SearchAgent-X.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.12135</link>
<guid>https://arxiv.org/abs/2505.12135</guid>
<content:encoded><![CDATA[
arXiv:2505.12135v1 Announce Type: cross 
Abstract: Assessing the capacity of Large Language Models (LLMs) to plan and reason within the constraints of interactive environments is crucial for developing capable AI agents. We introduce $\textbf{LLM-BabyBench}$, a new benchmark suite designed specifically for this purpose. Built upon a textual adaptation of the procedurally generated BabyAI grid world, this suite evaluates LLMs on three fundamental aspects of grounded intelligence: (1) predicting the consequences of actions on the environment state ($\textbf{Predict}$ task), (2) generating sequences of low-level actions to achieve specified objectives ($\textbf{Plan}$ task), and (3) decomposing high-level instructions into coherent subgoal sequences ($\textbf{Decompose}$ task). We detail the methodology for generating the three corresponding datasets ($\texttt{LLM-BabyBench-Predict}$, $\texttt{-Plan}$, $\texttt{-Decompose}$) by extracting structured information from an expert agent operating within the text-based environment. Furthermore, we provide a standardized evaluation harness and metrics, including environment interaction for validating generated plans, to facilitate reproducible assessment of diverse LLMs. Initial baseline results highlight the challenges posed by these grounded reasoning tasks. The benchmark suite, datasets, data generation code, and evaluation code are made publicly available ($\href{https://github.com/choukrani/llm-babybench}{\text{GitHub}}$, $\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\text{HuggingFace}}$).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective</title>
<link>https://arxiv.org/abs/2505.12185</link>
<guid>https://arxiv.org/abs/2505.12185</guid>
<content:encoded><![CDATA[
arXiv:2505.12185v1 Announce Type: cross 
Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering</title>
<link>https://arxiv.org/abs/2505.12189</link>
<guid>https://arxiv.org/abs/2505.12189</guid>
<content:encoded><![CDATA[
arXiv:2505.12189v1 Announce Type: cross 
Abstract: Large language models (LLMs) frequently demonstrate reasoning limitations, often conflating content plausibility (i.e., material inference) with logical validity (i.e., formal inference). This can result in biased inferences, where plausible arguments are incorrectly deemed logically valid or vice versa. Mitigating this limitation is critical, as it undermines the trustworthiness and generalizability of LLMs in applications that demand rigorous logical consistency. This paper investigates the problem of mitigating content biases on formal reasoning through activation steering. Specifically, we curate a controlled syllogistic reasoning dataset to disentangle formal validity from content plausibility. After localising the layers responsible for formal and material inference, we investigate contrastive activation steering methods for test-time interventions. An extensive empirical analysis on different LLMs reveals that contrastive steering consistently supports linear control over content biases. However, we observe that a static approach is insufficient for improving all the tested models. We then leverage the possibility to control content effects by dynamically determining the value of the steering parameters via fine-grained conditional methods. We found that conditional steering is effective on unresponsive models, achieving up to 15% absolute improvement in formal reasoning accuracy with a newly introduced kNN-based method (K-CAST). Finally, additional experiments reveal that steering for content effects is robust to prompt variations, incurs minimal side effects on language modeling capabilities, and can partially generalize to out-of-distribution reasoning tasks. Practically, this paper demonstrates that activation-level interventions can offer a scalable strategy for enhancing the robustness of LLMs, contributing towards more systematic and unbiased formal reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling</title>
<link>https://arxiv.org/abs/2505.12225</link>
<guid>https://arxiv.org/abs/2505.12225</guid>
<content:encoded><![CDATA[
arXiv:2505.12225v1 Announce Type: cross 
Abstract: High-quality reward models are crucial for unlocking the reasoning potential of large language models (LLMs), with best-of-N voting demonstrating significant performance gains. However, current reward models, which typically operate on the textual output of LLMs, are computationally expensive and parameter-heavy, limiting their real-world applications. We introduce the Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly parameter-efficient approach that leverages the rich information embedded in LLM hidden states to address these issues. ELHSR systematically outperform baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training. ELHSR also achieves orders-of-magnitude efficiency improvement with significantly less time and fewer FLOPs per sample than baseline reward models. Moreover, ELHSR exhibits robust performance even when trained only on logits, extending its applicability to some closed-source LLMs. In addition, ELHSR can also be combined with traditional reward models to achieve additional performance gains.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference</title>
<link>https://arxiv.org/abs/2505.12260</link>
<guid>https://arxiv.org/abs/2505.12260</guid>
<content:encoded><![CDATA[
arXiv:2505.12260v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode queries and documents into low-dimensional dense or high-dimensional sparse vectors. It retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval capabilities, serving deeply parameterized LLMs slows down query inference throughput and increases demands for online deployment resources. In this paper, we propose LightRetriever, a novel LLM-based hybrid retriever with extremely lightweight query encoders. Our method retains a full-sized LLM for document encoding, but reduces the workload of query encoding to no more than an embedding lookup. Compared to serving a full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for query inference with GPU acceleration, and even a 20x speedup without GPU. Experiments on large-scale retrieval benchmarks demonstrate that our method generalizes well across diverse retrieval tasks, retaining an average of 95% full-sized performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vague Knowledge: Evidence from Analyst Reports</title>
<link>https://arxiv.org/abs/2505.12269</link>
<guid>https://arxiv.org/abs/2505.12269</guid>
<content:encoded><![CDATA[
arXiv:2505.12269v1 Announce Type: cross 
Abstract: People in the real world often possess vague knowledge of future payoffs, for which quantification is not feasible or desirable. We argue that language, with differing ability to convey vague information, plays an important but less known-role in subjective expectations. Empirically, we find that in their reports, analysts include useful information in linguistic expressions but not numerical forecasts. Specifically, the textual tone of analyst reports has predictive power for forecast errors and subsequent revisions in numerical forecasts, and this relation becomes stronger when analyst's language is vaguer, when uncertainty is higher, and when analysts are busier. Overall, our theory and evidence suggest that some useful information is vaguely known and only communicated through language.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient RL Training for Reasoning Models via Length-Aware Optimization</title>
<link>https://arxiv.org/abs/2505.12284</link>
<guid>https://arxiv.org/abs/2505.12284</guid>
<content:encoded><![CDATA[
arXiv:2505.12284v1 Announce Type: cross 
Abstract: Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated remarkable performance on reasoning tasks but often incur a long reasoning path with significant memory and time costs. Existing methods primarily aim to shorten reasoning paths by introducing additional training data and stages. In this paper, we propose three critical reward designs integrated directly into the reinforcement learning process of large reasoning models, which reduce the response length without extra training stages. Experiments on four settings show that our method significantly decreases response length while maintaining or even improving performance. Specifically, in a logic reasoning setting, we achieve a 40% reduction in response length averaged by steps alongside a 14% gain in performance. For math problems, we reduce response length averaged by steps by 33% while preserving performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.12301</link>
<guid>https://arxiv.org/abs/2505.12301</guid>
<content:encoded><![CDATA[
arXiv:2505.12301v1 Announce Type: cross 
Abstract: LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with empirical human distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, considering that empirical distributions may derive from limited human annotations, we incorporate adversarial training to enhance model robustness against distribution perturbations. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with improved alignment quality, evaluation accuracy, and robustness.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?</title>
<link>https://arxiv.org/abs/2505.12307</link>
<guid>https://arxiv.org/abs/2505.12307</guid>
<content:encoded><![CDATA[
arXiv:2505.12307v1 Announce Type: cross 
Abstract: Recent advances in Large Multimodal Models (LMMs) have significantly improved their reasoning and Optical Character Recognition (OCR) capabilities. However, their performance on complex logical reasoning tasks involving text-rich images remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark comprising 1,100 multiple-choice questions designed to evaluate LMMs' logical reasoning abilities on text-rich images, while minimizing reliance on domain-specific knowledge (e.g., mathematics). We construct LogicOCR by curating a text corpus from the Chinese National Civil Servant Examination and develop a scalable, automated pipeline to convert it into multimodal samples. First, we design prompt templates to steer GPT-Image-1 to generate images with diverse backgrounds, interleaved text-illustration layouts, and varied fonts, ensuring contextual relevance and visual realism. Then, the generated images are manually verified, with low-quality examples discarded. We evaluate a range of representative open-source and proprietary LMMs under both Chain-of-Thought (CoT) and direct-answer settings. Our multi-dimensional analysis reveals key insights, such as the impact of test-time scaling, input modality differences, and sensitivity to visual-text orientation. Notably, LMMs still lag in multimodal reasoning compared to text-only inputs, indicating that they have not fully bridged visual reading with reasoning. We hope LogicOCR will serve as a valuable resource for advancing multimodal reasoning research. The dataset is available at https://github.com/MiliLab/LogicOCR.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visuospatial Cognitive Assistant</title>
<link>https://arxiv.org/abs/2505.12312</link>
<guid>https://arxiv.org/abs/2505.12312</guid>
<content:encoded><![CDATA[
arXiv:2505.12312v1 Announce Type: cross 
Abstract: Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
<link>https://arxiv.org/abs/2505.12363</link>
<guid>https://arxiv.org/abs/2505.12363</guid>
<content:encoded><![CDATA[
arXiv:2505.12363v1 Announce Type: cross 
Abstract: While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks</title>
<link>https://arxiv.org/abs/2505.12371</link>
<guid>https://arxiv.org/abs/2505.12371</guid>
<content:encoded><![CDATA[
arXiv:2505.12371v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at https://medagentboard.netlify.app/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.12442</link>
<guid>https://arxiv.org/abs/2505.12442</guid>
<content:encoded><![CDATA[
arXiv:2505.12442v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection</title>
<link>https://arxiv.org/abs/2505.12457</link>
<guid>https://arxiv.org/abs/2505.12457</guid>
<content:encoded><![CDATA[
arXiv:2505.12457v1 Announce Type: cross 
Abstract: Scaling RL for LLMs is computationally expensive, largely due to multi-sampling for policy optimization and evaluation, making efficient data selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory, we hypothesize LLMs learn best from data within their potential comprehension zone. Addressing the limitation of conventional, computationally intensive multi-sampling methods for data assessment, we introduce UFO-RL. This novel framework uses a computationally efficient single-pass uncertainty estimation to identify informative data instances, achieving up to 185x faster data evaluation. UFO-RL leverages this metric to select data within the estimated ZPD for training. Experiments show that training with just 10% of data selected by UFO-RL yields performance comparable to or surpassing full-data training, reducing overall training time by up to 16x while enhancing stability and generalization. UFO-RL offers a practical and highly efficient strategy for scaling RL fine-tuning of LLMs by focusing learning on valuable data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model</title>
<link>https://arxiv.org/abs/2505.12565</link>
<guid>https://arxiv.org/abs/2505.12565</guid>
<content:encoded><![CDATA[
arXiv:2505.12565v1 Announce Type: cross 
Abstract: Despite their ability to understand chemical knowledge and accurately generate sequential representations, large language models (LLMs) remain limited in their capacity to propose novel molecules with drug-like properties. In addition, the molecules that LLMs propose can often be challenging to make in the lab. To more effectively enable the discovery of functional small molecules, LLMs need to learn a molecular language. However, LLMs are currently limited by encoding molecules from atoms. In this paper, we argue that just like tokenizing texts into (sub-)word tokens instead of characters, molecules should be decomposed and reassembled at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis. This motivates us to propose mCLM, a modular Chemical-Language Model tokenizing molecules into building blocks and learning a bilingual language model of both natural language descriptions of functions and molecule building blocks. By reasoning on such functional building blocks, mCLM guarantees to generate efficiently synthesizable molecules thanks to recent progress in block-based chemistry, while also improving the functions of molecules in a principled manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of significantly improving 5 out of 6 chemical functions critical to determining drug potentials. More importantly, mCLM can reason on multiple functions and improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to greatly improve their shortcomings.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Latent Computation in Transformers with Latent Tokens</title>
<link>https://arxiv.org/abs/2505.12629</link>
<guid>https://arxiv.org/abs/2505.12629</guid>
<content:encoded><![CDATA[
arXiv:2505.12629v1 Announce Type: cross 
Abstract: Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents</title>
<link>https://arxiv.org/abs/2505.12632</link>
<guid>https://arxiv.org/abs/2505.12632</guid>
<content:encoded><![CDATA[
arXiv:2505.12632v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have sparked significant interest in developing GUI visual agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube), a large-scale dataset of 313K annotated frames from 20K instructional videos capturing diverse real-world mobile OS navigation across multiple platforms. Models that include MONDAY in their pre-training phases demonstrate robust cross-platform generalization capabilities, consistently outperforming models trained on existing single OS datasets while achieving an average performance gain of 18.11%p on an unseen mobile OS platform. To enable continuous dataset expansion as mobile platforms evolve, we present an automated framework that leverages publicly available video content to create comprehensive task datasets without manual annotation. Our framework comprises robust OCR-based scene detection (95.04% F1score), near-perfect UI element detection (99.87% hit ratio), and novel multi-step action identification to extract reliable action sequences across diverse interface configurations. We contribute both the MONDAY dataset and our automated collection framework to facilitate future research in mobile OS navigation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities</title>
<link>https://arxiv.org/abs/2505.12680</link>
<guid>https://arxiv.org/abs/2505.12680</guid>
<content:encoded><![CDATA[
arXiv:2505.12680v1 Announce Type: cross 
Abstract: LLM-based formal proof assistants (e.g., in Lean) hold great promise for automating mathematical discovery. But beyond syntactic correctness, do these systems truly understand mathematical structure as humans do? We investigate this question through the lens of mathematical inequalities -- a fundamental tool across many domains. While modern provers can solve basic inequalities, we probe their ability to handle human-intuitive compositionality. We introduce Ineq-Comp, a benchmark built from elementary inequalities through systematic transformations, including variable duplication, algebraic rewriting, and multi-step composition. Although these problems remain easy for humans, we find that most provers -- including Goedel, STP, and Kimina-7B -- struggle significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly because it is trained to decompose the problems into sub-problems -- but still suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor for all models even when formal proofs of the constituent parts are provided in context, revealing that the source of weakness is indeed in compositional reasoning. Our results expose a persisting gap between the generalization behavior of current AI provers and human mathematical intuition.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bullying the Machine: How Personas Increase LLM Vulnerability</title>
<link>https://arxiv.org/abs/2505.12692</link>
<guid>https://arxiv.org/abs/2505.12692</guid>
<content:encoded><![CDATA[
arXiv:2505.12692v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization</title>
<link>https://arxiv.org/abs/2505.12763</link>
<guid>https://arxiv.org/abs/2505.12763</guid>
<content:encoded><![CDATA[
arXiv:2505.12763v1 Announce Type: cross 
Abstract: Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization\textemdash a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[
arXiv:2505.12842v1 Announce Type: cross 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI Agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?</title>
<link>https://arxiv.org/abs/2505.12871</link>
<guid>https://arxiv.org/abs/2505.12871</guid>
<content:encoded><![CDATA[
arXiv:2505.12871v1 Announce Type: cross 
Abstract: Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA's training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective</title>
<link>https://arxiv.org/abs/2505.12886</link>
<guid>https://arxiv.org/abs/2505.12886</guid>
<content:encoded><![CDATA[
arXiv:2505.12886v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged--Reasoning Hallucination--where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
arXiv:2505.12891v1 Announce Type: cross 
Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME .
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models</title>
<link>https://arxiv.org/abs/2505.12900</link>
<guid>https://arxiv.org/abs/2505.12900</guid>
<content:encoded><![CDATA[
arXiv:2505.12900v1 Announce Type: cross 
Abstract: Geospatial code generation is emerging as a key direction in the integration of artificial intelligence and geoscientific analysis. However, there remains a lack of standardized tools for automatic evaluation in this domain. To address this gap, we propose AutoGEEval, the first multimodal, unit-level automated evaluation framework for geospatial code generation tasks on the Google Earth Engine (GEE) platform powered by large language models (LLMs). Built upon the GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench) comprising 1325 test cases that span 26 GEE data types. The framework integrates both question generation and answer verification components to enable an end-to-end automated evaluation pipeline-from function invocation to execution validation. AutoGEEval supports multidimensional quantitative analysis of model outputs in terms of accuracy, resource consumption, execution efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including general-purpose, reasoning-augmented, code-centric, and geoscience-specialized models-revealing their performance characteristics and potential optimization pathways in GEE code generation. This work provides a unified protocol and foundational resource for the development and assessment of geospatial code generation models, advancing the frontier of automated natural language to domain-specific code translation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM Inconsistency to Boost Pass@k Performance</title>
<link>https://arxiv.org/abs/2505.12938</link>
<guid>https://arxiv.org/abs/2505.12938</guid>
<content:encoded><![CDATA[
arXiv:2505.12938v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve impressive abilities in numerous domains, but exhibit inconsistent performance in response to minor input changes. Rather than view this as a drawback, in this paper we introduce a novel method for leveraging models' inconsistency to boost Pass@k performance. Specifically, we present a "Variator" agent that generates k variants of a given task and submits one candidate solution for each one. Our variant generation approach is applicable to a wide range of domains as it is task agnostic and compatible with free-form inputs. We demonstrate the efficacy of our agent theoretically using a probabilistic model of the inconsistency effect, and show empirically that it outperforms the baseline on the APPS dataset. Furthermore, we establish that inconsistency persists even in frontier reasoning models across coding and cybersecurity domains, suggesting our method is likely to remain relevant for future model generations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractured Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
arXiv:2505.12992v1 Announce Type: cross 
Abstract: Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.13028</link>
<guid>https://arxiv.org/abs/2505.13028</guid>
<content:encoded><![CDATA[
arXiv:2505.13028v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model owners.To evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</title>
<link>https://arxiv.org/abs/2505.13032</link>
<guid>https://arxiv.org/abs/2505.13032</guid>
<content:encoded><![CDATA[
arXiv:2505.13032v1 Announce Type: cross 
Abstract: We introduce MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. MMAR comprises 1,000 meticulously curated audio-question-answer triplets, collected from real-world internet videos and refined through iterative error corrections and quality checks to ensure high quality. Unlike existing benchmarks that are limited to specific domains of sound, music, or speech, MMAR extends them to a broad spectrum of real-world audio scenarios, including mixed-modality combinations of sound, music, and speech. Each question in MMAR is hierarchically categorized across four reasoning layers: Signal, Perception, Semantic, and Cultural, with additional sub-categories within each layer to reflect task diversity and complexity. To further foster research in this area, we annotate every question with a Chain-of-Thought (CoT) rationale to promote future advancements in audio reasoning. Each item in the benchmark demands multi-step deep reasoning beyond surface-level understanding. Moreover, a part of the questions requires graduate-level perceptual and domain-specific knowledge, elevating the benchmark's difficulty and depth. We evaluate MMAR using a broad set of models, including Large Audio-Language Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models (OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with audio caption inputs. The performance of these models on MMAR highlights the benchmark's challenging nature, and our analysis further reveals critical limitations of understanding and reasoning capabilities among current models. We hope MMAR will serve as a catalyst for future advances in this important but little-explored area.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs</title>
<link>https://arxiv.org/abs/2505.13098</link>
<guid>https://arxiv.org/abs/2505.13098</guid>
<content:encoded><![CDATA[
arXiv:2505.13098v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) can assist developing program code beside many other things, but can they support working with Knowledge Graphs (KGs) as well? Which LLM is offering the best capabilities in the field of Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to determine without checking many answers manually? The LLM-KG-Bench framework in Version 3.0 is designed to answer these questions. It consists of an extensible set of tasks for automated evaluation of LLM answers and covers different aspects of working with semantic technologies. In this paper the LLM-KG-Bench framework is presented in Version 3 along with a dataset of prompts, answers and evaluations generated with it and several state-of-the-art LLMs. Significant enhancements have been made to the framework since its initial release, including an updated task API that offers greater flexibility in handling evaluation tasks, revised tasks, and extended support for various open models through the vllm library, among other improvements. A comprehensive dataset has been generated using more than 30 contemporary open and proprietary LLMs, enabling the creation of exemplary model cards that demonstrate the models' capabilities in working with RDF and SPARQL, as well as comparing their performance on Turtle and JSON-LD RDF serialization tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2505.13109</link>
<guid>https://arxiv.org/abs/2505.13109</guid>
<content:encoded><![CDATA[
arXiv:2505.13109v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Iterative Formalization and Planning in Partially Observable Environments</title>
<link>https://arxiv.org/abs/2505.13126</link>
<guid>https://arxiv.org/abs/2505.13126</guid>
<content:encoded><![CDATA[
arXiv:2505.13126v1 Announce Type: cross 
Abstract: In planning, using LLMs not to predict plans but to formalize an environment into the Planning Domain Definition Language (PDDL) has been shown to greatly improve performance and control. While most work focused on fully observable environments, we tackle the more realistic and challenging partially observable environments where existing methods are incapacitated by the lack of complete information. We propose PDDLego+, a framework to iteratively formalize, plan, grow, and refine PDDL representations in a zero-shot manner, without needing access to any existing trajectories. On two textual simulated environments, we show that PDDLego+ not only achieves superior performance, but also shows robustness against problem complexity. We also show that the domain knowledge captured after a successful trial is interpretable and benefits future tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Generation of Parameterised Quantum Circuits from Large Texts</title>
<link>https://arxiv.org/abs/2505.13208</link>
<guid>https://arxiv.org/abs/2505.13208</guid>
<content:encoded><![CDATA[
arXiv:2505.13208v1 Announce Type: cross 
Abstract: Quantum approaches to natural language processing (NLP) are redefining how linguistic information is represented and processed. While traditional hybrid quantum-classical models rely heavily on classical neural networks, recent advancements propose a novel framework, DisCoCirc, capable of directly encoding entire documents as parameterised quantum circuits (PQCs), besides enjoying some additional interpretability and compositionality benefits. Following these ideas, this paper introduces an efficient methodology for converting large-scale texts into quantum circuits using tree-like representations of pregroup diagrams. Exploiting the compositional parallels between language and quantum mechanics, grounded in symmetric monoidal categories, our approach enables faithful and efficient encoding of syntactic and discourse relationships in long and complex texts (up to 6410 words in our experiments) to quantum circuits. The developed system is provided to the community as part of the augmented open-source quantum NLP package lambeq Gen II.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
arXiv:2505.13227v1 Announce Type: cross 
Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information</title>
<link>https://arxiv.org/abs/2505.13237</link>
<guid>https://arxiv.org/abs/2505.13237</guid>
<content:encoded><![CDATA[
arXiv:2505.13237v1 Announce Type: cross 
Abstract: Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space</title>
<link>https://arxiv.org/abs/2505.13308</link>
<guid>https://arxiv.org/abs/2505.13308</guid>
<content:encoded><![CDATA[
arXiv:2505.13308v1 Announce Type: cross 
Abstract: Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition</title>
<link>https://arxiv.org/abs/2505.13380</link>
<guid>https://arxiv.org/abs/2505.13380</guid>
<content:encoded><![CDATA[
arXiv:2505.13380v1 Announce Type: cross 
Abstract: Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar</title>
<link>https://arxiv.org/abs/2505.13393</link>
<guid>https://arxiv.org/abs/2505.13393</guid>
<content:encoded><![CDATA[
arXiv:2505.13393v1 Announce Type: cross 
Abstract: This article provides an overview of IG Parser, a software that facilitates qualitative content analysis of formal (e.g., legal) rules or informal (e.g., socio-normative) norms, and strategies (such as conventions) -- referred to as \emph{institutions} -- that govern social systems and operate configurally to describe \emph{institutional systems}. To this end, the IG Parser employs a distinctive syntax that ensures rigorous encoding of natural language, while automating the transformation into various formats that support the downstream analysis using diverse analytical techniques. The conceptual core of the IG Parser is an associated syntax, IG Script, that operationalizes the conceptual foundations of the Institutional Grammar, and more specifically Institutional Grammar 2.0, an analytical paradigm for institutional analysis. This article presents the IG Parser, including its conceptual foundations, syntactic specification of IG Script, alongside architectural principles. This introduction is augmented with selective illustrative examples that highlight the use and benefit associated with the tool.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimum Description Length Approach to Regularization in Neural Networks</title>
<link>https://arxiv.org/abs/2505.13398</link>
<guid>https://arxiv.org/abs/2505.13398</guid>
<content:encoded><![CDATA[
arXiv:2505.13398v1 Announce Type: cross 
Abstract: State-of-the-art neural networks can be trained to become remarkable solutions to many problems. But while these architectures can express symbolic, perfect solutions, trained models often arrive at approximations instead. We show that the choice of regularization method plays a crucial role: when trained on formal languages with standard regularization ($L_1$, $L_2$, or none), expressive architectures not only fail to converge to correct solutions but are actively pushed away from perfect initializations. In contrast, applying the Minimum Description Length (MDL) principle to balance model complexity with data fit provides a theoretically grounded regularization method. Using MDL, perfect solutions are selected over approximations, independently of the optimization algorithm. We propose that unlike existing regularization techniques, MDL introduces the appropriate inductive bias to effectively counteract overfitting and promote generalization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process</title>
<link>https://arxiv.org/abs/2505.13408</link>
<guid>https://arxiv.org/abs/2505.13408</guid>
<content:encoded><![CDATA[
arXiv:2505.13408v1 Announce Type: cross 
Abstract: Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Quantized Neural Networks with Zeroth-order Optimization</title>
<link>https://arxiv.org/abs/2505.13430</link>
<guid>https://arxiv.org/abs/2505.13430</guid>
<content:encoded><![CDATA[
arXiv:2505.13430v1 Announce Type: cross 
Abstract: As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
arXiv:2505.13438v1 Announce Type: cross 
Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.13445</link>
<guid>https://arxiv.org/abs/2505.13445</guid>
<content:encoded><![CDATA[
arXiv:2505.13445v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Linguistic Models: Investigating LLMs' metalinguistic abilities</title>
<link>https://arxiv.org/abs/2305.00948</link>
<guid>https://arxiv.org/abs/2305.00948</guid>
<content:encoded><![CDATA[
arXiv:2305.00948v4 Announce Type: replace 
Abstract: The performance of large language models (LLMs) has recently improved to the point where models can perform well on many language tasks. We show here that--for the first time--the models can also generate valid metalinguistic analyses of language data. We outline a research program where the behavioral interpretability of LLMs on these tasks is tested via prompting. LLMs are trained primarily on text--as such, evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. We show that OpenAI's (2024) o1 vastly outperforms other models on tasks involving drawing syntactic trees and phonological generalization. We speculate that OpenAI o1's unique advantage over other models may result from the model's chain-of-thought mechanism, which mimics the structure of human reasoning used in complex cognitive tasks, such as linguistic analysis.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics of Language Models: Part 1, Learning Hierarchical Language Structures</title>
<link>https://arxiv.org/abs/2305.13673</link>
<guid>https://arxiv.org/abs/2305.13673</guid>
<content:encoded><![CDATA[
arXiv:2305.13673v4 Announce Type: replace 
Abstract: Transformer-based language models are effective but complex, and understanding their inner workings and reasoning mechanisms is a significant challenge. Previous research has primarily explored how these models handle simple tasks like name copying or selection, and we extend this by investigating how these models perform recursive language structure reasoning defined by context-free grammars (CFGs). We introduce a family of synthetic CFGs that produce hierarchical rules, capable of generating lengthy sentences (e.g., hundreds of tokens) that are locally ambiguous and require dynamic programming to parse. Despite this complexity, we demonstrate that generative models like GPT can accurately learn and reason over CFG-defined hierarchies and generate sentences based on it. We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm.
  This paper also presents several corollaries, including showing why absolute positional embeddings is inferior to relative and rotary embeddings; uniform attention alone is surprisingly effective (motivating our follow-up work on Canon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep structure reasoning on CFGs compared to autoregressive models (e.g., GPT); and injecting structural or syntactic noise into pretraining data markedly improves robustness to corrupted language prompts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models</title>
<link>https://arxiv.org/abs/2310.10378</link>
<guid>https://arxiv.org/abs/2310.10378</guid>
<content:encoded><![CDATA[
arXiv:2310.10378v5 Announce Type: replace 
Abstract: Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically generating Riddles aiding Concept Attainment</title>
<link>https://arxiv.org/abs/2310.18290</link>
<guid>https://arxiv.org/abs/2310.18290</guid>
<content:encoded><![CDATA[
arXiv:2310.18290v2 Announce Type: replace 
Abstract: One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of riddles prove encouraging.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Sequence Transduction through Dynamic Compression</title>
<link>https://arxiv.org/abs/2402.01172</link>
<guid>https://arxiv.org/abs/2402.01172</guid>
<content:encoded><![CDATA[
arXiv:2402.01172v2 Announce Type: replace 
Abstract: We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Verify Step by Step for Incorrect Answer Detection?</title>
<link>https://arxiv.org/abs/2402.10528</link>
<guid>https://arxiv.org/abs/2402.10528</guid>
<content:encoded><![CDATA[
arXiv:2402.10528v4 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of $5.1\%$ increase in the F1 score and $2.97\%$ improvement in AUC-PR across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning</title>
<link>https://arxiv.org/abs/2402.12692</link>
<guid>https://arxiv.org/abs/2402.12692</guid>
<content:encoded><![CDATA[
arXiv:2402.12692v5 Announce Type: replace 
Abstract: The application of formulas (e.g., physics formulas) is a fundamental ability of humans when solving numerical reasoning problems. Existing numerical reasoning datasets seldom explicitly indicate the formulas employed in reasoning, as their questions rely on implicit commonsense mathematical knowledge. In contrast, in this paper, we introduce FormulaReasoning, a new dataset specifically designed for formula-based numerical reasoning. Each of the 4,751 questions in our dataset requires numerical calculation with external physics formulas, making it a more challenging benchmark for evaluating large language models (LLMs). We offer normalized fine-grained annotations for the questions, available in English and Chinese, including formula structures, parameter names, symbols, numerical values, and units, derived from extensive manual effort with LLM assistance for guaranteed quality. We also provide a consolidated formula database to serve as an external knowledge base accompanying the dataset. We employ FormulaReasoning to evaluate LLMs with 7B to over 100B parameters, and explore retrieval-augmented generation with the formula database. Our evaluation also covers supervised methods that break down the reasoning process into formula generation, parameter extraction, and numerical calculation, as well as direct preference optimization methods based on derived preference data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance</title>
<link>https://arxiv.org/abs/2402.12819</link>
<guid>https://arxiv.org/abs/2402.12819</guid>
<content:encoded><![CDATA[
arXiv:2402.12819v3 Announce Type: replace 
Abstract: When solving NLP tasks with limited labelled data, researchers typically either use a general large language model without further update, or use a small number of labelled samples to tune a specialised smaller model. In this work, we answer an important question -- how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 8 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average $100$) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with fine-tuning on binary datasets requiring significantly more samples. When performance variance is taken into consideration, the number of required labels increases on average by $100 - 200\%$. Finally, larger models do not consistently lead to better performance and lower variance, with 4-bit quantisation having negligible impact.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets</title>
<link>https://arxiv.org/abs/2404.17874</link>
<guid>https://arxiv.org/abs/2404.17874</guid>
<content:encoded><![CDATA[
arXiv:2404.17874v2 Announce Type: replace 
Abstract: Perceptions of hate can vary greatly across cultural contexts. Hate speech (HS) datasets, however, have traditionally been developed by language. This hides potential cultural biases, as one language may be spoken in different countries home to different cultures. In this work, we evaluate cultural bias in HS datasets by leveraging two interrelated cultural proxies: language and geography. We conduct a systematic survey of HS datasets in eight languages and confirm past findings on their English-language bias, but also show that this bias has been steadily decreasing in the past few years. For three geographically-widespread languages -- English, Arabic and Spanish -- we then leverage geographical metadata from tweets to approximate geo-cultural contexts by pairing language and country information. We find that HS datasets for these languages exhibit a strong geo-cultural bias, largely overrepresenting a handful of countries (e.g., US and UK for English) relative to their prominence in both the broader social media population and the general population speaking these languages. Based on these findings, we formulate recommendations for the creation of future HS datasets.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Matrix in Large Language Model Fine-tuning</title>
<link>https://arxiv.org/abs/2405.15525</link>
<guid>https://arxiv.org/abs/2405.15525</guid>
<content:encoded><![CDATA[
arXiv:2405.15525v3 Announce Type: replace 
Abstract: LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT) methods due to their ability to avoid excessive computational costs. However, an accuracy gap often exists between PEFT methods and full fine-tuning (FT), and this gap has yet to be systematically studied. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OR-Bench: An Over-Refusal Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2405.20947</link>
<guid>https://arxiv.org/abs/2405.20947</guid>
<content:encoded><![CDATA[
arXiv:2405.20947v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2406.10785</link>
<guid>https://arxiv.org/abs/2406.10785</guid>
<content:encoded><![CDATA[
arXiv:2406.10785v2 Announce Type: replace 
Abstract: In this paper, we introduce \textbf{Share}d \textbf{Lo}w \textbf{R}ank \textbf{A}daptation (ShareLoRA), a Large Language Model (LLM) fine-tuning technique that balances parameter efficiency, adaptability, and robustness without compromising performance. By strategically sharing the low-rank weight matrices across different layers, ShareLoRA achieves 44\% to 96\% reduction in trainable parameters compared to standard LoRA, alongside a substantial decrease in memory overhead. This efficiency gain scales with model size, making ShareLoRA particularly advantageous for resource-constrained environments. Importantly, ShareLoRA not only maintains model performance but also exhibits robustness in both classification and generation tasks across diverse models, including RoBERTa, GPT-2, and LLaMA series (1, 2, and 3). It consistently outperforms LoRA in zero-shot, few-shot, and continual fine-tuning scenarios, achieving up to 1.2\% average accuracy improvement, and enhanced generalization across domains. In continual learning settings, ShareLoRA achieves 1.2\% higher accuracy on GSM8K, 0.6\% on HumanEval, and 0.5\% on both MMLU and MMLU-Pro. Our results demonstrate that ShareLoRA supports high-quality fine-tuning while offering strong generalization and continual adaptation across various model scales and diverse tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging</title>
<link>https://arxiv.org/abs/2406.16330</link>
<guid>https://arxiv.org/abs/2406.16330</guid>
<content:encoded><![CDATA[
arXiv:2406.16330v2 Announce Type: replace 
Abstract: While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models</title>
<link>https://arxiv.org/abs/2406.17513</link>
<guid>https://arxiv.org/abs/2406.17513</guid>
<content:encoded><![CDATA[
arXiv:2406.17513v3 Announce Type: replace 
Abstract: Despite growing interest in Theory of Mind (ToM) tasks for evaluating language models (LMs), little is known about how LMs internally represent mental states of self and others. Understanding these internal mechanisms is critical - not only to move beyond surface-level performance, but also for model alignment and safety, where subtle misattributions of mental states may go undetected in generated outputs. In this work, we present the first systematic investigation of belief representations in LMs by probing models across different scales, training regimens, and prompts - using control tasks to rule out confounds. Our experiments provide evidence that both model size and fine-tuning substantially improve LMs' internal representations of others' beliefs, which are structured - not mere by-products of spurious correlations - yet brittle to prompt variations. Crucially, we show that these representations can be strengthened: targeted edits to model activations can correct wrong ToM inferences.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising</title>
<link>https://arxiv.org/abs/2407.00248</link>
<guid>https://arxiv.org/abs/2407.00248</guid>
<content:encoded><![CDATA[
arXiv:2407.00248v2 Announce Type: replace 
Abstract: Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to systems built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. The diffusion layer is trained on top of the existing classifier, ensuring seamless integration with any model in a plug-and-play manner. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over existing adversarial defense methods and achieves state-of-the-art performance against common black-box and white-box adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding</title>
<link>https://arxiv.org/abs/2407.01976</link>
<guid>https://arxiv.org/abs/2407.01976</guid>
<content:encoded><![CDATA[
arXiv:2407.01976v3 Announce Type: replace 
Abstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in KIE and VQA. Comprehensive benchmark evaluations reveal significant improvements of LayTextLLM, with a 15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA OCR-based LLMs. All resources are available at https://github.com/LayTextLLM/LayTextLLM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks</title>
<link>https://arxiv.org/abs/2407.18525</link>
<guid>https://arxiv.org/abs/2407.18525</guid>
<content:encoded><![CDATA[
arXiv:2407.18525v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in medicine. However, their utility in non-generative clinical prediction, often presumed inferior to specialized models, remains under-evaluated, leading to ongoing debate within the field and potential for misuse, misunderstanding, or over-reliance due to a lack of systematic benchmarking. Our ClinicRealm study addresses this by benchmarking 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods on unstructured clinical notes and structured Electronic Health Records (EHR). Key findings reveal a significant shift: for clinical note predictions, leading LLMs (e.g., DeepSeek R1/V3, GPT o3-mini-high) in zero-shot settings now decisively outperform finetuned BERT models. On structured EHRs, while specialized models excel with ample data, advanced LLMs (e.g., GPT-4o, DeepSeek R1/V3) show potent zero-shot capabilities, often surpassing conventional models in data-scarce settings. Notably, leading open-source LLMs can match or exceed proprietary counterparts. These results establish modern LLMs as powerful non-generative clinical prediction tools, particularly with unstructured text and offering data-efficient structured data options, thus necessitating a re-evaluation of model selection strategies. This research should serve as an important insight for medical informaticists, AI developers, and clinical researchers, potentially prompting a reassessment of current assumptions and inspiring new approaches to LLM application in predictive healthcare.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning</title>
<link>https://arxiv.org/abs/2408.05517</link>
<guid>https://arxiv.org/abs/2408.05517</guid>
<content:encoded><![CDATA[
arXiv:2408.05517v4 Announce Type: replace 
Abstract: Recent development in Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs) have leverage Attention-based Transformer architectures and achieved superior performance and generalization capabilities. They have since covered extensive areas of traditional learning tasks. For instance, text-based tasks such as text-classification and sequence-labeling, as well as multi-modal tasks like Visual Question Answering (VQA) and Optical Character Recognition (OCR), which were previously addressed using different models, can now be tackled based on one foundation model. Consequently, the training and lightweight fine-tuning of LLMs and MLLMs, especially those based on Transformer architecture, has become particularly important. In recognition of these overwhelming needs, we develop SWIFT, a customizable one-stop infrastructure for large models. With support of over $300+$ LLMs and $50+$ MLLMs, SWIFT stands as the open-source framework that provide the most comprehensive support for fine-tuning large models. In particular, it is the first training framework that provides systematic support for MLLMs. In addition to the core functionalities of fine-tuning, SWIFT also integrates post-training processes such as inference, evaluation, and model quantization, to facilitate fast adoptions of large models in various application scenarios. With a systematic integration of various training techniques, SWIFT offers helpful utilities such as benchmark comparisons among different training techniques for large models. For fine-tuning models specialized in agent framework, we show that notable improvements on the ToolBench leader-board can be achieved by training with customized dataset on SWIFT, with an increase of 5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in hallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling</title>
<link>https://arxiv.org/abs/2408.08696</link>
<guid>https://arxiv.org/abs/2408.08696</guid>
<content:encoded><![CDATA[
arXiv:2408.08696v2 Announce Type: replace 
Abstract: Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a widely recognized training method by 25\%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions</title>
<link>https://arxiv.org/abs/2408.08780</link>
<guid>https://arxiv.org/abs/2408.08780</guid>
<content:encoded><![CDATA[
arXiv:2408.08780v5 Announce Type: replace 
Abstract: With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks. However, the function of descriptive instructions during ICL remains under-explored. In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL performance. But to our surprise, LLMs might not care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since it could lead to improvement even with random descriptive nouns. We further apply this new ensemble framework on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions. Our code will be publicly available once this paper is published.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction</title>
<link>https://arxiv.org/abs/2408.12249</link>
<guid>https://arxiv.org/abs/2408.12249</guid>
<content:encoded><![CDATA[
arXiv:2408.12249v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly adopted for applications in healthcare, reaching the performance of domain experts on tasks such as question answering and document summarisation. Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extraction. To bridge this gap, in this paper, we systematically benchmark LLM performance in Medical Classification and Named Entity Recognition (NER) tasks. We aim to disentangle the contribution of different factors to the performance, particularly the impact of LLMs' task knowledge and reasoning capabilities, their (parametric) domain knowledge, and addition of external knowledge. To this end, we evaluate various open LLMs - including BioMistral and Llama-2 models - on a diverse set of biomedical datasets, using standard prompting, Chain of-Thought (CoT) and Self Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks, laying bare the limitations in the current application of CoT, self-consistency and RAG in the biomedical domain. Our findings suggest that advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required. This highlights the need for more effective integration of external knowledge and reasoning mechanisms in LLMs to enhance their performance in real-world biomedical applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices</title>
<link>https://arxiv.org/abs/2409.01893</link>
<guid>https://arxiv.org/abs/2409.01893</guid>
<content:encoded><![CDATA[
arXiv:2409.01893v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) with extended context windows have significantly improved tasks such as information extraction, question answering, and complex planning scenarios. In order to achieve success in long context tasks, a large amount of work has been done to enhance the long context capabilities of the model through synthetic data. Existing methods typically utilize the Self-Instruct framework to generate instruction tuning data for better long context capability improvement. However, our preliminary experiments indicate that less than 35% of generated samples are multi-hop, and more than 40% exhibit poor quality, limiting comprehensive understanding and further research. To improve the quality of synthetic data, we propose the Multi-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a Quality Verification Agent, a Single-hop Question Generation Agent, a Multiple Question Sampling Strategy, and a Multi-hop Question Merger Agent. This framework improves the data quality, with the proportion of high-quality, multi-hop, and diverse data exceeding 85%. Furthermore, we systematically investigate strategies for document selection, question merging, and validation techniques through extensive experiments across various models. Our findings show that our synthetic high-quality long-context instruction data significantly enhances model performance, even surpassing models trained on larger amounts of human-annotated data. Our code is available at: https://github.com/WowCZ/LongMIT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Efficient Recursive Numeral Systems via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.07170</link>
<guid>https://arxiv.org/abs/2409.07170</guid>
<content:encoded><![CDATA[
arXiv:2409.07170v4 Announce Type: replace 
Abstract: It has previously been shown that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems that are similar to human ones (Carlsson, 2021). However, it is a major challenge to show how more complex recursive numeral systems, similar to for example English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of efficient recursive number systems. We consider pairs of agents learning how to communicate about numerical quantities through a meta-grammar that can be gradually modified throughout the interactions. Utilising a slightly modified version of the meta-grammar of Hurford (1975), we demonstrate that our RL agents, shaped by the pressures for efficient communication, can effectively modify their lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems in terms of their efficiency.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PACE: Abstractions for Communicating Efficiently</title>
<link>https://arxiv.org/abs/2409.20120</link>
<guid>https://arxiv.org/abs/2409.20120</guid>
<content:encoded><![CDATA[
arXiv:2409.20120v3 Announce Type: replace 
Abstract: A central but unresolved aspect of problem-solving in AI is the capability to introduce and use abstractions, something humans excel at. Work in cognitive science has demonstrated that humans tend towards higher levels of abstraction when engaged in collaborative task-oriented communication, enabling gradually shorter and more information-efficient utterances. Several computational methods have attempted to replicate this phenomenon, but all make unrealistic simplifying assumptions about how abstractions are introduced and learned. Our method, Procedural Abstractions for Communicating Efficiently (PACE), overcomes these limitations through a neuro-symbolic approach. On the symbolic side, we draw on work from library learning for proposing abstractions. We combine this with neural methods for communication and reinforcement learning, via a novel use of bandit algorithms for controlling the exploration and exploitation trade-off in introducing new abstractions. PACE exhibits similar tendencies to humans on a collaborative construction task from the cognitive science literature, where one agent (the architect) instructs the other (the builder) to reconstruct a scene of block-buildings. PACE results in the emergence of an efficient language as a by-product of collaborative communication. Beyond providing mechanistic insights into human communication, our work serves as a first step to providing conversational agents with the ability for human-like communicative abstractions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR: Alignment-Aware Modality Connector for Speech Language Models</title>
<link>https://arxiv.org/abs/2410.00168</link>
<guid>https://arxiv.org/abs/2410.00168</guid>
<content:encoded><![CDATA[
arXiv:2410.00168v2 Announce Type: replace 
Abstract: Fusing speech into pre-trained language model (SpeechLM) usually suffers from inefficient encoding of long-form speech and catastrophic forgetting of pre-trained text modality. We propose SSR-Connector (Segmented Speech Representation Connector) for better modality fusion. Leveraging speech-text alignments, our approach segments and compresses speech features to match the granularity of text embeddings. Additionally, we introduce a two-stage training pipeline that includes the distillation and fine-tuning phases to mitigate catastrophic forgetting. SSR-Connector outperforms existing mechanism for speech-text modality fusion, consistently achieving better speech understanding (e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving pre-trained text ability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</title>
<link>https://arxiv.org/abs/2410.02707</link>
<guid>https://arxiv.org/abs/2410.02707</guid>
<content:encoded><![CDATA[
arXiv:2410.02707v4 Announce Type: replace 
Abstract: Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions</title>
<link>https://arxiv.org/abs/2410.06577</link>
<guid>https://arxiv.org/abs/2410.06577</guid>
<content:encoded><![CDATA[
arXiv:2410.06577v2 Announce Type: replace 
Abstract: Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are open-sourced at https://github.com/codefuse-ai/rodimus.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses</title>
<link>https://arxiv.org/abs/2410.07076</link>
<guid>https://arxiv.org/abs/2410.07076</guid>
<content:encoded><![CDATA[
arXiv:2410.07076v5 Announce Type: replace 
Abstract: Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Evaluations: The Garbling Trick</title>
<link>https://arxiv.org/abs/2411.01533</link>
<guid>https://arxiv.org/abs/2411.01533</guid>
<content:encoded><![CDATA[
arXiv:2411.01533v3 Announce Type: replace 
Abstract: As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments.
  To demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative abilities of these models, particularly highlighting the differences between base LLMs and more recent "reasoning" models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs</title>
<link>https://arxiv.org/abs/2411.11266</link>
<guid>https://arxiv.org/abs/2411.11266</guid>
<content:encoded><![CDATA[
arXiv:2411.11266v5 Announce Type: replace 
Abstract: As demonstrated by the proprietary Large Language Models (LLMs) such as GPT and Claude series, LLMs have the potential to achieve remarkable proficiency across a wide range of domains, including law, medicine, finance, science, code, etc., all within a single model. These capabilities are further augmented during the Supervised Fine-Tuning (SFT) phase. Despite their potential, existing work mainly focuses on domain-specific enhancements during fine-tuning, the challenge of which lies in catastrophic forgetting of knowledge across other domains. In this study, we introduce **VersaTune**, a novel data composition framework designed for enhancing LLMs' overall multi-domain capabilities during training. We begin with detecting the distribution of domain-specific knowledge within the base model, followed by the training data composition that aligns with the model's existing knowledge distribution. During the subsequent training process, domain weights are dynamically adjusted based on their learnable potential and forgetting degree. Experimental results indicate that VersaTune is effective in multi-domain fostering, with an improvement of 35.21\% in the overall multi-ability performances compared to uniform domain weights. Furthermore, we find that Qwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o, Claude3.5-Sonnet and DeepSeek-V3 by 0.86\%, 4.76\% and 4.60\%. Additionally, in scenarios where flexible expansion of a specific domain is required, VersaTune reduces the performance degradation in other domains by 38.77\%, while preserving the training efficacy of the target domain.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework</title>
<link>https://arxiv.org/abs/2411.16707</link>
<guid>https://arxiv.org/abs/2411.16707</guid>
<content:encoded><![CDATA[
arXiv:2411.16707v3 Announce Type: replace 
Abstract: The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths</title>
<link>https://arxiv.org/abs/2412.02466</link>
<guid>https://arxiv.org/abs/2412.02466</guid>
<content:encoded><![CDATA[
arXiv:2412.02466v3 Announce Type: replace 
Abstract: This study sets out to answer one major question: Can ChatGPT capture swearing nuances? It presents an empirical study on the ability of ChatGPT to translate Arabic oath expressions into English. 30 Arabic oath expressions were collected from the literature. These 30 oaths were first translated via ChatGPT and then analyzed and compared to the human translation in terms of types of gaps left unfulfilled by ChatGPT. Specifically, the gaps involved are: religious gap, cultural gap, both religious and cultural gaps, no gap, using non-oath particles, redundancy and noncapturing of Arabic script diacritics. It concludes that ChatGPT translation of oaths is still much unsatisfactory, unveiling the need of further developments of ChatGPT, and the inclusion of Arabic data on which ChatGPT should be trained including oath expressions, oath nuances, rituals, and practices.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intention Knowledge Graph Construction for User Intention Relation Modeling</title>
<link>https://arxiv.org/abs/2412.11500</link>
<guid>https://arxiv.org/abs/2412.11500</guid>
<content:encoded><![CDATA[
arXiv:2412.11500v2 Announce Type: replace 
Abstract: Understanding user intentions is challenging for online platforms. Recent work on intention knowledge graphs addresses this but often lacks focus on connecting intentions, which is crucial for modeling user behavior and predicting future actions. This paper introduces a framework to automatically generate an intention knowledge graph, capturing connections between user intentions. Using the Amazon m2 dataset, we construct an intention graph with 351 million edges, demonstrating high plausibility and acceptance. Our model effectively predicts new session intentions and enhances product recommendations, outperforming previous state-of-the-art methods and showcasing the approach's practical utility.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DateLogicQA: Benchmarking Temporal Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2412.13377</link>
<guid>https://arxiv.org/abs/2412.13377</guid>
<content:encoded><![CDATA[
arXiv:2412.13377v2 Announce Type: replace 
Abstract: This paper introduces DateLogicQA, a benchmark with 190 questions covering diverse date formats, temporal contexts, and reasoning types. We propose the Semantic Integrity Metric to assess tokenization quality and analyse two biases: Representation-Level Bias, affecting embeddings, and Logical-Level Bias, influencing reasoning outputs. Our findings provide a comprehensive evaluation of LLMs' capabilities and limitations in temporal reasoning, highlighting key challenges in handling temporal data accurately.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set</title>
<link>https://arxiv.org/abs/2412.14872</link>
<guid>https://arxiv.org/abs/2412.14872</guid>
<content:encoded><![CDATA[
arXiv:2412.14872v3 Announce Type: replace 
Abstract: Auto-regressive language models (LMs) have been widely used to generate data in data-scarce domains to train new LMs, compensating for the scarcity of real-world data. Previous work experimentally found that LMs collapse when trained on recursively generated data. This paper presents a theoretical proof: once a corpus (such as a subset of the World Wide Web) begins to incorporate generated data and no new real-world data is added to the corpus, then no matter how small the amount of data each LM generates and contributes to the corpus, LM collapse is inevitable after sufficient time. This finding suggests that attempts to mitigate collapse by limiting the quantity of synthetic data in the corpus are fundamentally insufficient. Instead, avoiding collapse hinges on ensuring the quality of synthetic data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration</title>
<link>https://arxiv.org/abs/2412.17061</link>
<guid>https://arxiv.org/abs/2412.17061</guid>
<content:encoded><![CDATA[
arXiv:2412.17061v2 Announce Type: replace 
Abstract: Scaling laws for inference compute in multi-agent systems remain under-explored compared to single-agent scenarios. This work aims to bridge this gap by investigating the problem of data synthesis through multi-agent sampling, where synthetic responses are generated by sampling from multiple distinct language models. Effective model coordination is crucial for successful multi-agent collaboration. Unlike previous approaches that rely on fixed workflows, we treat model coordination as a multi-step decision-making process, optimizing generation structures dynamically for each input question. We introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow evolves iteratively during the sequential sampling process. To achieve this, we leverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide real-time feedback and accelerate exploration. Our experiments on alignment, machine translation, and mathematical reasoning demonstrate that multi-agent sampling significantly outperforms single-agent sampling as inference compute scales. TOA is the most compute-efficient approach, achieving SOTA performance on WMT and a 72.2\% LC win rate on AlpacaEval. Moreover, fine-tuning with our synthesized alignment data surpasses strong preference learning methods on challenging benchmarks such as Arena-Hard and AlpacaEval.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use</title>
<link>https://arxiv.org/abs/2501.02506</link>
<guid>https://arxiv.org/abs/2501.02506</guid>
<content:encoded><![CDATA[
arXiv:2501.02506v3 Announce Type: replace 
Abstract: Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios</title>
<link>https://arxiv.org/abs/2501.11269</link>
<guid>https://arxiv.org/abs/2501.11269</guid>
<content:encoded><![CDATA[
arXiv:2501.11269v2 Announce Type: replace 
Abstract: Current multilingual large language models(MLLMs) still focus on simple question-answering formats, often overlooking more complex dialogue scenarios. In other words, their capabilities of multilingual large models have yet to be validated in dialogue tasks with intricate structures. We therefore ask, Q1: How well do LLMs generalize to more complex dialog scenarios? Q2: Can supervised fine-tuning on a high-quality parallel benchmark restore this ability? Q3: Does the "multilingual complementarity" effect survive in the setting? To answer these questions, we introduce XMP, a high-quality parallel Multilingual dataset sourced from Multi-party Podcast dialogues, which is the first parallel dataset focusing on multi-party dialogue scenarios. Most samples in the dataset feature three or more participants, discussing a wide range of topics. Through extensive experiments, we find that, R1: MLLMs fail to generalize to multi-party setting, R2 Fine-tuning on XMP improves only marginally, with the 70B model achieving at most a 1% absolute gain over its 8B counterpart; R3: Mixing languages during SFT is usually detrimental, with any benefits being marginal and limited to isolated cases in the 70B model.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning</title>
<link>https://arxiv.org/abs/2501.11292</link>
<guid>https://arxiv.org/abs/2501.11292</guid>
<content:encoded><![CDATA[
arXiv:2501.11292v2 Announce Type: replace 
Abstract: Multi-party dialogues, common in collaborative scenarios like brainstorming sessions and negotiations, pose significant challenges due to their complexity and diverse speaker roles. Current methods often use graph neural networks to model dialogue context, capturing structural dynamics but heavily relying on annotated graph structures and overlooking individual speaking styles. To address these challenges, we propose CMR, a Contrastive learning-based Multi-party dialogue Response generation framework. CMR employs a two-stage self-supervised contrastive learning framework. First, it captures global differences in speaking styles across individuals. Then, it focuses on intra-conversation comparisons to identify thematic transitions and contextually relevant facts. To the best of our knowledge, this is the first approach that applies contrastive learning in multi-party dialogue generation. Experimental results demonstrate that CMR not only significantly outperforms state-of-the-art models, but also generalizes well to large pre-trained language models, effectively enhancing their capability in handling multi-party conversations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2501.11496</link>
<guid>https://arxiv.org/abs/2501.11496</guid>
<content:encoded><![CDATA[
arXiv:2501.11496v2 Announce Type: replace 
Abstract: The global crisis of language endangerment meets a technological turning point as Generative AI (GenAI) and Large Language Models (LLMs) unlock new frontiers in automating corpus creation, transcription, translation, and tutoring. However, this promise is imperiled by fragmented practices and the critical lack of a methodology to navigate the fraught balance between LLM capabilities and the profound risks of data scarcity, cultural misappropriation, and ethical missteps. This paper introduces a novel analytical framework that systematically evaluates GenAI applications against language-specific needs, embedding community governance and ethical safeguards as foundational pillars. We demonstrate its efficacy through the Te Reo M\=aori revitalization, where it illuminates successes, such as community-led Automatic Speech Recognition achieving 92% accuracy, while critically surfacing persistent challenges in data sovereignty and model bias for digital archives and educational tools. Our findings underscore that GenAI can indeed revolutionize language preservation, but only when interventions are rigorously anchored in community-centric data stewardship, continuous evaluation, and transparent risk management. Ultimately, this framework provides an indispensable toolkit for researchers, language communities, and policymakers, aiming to catalyze the ethical and high-impact deployment of LLMs to safeguard the world's linguistic heritage.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding</title>
<link>https://arxiv.org/abs/2501.12162</link>
<guid>https://arxiv.org/abs/2501.12162</guid>
<content:encoded><![CDATA[
arXiv:2501.12162v2 Announce Type: replace 
Abstract: Modern large language model (LLM) applications exhibit diverse service-level objectives (SLOs), from low-latency requirements in interactive coding assistants to more relaxed constraints in data wrangling tasks. Existing LLM serving systems, which rely on uniform batching and scheduling strategies, often fail to meet these heterogeneous SLOs concurrently. We present AdaServe, the first LLM serving system designed to support efficient multi-SLO serving through SLO-customized speculative decoding. AdaServe formulates multi-SLO serving as a constrained optimization problem and introduces a hardware-aware algorithm that constructs a speculation tree tailored to each request's latency target. It features a speculate-select-verify pipeline that enables fine-grained control over decoding speed while maximizing system throughput. AdaServe further adapts to workload variation by dynamically adjusting speculation parameters. Evaluations across diverse workloads show that AdaServe reduces SLO violations by up to 4.3$\times$ and improves goodput by up to 1.9$\times$ compared to the best performing baselines, highlighting its effectiveness in multi-SLO serving.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Option-ID Based Elimination For Multiple Choice Questions</title>
<link>https://arxiv.org/abs/2501.15175</link>
<guid>https://arxiv.org/abs/2501.15175</guid>
<content:encoded><![CDATA[
arXiv:2501.15175v3 Announce Type: replace 
Abstract: Multiple choice questions (MCQs) are a popular and important task for evaluating large language models (LLMs). Based on common strategies people use when answering MCQs, the process of elimination (PoE) has been proposed as an effective problem-solving method. Existing PoE methods typically either have LLMs directly identify incorrect options or score options and replace lower-scoring ones with [MASK]. However, both methods suffer from inapplicability or suboptimal performance. To address these issues, this paper proposes a novel option-ID based PoE ($\text{PoE}_{\text{ID}}$). $\text{PoE}_{\text{ID}}$ critically incorporates a debiasing technique to counteract LLMs token bias, enhancing robustness over naive ID-based elimination. It features two strategies: $\text{PoE}_{\text{ID}}^{\text{log}}$, which eliminates options whose IDs have log probabilities below the average threshold, and $\text{PoE}_{\text{ID}}^{\text{seq}}$, which iteratively removes the option with the lowest ID probability. We conduct extensive experiments with 6 different LLMs on 4 diverse datasets. The results demonstrate that $\text{PoE}_{\text{ID}}$, especially $\text{PoE}_{\text{ID}}^{\text{log}}$, significantly improves zero-shot and few-shot MCQs performance, particularly in datasets with more options. Our analyses demonstrate that $\text{PoE}_{\text{ID}}^{\text{log}}$ enhances the LLMs' confidence in selecting the correct option, and the option elimination strategy outperforms methods relying on [MASK] replacement. We further investigate the limitations of LLMs in directly identifying incorrect options, which stem from their inherent deficiencies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Linguistics Learned to Stop Worrying and Love the Language Models</title>
<link>https://arxiv.org/abs/2501.17047</link>
<guid>https://arxiv.org/abs/2501.17047</guid>
<content:encoded><![CDATA[
arXiv:2501.17047v2 Announce Type: replace 
Abstract: Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure. We argue that both extremes are wrong. LMs can contribute to fundamental questions about linguistic structure, language processing, and learning. They force us to rethink arguments and ways of thinking that have been foundational in linguistics. While they do not replace linguistic structure and theory, they serve as model systems and working proofs of concept for gradient, usage-based approaches to language. We offer an optimistic take on the relationship between language models and linguistics.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
<link>https://arxiv.org/abs/2501.18280</link>
<guid>https://arxiv.org/abs/2501.18280</guid>
<content:encoded><![CDATA[
arXiv:2501.18280v3 Announce Type: replace 
Abstract: The security issue of large language models (LLMs) has gained wide attention recently, with various defense mechanisms developed to prevent harmful output, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the output distribution of text embedding models is severely biased with a large mean. Inspired by this observation, we propose novel, efficient methods to search for **universal magic words** that attack text embedding models. Universal magic words as suffixes can shift the embedding of any text towards the bias direction, thus manipulating the similarity of any text pair and misleading safeguards. Attackers can jailbreak the safeguards by appending magic words to user prompts and requiring LLMs to end answers with magic words. Experiments show that magic word attacks significantly degrade safeguard performance on JailbreakBench, cause real-world chatbots to produce harmful outputs in full-pipeline attacks, and generalize across input/output texts, models, and languages. To eradicate this security risk, we also propose defense methods against such attacks, which can correct the bias of text embeddings and improve downstream performance in a train-free manner.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-centric Token Compression in Large Language Model</title>
<link>https://arxiv.org/abs/2502.00791</link>
<guid>https://arxiv.org/abs/2502.00791</guid>
<content:encoded><![CDATA[
arXiv:2502.00791v3 Announce Type: replace 
Abstract: Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The source code will be released.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Localization and Activation Editing for Low-Resource Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.01179</link>
<guid>https://arxiv.org/abs/2502.01179</guid>
<content:encoded><![CDATA[
arXiv:2502.01179v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. These methods, due to their extremely small parameter counts, show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. The code for the method is released at https://github.com/wenlai-lavine/jola.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Embedding Layers in Language Models</title>
<link>https://arxiv.org/abs/2502.01637</link>
<guid>https://arxiv.org/abs/2502.01637</guid>
<content:encoded><![CDATA[
arXiv:2502.01637v2 Announce Type: replace 
Abstract: We propose SCONE ($S$calable, $C$ontextualized, $O$ffloaded, $N$-gram $E$mbedding), a new method for extending input embedding layers to enhance language model performance. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. After training, embeddings are precomputed and stored in off-accelerator memory; during inference, querying them has minimal impact on latency due to the low complexity of embedding lookups. SCONE enables two new scaling strategies: increasing the number of $n$-gram embeddings and scaling the model used to learn them, both while maintaining fixed accelerator usage during inference (in terms of FLOPS and memory). We show that scaling both aspects enables a model with 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline across diverse corpora, while using only about half the FLOPS and accelerator memory during inference.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models</title>
<link>https://arxiv.org/abs/2502.02444</link>
<guid>https://arxiv.org/abs/2502.02444</guid>
<content:encoded><![CDATA[
arXiv:2502.02444v4 Announce Type: replace 
Abstract: Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reformulation for Pretraining Data Augmentation</title>
<link>https://arxiv.org/abs/2502.04235</link>
<guid>https://arxiv.org/abs/2502.04235</guid>
<content:encoded><![CDATA[
arXiv:2502.04235v2 Announce Type: replace 
Abstract: Despite the impressive capabilities of large language models across various tasks, their continued scaling is severely hampered not only by data scarcity but also by the performance degradation associated with excessive data repetition during training. To overcome this critical bottleneck, we propose the Massive Genre-Audience(MGA) reformulation method, a lightweight and scalable data augmentation technique inspired by synthetic data methodologies. MGA systematically reformulates existing corpora into diverse, contextually-rich variations to mitigate the negative effects of repetition, and we introduce this approach along with the resulting 770 billion token MGACorpus in this work. We experimentally validate its core benefit by demonstrating superior performance against data repetition and upsampling in scaling scenarios (up to 13B parameters). Furthermore, comprehensive analysis investigates the role of prompt engineering in generation quality and reveals nuances in evaluating model capabilities using standard loss metrics. Our work shows that MGA provides a reliable pathway to substantially augment training datasets, effectively alleviating repetition bottlenecks and enabling more efficient scaling of large language models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data</title>
<link>https://arxiv.org/abs/2502.05567</link>
<guid>https://arxiv.org/abs/2502.05567</guid>
<content:encoded><![CDATA[
arXiv:2502.05567v2 Announce Type: replace 
Abstract: Autoformalization, the automatic translation of mathematical content from natural language into machine-verifiable formal languages, has seen significant progress driven by advances in large language models (LLMs). Nonetheless, a primary barrier to further improvements is the limited availability of parallel corpora that map informal mathematical text to its formal counterpart. To address this limitation, we propose ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), a novel data generation framework designed to produce large-scale, high-quality parallel corpora of theorem statements. Distinct from prior approaches, ATLAS begins with a concept repository, accelerates the improvement of student model through expert iteration combined with knowledge distillation, and introduces two novel augmentation strategies that exploit the structural characteristics of formal languages. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 117k theorem statements and develop ATLAS Translator, which demonstrates statistically significant improvements over both the HERALD Translator and the Kimina-Autoformalizer across all benchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The datasets, model, and code will be released to the public soon.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization</title>
<link>https://arxiv.org/abs/2502.05605</link>
<guid>https://arxiv.org/abs/2502.05605</guid>
<content:encoded><![CDATA[
arXiv:2502.05605v3 Announce Type: replace 
Abstract: While large language models (LLMs) have demonstrated remarkable general performance, enabling smaller models to achieve capabilities comparable to their larger counterparts remains a critical challenge. For humans, iterative refinement of problem analysis and responses is a common strategy to enhance answer quality. However, we observe that existing LLMs exhibit limited ability to refine their outputs for quality improvement. In this paper, we first investigate mechanisms to unlock and progressively enhance self-refinement ability in smaller models within an iterative preference optimization framework, aiming to bridge the performance gap with larger models. To this end, we propose EVOLVE, a novel post-training and inference framework that iteratively integrates preference training with self-refinement-driven data collection. During training, EVOLVE strengthens the model's direct question-answering ability while simultaneously unlocking its self-refinement potential. At inference, the framework leverages this capability to generate progressively refined responses, which are filtered to construct datasets for subsequent rounds of preference training. Experiments demonstrate EVOLVE's exceptional performance: when applied to Llama-3.1-8B base model and under the self-refinement setting, it surpasses state-of-the-art models including Llama-3.1-405B-Instruct and GPT-4o, achieving a 62.3% length-controlled win rate and 63.3% raw win rate on AlpacaEval 2, along with a 50.3% win rate on Arena-Hard. Furthermore, EVOLVE consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement</title>
<link>https://arxiv.org/abs/2502.06207</link>
<guid>https://arxiv.org/abs/2502.06207</guid>
<content:encoded><![CDATA[
arXiv:2502.06207v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Taught You That? Tracing Teachers in Model Distillation</title>
<link>https://arxiv.org/abs/2502.06659</link>
<guid>https://arxiv.org/abs/2502.06659</guid>
<content:encoded><![CDATA[
arXiv:2502.06659v2 Announce Type: replace 
Abstract: Model distillation -- using outputs from a large teacher model to teach a small student model -- is a practical means of creating efficient models for a particular task. We ask: Can we identify a students' teacher based on its outputs? Such "footprints" left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that $n$-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues</title>
<link>https://arxiv.org/abs/2502.09120</link>
<guid>https://arxiv.org/abs/2502.09120</guid>
<content:encoded><![CDATA[
arXiv:2502.09120v3 Announce Type: replace 
Abstract: This study investigates whether vision-language models (VLMs) can perform pragmatic inference, focusing on ignorance implicatures, utterances that imply the speaker's lack of precise knowledge. To test this, we systematically manipulated contextual cues: the visually depicted situation (visual cue) and QUD-based linguistic prompts (linguistic cue). When only visual cues were provided, three state-of-the-art VLMs (GPT-4o, Gemini 1.5 Pro, and Claude 3.5 sonnet) produced interpretations largely based on the lexical meaning of the modified numerals. When linguistic cues were added to enhance contextual informativeness, Claude exhibited more human-like inference by integrating both types of contextual cues. In contrast, GPT and Gemini favored precise, literal interpretations. Although the influence of contextual cues increased, they treated each contextual cue independently and aligned them with semantic features rather than engaging in context-driven reasoning. These findings suggest that although the models differ in how they handle contextual cues, Claude's ability to combine multiple cues may signal emerging pragmatic competence in multimodal models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation</title>
<link>https://arxiv.org/abs/2502.10996</link>
<guid>https://arxiv.org/abs/2502.10996</guid>
<content:encoded><![CDATA[
arXiv:2502.10996v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved impressive performance on knowledge-intensive tasks, yet they often struggle with multi-step reasoning due to the unstructured nature of retrieved context. While retrieval-augmented generation (RAG) methods provide external information, the lack of explicit organization among retrieved passages limits their effectiveness, leading to brittle reasoning pathways. Recent interpretability studies highlighting the importance of structured intermediate reasoning further align with this perspective. We propose Retrieval-And-Structuring (RAS), a framework that dynamically constructs query-specific knowledge graphs through iterative retrieval and structured knowledge building. RAS interleaves targeted retrieval planning with incremental graph construction, enabling models to assemble and reason over evolving knowledge structures tailored to each query. On seven knowledge-intensive benchmarks, RAS consistently outperforms strong baselines, achieving up to 6.4% and 7.0% gains with open-source and proprietary LLMs, respectively. Our results demonstrate that dynamic, query-specific knowledge structuring offers a robust path to improving reasoning accuracy and robustness in language model generation. Our data and code can be found at https://github.com/pat-jj/RAS.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pairwise: Global Zero-shot Temporal Graph Generation</title>
<link>https://arxiv.org/abs/2502.11114</link>
<guid>https://arxiv.org/abs/2502.11114</guid>
<content:encoded><![CDATA[
arXiv:2502.11114v2 Announce Type: replace 
Abstract: Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relationships between events in a document. Despite the advances in large language models (LLMs), their application to TRE remains limited. Most existing approaches rely on pairwise classification, where event pairs are classified in isolation, leading to computational inefficiency and a lack of global consistency in the resulting temporal graph. In this work, we propose a novel zero-shot method for TRE that generates a document's complete temporal graph in a single step, followed by temporal constraint optimization to refine predictions and enforce temporal consistency across relations. Additionally, we introduce OmniTemp, a new dataset with complete annotations for all pairs of targeted events within a document. Through experiments and analyses, we demonstrate that our method outperforms existing zero-shot approaches and offers a competitive alternative to supervised TRE models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures</title>
<link>https://arxiv.org/abs/2502.11150</link>
<guid>https://arxiv.org/abs/2502.11150</guid>
<content:encoded><![CDATA[
arXiv:2502.11150v2 Announce Type: replace 
Abstract: Automated text readability prediction is widely used in many real-world scenarios. Over the past century, such measures have primarily been developed and evaluated on reading comprehension outcomes and on human annotations of text readability levels. In this work, we propose an alternative, eye tracking-based cognitive framework which directly taps into a key aspect of readability: reading ease. We use this framework for evaluating a broad range of prominent readability measures, including two systems widely used in education, by quantifying their ability to account for reading facilitation effects in text simplification, as well as text reading ease more broadly. Our analyses suggest that existing readability measures are poor predictors of reading facilitation and reading ease, outperformed by word properties commonly used in psycholinguistics, and in particular by surprisal.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirage of Model Editing: Revisiting Evaluation in the Wild</title>
<link>https://arxiv.org/abs/2502.11177</link>
<guid>https://arxiv.org/abs/2502.11177</guid>
<content:encoded><![CDATA[
arXiv:2502.11177v4 Announce Type: replace 
Abstract: Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs</title>
<link>https://arxiv.org/abs/2502.11380</link>
<guid>https://arxiv.org/abs/2502.11380</guid>
<content:encoded><![CDATA[
arXiv:2502.11380v2 Announce Type: replace 
Abstract: Lexico-semantic networks represent words as nodes and their semantic relatedness as edges. While such networks are traditionally constructed using embeddings from encoder-based models or static vectors, embeddings from decoder-only large language models (LLMs) remain underexplored. Unlike encoder models, LLMs are trained with a next-token prediction objective, which does not directly encode the meaning of the current token. In this paper, we construct lexico-semantic networks from the input embeddings of LLMs with varying parameter scales and conduct a comparative analysis of their global and local structures. Our results show that these networks exhibit small-world properties, characterized by high clustering and short path lengths. Moreover, larger LLMs yield more intricate networks with less small-world effects and longer paths, reflecting richer semantic structures and relations. We further validate our approach through analyses of common conceptual pairs, structured lexical relations derived from WordNet, and a cross-lingual semantic network for qualitative words.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs</title>
<link>https://arxiv.org/abs/2502.11525</link>
<guid>https://arxiv.org/abs/2502.11525</guid>
<content:encoded><![CDATA[
arXiv:2502.11525v2 Announce Type: replace 
Abstract: Length generalization, the ability to solve problems longer than those seen during training, remains a critical challenge for large language models (LLMs). Previous work modifies positional encodings (PEs) and data formats to improve length generalization on specific symbolic tasks such as addition and sorting. However, these approaches are fundamentally limited to special tasks, often degrading general language performance. Furthermore, they are typically evaluated on small transformers trained from scratch on single tasks and can cause performance drop when applied during post-training stage of practical LLMs with general capabilities. Hu et al., (2024) proposed Rule-Following Fine-Tuning (RFFT) to improve length generalization in the post-training stage of LLMs. Despite its compatibility with practical models and strong performance, RFFT is proposed for single tasks too, requiring re-training for each individual task with extensive examples. In this paper, we study length generalization in multi-task settings and propose Meta Rule-Following Fine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length generalization. As our first contribution, we construct a large length generalization dataset containing 86 tasks spanning code execution, number processing, symbolic and logical reasoning tasks, beyond the common addition or multiplication tasks. Secondly, we show that cross-task length generalization is possible with Meta-RFFT. After training on a large number of tasks and instances, the models achieve remarkable length generalization ability on unseen tasks with minimal fine-tuning or one-shot prompting. For example, after fine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30 digit addition, significantly outperforming the state-of-the-art reasoning models (DeepSeek-R1-671B: 72%), despite never seeing this task during RF-pretraining.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaMTEB: Massive Text Embedding Benchmark in Persian Language</title>
<link>https://arxiv.org/abs/2502.11571</link>
<guid>https://arxiv.org/abs/2502.11571</guid>
<content:encoded><![CDATA[
arXiv:2502.11571v2 Announce Type: replace 
Abstract: In this paper, we introduce a comprehensive benchmark for Persian (Farsi) text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our benchmark includes 63 datasets spanning seven different tasks: classification, clustering, pair classification, reranking, retrieval, summary retrieval, and semantic textual similarity. The datasets are formed as a combination of existing, translated, and newly generated data, offering a diverse evaluation framework for Persian language models. Given the increasing use of text embedding models in chatbots, evaluation datasets are becoming inseparable ingredients in chatbot challenges and Retrieval-Augmented Generation systems. As a contribution, we include chatbot evaluation datasets in the MTEB benchmark for the first time. In addition, in this paper, we introduce the new task of summary retrieval which is not part of the tasks included in standard MTEB. Another contribution of this paper is the introduction of a substantial number of new Persian language NLP datasets suitable for training and evaluation, some of which have no previous counterparts in Persian. We evaluate the performance of several Persian and multilingual embedding models in a range of tasks. This work introduces an open-source benchmark with datasets, code and a public leaderboard.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2502.12202</link>
<guid>https://arxiv.org/abs/2502.12202</guid>
<content:encoded><![CDATA[
arXiv:2502.12202v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) are designed to solve complex tasks by generating explicit reasoning traces before producing final answers. However, we reveal a critical vulnerability in LRMs -- termed Unthinking Vulnerability -- wherein the thinking process can be bypassed by manipulating special delimiter tokens. It is empirically demonstrated to be widespread across mainstream LRMs, posing both a significant risk and potential utility, depending on how it is exploited. In this paper, we systematically investigate this vulnerability from both malicious and beneficial perspectives. On the malicious side, we introduce Breaking of Thought (BoT), a novel attack that enables adversaries to bypass the thinking process of LRMs, thereby compromising their reliability and availability. We present two variants of BoT: a training-based version that injects backdoor during the fine-tuning stage, and a training-free version based on adversarial attack during the inference stage. As a potential defense, we propose thinking recovery alignment to partially mitigate the vulnerability. On the beneficial side, we introduce Monitoring of Thought (MoT), a plug-and-play framework that allows model owners to enhance efficiency and safety. It is implemented by leveraging the same vulnerability to dynamically terminate redundant or risky reasoning through external monitoring. Extensive experiments show that BoT poses a significant threat to reasoning reliability, while MoT provides a practical solution for preventing overthinking and jailbreaking. Our findings expose an inherent flaw in current LRM architectures and underscore the need for more robust reasoning systems in the future.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</title>
<link>https://arxiv.org/abs/2502.12464</link>
<guid>https://arxiv.org/abs/2502.12464</guid>
<content:encoded><![CDATA[
arXiv:2502.12464v2 Announce Type: replace 
Abstract: Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora</title>
<link>https://arxiv.org/abs/2502.13691</link>
<guid>https://arxiv.org/abs/2502.13691</guid>
<content:encoded><![CDATA[
arXiv:2502.13691v2 Announce Type: replace 
Abstract: As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach</title>
<link>https://arxiv.org/abs/2502.14285</link>
<guid>https://arxiv.org/abs/2502.14285</guid>
<content:encoded><![CDATA[
arXiv:2502.14285v2 Announce Type: replace 
Abstract: Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-generated text detection prevents language model collapse</title>
<link>https://arxiv.org/abs/2502.15654</link>
<guid>https://arxiv.org/abs/2502.15654</guid>
<content:encoded><![CDATA[
arXiv:2502.15654v5 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, converge to a low variance output distribution, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the text characteristics at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2), across a range of model sizes (124M to 1.7B), on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. Source code: github.com/GeorgeDrayson/model_collapse.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FANformer: Improving Large Language Models Through Effective Periodicity Modeling</title>
<link>https://arxiv.org/abs/2502.21309</link>
<guid>https://arxiv.org/abs/2502.21309</guid>
<content:encoded><![CDATA[
arXiv:2502.21309v2 Announce Type: replace 
Abstract: Periodicity, as one of the most important basic characteristics, lays the foundation for facilitating structured knowledge acquisition and systematic cognitive processes within human learning paradigms. However, the potential flaws of periodicity modeling in Transformer affect the learning efficiency and establishment of underlying principles from data for large language models (LLMs) built upon it. In this paper, we demonstrate that integrating effective periodicity modeling can improve the learning efficiency and performance of LLMs. We introduce FANformer, which adapts Fourier Analysis Network (FAN) into attention mechanism to achieve efficient periodicity modeling, by modifying the feature projection process of attention mechanism. Extensive experimental results on language modeling show that FANformer consistently outperforms Transformer when scaling up model size and training tokens, underscoring its superior learning efficiency. Our pretrained FANformer-1B exhibits marked improvements on downstream tasks compared to open-source LLMs with similar model parameters or training tokens. Moreover, we reveal that FANformer exhibits superior ability to learn and apply rules for reasoning compared to Transformer. The results position FANformer as an effective and promising architecture for advancing LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings</title>
<link>https://arxiv.org/abs/2503.03008</link>
<guid>https://arxiv.org/abs/2503.03008</guid>
<content:encoded><![CDATA[
arXiv:2503.03008v2 Announce Type: replace 
Abstract: Deploying language models often requires navigating accuracy vs. performance trade-offs to meet latency constraints while preserving utility. Traditional model distillation reduces size but incurs substantial costs through training separate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter multi-exit encoder for code retrieval and classification that employs a novel Self-Distillation mechanism. This approach significantly enhances lower-layer representations, enabling flexible deployment of different model portions with favorable performance trade-offs. Our architecture improves text-to-code and code-to-code search by targeting specific encoder layers as exit heads, where higher layers guide earlier ones during training-improving intermediate representations at minimal additional cost. We further enhance MoSE with a repository-level contextual loss that maximizes training context window utilization. Additionally, we release a new dataset created through code translation that extends text-to-code benchmarks with cross-language code-to-code pairs. Evaluations demonstrate the effectiveness of Self-Distillation as a principled approach to trading inference cost for accuracy across various code understanding tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions</title>
<link>https://arxiv.org/abs/2503.03261</link>
<guid>https://arxiv.org/abs/2503.03261</guid>
<content:encoded><![CDATA[
arXiv:2503.03261v2 Announce Type: replace 
Abstract: Multiple previous studies have reported suboptimal performance of LLMs in biomedical text mining. By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow. We experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines. Our results show that frontier LLMs can approach or surpass the performance of SOTA BERT-based models with minimal reliance on manually annotated data and without fine-tuning. Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance. Based on these findings, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedical text mining.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios</title>
<link>https://arxiv.org/abs/2503.06218</link>
<guid>https://arxiv.org/abs/2503.06218</guid>
<content:encoded><![CDATA[
arXiv:2503.06218v2 Announce Type: replace 
Abstract: Currently, long-chain reasoning remains a key challenge for large language models (LLMs) because natural texts lack sufficient explicit reasoning data. However, existing benchmarks suffer from limitations such as narrow coverage, short reasoning paths, or high construction costs. We introduce SCoRE (Scenario-based Commonsense Reasoning Evaluation), a benchmark that synthesizes multi-hop questions from scenario schemas of entities, relations, and logical rules to assess long-chain commonsense reasoning. SCoRE contains 100k bilingual (Chinese-English) multiple-choice questions whose reasoning chains span 2-11 hops and are grouped into various difficulty levels. Each question is accompanied by fine-grained knowledge labels, explicit reasoning chains, and difficulty levels for diagnostic evaluation. Evaluation results on cutting-edge LLMs such as o3-mini and Deepseek R1 shows that even the best model attains only 69.78% accuracy on SCoRE (even only 47.91% on the hard set), with errors often stemming from rare knowledge, logical inconsistency, and over-interpretation of simple questions. SCoRE offers a scalable, extensible framework for evaluating and diagnosing the long-chain commonsense reasoning abilities of LLMs and guiding future advances in model design and training.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs</title>
<link>https://arxiv.org/abs/2503.09543</link>
<guid>https://arxiv.org/abs/2503.09543</guid>
<content:encoded><![CDATA[
arXiv:2503.09543v2 Announce Type: replace 
Abstract: The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed -- i.e., parameters' initialisation and data order -- on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Reasoning with LLMs for k-anonymity Estimation</title>
<link>https://arxiv.org/abs/2503.09674</link>
<guid>https://arxiv.org/abs/2503.09674</guid>
<content:encoded><![CDATA[
arXiv:2503.09674v2 Announce Type: replace 
Abstract: Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality</title>
<link>https://arxiv.org/abs/2503.10669</link>
<guid>https://arxiv.org/abs/2503.10669</guid>
<content:encoded><![CDATA[
arXiv:2503.10669v2 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs) with human values. However, existing approaches struggle to capture the multi-dimensional, distributional nuances of human preferences. Methods such as RiC that directly inject raw reward values into prompts face significant numerical sensitivity issues--for instance, LLMs may fail to distinguish between 9.11 and 9.8--while alternatives like MORLHF, Rewarded Soups, and MODPO incur high computational costs by training multiple models. In this work, we introduce Utility-Conditioned Multi-Objective Alignment (UC-MOA), a novel framework that overcomes these limitations. Our approach leverages a diverse set of strictly increasing, non-linear utility functions to transform user-specified preferences into symbolic tokens, which are then used to condition a single LLM. This design not only mitigates numerical reasoning challenges but also substantially reduces training overhead, yielding models that achieve superior Pareto fronts and robust alignment across complex reward dimensions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2503.12908</link>
<guid>https://arxiv.org/abs/2503.12908</guid>
<content:encoded><![CDATA[
arXiv:2503.12908v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model's prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more "contrast-effective" hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models</title>
<link>https://arxiv.org/abs/2503.14411</link>
<guid>https://arxiv.org/abs/2503.14411</guid>
<content:encoded><![CDATA[
arXiv:2503.14411v2 Announce Type: replace 
Abstract: Temporal graph neural networks (TGNNs) have shown remarkable performance in temporal graph modeling. However, real-world temporal graphs often possess rich textual information, giving rise to temporal text-attributed graphs (TTAGs). Such combination of dynamic text semantics and evolving graph structures introduces heightened complexity. Existing TGNNs embed texts statically and rely heavily on encoding mechanisms that biasedly prioritize structural information, overlooking the temporal evolution of text semantics and the essential interplay between semantics and structures for synergistic reinforcement. To tackle these issues, we present \textbf{CROSS}, a flexible framework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is designed by decomposing the TTAG modeling process into two phases: (i) temporal semantics extraction; and (ii) semantic-structural information unification. The key idea is to advance the large language models (LLMs) to dynamically extract the temporal semantics in text space and then generate cohesive representations unifying both semantics and structures. Specifically, we propose a Temporal Semantics Extractor in the CROSS framework, which empowers LLMs to offer the temporal semantic understanding of node's evolving contexts of textual neighborhoods, facilitating semantic dynamics. Subsequently, we introduce the Semantic-structural Co-encoder, which collaborates with the above Extractor for synthesizing illuminating representations by jointly considering both semantic and structural information while encouraging their mutual reinforcement. Extensive experiments show that CROSS achieves state-of-the-art results on four public datasets and one industrial dataset, with 24.7% absolute MRR gain on average in temporal link prediction and 3.7% AUC gain in node classification of industrial application.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2503.21380</link>
<guid>https://arxiv.org/abs/2503.21380</guid>
<content:encoded><![CDATA[
arXiv:2503.21380v2 Announce Type: replace 
Abstract: In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5 Pro Exp demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the benchmark, evaluation code, detailed results and a data visualization tool at https://github.com/RUCAIBox/OlymMATH.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
arXiv:2503.21729v3 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImF: Implicit Fingerprint for Large Language Models</title>
<link>https://arxiv.org/abs/2503.21805</link>
<guid>https://arxiv.org/abs/2503.21805</guid>
<content:encoded><![CDATA[
arXiv:2503.21805v2 Announce Type: replace 
Abstract: Training large language models (LLMs) is resource-intensive and expensive, making protecting intellectual property (IP) for LLMs crucial. Recently, embedding fingerprints into LLMs has emerged as a prevalent method for establishing model ownership. However, existing fingerprinting techniques typically embed identifiable patterns with weak semantic coherence, resulting in fingerprints that significantly differ from the natural question-answering (QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of the embedded fingerprints and makes them vulnerable to adversarial attacks. In this paper, we first demonstrate the critical vulnerability of existing fingerprint embedding methods by introducing a novel adversarial attack named Generation Revision Intervention (GRI) attack. GRI attack exploits the semantic fragility of current fingerprinting methods, effectively erasing fingerprints by disrupting their weakly correlated semantic structures. Our empirical evaluation highlights that traditional fingerprinting approaches are significantly compromised by the GRI attack, revealing severe limitations in their robustness under realistic adversarial conditions. To advance the state-of-the-art in model fingerprinting, we propose a novel model fingerprint paradigm called Implicit Fingerprints (ImF). ImF leverages steganography techniques to subtly embed ownership information within natural texts, subsequently using Chain-of-Thought (CoT) prompting to construct semantically coherent and contextually natural QA pairs. This design ensures that fingerprints seamlessly integrate with the standard model behavior, remaining indistinguishable from regular outputs and substantially reducing the risk of accidental triggering and targeted removal. We conduct a comprehensive evaluation of ImF on 15 diverse LLMs, spanning different architectures and varying scales.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors</title>
<link>https://arxiv.org/abs/2503.22388</link>
<guid>https://arxiv.org/abs/2503.22388</guid>
<content:encoded><![CDATA[
arXiv:2503.22388v2 Announce Type: replace 
Abstract: LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RARE: Retrieval-Augmented Reasoning Modeling</title>
<link>https://arxiv.org/abs/2503.23513</link>
<guid>https://arxiv.org/abs/2503.23513</guid>
<content:encoded><![CDATA[
arXiv:2503.23513v2 Announce Type: replace 
Abstract: Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts with masked losses, RARE transforms learning objectives from rote memorization to contextualized reasoning. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Extensive experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\% accuracy. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FISH-Tuning: Enhancing PEFT Methods with Fisher Information</title>
<link>https://arxiv.org/abs/2504.04050</link>
<guid>https://arxiv.org/abs/2504.04050</guid>
<content:encoded><![CDATA[
arXiv:2504.04050v2 Announce Type: replace 
Abstract: The rapid growth in the parameter size of Large Language Models (LLMs) has spurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods to mitigate the substantial computational costs of fine-tuning. Among these, Fisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT technique that identifies a critical subset of pre-trained parameters using approximate Fisher information. While addition-based and reparameterization-based PEFT methods like LoRA and Adapter already fine-tune only a small number of parameters, the newly introduced parameters within these methods themselves present an opportunity for further optimization. Selectively fine-tuning only the most impactful among these new parameters could further reduce resource consumption while maintaining, or even improving, fine-tuning effectiveness. In this paper, we propose \textbf{FISH-Tuning}, a novel approach that incorporates FISH Mask into such PEFT methods, including LoRA, Adapter, and their variants. By leveraging Fisher information to identify and update only the most significant parameters within these added or reparameterized components, FISH-Tuning aims to achieve superior performance without increasing training time or inference latency compared to the vanilla PEFT methods. Experimental results across various datasets and pre-trained models demonstrate that FISH-Tuning consistently outperforms the vanilla PEFT methods when using the same proportion of trainable parameters. Code is available at https://anonymous.4open.science/r/FISH-Tuning-6F7C.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Robust Optimization for LLM Alignment under Distribution Shifts</title>
<link>https://arxiv.org/abs/2504.05831</link>
<guid>https://arxiv.org/abs/2504.05831</guid>
<content:encoded><![CDATA[
arXiv:2504.05831v2 Announce Type: replace 
Abstract: Preference alignment methods are increasingly critical for steering large language models (LLMs) to generate outputs consistent with human values. While recent approaches often rely on synthetic data generated by LLMs for scalability and cost-efficiency reasons, this reliance can introduce distribution shifts that undermine the nuanced representation of human preferences needed for desirable outputs. In this paper, we propose a novel distribution-aware optimization framework that improves preference alignment despite such shifts. Our approach first leverages well-learned classifiers to assign a calibration value to each training sample, quantifying its alignment with the target human-preferred distribution. These values are then incorporated into a robust optimization objective that minimizes the worst-case loss over regions of the data space most relevant to human preferences. By explicitly focusing optimization on the target distribution, our approach mitigates the impact of distributional mismatch and improves the generation of responses that better reflect intended values.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSR-MCTS: Alleviating Long Range Dependency in Code Generation</title>
<link>https://arxiv.org/abs/2504.07433</link>
<guid>https://arxiv.org/abs/2504.07433</guid>
<content:encoded><![CDATA[
arXiv:2504.07433v3 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Could Be Rote Learners</title>
<link>https://arxiv.org/abs/2504.08300</link>
<guid>https://arxiv.org/abs/2504.08300</guid>
<content:encoded><![CDATA[
arXiv:2504.08300v4 Announce Type: replace 
Abstract: Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework reformulating MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.10198</link>
<guid>https://arxiv.org/abs/2504.10198</guid>
<content:encoded><![CDATA[
arXiv:2504.10198v2 Announce Type: replace 
Abstract: Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events</title>
<link>https://arxiv.org/abs/2504.12052</link>
<guid>https://arxiv.org/abs/2504.12052</guid>
<content:encoded><![CDATA[
arXiv:2504.12052v3 Announce Type: replace 
Abstract: We present a Bayesian dynamic borrowing (BDB) approach to enhance the quantitative identification of adverse events (AEs) in spontaneous reporting systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior with a Bayesian hierarchical model and incorporates semantic similarity measures (SSMs) to enable weighted information sharing from clinically similar MedDRA Preferred Terms (PTs) to the target PT. This continuous similarity-based borrowing overcomes limitations of rigid hierarchical grouping in current disproportionality analysis (DPA).
  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015 and 2019, we evaluate our approach -- termed IC SSM -- against traditional Information Component (IC) analysis and IC with borrowing at the MedDRA high-level group term level (IC HLGT). A reference set (PVLens), derived from FDA product label update, enabled prospective evaluation of method performance in identifying AEs prior to official labeling.
  The IC SSM approach demonstrated higher sensitivity (1332/2337=0.570, Youden's J=0.246) than traditional IC (Se=0.501, J=0.250) and IC HLGT (Se=0.556, J=0.225), consistently identifying more true positives and doing so on average 5 months sooner than traditional IC. Despite a marginally lower aggregate F1-score and Youden's index, IC SSM showed higher performance in early post-marketing periods or when the detection threshold was raised, providing more stable and relevant alerts than IC HLGT and traditional IC.
  These findings support the use of SSM-informed Bayesian borrowing as a scalable and context-aware enhancement to traditional DPA methods, with potential for validation across other datasets and exploration of additional similarity metrics and Bayesian strategies using case-level data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.13534</link>
<guid>https://arxiv.org/abs/2504.13534</guid>
<content:encoded><![CDATA[
arXiv:2504.13534v2 Announce Type: replace 
Abstract: Chain-of-thought (CoT) reasoning boosts large language models' (LLMs) performance on complex tasks but faces two key limitations: a lack of reliability when solely relying on LLM-generated reasoning chains and interference from natural language reasoning steps with the models' inference process, also known as the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which promotes greater logical rigor by guiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine public datasets spanning three reasoning tasks reveal significant accuracy gains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore, tests on four domain-specific datasets demonstrate exceptional accuracy and efficient execution, underscoring its practical applicability and scalability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach</title>
<link>https://arxiv.org/abs/2504.14321</link>
<guid>https://arxiv.org/abs/2504.14321</guid>
<content:encoded><![CDATA[
arXiv:2504.14321v2 Announce Type: replace 
Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing mutimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources. To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We will release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data</title>
<link>https://arxiv.org/abs/2504.14669</link>
<guid>https://arxiv.org/abs/2504.14669</guid>
<content:encoded><![CDATA[
arXiv:2504.14669v2 Announce Type: replace 
Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Early Exit in Reasoning Models</title>
<link>https://arxiv.org/abs/2504.15895</link>
<guid>https://arxiv.org/abs/2504.15895</guid>
<content:encoded><![CDATA[
arXiv:2504.15895v2 Announce Type: replace 
Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and LiveCodeBench) show that the proposed method is consistently effective on 11 cutting-edge reasoning LLMs of varying series and sizes, reducing the length of CoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3% to 5.0%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
arXiv:2504.16074v2 Announce Type: replace 
Abstract: Current benchmarks for evaluating the reasoning capabilities of Large Language Models (LLMs) face significant limitations: task oversimplification, data contamination, and flawed evaluation items. These deficiencies necessitate more rigorous assessment methods. To address these limitations, we introduce PHYBench, a benchmark of 500 original physics problems ranging from high school to Physics Olympiad difficulty. PHYBench addresses data contamination through original content and employs a systematic curation pipeline to eliminate flawed items. Evaluations show that PHYBench activates more tokens and provides stronger differentiation between reasoning models compared to other baselines like AIME 2024, OlympiadBench and GPQA. Even the best-performing model, Gemini 2.5 Pro, achieves only 36.9% accuracy compared to human experts' 61.9%. To further enhance evaluation precision, we introduce the Expression Edit Distance (EED) Score for mathematical expression assessment, which improves sample efficiency by 204% over binary scoring. Moreover, PHYBench effectively elicits multi-step and multi-condition reasoning, providing a platform for examining models' reasoning robustness, preferences, and deficiencies. The benchmark results and dataset are publicly available at https://www.phybench.cn/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[
arXiv:2504.16918v2 Announce Type: replace 
Abstract: Optimization plays a vital role in scientific research and practical applications. However, formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce OptimAI, a framework for solving Optimization problems described in natural language by leveraging LLM-powered AI agents, and achieve superior performance over current state-of-the-art methods. Our framework is built upon the following key roles: (1) a formulator that translates natural language problem descriptions into precise mathematical formulations; (2) a planner that constructs a high-level solution strategy prior to execution; and (3) a coder and a code critic capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, and our experiments confirm that combining diverse models leads to performance gains. Our approach attains 88.1% accuracy on the NLP4LP dataset and 82.3% on the Optibench dataset, reducing error rates by 58% and 52%, respectively, over prior best results.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
arXiv:2504.17192v3 Announce Type: replace 
Abstract: Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, particularly from the authors of those papers, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at: https://github.com/going-doer/Paper2Code.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
arXiv:2504.19162v2 Announce Type: replace 
Abstract: Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a "sneaky generator" that deliberately produces erroneous steps designed to be difficult to detect, and a "critic" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19627</link>
<guid>https://arxiv.org/abs/2504.19627</guid>
<content:encoded><![CDATA[
arXiv:2504.19627v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
arXiv:2504.20157v2 Announce Type: replace 
Abstract: Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, from essay writing to mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and data can be accessed at: https://github.com/minnesotanlp/mpo
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
arXiv:2504.20734v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single aggregated corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over various modality-specific and unified baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2504.20771</link>
<guid>https://arxiv.org/abs/2504.20771</guid>
<content:encoded><![CDATA[
arXiv:2504.20771v2 Announce Type: replace 
Abstract: With the rapid development and widespread application of Large Language Models (LLMs), multidimensional evaluation has become increasingly critical. However, current evaluations are often domain-specific and overly complex, limiting their effectiveness as cross-domain proxies for core capabilities. To address these limitations and enable a unified and simple evaluation framework, an ideal proxy task should target a basic capability that generalizes across tasks and is independent of domain-specific knowledge. Turing machine provides a powerful theoretical lens by reducing complex processes to basic, domain-agnostic computational operations. This perspective offers a principled framework for evaluating basic computational abilities essential to a wide range of tasks. Motivated by this abstraction, we introduce \textbf{Turing Machine Bench}, a benchmark designed to assess the ability of LLMs to \textbf{strictly follow rules} and \textbf{accurately manage internal states} for multi-step, referred to as \textbf{computational reasoning}. TMBench incorporates four key features: self-contained and knowledge-agnostic reasoning, a minimalistic multi-step structure, controllable difficulty, and a solid theoretical foundation based on Turing machine. Empirical results demonstrate that TMBench serves as an effective proxy for evaluating computational reasoning on representative LLMs. It produces clear step-wise accuracy curves, revealing LLMs' ability to execute multi-step reasoning processes. By analyzing performance trends across TMBench and established reasoning benchmarks, we find strong correlations with real-world tasks, bridging real-task evaluation with basic ability assessment. These findings suggest that TMBench holds potential as a cross-domain dimension for evaluating reasoning in LLMs. Code and data are available at \href{https://github.com/HaitaoWuTJU/Turing-Machine-Bench}{Repo}.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</title>
<link>https://arxiv.org/abs/2505.00570</link>
<guid>https://arxiv.org/abs/2505.00570</guid>
<content:encoded><![CDATA[
arXiv:2505.00570v2 Announce Type: replace 
Abstract: Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent</title>
<link>https://arxiv.org/abs/2309.12555</link>
<guid>https://arxiv.org/abs/2309.12555</guid>
<content:encoded><![CDATA[
arXiv:2309.12555v2 Announce Type: replace-cross 
Abstract: Creating personalized and actionable exercise plans often requires iteration with experts, which can be costly and inaccessible to many individuals. This work explores the capabilities of Large Language Models (LLMs) in addressing these challenges. We present PlanFitting, an LLM-driven conversational agent that assists users in creating and refining personalized weekly exercise plans. By engaging users in free-form conversations, PlanFitting helps elicit users' goals, availabilities, and potential obstacles, and enables individuals to generate personalized exercise plans aligned with established exercise guidelines. Our study -- involving a user study, intrinsic evaluation, and expert evaluation -- demonstrated PlanFitting's ability to guide users to create tailored, actionable, and evidence-based plans. We discuss future design opportunities for LLM-driven conversational agents to create plans that better comply with exercise principles and accommodate personal constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAT: Learning to Reason about Spatial Sounds with Large Language Models</title>
<link>https://arxiv.org/abs/2402.01591</link>
<guid>https://arxiv.org/abs/2402.01591</guid>
<content:encoded><![CDATA[
arXiv:2402.01591v3 Announce Type: replace-cross 
Abstract: Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era</title>
<link>https://arxiv.org/abs/2403.08946</link>
<guid>https://arxiv.org/abs/2403.08946</guid>
<content:encoded><![CDATA[
arXiv:2403.08946v2 Announce Type: replace-cross 
Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended toward explaining Large Language Models (LLMs). This extension calls for a significant transformation in the XAI methodologies for two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity and advanced capabilities. Second, as LLMs are increasingly deployed in diverse applications, the role of XAI shifts from merely opening the ``black box'' to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, the conversation and generation abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can explain and improve LLM-based AI systems and (2) how XAI techniques can be improved by using LLMs. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Training Data Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2403.15309</link>
<guid>https://arxiv.org/abs/2403.15309</guid>
<content:encoded><![CDATA[
arXiv:2403.15309v2 Announce Type: replace-cross 
Abstract: We present a method to control a text-to-image generative model to produce training data useful for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak</title>
<link>https://arxiv.org/abs/2405.20015</link>
<guid>https://arxiv.org/abs/2405.20015</guid>
<content:encoded><![CDATA[
arXiv:2405.20015v2 Announce Type: replace-cross 
Abstract: This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreak methods that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) built upon the target LLM. Subsequently, we perform an efficient MLLM jailbreak and obtain a jailbreaking embedding. Finally, we convert the embedding into a textual jailbreaking suffix to carry out the jailbreak of target LLM. Compared to the direct LLM-jailbreak methods, our indirect jailbreaking approach is more efficient, as MLLMs are more vulnerable to jailbreak than pure LLM. Additionally, to improve the attack success rate of jailbreak, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art jailbreak methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class generalization abilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$S^3$ -- Semantic Signal Separation</title>
<link>https://arxiv.org/abs/2406.09556</link>
<guid>https://arxiv.org/abs/2406.09556</guid>
<content:encoded><![CDATA[
arXiv:2406.09556v3 Announce Type: replace-cross 
Abstract: Topic models are useful tools for discovering latent semantic structures in large textual corpora. Recent efforts have been oriented at incorporating contextual representations in topic modeling and have been shown to outperform classical topic models. These approaches are typically slow, volatile, and require heavy preprocessing for optimal results. We present Semantic Signal Separation ($S^3$), a theory-driven topic modeling approach in neural embedding spaces. $S^3$ conceptualizes topics as independent axes of semantic space and uncovers these by decomposing contextualized document embeddings using Independent Component Analysis. Our approach provides diverse and highly coherent topics, requires no preprocessing, and is demonstrated to be the fastest contextual topic model, being, on average, 4.5x faster than the runner-up BERTopic. We offer an implementation of $S^3$, and all contextual baselines, in the Turftopic Python package.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Language Models with Error Correcting Codes</title>
<link>https://arxiv.org/abs/2406.10281</link>
<guid>https://arxiv.org/abs/2406.10281</guid>
<content:encoded><![CDATA[
arXiv:2406.10281v3 Announce Type: replace-cross 
Abstract: Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating $p$-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Facet Learning: A Structured Approach to Prompt Optimization</title>
<link>https://arxiv.org/abs/2406.10504</link>
<guid>https://arxiv.org/abs/2406.10504</guid>
<content:encoded><![CDATA[
arXiv:2406.10504v2 Announce Type: replace-cross 
Abstract: Given a task in the form of a basic description and its training examples, prompt optimization is the problem of synthesizing the given information into a text prompt for a large language model. Humans solve this problem by also considering the different facets that define a task (e.g., counter-examples, explanations, analogies) and including them in the prompt. However, it is unclear whether existing algorithmic approaches, based on iteratively editing a given prompt or automatically selecting a few in-context examples, can cover the multiple facets required to solve a complex task. In this work, we view prompt optimization as that of learning multiple facets of a task from a set of training examples. We exploit structure in the prompt optimization problem and break down a prompt into loosely coupled semantic sections. The proposed algorithm, UniPrompt, (1) clusters the input space and uses clustered batches so that each batch likely corresponds to a different facet of the task, and (2) utilizes a feedback mechanism to propose adding, editing or deleting a section, which in turn is aggregated over a batch to capture generalizable facets. Empirical evaluation on multiple datasets and a real-world task shows that prompts generated using \shortname{} obtain higher accuracy than human-tuned prompts and those from state-of-the-art methods. In particular, our algorithm can generate long, complex prompts that existing methods are unable to generate. Code for UniPrompt is available at https://aka.ms/uniprompt.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient descent with generalized Newton's method</title>
<link>https://arxiv.org/abs/2407.02772</link>
<guid>https://arxiv.org/abs/2407.02772</guid>
<content:encoded><![CDATA[
arXiv:2407.02772v3 Announce Type: replace-cross 
Abstract: We propose the generalized Newton's method (GeN) -- a Hessian-informed approach that applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case. Our method automatically and dynamically selects the learning rate that accelerates the convergence, without the intensive tuning of the learning rate scheduler. In practice, our method is easily implementable, since it only requires additional forward passes with almost zero computational overhead (in terms of training time and memory cost), if the overhead is amortized over many iterations. We present extensive experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that GeN optimizers match the state-of-the-art performance, which was achieved with carefully tuned learning rate schedulers.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientQAT: Efficient Quantization-Aware Training for Large Language Models</title>
<link>https://arxiv.org/abs/2407.11062</link>
<guid>https://arxiv.org/abs/2407.11062</guid>
<content:encoded><![CDATA[
arXiv:2407.11062v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are crucial in modern natural language processing and artificial intelligence. However, they face challenges in managing their significant memory requirements. Although quantization-aware training (QAT) offers a solution by reducing memory consumption through low-bit representations with minimal accuracy loss, it is impractical due to substantial training resources. To address this, we propose Efficient Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm. EfficientQAT involves two consecutive phases: Block-wise training of all parameters (Block-AP) and end-to-end training of quantization parameters (E2E-QP). To the best of our knowledge, Block-AP is the first method to enable direct training of all parameters in a block-wise manner, reducing accuracy loss in low-bit scenarios by enhancing the solution space during optimization. E2E-QP then trains only the quantization parameters (step sizes) end-to-end, further improving the performance of quantized models by considering interactions among all sub-modules. Extensive experiments demonstrate that EfficientQAT outperforms previous quantization methods across a range of models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with scales from 7B to 70B parameters at various quantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3 points accuracy degradation compared to the full precision (69.48 vs. 72.41). Code is available at https://github.com/OpenGVLab/EfficientQAT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation</title>
<link>https://arxiv.org/abs/2409.02718</link>
<guid>https://arxiv.org/abs/2409.02718</guid>
<content:encoded><![CDATA[
arXiv:2409.02718v3 Announce Type: replace-cross 
Abstract: Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection</title>
<link>https://arxiv.org/abs/2410.02647</link>
<guid>https://arxiv.org/abs/2410.02647</guid>
<content:encoded><![CDATA[
arXiv:2410.02647v2 Announce Type: replace-cross 
Abstract: Immunogenicity prediction is a central topic in reverse vaccinology for finding candidate vaccines that can trigger protective immune responses. Existing approaches typically rely on highly compressed features and simple model architectures, leading to limited prediction accuracy and poor generalizability. To address these challenges, we introduce VenusVaccine, a novel deep learning solution with a dual attention mechanism that integrates pre-trained latent vector representations of protein sequences and structures. We also compile the most comprehensive immunogenicity dataset to date, encompassing over 7000 antigen sequences, structures, and immunogenicity labels from bacteria, virus, and tumor. Extensive experiments demonstrate that VenusVaccine outperforms existing methods across a wide range of evaluation metrics. Furthermore, we establish a post-hoc validation protocol to assess the practical significance of deep learning models in tackling vaccine design challenges. Our work provides an effective tool for vaccine design and sets valuable benchmarks for future research. The implementation is at https://github.com/songleee/VenusVaccine.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference and Verbalization Functions During In-Context Learning</title>
<link>https://arxiv.org/abs/2410.09349</link>
<guid>https://arxiv.org/abs/2410.09349</guid>
<content:encoded><![CDATA[
arXiv:2410.09349v2 Announce Type: replace-cross 
Abstract: Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., "true"/"false" to "cat"/"dog"), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and LLAMA-3.1-70B.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Similarity Across Large Language Models</title>
<link>https://arxiv.org/abs/2410.12010</link>
<guid>https://arxiv.org/abs/2410.12010</guid>
<content:encoded><![CDATA[
arXiv:2410.12010v3 Announce Type: replace-cross 
Abstract: Bias in Large Language Models remains a critical concern as these systems are increasingly deployed in high-stakes applications. Yet most fairness evaluations rely on scalar metrics or single-model analysis, overlooking how biases align -- or diverge -- across model families, scales, and tuning strategies. In this work, we reframe bias similarity as a form of functional similarity and evaluate 24 LLMs from four major families on over one million structured prompts spanning four bias dimensions. Our findings uncover that fairness is not strongly determined by model size, architecture, instruction tuning, or openness. Instead, bias behaviors are highly context-dependent and structurally persistent, often resistant to current alignment techniques. Contrary to common assumptions, we find that open-source models frequently match or outperform proprietary models in both fairness and utility. These results call into question the default reliance on proprietary systems and highlight the need for behaviorally grounded, model-specific audits to better understand how bias manifests and endures across the LLM landscape.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation</title>
<link>https://arxiv.org/abs/2410.14971</link>
<guid>https://arxiv.org/abs/2410.14971</guid>
<content:encoded><![CDATA[
arXiv:2410.14971v2 Announce Type: replace-cross 
Abstract: Current EEG/MEG-to-text decoding systems suffer from three key limitations: (1) reliance on teacher-forcing methods, which compromises robustness during inference, (2) sensitivity to session-specific noise, hindering generalization across subjects, and (3) misalignment between brain signals and linguistic representations due to pre-trained language model over-dominance. To overcome these challenges, we propose BrainECHO (Brain signal decoding via vEctor-quantized speCtrogram reconstruction for WHisper-enhanced text generatiOn), a multi-stage framework that employs decoupled representation learning to achieve state-of-the-art performance on both EEG and MEG datasets. Specifically, BrainECHO consists of three stages: (1) Discrete autoencoding, which transforms continuous Mel spectrograms into a finite set of high-quality discrete representations for subsequent stages. (2) Frozen alignment, where brain signal embeddings are mapped to corresponding Mel spectrogram embeddings in a frozen latent space, effectively filtering session-specific noise through vector-quantized reconstruction, yielding a 3.65% improvement in BLEU-4 score. (3) Constrained decoding fine-tuning, which leverages the pre-trained Whisper model for audio-to-text translation, balancing signal adaptation with knowledge preservation, and achieving 74%-89% decoding BLEU scores without excessive reliance on teacher forcing. BrainECHO demonstrates robustness across sentence, session, and subject-independent conditions, passing Gaussian noise tests and showcasing its potential for enhancing language-based brain-computer interfaces.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMScan: Causal Scan for LLM Misbehavior Detection</title>
<link>https://arxiv.org/abs/2410.16638</link>
<guid>https://arxiv.org/abs/2410.16638</guid>
<content:encoded><![CDATA[
arXiv:2410.16638v3 Announce Type: replace-cross 
Abstract: Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful, biased and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when misbehaving. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation</title>
<link>https://arxiv.org/abs/2410.17462</link>
<guid>https://arxiv.org/abs/2410.17462</guid>
<content:encoded><![CDATA[
arXiv:2410.17462v3 Announce Type: replace-cross 
Abstract: Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title>
<link>https://arxiv.org/abs/2411.19939</link>
<guid>https://arxiv.org/abs/2411.19939</guid>
<content:encoded><![CDATA[
arXiv:2411.19939v3 Announce Type: replace-cross 
Abstract: Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counterintuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs aligned with image text pairs. To explain such a phenomenon, we discover a Visual Safety Information Leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky content in the image has been revealed in the textual query. Thus, MLLMs can easily refuse these sensitive image-text pairs according to textual queries only, leading to unreliable cross-modality safety evaluation of MLLMs. We also conduct a further comparison experiment between textual alignment and multimodal alignment to highlight this drawback. To this end, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k image-text pairs through an automated data pipeline. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically compare textual and multimodal alignment methods on VLSBench and find that textual alignment is effective enough for multimodal safety scenarios with VSIL, while multimodal alignment is preferable for safety scenarios without VSIL. Code and data are released under https://github.com/AI45Lab/VLSBench
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Bayesianization for Low-Rank Adapters of Large Language Models</title>
<link>https://arxiv.org/abs/2412.05723</link>
<guid>https://arxiv.org/abs/2412.05723</guid>
<content:encoded><![CDATA[
arXiv:2412.05723v2 Announce Type: replace-cross 
Abstract: Estimating the uncertainty of responses from Large Language Models (LLMs) remains a critical challenge. While recent Bayesian methods have demonstrated effectiveness in quantifying uncertainty through low-rank weight updates, they typically require complex fine-tuning or post-training procedures. In this paper, we propose Training-Free Bayesianization (TFB), a simple yet theoretically grounded framework that efficiently transforms trained low-rank adapters into Bayesian ones without additional training. TFB systematically searches for the maximally acceptable level of variance in the weight posterior, constrained within a family of low-rank isotropic Gaussian distributions. Our theoretical analysis shows that under mild conditions, this search process is equivalent to KL-regularized variational optimization, a generalized form of variational inference. Through comprehensive experiments, we show that TFB achieves superior uncertainty estimation and generalization compared to existing methods while eliminating the need for complex Bayesianization training procedures. Code will be available at https://github.com/Wang-ML-Lab/bayesian-peft.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization</title>
<link>https://arxiv.org/abs/2412.06141</link>
<guid>https://arxiv.org/abs/2412.06141</guid>
<content:encoded><![CDATA[
arXiv:2412.06141v2 Announce Type: replace-cross 
Abstract: The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately mitigated clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. To address this challenge, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, and integrate these scores into the preference optimization process as weights, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by averaging 14.2% and 51.7% across the Med-VQA and report generation tasks. Our code are available in https://github.com/aiming-lab/MMedPO.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superhuman performance of a large language model on the reasoning tasks of a physician</title>
<link>https://arxiv.org/abs/2412.10849</link>
<guid>https://arxiv.org/abs/2412.10849</guid>
<content:encoded><![CDATA[
arXiv:2412.10849v2 Announce Type: replace-cross 
Abstract: A seminal paper published by Ledley and Lusted in 1959 introduced complex clinical diagnostic reasoning cases as the gold standard for the evaluation of expert medical computing systems, a standard that has held ever since. Here, we report the results of a physician evaluation of a large language model (LLM) on challenging clinical cases against a baseline of hundreds of physicians. We conduct five experiments to measure clinical reasoning across differential diagnosis generation, display of diagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management reasoning, all adjudicated by physician experts with validated psychometrics. We then report a real-world study comparing human expert and AI second opinions in randomly-selected patients in the emergency room of a major tertiary academic medical center in Boston, MA. We compared LLMs and board-certified physicians at three predefined diagnostic touchpoints: triage in the emergency room, initial evaluation by a physician, and admission to the hospital or intensive care unit. In all experiments--both vignettes and emergency room second opinions--the LLM displayed superhuman diagnostic and reasoning abilities, as well as continued improvement from prior generations of AI clinical decision support. Our study suggests that LLMs have achieved superhuman performance on general medical diagnostic and management reasoning, fulfilling the vision put forth by Ledley and Lusted, and motivating the urgent need for prospective trials.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Vision-Language Alignment with Minimal Human Supervision</title>
<link>https://arxiv.org/abs/2501.04568</link>
<guid>https://arxiv.org/abs/2501.04568</guid>
<content:encoded><![CDATA[
arXiv:2501.04568v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation of these image-text pairs is both time-consuming and computationally expensive. To address this challenge, we introduce SVP (Sampling-based Visual Projection), a novel framework that enhances vision-language alignment without relying on manually curated text-image pairs or preference annotation. SVP leverages a small set of manually selected images, self-captioning and a pre-trained grounding model as a feedback mechanism to elicit latent information in VLMs. We evaluate our approach across six key areas: captioning, referring, visual question answering, multitasking, hallucination control, and object recall. Results demonstrate significant improvements, including a 14 % average improvement in captioning tasks, up to 12 % increase in object recall, and significantly reduced hallucinations, while maintaining question-answering capabilities. Using SVP, a small VLM achieves hallucination reductions similar to a model five times larger, while a VLM with initially poor referring capabilities more than doubles its performance, approaching parity with a model twice its size.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling</title>
<link>https://arxiv.org/abs/2502.00814</link>
<guid>https://arxiv.org/abs/2502.00814</guid>
<content:encoded><![CDATA[
arXiv:2502.00814v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved considerable success in aligning large language models (LLMs) by modeling human preferences with a learnable reward model and employing a reinforcement learning algorithm to maximize the reward model's scores. However, these reward models are susceptible to exploitation through various superficial confounding factors, with length bias emerging as a particularly significant concern. Moreover, while the pronounced impact of length bias on preference modeling suggests that LLMs possess an inherent sensitivity to length perception, our preliminary investigations reveal that fine-tuned LLMs consistently struggle to adhere to explicit length instructions. To address these two limitations, we propose a novel framework wherein the reward model explicitly differentiates between human semantic preferences and response length requirements. Specifically, we introduce a $\textbf{R}$esponse-$\textbf{c}$onditioned $\textbf{B}$radley-$\textbf{T}$erry (Rc-BT) model that enhances the model's capability in length bias mitigating and length instruction following, through training on our augmented dataset. Furthermore, we propose the Rc-RM and Rc-DPO algorithm to leverage the Rc-BT model for reward modeling and direct policy optimization (DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to length instructions. Extensive experiments across various foundational models and datasets demonstrate the effectiveness and generalizability of our approach.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Context Length Scaling and Bounds for Language Models</title>
<link>https://arxiv.org/abs/2502.01481</link>
<guid>https://arxiv.org/abs/2502.01481</guid>
<content:encoded><![CDATA[
arXiv:2502.01481v3 Announce Type: replace-cross 
Abstract: Long Context Language Models have drawn great attention in the past few years. There has been work discussing the impact of long context on Language Model performance: some find that long irrelevant context could harm performance, while some experimentally summarize loss reduction by relevant long context as Scaling Laws. This calls for a more thorough understanding on how long context impacts Language Modeling. In this work, we (1) propose a clean and effective theoretical framework for explaining the impact of context length on Language Modeling, from an Intrinsic Space perspective; and (2) conduct experiments on natural language and synthetic data, validating our proposed theoretical assumptions and deductions. Our theoretical framework can provide practical insights such as establishing that training dataset size dictates an optimal context length and bounds context length scaling for certain cases. We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models. Code for our experiments is available at: https://github.com/JingzheShi/NLPCtlScalingAndBounds.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the true depth of LLMs</title>
<link>https://arxiv.org/abs/2502.02790</link>
<guid>https://arxiv.org/abs/2502.02790</guid>
<content:encoded><![CDATA[
arXiv:2502.02790v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities at the cost of high compute requirements. Recent studies have demonstrated that intermediate layers in LLMs can be removed or reordered without substantial accuracy loss; however, this insight has not yet been exploited to improve inference efficiency. Leveraging observed layer independence, we propose a novel method that groups consecutive layers into pairs evaluated in parallel, effectively restructuring the computational graph to enhance parallelism. Without requiring retraining or fine-tuning, this approach achieves an inference throughput improvement of 1.05x-1.20x on standard benchmarks, retaining 95\%-99\% of the original model accuracy. Empirical results demonstrate the practicality of this method in significantly reducing inference cost for large-scale LLM deployment. Additionally, we demonstrate that modest performance degradation can be substantially mitigated through lightweight fine-tuning, further enhancing the method's applicability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v2 Announce Type: replace-cross 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning</title>
<link>https://arxiv.org/abs/2502.11799</link>
<guid>https://arxiv.org/abs/2502.11799</guid>
<content:encoded><![CDATA[
arXiv:2502.11799v2 Announce Type: replace-cross 
Abstract: Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARS: Automatic Routing Solver with Large Language Models</title>
<link>https://arxiv.org/abs/2502.15359</link>
<guid>https://arxiv.org/abs/2502.15359</guid>
<content:encoded><![CDATA[
arXiv:2502.15359v3 Announce Type: replace-cross 
Abstract: Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of practical constraints, making manual solver design both knowledge-intensive and time-consuming. Although there is increasing interest in automating the design of routing algorithms, existing research has explored only a limited array of VRP variants and fails to adequately address the complex and prevalent constraints encountered in real-world situations. To fill this gap, this paper introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24 attributes, for evaluating the effectiveness of automatic routing solvers in addressing complex constraints. Along with RoutBench, we present the Automatic Routing Solver (ARS), which employs Large Language Model (LLM) agents to enhance a backbone algorithm framework by automatically generating constraint-aware heuristic code, based on problem descriptions and several representative constraints selected from a database. Our experiments show that ARS outperforms state-of-the-art LLM-based methods and commonly used solvers, automatically solving 91.67% of common VRPs and achieving at least a 30% improvement across all benchmarks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.16565</link>
<guid>https://arxiv.org/abs/2502.16565</guid>
<content:encoded><![CDATA[
arXiv:2502.16565v2 Announce Type: replace-cross 
Abstract: Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore</title>
<link>https://arxiv.org/abs/2502.20034</link>
<guid>https://arxiv.org/abs/2502.20034</guid>
<content:encoded><![CDATA[
arXiv:2502.20034v2 Announce Type: replace-cross 
Abstract: Recently, Large Vision-Language Models (LVLMs) show remarkable performance across various domains. However, these models suffer from object hallucination. This study revisits the previous claim that the primary cause of such hallucination lies in the limited representational capacity of the vision encoder. Our analysis reveals that the capacity of the vision encoder itself is already adequate for detecting object hallucination. Based on this insight, we propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective evaluation metric that enhances object-level granularity by incorporating text embeddings at the noun level. Evaluations on the OHD-Caps benchmark show that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a large margin of 39.6\% without additional training. We further demonstrate that F-CLIPScore-based data filtering reduces object hallucination in LVLMs (4.9\% in POPE).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.20854</link>
<guid>https://arxiv.org/abs/2502.20854</guid>
<content:encoded><![CDATA[
arXiv:2502.20854v3 Announce Type: replace-cross 
Abstract: The integration of Knowledge Graphs (KGs) into the Retrieval Augmented Generation (RAG) framework has attracted significant interest, with early studies showing promise in mitigating hallucinations and improving model accuracy. However, a systematic understanding and comparative analysis of the rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the foundation for systematically answering the question of when and how to use KG-RAG by analyzing their performance in various application scenarios associated with different technical configurations. After outlining the mind map using KG-RAG framework and summarizing its popular pipeline, we conduct a pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG methods across 9 datasets in diverse domains and scenarios, analyzing the impact of 9 KG-RAG configurations in combination with 17 LLMs, and combining Metacognition with KG-RAG as a pilot attempt. Our results underscore the critical role of appropriate application conditions and optimal configurations of KG-RAG components.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</title>
<link>https://arxiv.org/abs/2503.01711</link>
<guid>https://arxiv.org/abs/2503.01711</guid>
<content:encoded><![CDATA[
arXiv:2503.01711v4 Announce Type: replace-cross 
Abstract: Personalized product search aims to retrieve and rank items that match users' preferences and search intent. Despite their effectiveness, existing approaches typically assume that users' query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding</title>
<link>https://arxiv.org/abs/2503.13139</link>
<guid>https://arxiv.org/abs/2503.13139</guid>
<content:encoded><![CDATA[
arXiv:2503.13139v2 Announce Type: replace-cross 
Abstract: Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to "finding a needle in a haystack." To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation</title>
<link>https://arxiv.org/abs/2503.18225</link>
<guid>https://arxiv.org/abs/2503.18225</guid>
<content:encoded><![CDATA[
arXiv:2503.18225v2 Announce Type: replace-cross 
Abstract: Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaintainCoder: Maintainable Code Generation Under Dynamic Requirements</title>
<link>https://arxiv.org/abs/2503.24260</link>
<guid>https://arxiv.org/abs/2503.24260</guid>
<content:encoded><![CDATA[
arXiv:2503.24260v2 Announce Type: replace-cross 
Abstract: Modern code generation has made significant strides in functional correctness and execution efficiency. However, these systems often overlook a critical dimension in real-world software development: \textit{maintainability}. To handle dynamic requirements with minimal rework, we propose \textbf{MaintainCoder} as a pioneering solution. It integrates the Waterfall model, design patterns, and multi-agent collaboration to systematically enhance cohesion, reduce coupling, achieving clear responsibility boundaries and better maintainability. We also introduce \textbf{MaintainBench}, a benchmark comprising requirement changes and novel dynamic metrics on maintenance efforts. Experiments demonstrate that existing code generation methods struggle to meet maintainability standards when requirements evolve. In contrast, MaintainCoder improves dynamic maintainability metrics by more than 60\% with even higher correctness of initial codes. Furthermore, while static metrics fail to accurately reflect maintainability and even contradict each other, our proposed dynamic metrics exhibit high consistency. Our work not only provides the foundation for maintainable code generation, but also highlights the need for more realistic and comprehensive code generation research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
arXiv:2503.24370v2 Announce Type: replace-cross 
Abstract: Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation</title>
<link>https://arxiv.org/abs/2504.04453</link>
<guid>https://arxiv.org/abs/2504.04453</guid>
<content:encoded><![CDATA[
arXiv:2504.04453v2 Announce Type: replace-cross 
Abstract: Unlocking the next generation of biotechnology and therapeutic innovation demands overcoming the inherent complexity and resource-intensity of conventional protein engineering methods. Recent GenAI-powered computational techniques often rely on the availability of the target protein's 3D structures and specific binding sites to generate high-affinity binders, constraints exhibited by models such as AlphaProteo and RFdiffusion. In this work, we explore the use of Protein Language Models (pLMs) for high-affinity binder generation. We introduce Prot42, a novel family of Protein Language Models (pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing deep evolutionary, structural, and functional insights through an advanced auto-regressive, decoder-only architecture inspired by breakthroughs in natural language processing, Prot42 dramatically expands the capabilities of computational protein design based on language only. Remarkably, our models handle sequences up to 8,192 amino acids, significantly surpassing standard limitations and enabling precise modeling of large proteins and complex multi-domain sequences. Demonstrating powerful practical applications, Prot42 excels in generating high-affinity protein binders and sequence-specific DNA-binding proteins. Our innovative models are publicly available, offering the scientific community an efficient and precise computational toolkit for rapid protein engineering.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signatures of human-like processing in Transformer forward passes</title>
<link>https://arxiv.org/abs/2504.14107</link>
<guid>https://arxiv.org/abs/2504.14107</guid>
<content:encoded><![CDATA[
arXiv:2504.14107v2 Announce Type: replace-cross 
Abstract: Modern AI models are increasingly being used as theoretical tools to study human cognition. One dominant approach is to evaluate whether human-derived measures are predicted by a model's output: that is, the end-product of a forward pass. However, recent advances in mechanistic interpretability have begun to reveal the internal processes that give rise to model outputs, raising the question of whether models might use human-like processing strategies. Here, we investigate the relationship between real-time processing in humans and layer-time dynamics of computation in Transformers, testing 20 open-source models in 6 domains. We first explore whether forward passes show mechanistic signatures of competitor interference, taking high-level inspiration from cognitive theories. We find that models indeed appear to initially favor a competing incorrect answer in the cases where we would expect decision conflict in humans. We then systematically test whether forward-pass dynamics predict signatures of processing in humans, above and beyond properties of the model's output probability distribution. We find that dynamic measures improve prediction of human processing measures relative to static final-layer measures. Moreover, across our experiments, larger models do not always show more human-like processing patterns. Our work suggests a new way of using AI models to study human cognition: not just as a black box mapping stimuli to responses, but potentially also as explicit processing models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2504.14858</link>
<guid>https://arxiv.org/abs/2504.14858</guid>
<content:encoded><![CDATA[
arXiv:2504.14858v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has become a widely adopted paradigm for enabling knowledge-grounded large language models (LLMs). However, standard RAG pipelines often fail to ensure that model reasoning remains consistent with the evidence retrieved, leading to factual inconsistencies or unsupported conclusions. In this work, we reinterpret RAG as Retrieval-Augmented Reasoning and identify a central but underexplored problem: \textit{Reasoning Misalignment}-the divergence between an LLM's internal reasoning trajectory and the evidential constraints provided by retrieval. To address this issue, we propose \textsc{AlignRAG}, a novel iterative framework grounded in Critique-Driven Alignment (CDA). At the heart of \textsc{AlignRAG} lies a \textit{contrastive critique synthesis} mechanism that generates retrieval-sensitive critiques while mitigating self-bias. This mechanism trains a dedicated retrieval-augmented \textit{Critic Language Model (CLM)} using labeled critiques that distinguish between evidence-aligned and misaligned reasoning. Alignment signals for supervision are obtained through self-supervised or externally guided labeling strategies. The resulting CLM is explicitly optimized for evidence sensitivity, enabling it to detect and revise reasoning errors during inference without relying solely on self-generated feedback. Empirical evaluations show that our 8B-parameter CLM improves performance over the Self-Refine baseline by 12.1\% on out-of-domain tasks and outperforms a standard 72B-parameter CLM by 2.2\%, while remaining compatible with existing RAG architectures as a plug-and-play module. Overall, AlignRAG offers a principled solution for aligning model reasoning with retrieved evidence, substantially improving the factual reliability and robustness of RAG systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v2 Announce Type: replace-cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
arXiv:2504.16828v2 Announce Type: replace-cross 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
arXiv:2505.00234v3 Announce Type: replace-cross 
Abstract: Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering--custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93% success on ALFWorld--surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Bias on English Language Learners in Automatic Scoring</title>
<link>https://arxiv.org/abs/2505.10643</link>
<guid>https://arxiv.org/abs/2505.10643</guid>
<content:encoded><![CDATA[
<div> bias, disparities, English Language Learners, automatic scoring systems, BERT

Summary:
The study investigated scoring biases and disparities in automatic scoring systems for middle school science assessments of English Language Learners (ELLs). By fine-tuning BERT with different training datasets, the researchers found no bias or disparities when the dataset size was sufficient (30,000 ELL responses or 1,000 ELL responses). However, concerns arose when the sample size was limited (200 ELL responses). Scoring accuracy was compared using Friedman tests, and Mean Score Gaps (MSGs) were measured to identify disparities between ELLs and non-ELLs. The study suggests that ensuring a large enough training dataset is crucial to avoid bias and disparities in scoring ELLs' written responses. <div>
arXiv:2505.10643v1 Announce Type: new 
Abstract: This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to scoring bias and disparities. We fine-tuned BERT with four datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced mixed dataset with equal representation of both groups. The study analyzed 21 assessment items: 10 items with about 30,000 ELL responses, five items with about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring accuracy (Acc) was calculated and compared to identify bias using Friedman tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and then calculated the differences in MSGs generated through both the human and AI models to identify the scoring disparities. We found that no AI bias and distorted disparities between ELLs and non-ELLs were found when the training dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could exist if the sample size is limited (ELL = 200).
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?</title>
<link>https://arxiv.org/abs/2505.10714</link>
<guid>https://arxiv.org/abs/2505.10714</guid>
<content:encoded><![CDATA[
<div> climate variables, foundation models, GeoGrid-Bench, geo-spatial data, scientific research
<br />
Summary: 
The article introduces GeoGrid-Bench, a benchmark created to evaluate foundation models' ability to comprehend geo-spatial data in a grid format. Geo-spatial datasets present unique challenges due to their dense numerical values, spatial and temporal dependencies, and multimodal representations. The benchmark encompasses real-world data on 16 climate variables across 150 locations and extended time frames, with approximately 3,200 question-answer pairs generated from expert-curated templates. These tasks simulate scenarios encountered by scientists, ranging from basic queries to complex spatiotemporal comparisons. Vision-language models demonstrated superior performance overall, with a detailed analysis of strengths and limitations in various geo-spatial tasks provided. GeoGrid-Bench aims to enhance understanding of how foundation models can support scientific research in the analysis of geo-spatial data. 
<br /><br /> <div>
arXiv:2505.10714v1 Announce Type: new 
Abstract: We present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundation models can support scientific research in this domain, GeoGrid-Bench features large-scale, real-world data covering 16 climate variables across 150 locations and extended time frames. The benchmark includes approximately 3,200 question-answer pairs, systematically generated from 8 domain expert-curated templates to reflect practical tasks encountered by human scientists. These range from basic queries at a single location and time to complex spatiotemporal comparisons across regions and periods. Our evaluation reveals that vision-language models perform best overall, and we provide a fine-grained analysis of the strengths and limitations of different foundation models in different geo-spatial tasks. This benchmark offers clearer insights into how foundation models can be effectively applied to geo-spatial data analysis and used to support scientific research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment</title>
<link>https://arxiv.org/abs/2505.10717</link>
<guid>https://arxiv.org/abs/2505.10717</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, clinical settings, biomedical domain adaptation, clinical data, MediPhi collection

Summary: 
Small language models (SLMs) like GPT-4 offer a cost-effective alternative to large language models but require adaptation to the biomedical domain, a challenging task due to limited capacity. To address this, a new framework has been proposed for adapting SLMs into high-performing clinical models. The framework involves pre-instruction tuning of experts on medical and clinical corpora, model merging, and alignment of clinical tasks. The resulting MediPhi collection of SLMs outperforms the base model on various clinical tasks, including medical entities, radiology reports, and ICD-10 coding. Additionally, the CLUE benchmark has been extended to CLUE+ to cover most clinical tasks, doubling its size. The expert models are unified into MediPhi via model merging, achieving gains across benchmarks. Furthermore, a synthetic dataset called MediFlow has been created for 14 medical NLP tasks, leading to further performance improvements through alignment and fine-tuning. <div>
arXiv:2505.10717v1 Announce Type: new 
Abstract: High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-enhanced semantic feature norms for 786 concepts</title>
<link>https://arxiv.org/abs/2505.10718</link>
<guid>https://arxiv.org/abs/2505.10718</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic feature norms, human conceptual knowledge, large language models, NOVA dataset, cognitive science research

Summary:
NOVA, a dataset combining human-generated feature norms with responses from large language models (LLMs), enhances concept/feature coverage without sacrificing quality. The dataset, NOVA: Norms Optimized Via AI, demonstrates higher feature density and overlap among concepts compared to human-only norms and word-embedding models. AI-enhanced norms also outperform traditional models in predicting semantic similarity judgments. The study showcases the richness of human conceptual knowledge beyond previous norm datasets. By validating LLM responses with reliable human judgments, researchers can leverage these powerful tools for cognitive science research.
<br /><br />Summary: 
- NOVA dataset combines human and AI-generated feature norms
- Higher feature density and overlap among concepts in NOVA dataset
- NOVA outperforms human-only norms and word-embedding models
- AI-enhanced norms excel in predicting semantic similarity judgments
- LLMs can be valuable tools for cognitive science research <div>
arXiv:2505.10718v1 Announce Type: new 
Abstract: Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracr-Injection: Distilling Algorithms into Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2505.10719</link>
<guid>https://arxiv.org/abs/2505.10719</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer architecture, RASP, tracr-injection, language model, symbolic abilities

Summary: 
The article discusses the formal characterization of symbolic abilities in the transformer architecture through a programming language called RASP. It highlights a mismatch between theoretical capabilities of transformers and their practical learnability from unsupervised data. The authors introduce tracr-injection, a method to distill RASP algorithms into pre-trained language models, demonstrating improved out-of-distribution performance compared to baseline. They showcase the injection of three algorithms into a language model, creating an interpretable subspace within the model's residual stream. The method decodes variables present in the RASP algorithm code, indicating a more symbolic mechanism at work in the model. The release of the experiment code allows for further exploration and replication of the results.<br /><br />Summary: <div>
arXiv:2505.10719v1 Announce Type: new 
Abstract: Motivated by the surge of large language models, there has been a push to formally characterize the symbolic abilities intrinsic to the transformer architecture. A programming language, called RASP, has been proposed, which can be directly compiled into transformer weights to implement these algorithms. However, the tasks that can be implemented in RASP are often uncommon to learn from natural unsupervised data, showing a mismatch between theoretical capabilities of the transformer architecture, and the practical learnability of these capabilities from unsupervised data. We propose tracr-injection, a method that allows us to distill algorithms written in RASP directly into a pre-trained language model. We showcase our method by injecting 3 different algorithms into a language model. We show how our method creates an interpretable subspace within the model's residual stream, which can be decoded into the variables present in the code of the RASP algorithm. Additionally, we found that the proposed method can improve out of distribution performance compared to our baseline, indicating that indeed a more symbolic mechanism is taking place in the inner workings of the model. We release the code used to run our experiments.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization</title>
<link>https://arxiv.org/abs/2505.10736</link>
<guid>https://arxiv.org/abs/2505.10736</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Prompt Optimization, Iterative evaluation data selection, Real-time Model Performance, Coreset Selection

Summary:
IPOMP is a new automated technique for optimizing prompts in Large Language Models (LLMs) that addresses the limitations of existing methods by selecting representative and diverse samples using semantic clustering and boundary analysis. It then iteratively refines the selected samples based on real-time model performance data. The evaluations on the BIG-bench dataset demonstrate a significant improvement in effectiveness (1.6% to 5.3%) and stability (at least 57%) compared to state-of-the-art baseline methods, with minimal computational overhead. The results also show that the real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods. Overall, IPOMP offers a more efficient and effective solution for prompt optimization in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.10736v1 Announce Type: new 
Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2505.10740</link>
<guid>https://arxiv.org/abs/2505.10740</guid>
<content:encoded><![CDATA[
<div> shared task, multilingual claim retrieval, machine learning, online disinformation, fact-checking 
Summary: 
The article discusses a shared task on multilingual claim retrieval at SemEval 2025, focusing on identifying fact-checked claims across different languages, addressing the neglect of low-resource languages in the field of online disinformation. The task includes a monolingual track and a crosslingual track, with 179 participants and 52 test submissions. The best-performing systems and common approaches in both subtracks are reported, providing insights into multilingual claim retrieval and automated fact-checking. The shared task dataset and participating systems aim to support future research in this field. <div>
arXiv:2505.10740v1 Announce Type: new 
Abstract: The rapid spread of online disinformation presents a global challenge, and machine learning has been widely explored as a potential solution. However, multilingual settings and low-resource languages are often neglected in this field. To address this gap, we conducted a shared task on multilingual claim retrieval at SemEval 2025, aimed at identifying fact-checked claims that match newly encountered claims expressed in social media posts across different languages. The task includes two subtracks: (1) a monolingual track, where social posts and claims are in the same language, and (2) a crosslingual track, where social posts and claims might be in different languages. A total of 179 participants registered for the task contributing to 52 test submissions. 23 out of 31 teams have submitted their system papers. In this paper, we report the best-performing systems as well as the most common and the most effective approaches across both subtracks. This shared task, along with its dataset and participating systems, provides valuable insights into multilingual claim retrieval and automated fact-checking, supporting future research in this field.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranked Voting based Self-Consistency of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10772</link>
<guid>https://arxiv.org/abs/2505.10772</guid>
<content:encoded><![CDATA[
<div> Ranked voting, chain-of-thought reasoning, self-consistency, multiple-choice, open-ended <br />
Summary: 

This work introduces a novel approach to chain-of-thought reasoning by generating ranked answers and using ranked voting methods for enhanced reliability of self-consistency. The proposed method outperforms existing techniques by considering multiple potential answers in each reasoning process and incorporating them into the voting process. Three ranked voting methods - Instant-runoff voting, Borda count voting, and mean reciprocal rank voting - are utilized and validated on six datasets comprising both multiple-choice and open-ended question-answering tasks. Results show significant improvement in reasoning performance, highlighting the effectiveness of leveraging ranked answers and implementing ranked voting. The code for the proposed method is available on GitHub for further exploration and experimentation. <br /> <div>
arXiv:2505.10772v1 Announce Type: new 
Abstract: Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest "self-consistency" among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. The code is available at https://github.com/szu-tera/RankedVotingSC.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Analysis of Base Model Choice for Reward Modeling</title>
<link>https://arxiv.org/abs/2505.10775</link>
<guid>https://arxiv.org/abs/2505.10775</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, human feedback, reward modeling, large language models, base model <br />
Summary: 
Reinforcement learning from human feedback and reward modeling are crucial for training large language models. The choice of the base model significantly affects reward modeling performance, with potential improvements of up to 14%. Benchmarks play a key role in predicting downstream performance, and a combination of select benchmarks can enhance model selection by 18% on average. Post-training steps also impact final performance, and using estimated data distributions can reduce prediction errors. <div>
arXiv:2505.10775v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection ($+$18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.10792</link>
<guid>https://arxiv.org/abs/2505.10792</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Finetune-RAG, factual accuracy, hallucinations, LLM-as-a-judge

Summary: 
Retrieval-Augmented Generation (RAG) aims to enhance the factuality of large language models (LLMs) by grounding their outputs in retrieved documents. However, challenges persist in achieving perfect retrieval, leading to the risk of hallucinations when irrelevant information is incorporated into LLM outputs. To address this, the Finetune-RAG approach is proposed, involving fine-tuning with a specialized dataset mimicking real-world imperfections. This method demonstrates a significant improvement in factual accuracy by 21.2% compared to the base model. Additionally, a novel evaluation pipeline called Bench-RAG is introduced to assess LLM performance in imperfect retrieval scenarios. The codebase and dataset associated with this research are made openly available for community use. 

<br /><br />Summary: <div>
arXiv:2505.10792v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose a Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets</title>
<link>https://arxiv.org/abs/2505.10798</link>
<guid>https://arxiv.org/abs/2505.10798</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, text extraction, AffilKG, dataset, social science research
<br />
Summary: 
The article introduces AffilKG, a collection of datasets that pair complete book scans with large, labeled knowledge graphs. These datasets feature affiliation graphs capturing relationships between Person and Organization entities, useful for studying social phenomena. The datasets enable benchmarking of extraction errors in graph-level analyses and validation of KG extraction methods for social science research. Preliminary experiments show varying model performance across datasets, highlighting AffilKG's importance in evaluating accuracy for downstream analysis. <div>
arXiv:2505.10798v1 Announce Type: new 
Abstract: When knowledge graphs (KGs) are automatically extracted from text, are they accurate enough for downstream analysis? Unfortunately, current annotated datasets can not be used to evaluate this question, since their KGs are highly disconnected, too small, or overly complex. To address this gap, we introduce AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six datasets that are the first to pair complete book scans with large, labeled knowledge graphs. Each dataset features affiliation graphs, which are simple KGs that capture Member relationships between Person and Organization entities -- useful in studies of migration, community interactions, and other social phenomena. In addition, three datasets include expanded KGs with a wider variety of relation types. Our preliminary experiments demonstrate significant variability in model performance across datasets, underscoring AffilKG's ability to enable two critical advances: (1) benchmarking how extraction errors propagate to graph-level analyses (e.g., community structure), and (2) validating KG extraction methods for real-world social science research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances</title>
<link>https://arxiv.org/abs/2505.10829</link>
<guid>https://arxiv.org/abs/2505.10829</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resource languages, Large Language Models, Retrieval-Augmented Generation, translation challenges, cultural preservation<br />
Summary:<br />
- The study examines the difficulties of translating low-resource languages using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). 
- Different model configurations were tested on Hakka translations, with the highest BLEU score of 31% achieved by RAG with Gemini 2.0.
- Model 4, combining retrieval and advanced language modeling, proved to be the most effective, enhancing lexical coverage and grammatical coherence.
- Model 3, a two-stage method combining dictionary outputs and Gemini 2.0, achieved a BLEU score of 26%, showcasing the value of iterative correction and the challenges posed by domain-specific expressions.
- Static dictionary-based approaches struggled with context-sensitive content, highlighting the limitations of relying solely on predefined resources.
<br /><br />Summary: This research delves into the complexities of translating low-resource languages by leveraging Large Language Models and Retrieval-Augmented Generation. The findings emphasize the importance of curated resources, domain knowledge, and ethical collaboration with local communities to improve translation accuracy and fluency while also supporting cultural preservation. <div>
arXiv:2505.10829v1 Announce Type: new 
Abstract: This study investigates the challenges of translating low-resource languages by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG). Various model configurations were tested on Hakka translations, with BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0). The best-performing model (Model 4) combined retrieval and advanced language modeling, improving lexical coverage, particularly for specialized or culturally nuanced terms, and enhancing grammatical coherence. A two-stage method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU score of 26%, highlighting iterative correction's value and the challenges of domain-specific expressions. Static dictionary-based approaches struggled with context-sensitive content, demonstrating the limitations of relying solely on predefined resources. These results emphasize the need for curated resources, domain knowledge, and ethical collaboration with local communities, offering a framework that improves translation accuracy and fluency while supporting cultural preservation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL</title>
<link>https://arxiv.org/abs/2505.10832</link>
<guid>https://arxiv.org/abs/2505.10832</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, adaptive thinking, AutoThink, reinforcement learning, scalability <br />
Summary: <br />
Large reasoning models (LRMs) excel in producing detailed reasoning sequences, but this can lead to unnecessary computational overhead. To address this, the concept of adaptive thinking is introduced to LRMs through the AutoThink framework. By dynamically deciding when to engage in explicit reasoning based on problem complexity, AutoThink optimizes the trade-off between accuracy and efficiency. By leveraging a stochastic trigger in the prompt, AutoThink learns to activate explicit reasoning only when necessary and default to succinct responses for simpler tasks. Through a multi-stage reinforcement learning approach, AutoThink improves accuracy by 6.4 percent while reducing token usage by 52 percent on a specific model. This scalable and adaptive reasoning paradigm enhances the performance of LRMs on various mathematical benchmarks, making it a valuable addition to the field of AI reasoning models. <br /> <div>
arXiv:2505.10832v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis ("...") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs</title>
<link>https://arxiv.org/abs/2505.10836</link>
<guid>https://arxiv.org/abs/2505.10836</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, event detection, multimodal models, generative models, supervised methods 

Summary: 
In this paper, the authors investigate the challenges of detecting events on social media platforms, where traditional unimodal systems struggle due to the rapid and multimodal nature of data dissemination. They compare various models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o and LLaVA. The study shows that multimodal approaches outperform unimodal models, but generative models, despite having a large number of parameters, fall behind supervised methods in terms of precision. It is also noted that generative models have difficulty accurately generating event classes, leading to lower performance compared to instruction-tuned models. Through error analysis, the authors find that generative models excel in handling common social media issues such as leet speak and text elongation, which are challenging for supervised approaches. <div>
arXiv:2505.10836v1 Announce Type: new 
Abstract: In this paper, we study the challenges of detecting events on social media, where traditional unimodal systems struggle due to the rapid and multimodal nature of data dissemination. We employ a range of models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o, and LLaVA. Additionally, we also study the effect of providing multimodal generative models (such as GPT-4o) with a single modality to assess their efficacy. Our results indicate that while multimodal approaches notably outperform unimodal counterparts, generative approaches despite having a large number of parameters, lag behind supervised methods in precision. Furthermore, we also found that they lag behind instruction-tuned models because of their inability to generate event classes correctly. During our error analysis, we discovered that common social media issues such as leet speak, text elongation, etc. are effectively handled by generative approaches but are hard to tackle using supervised approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?</title>
<link>https://arxiv.org/abs/2505.10862</link>
<guid>https://arxiv.org/abs/2505.10862</guid>
<content:encoded><![CDATA[
<div> clocks, multimodal large language models, GPT-4.1, fine-tuning, limitations

Summary:
Multimodal Large Language Models (MLLMs) struggle to tell the time on analog clocks, likely due to a lack of diverse clock images in their training data. In this study, the researchers investigate this issue using GPT-4.1 and explore whether fine-tuning can improve clock-reading capabilities. Results show progress in time-reading, indicating some level of learning. However, it is uncertain if the models truly understand time or simply recognize patterns in their training sets. Testing the models with various clocks highlights their limitations in abstracting and generalizing concepts. This study sheds light on the challenges MLLMs face in understanding and interpreting analog clocks, emphasizing the need for diverse and comprehensive training data for improved performance. 

<br /><br />Summary: <div>
arXiv:2505.10862v1 Announce Type: new 
Abstract: Multimodal Large Language Models which can answer complex questions on an image struggle to tell the time on analog clocks. This is probably due to the lack of images with clocks at different times in their training set. In this work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand why MLLMs fail to tell the time and whether fine-tuning can solve the problem. The results show how models are making progress in reading the time on analog clocks. But have they really learned to do it, or have they only learned patterns in their training datasets? In this work we put the models to the test with different clocks to illustrate the limitations of MLLMs to abstract and generalize.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate</title>
<link>https://arxiv.org/abs/2505.10870</link>
<guid>https://arxiv.org/abs/2505.10870</guid>
<content:encoded><![CDATA[
<div> Keyword: rule retrieval, Large Language Models, query augmentation, reasoning performance, relevance reestimation

Summary:
Self-Induction Augmented Retrieval (SIAR) addresses challenges of rule retrieval by utilizing Large Language Models (LLMs) to induce potential inferential rules that abstract underlying knowledge in queries, improving retrieval effectiveness. Rule Relevance ReEstimate (R3) method re-estimates the relevance of retrieved rules by aligning abstract knowledge with query facts, enhancing reasoning performance. The significant semantic gap between query facts and rule representations is overcome, leading to improved retrieval quality. SIAR's query augmentation with induced rules enhances retrieval accuracy. R3 assesses the helpfulness of retrieved rules for reasoning by instantiating abstract knowledge with query facts. Extensive experiments verify the effectiveness and versatility of SIAR and R3 in rule retrieval for downstream reasoning tasks. <div>
arXiv:2505.10870v1 Announce Type: new 
Abstract: This paper systematically addresses the challenges of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?</title>
<link>https://arxiv.org/abs/2505.10924</link>
<guid>https://arxiv.org/abs/2505.10924</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-driven interactions, Computer-Using Agents (CUAs), safety threats, defensive strategies, evaluation metrics 

Summary: 
AI-driven interactions in computing devices have evolved to create Computer-Using Agents (CUAs) capable of autonomous tasks, leading to new safety and security risks. Vulnerabilities in reasoning and the complexity of integrating software components pose challenges. This paper systematizes knowledge on safety and security threats of CUAs by defining suitable safety analysis, categorizing current threats, proposing defensive strategies, and summarizing evaluation metrics. It offers researchers a foundation to explore vulnerabilities and provides practitioners with guidance to design secure CUAs.<br /><br /> <div>
arXiv:2505.10924v1 Announce Type: new 
Abstract: Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents</title>
<link>https://arxiv.org/abs/2505.10936</link>
<guid>https://arxiv.org/abs/2505.10936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Chain-of-thought, Multi-agent systems, Collaboration prompting framework, Business workflow

Summary:
Large Language Models (LLMs) and multi-agent systems have shown great potential in enhancing reasoning capabilities. However, designing cross-domain prompts and token consumption pose challenges in collaboration and problem-solving tasks. To address these issues, the Cochain framework is proposed. Cochain integrates knowledge and prompts efficiently, using an integrated knowledge graph and prompts tree to enhance collaboration in business workflow tasks. Extensive evaluations show that Cochain outperforms existing methods in prompt engineering and multi-agent LLMs, with expert evaluations favoring the use of Cochain with a smaller model over GPT-4. Cochain presents a promising solution to improve collaboration and problem-solving in complex tasks. 

<br /><br />Summary: <div>
arXiv:2505.10936v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations</title>
<link>https://arxiv.org/abs/2505.10937</link>
<guid>https://arxiv.org/abs/2505.10937</guid>
<content:encoded><![CDATA[
<div> Keywords: large reasoning models, chain-of-thought processes, OmniThought dataset, reasoning verbosity, cognitive difficulty <br />
<br />
Summary: 
The article introduces the OmniThought dataset, comprising 2 million chain-of-thought processes generated by two large reasoning models (LRMs). Each process is annotated with Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores to describe the coherence and complexity of the reasoning. The dataset addresses the lack of comprehensive CoT resources and aims to enhance LRM training effectiveness. Experimentation with Qwen2.5 models demonstrates the positive impact of RV and CD scores. The dataset enables the training of high-performing LRMs with improved reasoning abilities and optimal CoT outputs. These contributions advance the development of LRMs for complex tasks, providing a valuable resource for researchers in the field. <br /><br /> <div>
arXiv:2505.10937v1 Announce Type: new 
Abstract: The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However, the advancement of LRMs is hindered by the lack of comprehensive CoT datasets. Current resources often fail to provide extensive reasoning problems with coherent CoT processes distilled from multiple teacher models and do not account for multifaceted properties describing the internal characteristics of CoTs. To address these challenges, we introduce OmniThought, a large-scale dataset featuring 2 million CoT processes generated and validated by two powerful LRMs as teacher models. Each CoT process in OmniThought is annotated with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which describe the appropriateness of CoT verbosity and cognitive difficulty level for models to comprehend these reasoning processes. We further establish a self-reliant pipeline to curate this dataset. Extensive experiments using Qwen2.5 models of various sizes demonstrate the positive impact of our proposed scores on LRM training effectiveness. Based on the proposed OmniThought dataset, we further train and release a series of high-performing LRMs, specifically equipped with stronger reasoning abilities and optimal CoT output length and difficulty level. Our contributions significantly enhance the development and training of LRMs for solving complex tasks.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate KV Cache Quantization with Outlier Tokens Tracing</title>
<link>https://arxiv.org/abs/2505.10938</link>
<guid>https://arxiv.org/abs/2505.10938</guid>
<content:encoded><![CDATA[
<div> quantization, Large Language Models, KV Cache, memory usage, throughput
<br />
The article discusses the use of quantization in Large Language Models (LLMs) to reduce memory usage and increase throughput during deployment. While KV Cache can assist in reducing recomputation during inference, it can also introduce additional memory overhead. The common practice of applying channel-wise quantization to Keys and token-wise quantization to Values may lead to accuracy issues for unusual tokens with unique characteristics. A simple method is proposed to identify and exclude these outlier tokens during decoding, resulting in significant accuracy improvements under 2-bit quantization. Experiments show a 6.4 times reduction in memory usage and a 2.3 times increase in throughput with this approach.
<br /><br />Summary: <div>
arXiv:2505.10938v1 Announce Type: new 
Abstract: The impressive capabilities of Large Language Models (LLMs) come at the cost of substantial computational resources during deployment. While KV Cache can significantly reduce recomputation during inference, it also introduces additional memory overhead. KV Cache quantization presents a promising solution, striking a good balance between memory usage and accuracy. Previous research has shown that the Keys are distributed by channel, while the Values are distributed by token. Consequently, the common practice is to apply channel-wise quantization to the Keys and token-wise quantization to the Values. However, our further investigation reveals that a small subset of unusual tokens exhibit unique characteristics that deviate from this pattern, which can substantially impact quantization accuracy. To address this, we develop a simple yet effective method to identify these tokens accurately during the decoding process and exclude them from quantization as outlier tokens, significantly improving overall accuracy. Extensive experiments show that our method achieves significant accuracy improvements under 2-bit quantization and can deliver a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction</title>
<link>https://arxiv.org/abs/2505.10939</link>
<guid>https://arxiv.org/abs/2505.10939</guid>
<content:encoded><![CDATA[
<div> General knowledge subtraction, modular framework, LoRA modules, zero-shot generalization, task-specific adaptations<br />
<br />
Summary: <br />
Large language models often struggle with zero-shot generalization, despite the introduction of modular approaches. However, the entanglement of general knowledge with task-specific adaptations continues to be a limiting factor. To address this issue, a modular framework that disentangles these components by utilizing task-specific LoRA modules alongside a general-domain LoRA is proposed. Through the process of general knowledge subtraction (GenKnowSub), residual modules are obtained that focus more exclusively on task-relevant information. By leveraging refined task-specific modules and the Arrow routing algorithm, the framework dynamically selects and combines modules for new inputs without additional training. Experimental studies on the Phi-3 model and standard Arrow baselines confirm performance gains in both monolingual and cross-lingual settings across various benchmarks. Additionally, experiments on weaker LLMs demonstrate the generalizability of GenKnowSub. <div>
arXiv:2505.10939v1 Announce Type: new 
Abstract: Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer</title>
<link>https://arxiv.org/abs/2505.10945</link>
<guid>https://arxiv.org/abs/2505.10945</guid>
<content:encoded><![CDATA[
<div> transfer learning, language models, multilingual, embeddings, cross-lingual understanding <br />
Summary: 
The paper introduces a new cross-lingual transfer technique called Semantic Aware Linear Transfer (SALT) to enhance the performance of Large Language Models (LLMs) when transferring them into target language-specific models. SALT recycles embeddings from target language Pre-trained Language Models (PLMs) to maintain deep representational strengths during transfer. By deriving unique regression lines based on vocabulary overlap between source and target languages, SALT handles non-overlapping tokens effectively. Experimental results demonstrate that SALT outperforms existing transfer methods, achieves lower loss, and converges faster during language adaptation. Additionally, SALT shows significant improvements in cross-lingual understanding tasks compared to other techniques, highlighting its effectiveness in enhancing the functionality of LLMs. The scalability of PLMs in enhancing different LLM architectures is also demonstrated through experiments. <br /> <div>
arXiv:2505.10945v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly incorporate multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model's embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies, to handle each non-overlapping token's embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods and achieves lower loss with accelerating faster convergence during language adaptation. Notably, SALT obtains remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs</title>
<link>https://arxiv.org/abs/2505.10948</link>
<guid>https://arxiv.org/abs/2505.10948</guid>
<content:encoded><![CDATA[
<div> Conceptual Blending Theory, Large Language Models, Prompt-Induced Transitions, Prompt-Induced Hallucinations, Artificial Intelligence <br />
Summary:
The study focuses on understanding the behaviors exhibited by Large Language Models (LLMs) through the lens of Conceptual Blending Theory (CBT). Using prompt-based methods, the researchers investigate Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH) to uncover similarities and differences between artificial and biological cognition. By bridging linguistics, neuroscience, and empirical AI research, the study highlights the potential for human-AI collaboration to inform cognitive science. The research suggests that prompt engineering can not only be used as a technical tool but also as a scientific method to delve into the deep structure of meaning. This interdisciplinary approach sheds light on the mechanisms behind the seemingly intelligent and personality-like behaviors of LLMs, offering insights into the future of cognitive science. <br /><br />Summary: <div>
arXiv:2505.10948v1 Announce Type: new 
Abstract: Large language models (LLMs), inspired by neuroscience, exhibit behaviors that often evoke a sense of personality and intelligence-yet the mechanisms behind these effects remain elusive. Here, we operationalize Conceptual Blending Theory (CBT) as an experimental framework, using prompt-based methods to reveal how LLMs blend and compress meaning. By systematically investigating Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we uncover structural parallels and divergences between artificial and biological cognition. Our approach bridges linguistics, neuroscience, and empirical AI research, demonstrating that human-AI collaboration can serve as a living prototype for the future of cognitive science. This work proposes prompt engineering not just as a technical tool, but as a scientific method for probing the deep structure of meaning itself.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio</title>
<link>https://arxiv.org/abs/2505.10975</link>
<guid>https://arxiv.org/abs/2505.10975</guid>
<content:encoded><![CDATA[
<div> E2E, neural approaches, multi-speaker, ASR, speech recognition, monaural

Summary:
This survey reviews recent developments in monaural multi-speaker automatic speech recognition (ASR) using end-to-end (E2E) neural approaches. It highlights architectural paradigms (SIMO vs. SISO) for pre-segmented audio and recent improvements based on these paradigms. The survey also discusses extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Methods are evaluated and compared across standard benchmarks to provide a comprehensive analysis. The survey concludes with a discussion of open challenges and future research directions for building robust and scalable multi-speaker ASR. 

<br /><br />Summary: <div>
arXiv:2505.10975v1 Announce Type: new 
Abstract: Monaural multi-speaker automatic speech recognition (ASR) remains challenging due to data scarcity and the intrinsic difficulty of recognizing and attributing words to individual speakers, particularly in overlapping speech. Recent advances have driven the shift from cascade systems to end-to-end (E2E) architectures, which reduce error propagation and better exploit the synergy between speech content and speaker identity. Despite rapid progress in E2E multi-speaker ASR, the field lacks a comprehensive review of recent developments. This survey provides a systematic taxonomy of E2E neural approaches for multi-speaker ASR, highlighting recent advances and comparative analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO) for pre-segmented audio, analyzing their distinct characteristics and trade-offs; (2) recent architectural and algorithmic improvements based on these two paradigms; (3) extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate and compare methods across standard benchmarks. We conclude with a discussion of open challenges and future research directions towards building robust and scalable multi-speaker ASR.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning</title>
<link>https://arxiv.org/abs/2505.11004</link>
<guid>https://arxiv.org/abs/2505.11004</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer language models, in-context learning, downstream tasks, training dynamics, AI security<br />
Summary:<br />
The study investigates in-context learning (ICL) in large-scale Transformer language models trained on next-token prediction with web-scale data. It aims to understand the mechanisms behind ICL and its implications. The research introduces investigative tasks and a novel method to systematically explore ICL using the Pythia scaling suite. By analyzing ICL performance on downstream tasks and delving into the residual stream's subspace, the study shows that ICL goes beyond mere data memorization but does not constitute the implementation of an independent symbolic algorithm. Findings highlight the impact of training dynamics, model capabilities, and aspects of mechanistic interpretability on ICL. The results offer insights for model developers on potential improvements and provide AI security practitioners with informed guidelines. <br /><br />Summary: <div>
arXiv:2505.11004v1 Announce Type: new 
Abstract: Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs</title>
<link>https://arxiv.org/abs/2505.11008</link>
<guid>https://arxiv.org/abs/2505.11008</guid>
<content:encoded><![CDATA[
<div> Keywords: Abugida languages, Transformer-based models, syllable sequence prediction, incomplete input types, Asian Language Treebank.

Summary:
This paper investigates syllable sequence prediction in Abugida languages using Transformer-based models. The study focuses on languages such as Bengali, Hindi, Khmer, Lao, Myanmar, and Thai from the ALT dataset. Various incomplete input types, including consonant sequences, vowel sequences, partial syllables, and masked syllables, were explored for complete syllable sequence reconstruction. Results show that consonant sequences play a crucial role in accurate syllable prediction, with high BLEU scores, while vowel sequences pose a greater challenge. The model demonstrates strong performance in tasks involving partial and masked syllable reconstruction, especially with consonant information and syllable masking. This research contributes to advancing the understanding of sequence prediction in Abugida languages and offers practical insights for applications like text prediction, spelling correction, and data augmentation in these scripts.
<br /><br />Summary: <div>
arXiv:2505.11008v1 Announce Type: new 
Abstract: This paper explores syllable sequence prediction in Abugida languages using Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer, Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We investigate the reconstruction of complete syllable sequences from various incomplete input types, including consonant sequences, vowel sequences, partial syllables (with random character deletions), and masked syllables (with fixed syllable deletions). Our experiments reveal that consonant sequences play a critical role in accurate syllable prediction, achieving high BLEU scores, while vowel sequences present a significantly greater challenge. The model demonstrates robust performance across tasks, particularly in handling partial and masked syllable reconstruction, with strong results for tasks involving consonant information and syllable masking. This study advances the understanding of sequence prediction for Abugida languages and provides practical insights for applications such as text prediction, spelling correction, and data augmentation in these scripts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11010</link>
<guid>https://arxiv.org/abs/2505.11010</guid>
<content:encoded><![CDATA[
<div> multi-turn dialogue, large language models, conversational AI, Review-Instruct framework, dataset construction

Summary:
The paper introduces the Review-Instruct framework for generating diverse and high-quality multi-turn dialogue data for large language models (LLMs) in conversational AI. The framework involves an iterative process with three agent roles: a Candidate, multiple Reviewers, and a Chairman, to refine instructions and enhance dialogue diversity and difficulty. By fine-tuning the LLaMA2-13B model on a new multi-turn dataset created using this framework, the study shows significant improvements in various evaluation metrics such as MT-Bench and MMLU-Pro. Ablation studies confirm the importance of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. This work showcases the potential of review-driven, multi-agent frameworks in generating high-quality conversational data at scale. 

<br /><br />Summary: <div>
arXiv:2505.11010v1 Announce Type: new 
Abstract: The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative "Ask-Respond-Review" process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StRuCom: A Novel Dataset of Structured Code Comments in Russian</title>
<link>https://arxiv.org/abs/2505.11026</link>
<guid>https://arxiv.org/abs/2505.11026</guid>
<content:encoded><![CDATA[
<div> Keywords: code documentation, machine learning, Russian, dataset, model fine-tuning

Summary:<br />
Structured code comments are crucial for code understanding and maintenance. Existing machine learning models struggle with generating Russian comments compared to English. To address this gap, the article introduces StRuCom, a new dataset specifically tailored for Russian code documentation. It comprises 153K examples sourced from Russian GitHub repositories and synthetic comments. The dataset ensures adherence to Python, Java, JavaScript, C#, and Go standards through automated validation. By fine-tuning Qwen2.5-Coder models on StRuCom, significant enhancements in chrf++ and BERTScore metrics are achieved compared to baseline models. This work showcases the importance of language-specific datasets in enhancing machine learning capabilities for code comment generation. The StRuCom dataset fills a critical gap in the field and sets the stage for further advancements in Russian code documentation using machine learning techniques. 

<br /><br />Summary: <div>
arXiv:2505.11026v1 Announce Type: new 
Abstract: Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom shows statistically significant improvements of chrf++ and BERTScore over baseline models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning</title>
<link>https://arxiv.org/abs/2505.11031</link>
<guid>https://arxiv.org/abs/2505.11031</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, ontologies, symbolic knowledge, natural language processing

Summary: 
- This article introduces a taxonomy to evaluate large language models' (LLMs) ability to process structured symbolic knowledge.
- The OntoURL benchmark is designed to assess LLMs' proficiency in handling ontologies, representing domain knowledge through concepts, relationships, and instances.
- OntoURL evaluates LLMs in three dimensions: understanding, reasoning, and learning, through 15 tasks derived from 40 ontologies across 8 domains.
- Experiments with 20 open-source LLMs reveal performance variations across models, tasks, and domains, indicating weaknesses in reasoning and learning tasks.
- Current LLMs demonstrate strength in understanding ontological knowledge but lack capability in reasoning and learning tasks. OntoURL is established as a crucial benchmark to advance the integration of LLMs with formal knowledge representations.

<br /><br />Summary: <div>
arXiv:2505.11031v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing tasks, yet their ability to process structured symbolic knowledge remains underexplored. To address this gap, we propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the first comprehensive benchmark designed to systematically evaluate LLMs' proficiency in handling ontologies -- formal, symbolic representations of domain knowledge through concepts, relationships, and instances. Based on the proposed taxonomy, OntoURL systematically assesses three dimensions: understanding, reasoning, and learning through 15 distinct tasks comprising 58,981 questions derived from 40 ontologies across 8 domains. Experiments with 20 open-source LLMs reveal significant performance differences across models, tasks, and domains, with current LLMs showing proficiency in understanding ontological knowledge but substantial weaknesses in reasoning and learning tasks. These findings highlight fundamental limitations in LLMs' capability to process symbolic knowledge and establish OntoURL as a critical benchmark for advancing the integration of LLMs with formal knowledge representations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMEO: Collection of Multilingual Emotional Speech Corpora</title>
<link>https://arxiv.org/abs/2505.11051</link>
<guid>https://arxiv.org/abs/2505.11051</guid>
<content:encoded><![CDATA[
<div> Keywords: CAMEO, emotional speech datasets, emotion recognition, speech-related tasks, Hugging Face platform

Summary: 
CAMEO is a curated collection of multilingual emotional speech datasets aimed at facilitating research in emotion recognition and other speech-related tasks. The main goals of CAMEO are to provide easy access to data, ensure reproducibility of results, and establish a standardized benchmark for evaluating speech emotion recognition systems across various emotional states and languages. The paper outlines the dataset selection criteria, the curation and normalization process, and presents performance results for different models. The collection, metadata, and leaderboard can be accessed through the Hugging Face platform. <div>
arXiv:2505.11051v1 Announce Type: new 
Abstract: This paper presents CAMEO -- a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a standardized benchmark for evaluating speech emotion recognition (SER) systems across different emotional states and languages. The paper describes the dataset selection criteria, the curation and normalization process, and provides performance results for several models. The collection, along with metadata, and a leaderboard, is publicly available via the Hugging Face platform.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLEUBERI: BLEU is a surprisingly effective reward for instruction following</title>
<link>https://arxiv.org/abs/2505.11080</link>
<guid>https://arxiv.org/abs/2505.11080</guid>
<content:encoded><![CDATA[
<div> Keywords: reward models, LLMs, instruction-following datasets, BLEU, BLEUBERI

Summary: 
BLEU, a basic string-matching metric, has been shown to match strong reward models in agreement with human preferences on instruction-following datasets. Based on this discovery, BLEUBERI, a method utilizing challenging instructions and Group Relative Policy Optimization with BLEU as a reward function, proves to be competitive with reward model-guided RL on instruction-following benchmarks. Human evaluation confirms the quality of BLEUBERI model outputs to be comparable to reward model-aligned models. Additionally, BLEUBERI models are more factually grounded than other methods. Access to high-quality reference outputs enables string matching-based metrics to serve as cost-effective proxies for reward models during alignment. The code and data for BLEUBERI are available at https://github.com/lilakk/BLEUBERI.<br /><br />Summary: <div>
arXiv:2505.11080v1 Announce Type: new 
Abstract: Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Evaluation for Generated Patent Claims</title>
<link>https://arxiv.org/abs/2505.11095</link>
<guid>https://arxiv.org/abs/2505.11095</guid>
<content:encoded><![CDATA[
<div> Patent claims, automation, large language models, evaluation metrics, benchmark <br />
<br />
Summary: 
Researchers have developed a benchmark, Patent-CE, to evaluate patent claims automatically generated by large language models. The benchmark includes comparative evaluations by patent experts on key criteria such as feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. A novel multi-dimensional evaluation method, PatClaimEval, is proposed to achieve the highest correlation with human expert assessments. This approach addresses the inconsistencies found in existing automated evaluation metrics. The research aims to improve the accuracy of assessing automated patent claim generation systems, reducing barriers for small enterprises in the patent drafting process. <div>
arXiv:2505.11095v1 Announce Type: new 
Abstract: Patent claims define the scope of protection and establish the legal boundaries of an invention. Drafting these claims is a complex and time-consuming process that usually requires the expertise of skilled patent attorneys, which can form a large access barrier for many small enterprises. To solve these challenges, researchers have investigated the use of large language models (LLMs) for automating patent claim generation. However, existing studies highlight inconsistencies between automated evaluation metrics and human expert assessments. To bridge this gap, we introduce Patent-CE, the first comprehensive benchmark for evaluating patent claims. Patent-CE includes comparative claim evaluations annotated by patent experts, focusing on five key criteria: feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. Additionally, we propose PatClaimEval, a novel multi-dimensional evaluation method specifically designed for patent claims. Our experiments demonstrate that PatClaimEval achieves the highest correlation with human expert evaluations across all assessment criteria among all tested metrics. This research provides the groundwork for more accurate evaluations of automated patent claim generation systems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Reasoning can Improve Factuality in Large Language Models</title>
<link>https://arxiv.org/abs/2505.11140</link>
<guid>https://arxiv.org/abs/2505.11140</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, reasoning traces, test-time compute, token budgets, factual accuracy

Summary: 
- The study focuses on examining the reasoning capabilities of large language models (LLMs) in open-domain question-answering scenarios.
- Reasoning traces from advanced models like QwQ-32B and DeepSeek-R1-671B are distilled and fine-tuned using knowledge graph paths.
- Smaller reasoning models show improved factual accuracy compared to larger instruction-tuned models.
- Adding test-time compute and token budgets results in consistent 2-8% improvement in factual accuracy.
- The study releases all experimental artifacts for further research.<br /><br /> <div>
arXiv:2505.11140v1 Announce Type: new 
Abstract: Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization</title>
<link>https://arxiv.org/abs/2505.11166</link>
<guid>https://arxiv.org/abs/2505.11166</guid>
<content:encoded><![CDATA[
<div> framework, SoLoPO, long-context preference optimization, short-context PO, short-to-long reward alignment <br />
<br />
Summary: The article introduces the SoLoPO framework, which aims to improve the utilization of long-context information in large language models by separating preference optimization into short-context PO and SoLo-RA. Short-context PO enhances contextual knowledge utilization with preference pairs from short contexts, while SoLo-RA promotes reward score consistency in responses conditioned on both short and long contexts. The framework is compatible with existing preference optimization algorithms and improves data construction and training efficiency. Experimental results demonstrate enhanced length and domain generalization abilities across various benchmarks, along with improved computational and memory efficiency. <div>
arXiv:2505.11166v1 Announce Type: new 
Abstract: Despite advances in pretraining with extended context lengths, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng $\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency utilization for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline</title>
<link>https://arxiv.org/abs/2505.11177</link>
<guid>https://arxiv.org/abs/2505.11177</guid>
<content:encoded><![CDATA[
<div> OCR, multilingual information extraction, image-based documents, language model APIs, document comprehension <br />
<br />
Summary: 
This paper introduces a complete suite for extracting and processing multilingual information from image-based documents. Using Optical Character Recognition (OCR) such as Tesseract, the system extracts text in languages like English, Hindi, and Tamil. It then employs large language model APIs like Gemini for tasks such as cross-lingual translation, abstractive summarization, and re-translation into a desired language. Additional modules for sentiment analysis, topic classification, and date extraction enhance document understanding. The research presents a practical application of libraries, models, and APIs to bridge language barriers and improve access to information in image media across diverse linguistic settings. <div>
arXiv:2505.11177v1 Announce Type: new 
Abstract: This paper presents an end-to-end suite for multilingual information extraction and processing from image-based documents. The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil, and then a pipeline involving large language model APIs (Gemini) for cross-lingual translation, abstractive summarization, and re-translation into a target language. Additional modules add sentiment analysis (TensorFlow), topic classification (Transformers), and date extraction (Regex) for better document comprehension. Made available in an accessible Gradio interface, the current research shows a real-world application of libraries, models, and APIs to close the language gap and enhance access to information in image media across different linguistic environments
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoPE: The Counting Power of Transformers with No Positional Encodings</title>
<link>https://arxiv.org/abs/2505.11199</link>
<guid>https://arxiv.org/abs/2505.11199</guid>
<content:encoded><![CDATA[
<div> expressiveness, transformers, Positional Encodings, NoPE-transformers, Diophantine equations 
Summary: 
Average Hard Attention NoPE-Transformers (NoPE-AHATs) are shown to be surprisingly expressive, able to express counting languages corresponding to nonnegative integer solutions to multivariate polynomial equations. These transformers can reason about undecidable problems, as their expressibility corresponds to finite unions of sets of nonnegative integer solutions to systems of multivariate polynomial inequations. NoPE-transformers can handle complex counting properties surpassing models like simplified counter machines and Petri nets but fail to express simple properties like PARITY. Analyzing NoPE-transformers is undecidable, presenting challenges in classifying all input strings into one class. Additionally, a counting language exists that is expressible in the circuit complexity class TC$^0 but not by average hard attention transformers even with arbitrary Positional Encodings, resolving an open problem. <div>
arXiv:2505.11199v1 Announce Type: new 
Abstract: Positional Encodings (PEs) seem to be indispensable for ensuring expressiveness of transformers; without them attention transformers reduce to a bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard attention mechanisms were very recently shown to only be able to express regular languages, i.e., with limited counting ability. This paper shows that, with average hard attention mechanisms, NoPE-transformers are still surprisingly expressive: they can express counting languages corresponding to nonnegative integer solutions to multivariate polynomial equations (i.e. Diophantine equations), reasoning about which is well-known to be undecidable. In fact, we provide a precise characterization of languages expressible by Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of sets of nonnegative integer solutions to systems of multivariate polynomial inequations. We obtain several interesting consequences of our characterization. Firstly, NoPE-transformers can express counting properties that are far more complex than established models like simplified counter machines and Petri nets, but cannot express a very simple counting property of PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable, e.g., whether a given NoPE transformer classifies all input strings in one class. To complement our results, we exhibit a counting language that is not expressible by average hard attention transformers even with arbitrary PEs but is expressible in the circuit complexity class TC$^0$, answering an open problem.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2505.11225</link>
<guid>https://arxiv.org/abs/2505.11225</guid>
<content:encoded><![CDATA[
<div> Keywords: efficient test-time scaling, large language models, history-aware policy optimization, concise reasoning abilities, math benchmarks

Summary:<br /><br />
- The article introduces a new approach called History-Aware Policy Optimization (HAPO) for efficient test-time scaling of large language models (LLMs).
- HAPO tracks historical information from previous encounters to incentivize the discovery of more concise correct solutions.
- HAPO employs a novel length reward function that encourages the generation of shorter responses while balancing correctness.
- By combining length and correctness rewards, HAPO optimizes LLMs for both efficiency and accuracy.
- Experiment results show that HAPO successfully induces LLMs' concise reasoning abilities, leading to significant length reductions with minimal accuracy drops on various math benchmarks. 

Summary: <div>
arXiv:2505.11225v1 Announce Type: new 
Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models</title>
<link>https://arxiv.org/abs/2505.11271</link>
<guid>https://arxiv.org/abs/2505.11271</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, semantic caching, efficient information reuse, computational cost, real-time AI assistants

Summary:
This paper introduces a semantic caching approach for efficiently storing and reusing contextual summaries in Large Language Models (LLMs) used for question-answering and generation tasks. By reusing intermediate summaries, redundant computations can be reduced by up to 50-60%, leading to lower computational overhead, memory usage, and network bandwidth in distributed systems. The method maintains answer accuracy comparable to full document processing, as shown in experiments on various datasets including NaturalQuestions and TriviaQA. This approach strikes a balance between computational cost and response quality, making it crucial for real-time AI assistants deployed across edge and cloud platforms. <div>
arXiv:2505.11271v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed across edge and cloud platforms for real-time question-answering and retrieval-augmented generation. However, processing lengthy contexts in distributed systems incurs high computational overhead, memory usage, and network bandwidth. This paper introduces a novel semantic caching approach for storing and reusing intermediate contextual summaries, enabling efficient information reuse across similar queries in LLM-based QA workflows. Our method reduces redundant computations by up to 50-60% while maintaining answer accuracy comparable to full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a synthetic ArXiv dataset. This approach balances computational cost and response quality, critical for real-time AI assistants.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs</title>
<link>https://arxiv.org/abs/2505.11277</link>
<guid>https://arxiv.org/abs/2505.11277</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, retrieval-augmented reasoning, knowledge refinement, multi-hop reasoning

Summary:
AutoRefine is a reinforcement learning post-training framework that enhances the reasoning capabilities of large language models by introducing a "search-and-refine-during-think" paradigm. It includes explicit knowledge refinement steps between search calls, allowing the model to iteratively filter, distill, and organize information before generating an answer. Tailored retrieval-specific rewards are used alongside answer correctness rewards to optimize the model's performance. Experimental results on both single-hop and multi-hop question-answering benchmarks show that AutoRefine outperforms existing approaches, especially in complex, multi-hop reasoning scenarios. The framework demonstrates the ability to issue frequent, high-quality searches and effectively synthesize evidence for accurate reasoning. <div>
arXiv:2505.11277v1 Announce Type: new 
Abstract: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal fine-tuning for early risk detection</title>
<link>https://arxiv.org/abs/2505.11280</link>
<guid>https://arxiv.org/abs/2505.11280</guid>
<content:encoded><![CDATA[
<div> Keywords: Early Risk Detection, Web, Multi-objective approach, Transformer-based models, Temporal fine-tuning

Summary: 
Early Risk Detection (ERD) on the Web involves identifying users facing social and health issues promptly, optimizing precision and minimizing detection delay. Standard classification metrics may not be sufficient, so ERDE(theta) metric is used to consider precision and delay. A multi-objective approach is employed, prioritizing classification performance and incorporating a separate criterion for decision time. A new strategy called temporal fine-tuning is proposed, utilizing transformer-based models and explicitly incorporating time in the learning process. The method allows for analyzing complete user post histories, considering different contexts, and evaluating training performance using temporal metrics. Competitive results were achieved in depression and eating disorders tasks for the Spanish language. Temporal fine-tuning optimized decisions by considering context and time progress. By leveraging transformer models effectively, ERD can be addressed by combining precision and speed as a single objective. 

<br /><br />Summary: <div>
arXiv:2505.11280v1 Announce Type: new 
Abstract: Early Risk Detection (ERD) on the Web aims to identify promptly users facing social and health issues. Users are analyzed post-by-post, and it is necessary to guarantee correct and quick answers, which is particularly challenging in critical scenarios. ERD involves optimizing classification precision and minimizing detection delay. Standard classification metrics may not suffice, resorting to specific metrics such as ERDE(theta) that explicitly consider precision and delay. The current research focuses on applying a multi-objective approach, prioritizing classification performance and establishing a separate criterion for decision time. In this work, we propose a completely different strategy, temporal fine-tuning, which allows tuning transformer-based models by explicitly incorporating time within the learning process. Our method allows us to analyze complete user post histories, tune models considering different contexts, and evaluate training performance using temporal metrics. We evaluated our proposal in the depression and eating disorders tasks for the Spanish language, achieving competitive results compared to the best models of MentalRiskES 2023. We found that temporal fine-tuning optimized decisions considering context and time progress. In this way, by properly taking advantage of the power of transformers, it is possible to address ERD by combining precision and speed as a single objective.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Subphonemes in Morphology Models</title>
<link>https://arxiv.org/abs/2505.11297</link>
<guid>https://arxiv.org/abs/2505.11297</guid>
<content:encoded><![CDATA[
<div> transformers, morphological inflection tasks, phonological feature encoding, phoneme embeddings, subphonemic feature acquisition <br />
<br />
The article explores the limitations of transformer models in generalizing across languages and morphological rules in morphological inflection tasks. It suggests that the models' inability to capture implicit phonological and subphonemic phenomena could be a contributing factor. The study introduces a language-agnostic probing method to examine phonological feature encoding in transformers trained on phonemes across multiple languages. The results indicate that local phonological features such as final-obstruent devoicing in Turkish are well represented in phoneme embeddings, while long-distance dependencies like vowel harmony are better captured in the transformer's encoder. The findings shed light on the importance of considering subphonemic feature acquisition when training morphological models, offering insights for improving language generalization and morphological rule application in transformer-based systems. <br /><br />Summary: <div>
arXiv:2505.11297v1 Announce Type: new 
Abstract: Transformers have achieved state-of-the-art performance in morphological inflection tasks, yet their ability to generalize across languages and morphological rules remains limited. One possible explanation for this behavior can be the degree to which these models are able to capture implicit phenomena at the phonological and subphonemic levels. We introduce a language-agnostic probing method to investigate phonological feature encoding in transformers trained directly on phonemes, and perform it across seven morphologically diverse languages. We show that phonological features which are local, such as final-obstruent devoicing in Turkish, are captured well in phoneme embeddings, whereas long-distance dependencies like vowel harmony are better represented in the transformer's encoder. Finally, we discuss how these findings inform empirical strategies for training morphological models, particularly regarding the role of subphonemic feature acquisition.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision</title>
<link>https://arxiv.org/abs/2505.11336</link>
<guid>https://arxiv.org/abs/2505.11336</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, academic writing, research communication, human-AI collaboration, XtraGPT

Summary: 
XtraGPT proposes a human-AI collaboration framework for academic paper revision, addressing the limitations of existing systems in supporting high-quality scientific writing. A dataset of 7,040 research papers annotated with instruction-response pairs is introduced, enabling context-aware, instruction-guided writing assistance through XtraGPT, a suite of open-source LLMs. These models significantly outperform baselines and approach proprietary systems' quality, as validated through automated preference assessments and human evaluations. The framework supports sophisticated demands of research communication, such as conceptual coherence, and facilitates the iterative and revision-driven nature of academic writing. <div>
arXiv:2505.11336v1 Announce Type: new 
Abstract: Despite the growing adoption of large language models (LLMs) in academic workflows, their capabilities remain limited when it comes to supporting high-quality scientific writing. Most existing systems are designed for general-purpose scientific text generation and fail to meet the sophisticated demands of research communication beyond surface-level polishing, such as conceptual coherence across sections. Furthermore, academic writing is inherently iterative and revision-driven, a process not well supported by direct prompting-based paradigms. To address these scenarios, we propose a human-AI collaboration framework for academic paper revision. We first introduce a comprehensive dataset of 7,040 research papers from top-tier venues annotated with over 140,000 instruction-response pairs that reflect realistic, section-level scientific revisions. Building on the dataset, we develop XtraGPT, the first suite of open-source LLMs, designed to provide context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B parameters. Extensive experiments validate that XtraGPT significantly outperforms same-scale baselines and approaches the quality of proprietary systems. Both automated preference assessments and human evaluations confirm the effectiveness of our models in improving scientific drafts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11341</link>
<guid>https://arxiv.org/abs/2505.11341</guid>
<content:encoded><![CDATA[
<div> Keywords: Critical Questions Generation, Dataset, Automatic Evaluation, Large Language Models, Benchmarking

Summary:
This paper introduces the task of Critical Questions Generation (CQs-Gen), which aims to enhance critical thinking by generating questions that challenge assumptions in arguments. The lack of suitable datasets and automatic evaluation methods has hindered progress in this area. The authors present the first large-scale manually-annotated dataset for CQs-Gen and investigate automatic evaluation techniques. They find that using large language models (LLMs) for reference-based evaluation correlates best with human judgments. Zero-shot evaluation of 11 LLMs establishes a strong baseline, highlighting the difficulty of the task. The authors provide data, code, and a public leaderboard to encourage further research on both model performance and the practical applications of CQs-Gen for automated reasoning and human critical thinking. <br /><br />Summary: <div>
arXiv:2505.11341v1 Announce Type: new 
Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose assumptions and challenge the reasoning in arguments. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This work presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale manually-annotated dataset. We also investigate automatic evaluation methods and identify a reference-based technique using large language models (LLMs) as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data, code, and a public leaderboard are provided to encourage further research not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors</title>
<link>https://arxiv.org/abs/2505.11352</link>
<guid>https://arxiv.org/abs/2505.11352</guid>
<content:encoded><![CDATA[
<div> Keywords: speech encoders, Large Language Models, ASR, LegoSLM, Connectionist Temporal Classification (CTC) 

Summary: 
The paper introduces a new approach, LegoSLM, that combines speech encoders and Large Language Models (LLMs) for improved performance in spoken language processing tasks. It bridges the gap between the models using ASR posterior matrices, allowing for flexible integration and better results. By training the speech encoder to generate CTC posteriors over the LLM vocabulary, pseudo-audio embeddings can be reconstructed to enhance ASR and speech translation tasks. The method shows promising results, achieving a 49% WERR improvement over baselines on MLS test sets. The model also exhibits modularity, enabling seamless integration with different models. By controlling the decode-time influence using a softmax temperature, effective domain adaptation is achieved. Overall, the LegoSLM approach presents a novel way to leverage the strengths of speech encoders and LLMs for enhanced performance in spoken language processing tasks.<br /><br />Summary: <div>
arXiv:2505.11352v1 Announce Type: new 
Abstract: Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a range of spoken language processing tasks including Automatic Speech Recognition (ASR). To effectively combine both models for better performance, continuous speech prompts, and ASR error correction have been adopted. However, these methods are prone to suboptimal performance or are inflexible. In this paper, we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using the ASR posterior matrices. The speech encoder is trained to generate Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary, which are used to reconstruct pseudo-audio embeddings by computing a weighted sum of the LLM input embeddings. These embeddings are concatenated with text embeddings in the LLM input space. Using the well-performing USM and Gemma models as an example, we demonstrate that our proposed LegoSLM method yields good performance on both ASR and speech translation tasks. By connecting USM with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline on 8 MLS testsets. The trained model also exhibits modularity in a range of settings -- after fine-tuning the Gemma model weights, the speech encoder can be switched and combined with the LLM in a zero-shot fashion. Additionally, we propose to control the decode-time influence of the USM and LLM using a softmax temperature, which shows effectiveness in domain adaptation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents</title>
<link>https://arxiv.org/abs/2505.11368</link>
<guid>https://arxiv.org/abs/2505.11368</guid>
<content:encoded><![CDATA[
<div> benchmarks, domain-oriented guidelines, large language models, instruction following, guideline following
Summary:
GuideBench is a new benchmark introduced to evaluate the performance of large language models (LLMs) in following domain-oriented guidelines. The benchmark assesses LLMs on three key aspects: adherence to various rules, robustness to rule updates, and alignment with human preferences. Experimental results reveal room for improvement in LLMs' capability to follow domain-oriented guidelines. This benchmark addresses the challenges posed by domain-specific guidelines that may conflict with an LLM's commonsense knowledge, providing a comprehensive evaluation framework for assessing and enhancing LLMs' performance in real-world applications. <div>
arXiv:2505.11368v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography</title>
<link>https://arxiv.org/abs/2505.11379</link>
<guid>https://arxiv.org/abs/2505.11379</guid>
<content:encoded><![CDATA[
<div> Keywords: Contemporary Quranic Orthography, phonetic notation, tajwid rules, Cairo Quran, computational analysis 

Summary: 
Contemporary Quranic Orthography (CQO) utilizes a precise phonetic notation system rooted in early Islam's oral tradition. The tajwid rules of recitation, represented through diacritical marks on the Quranic Consonantal Text, were systematically explored in the Cairo Quran using digital tools. A python module was developed to manipulate the orthographic layer of tajwid in CQO texts. This analysis of the Cairo Quran, complete and rich in content, can serve as a linchpin for aligning and comparing Quranic manuscripts computationally. The interconnectedness of these texts allows for studying the nature of diacritic notation systems across different manuscripts. This framework enhances understanding of Arabic script and textual phenomena within Quranic manuscripts. Computational analysis based on these tajwid rules opens avenues for in-depth exploration of the Quranic text's phonetic and prosodic aspects.<br /><br />Summary: <div>
arXiv:2505.11379v1 Announce Type: new 
Abstract: Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic notation that can be traced back to the early stages of Islam, when the Quran was mainly oral in nature and the first written renderings of it served as memory aids for this oral tradition. The early systems of diacritical marks created on top of the Quranic Consonantal Text (QCT) motivated the creation and further development of a fine-grained system of phonetic notation that represented tajwid-the rules of recitation. We explored the systematicity of the rules of tajwid, as they are encountered in the Cairo Quran, using a fully and accurately encoded digital edition of the Quranic text. For this purpose, we developed a python module that can remove or add the orthographic layer of tajwid from a Quranic text in CQO. The interesting characteristic of these two sets of rules is that they address the complete Quranic text of the Cairo Quran, so they can be used as precise witnesses to study its phonetic and prosodic processes. From a computational point of view, the text of the Cairo Quran can be used as a linchpin to align and compare Quranic manuscripts, due to its richness and completeness. This will let us create a very powerful framework to work with the Arabic script, not just within an isolated text, but automatically exploring a specific textual phenomenon in other connected manuscripts. Having all the texts mapped among each other can serve as a powerful tool to study the nature of the notation systems of diacritics added to the consonantal skeleton.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs</title>
<link>https://arxiv.org/abs/2505.11413</link>
<guid>https://arxiv.org/abs/2505.11413</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, healthcare, adversarial manipulation, safety evaluation, mitigation strategy <br />
Summary:<br />
The article introduces CARES, a benchmark for evaluating the safety of large language models (LLMs) in healthcare settings. CARES includes prompts covering various medical safety principles, harm levels, and prompting styles to simulate benign and malicious use cases. A three-way response evaluation protocol is proposed to assess model behavior, revealing vulnerabilities in state-of-the-art LLMs to jailbreak-style attacks and over-refusal of safe queries. A mitigation strategy using a lightweight classifier is suggested to detect jailbreak attempts and guide models towards safer responses. By providing a comprehensive framework for testing and enhancing LLM safety under adversarial and ambiguous conditions, CARES aims to address critical concerns about model alignment, susceptibility to manipulation, and overall safety in medical contexts. <br /> <div>
arXiv:2505.11413v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model</title>
<link>https://arxiv.org/abs/2505.11421</link>
<guid>https://arxiv.org/abs/2505.11421</guid>
<content:encoded><![CDATA[
<div> Keywords: Bahnaric, Vietnamese, translation, transfer learning, data augmentation

Summary: 
<br />
This work discusses the challenges of translating Bahnaric to Vietnamese due to limited available resources in the source language. The authors propose a transfer learning approach using a pre-trained Vietnamese language model to overcome this limitation. By training a sequence-to-sequence model with bilingual resources, the model can effectively translate between the two languages. The use of data augmentation techniques and heuristic methods further enhance the translation accuracy. The approach has been proven to be highly effective in bridging the linguistic gap between the Bahnaric and Vietnamese ethnic groups, leading to better mutual understanding and language preservation. 

Summary: <div>
arXiv:2505.11421v1 Announce Type: new 
Abstract: This work explores the journey towards achieving Bahnaric-Vietnamese translation for the sake of culturally bridging the two ethnic groups in Vietnam. However, translating from Bahnaric to Vietnamese also encounters some difficulties. The most prominent challenge is the lack of available original Bahnaric resources source language, including vocabulary, grammar, dialogue patterns and bilingual corpus, which hinders the data collection process for training. To address this, we leverage a transfer learning approach using sequence-to-sequence pre-training language model. First of all, we leverage a pre-trained Vietnamese language model to capture the characteristics of this language. Especially, to further serve the purpose of machine translation, we aim for a sequence-to-sequence model, not encoder-only like BERT or decoder-only like GPT. Taking advantage of significant similarity between the two languages, we continue training the model with the currently limited bilingual resources of Vietnamese-Bahnaric text to perform the transfer learning from language model to machine translation. Thus, this approach can help to handle the problem of imbalanced resources between two languages, while also optimizing the training and computational processes. Additionally, we also enhanced the datasets using data augmentation to generate additional resources and defined some heuristic methods to help the translation more precise. Our approach has been validated to be highly effective for the Bahnaric-Vietnamese translation model, contributing to the expansion and preservation of languages, and facilitating better mutual understanding between the two ethnic people.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs</title>
<link>https://arxiv.org/abs/2505.11423</link>
<guid>https://arxiv.org/abs/2505.11423</guid>
<content:encoded><![CDATA[
<div> instruction-following accuracy, reasoning-enhanced large language models, chain-of-thought prompting, CoT reasoning, constraint attention

Summary:
Reasoning-enhanced large language models have shown impressive performance on complex reasoning tasks, but a new study reveals that explicit CoT reasoning may actually decrease instruction-following accuracy. Evaluating models on two benchmarks, researchers found that CoT prompting led to performance drops. Through case studies and analysis, they identified ways in which reasoning can either help or hinder performance. A metric called constraint attention was introduced to measure model focus during generation, showing that CoT reasoning often diverts attention from relevant instructions. To address these issues, researchers proposed and tested four strategies to mitigate the negative effects of reasoning, with classifier-selective reasoning proving to be particularly effective in restoring lost performance. This study is the first to systematically uncover the failures induced by reasoning in instruction-following tasks and offers practical solutions to improve model performance. 

<br /><br />Summary: <div>
arXiv:2505.11423v1 Announce Type: new 
Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art</title>
<link>https://arxiv.org/abs/2505.11436</link>
<guid>https://arxiv.org/abs/2505.11436</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Comment Art, Multimodal Large Language Models, Chain-of-Thought, GODBench, Ripple of Thought

Summary: <br /><br /> This article introduces a new benchmark called GODBench that evaluates the abilities of Multimodal Large Language Models (MLLMs) in composing creative video comments. While MLLMs and Chain-of-Thought (CoT) have shown strong reasoning skills in STEM tasks, they struggle to generate creative expressions like jokes and satire. The GODBench benchmark integrates video and text modalities to assess MLLMs' creativity. The Ripple of Thought (RoT) framework, inspired by physics wave propagation patterns, is proposed to enhance MLLMs' creativity by leveraging multi-step reasoning. Experiments reveal that existing MLLMs and CoT methods face challenges in understanding and generating creative video comments. In contrast, the RoT framework shows promise in improving creative composition, indicating potential advancements in MLLM-based creativity. The GODBench benchmark is publicly available for further research and development. <div>
arXiv:2505.11436v1 Announce Type: new 
Abstract: Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Compression Really Linear with Code Intelligence?</title>
<link>https://arxiv.org/abs/2505.11441</link>
<guid>https://arxiv.org/abs/2505.11441</guid>
<content:encoded><![CDATA[
arXiv:2505.11441v1 Announce Type: new 
Abstract: Understanding the relationship between data compression and the capabilities of Large Language Models (LLMs) is crucial, especially in specialized domains like code intelligence. Prior work posited a linear relationship between compression and general intelligence. However, it overlooked the multifaceted nature of code that encompasses diverse programming languages and tasks, and struggled with fair evaluation of modern Code LLMs. We address this by evaluating a diverse array of open-source Code LLMs on comprehensive multi-language, multi-task code benchmarks. To address the challenge of efficient and fair evaluation of pre-trained LLMs' code intelligence, we introduce \textit{Format Annealing}, a lightweight, transparent training methodology designed to assess the intrinsic capabilities of these pre-trained models equitably. Compression efficacy, measured as bits-per-character (BPC), is determined using a novel, large-scale, and previously unseen code validation set derived from GitHub. Our empirical results reveal a fundamental logarithmic relationship between measured code intelligence and BPC. This finding refines prior hypotheses of linearity, which we suggest are likely observations of the logarithmic curve's tail under specific, limited conditions. Our work provides a more nuanced understanding of compression's role in developing code intelligence and contributes a robust evaluation framework in the code domain.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Reasoning and Knowledge in Medical Large Language Models</title>
<link>https://arxiv.org/abs/2505.11462</link>
<guid>https://arxiv.org/abs/2505.11462</guid>
<content:encoded><![CDATA[
arXiv:2505.11462v1 Announce Type: new 
Abstract: Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, m1 scores 60.5 on knowledge but only 47.1 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies</title>
<link>https://arxiv.org/abs/2505.11470</link>
<guid>https://arxiv.org/abs/2505.11470</guid>
<content:encoded><![CDATA[
arXiv:2505.11470v1 Announce Type: new 
Abstract: We introduce two reference-free metrics for quality evaluation of taxonomies. The first metric evaluates robustness by calculating the correlation between semantic and taxonomic similarity, covering a type of error not handled by existing metrics. The second uses Natural Language Inference to assess logical adequacy. Both metrics are tested on five taxonomies and are shown to correlate well with F1 against gold-standard taxonomies.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages</title>
<link>https://arxiv.org/abs/2505.11475</link>
<guid>https://arxiv.org/abs/2505.11475</guid>
<content:encoded><![CDATA[
arXiv:2505.11475v1 Announce Type: new 
Abstract: Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Assembly Code Performance with Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11480</link>
<guid>https://arxiv.org/abs/2505.11480</guid>
<content:encoded><![CDATA[
arXiv:2505.11480v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.11484</link>
<guid>https://arxiv.org/abs/2505.11484</guid>
<content:encoded><![CDATA[
arXiv:2505.11484v1 Announce Type: new 
Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling cognitive processes of natural reading with transformer-based Language Models</title>
<link>https://arxiv.org/abs/2505.11485</link>
<guid>https://arxiv.org/abs/2505.11485</guid>
<content:encoded><![CDATA[
arXiv:2505.11485v1 Announce Type: new 
Abstract: Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10583</link>
<guid>https://arxiv.org/abs/2505.10583</guid>
<content:encoded><![CDATA[
arXiv:2505.10583v1 Announce Type: cross 
Abstract: Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to the similar area in the latent space as a textual description of the strokes that conform the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper we evaluate the complexity of teaching visual-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</title>
<link>https://arxiv.org/abs/2505.10586</link>
<guid>https://arxiv.org/abs/2505.10586</guid>
<content:encoded><![CDATA[
arXiv:2505.10586v1 Announce Type: cross 
Abstract: Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action. However, the manual analysis of vast and heterogeneous data sources often results in delays, limiting the effectiveness of interventions. This paper introduces a dynamic Retrieval-Augmented Generation (RAG) system that autonomously generates situation awareness reports by integrating real-time data from diverse sources, including news articles, conflict event databases, and economic indicators. Our system constructs query-specific knowledge bases on demand, ensuring timely, relevant, and accurate insights.
  To ensure the quality of generated reports, we propose a three-level evaluation framework that combines semantic similarity metrics, factual consistency checks, and expert feedback. The first level employs automated NLP metrics to assess coherence and factual accuracy. The second level involves human expert evaluation to verify the relevance and completeness of the reports. The third level utilizes LLM-as-a-Judge, where large language models provide an additional layer of assessment to ensure robustness. The system is tested across multiple real-world scenarios, demonstrating its effectiveness in producing coherent, insightful, and actionable reports. By automating report generation, our approach reduces the burden on human analysts and accelerates decision-making processes. To promote reproducibility and further research, we openly share our code and evaluation tools with the community via GitHub.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation</title>
<link>https://arxiv.org/abs/2505.10588</link>
<guid>https://arxiv.org/abs/2505.10588</guid>
<content:encoded><![CDATA[
arXiv:2505.10588v1 Announce Type: cross 
Abstract: This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment</title>
<link>https://arxiv.org/abs/2505.10597</link>
<guid>https://arxiv.org/abs/2505.10597</guid>
<content:encoded><![CDATA[
arXiv:2505.10597v1 Announce Type: cross 
Abstract: Reward models (RMs) are essential for aligning large language models (LLMs) with human values. However, noisy preferences in human feedback often lead to reward misgeneralization, where RMs overfit to spurious patterns and provide misleading signals during policy optimization. We systematically analyze the training dynamics of preference pairs and identify that noisy examples are harder to fit and introduce instability. Empirical evidence shows that LLMs optimized using reward models trained on full noisy datasets perform worse than those trained on filtered, high-quality preferences. To address this, we propose Collaborative Reward Modeling (CRM), an online framework that enhances robustness by combining peer review and curriculum learning. Two reward models are trained in parallel and assess each other's data selections to filter out potential noise. Curriculum learning structures the preference data from easy to hard, ensuring synchronized training and stable feedback. Extensive experiments demonstrate that CRM improves generalization, with up to 9.94 points of accuracy gain on RewardBench under 40 percent label noise. CRM is also compatible with implicit-reward alignment methods, offering a practical and versatile strategy for robust alignment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech</title>
<link>https://arxiv.org/abs/2505.10599</link>
<guid>https://arxiv.org/abs/2505.10599</guid>
<content:encoded><![CDATA[
arXiv:2505.10599v1 Announce Type: cross 
Abstract: Recent neural codec language models have made great progress in the field of text-to-speech (TTS), but controllable emotional TTS still faces many challenges. Traditional methods rely on predefined discrete emotion labels to control emotion categories and intensities, which can't capture the complexity and continuity of human emotional perception and expression. The lack of large-scale emotional speech datasets with balanced emotion distributions and fine-grained emotion annotations often causes overfitting in synthesis models and impedes effective emotion control. To address these issues, we propose UDDETTS, a neural codec language model unifying discrete and dimensional emotions for controllable emotional TTS. This model introduces the interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion description and supports emotion control driven by either discrete emotion labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised training strategy is designed to comprehensively utilize diverse speech datasets with different types of emotion annotations to train the UDDETTS. Experiments show that UDDETTS achieves linear emotion control along the three dimensions of ADV space, and exhibits superior end-to-end emotional speech synthesis capabilities.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly</title>
<link>https://arxiv.org/abs/2505.10610</link>
<guid>https://arxiv.org/abs/2505.10610</guid>
<content:encoded><![CDATA[
arXiv:2505.10610v1 Announce Type: cross 
Abstract: The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating General User Models from Computer Use</title>
<link>https://arxiv.org/abs/2505.10831</link>
<guid>https://arxiv.org/abs/2505.10831</guid>
<content:encoded><![CDATA[
arXiv:2505.10831v1 Announce Type: cross 
Abstract: Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs</title>
<link>https://arxiv.org/abs/2505.10838</link>
<guid>https://arxiv.org/abs/2505.10838</guid>
<content:encoded><![CDATA[
arXiv:2505.10838v1 Announce Type: cross 
Abstract: Efficient red-teaming method to uncover vulnerabilities in Large Language Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers, the discrete language space make gradient-based methods struggle. We introduce LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel latent self-reflection attack that reasserts the power of gradient-based optimization for generating fluent jailbreaking prompts. By operating within the LLM's continuous latent space, LARGO first optimizes an adversarial latent vector and then recursively call the same LLM to decode the latent into natural language. This methodology yields a fast, effective, and transferable attack that produces fluent and stealthy prompts. On standard benchmarks like AdvBench and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent alternative to agentic LLM prompting, highlighting the efficacy of interpreting and attacking LLM internals through gradient optimization.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10844</link>
<guid>https://arxiv.org/abs/2505.10844</guid>
<content:encoded><![CDATA[
arXiv:2505.10844v1 Announce Type: cross 
Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatTools: Benchmarking Large Language Models for Materials Science Tools</title>
<link>https://arxiv.org/abs/2505.10852</link>
<guid>https://arxiv.org/abs/2505.10852</guid>
<content:encoded><![CDATA[
arXiv:2505.10852v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?</title>
<link>https://arxiv.org/abs/2505.10872</link>
<guid>https://arxiv.org/abs/2505.10872</guid>
<content:encoded><![CDATA[
arXiv:2505.10872v1 Announce Type: cross 
Abstract: Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory</title>
<link>https://arxiv.org/abs/2505.10981</link>
<guid>https://arxiv.org/abs/2505.10981</guid>
<content:encoded><![CDATA[
arXiv:2505.10981v1 Announce Type: cross 
Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies $\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a method according to probability theory to quickly and accurately predict the scaling performance and select the best strategy under large sampling times without extra resource-intensive inference in practice. It can serve as the test-time scaling law for majority voting. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.11079</link>
<guid>https://arxiv.org/abs/2505.11079</guid>
<content:encoded><![CDATA[
arXiv:2505.11079v1 Announce Type: cross 
Abstract: Audio deepfake detection (ADD) has grown increasingly important due to the rise of high-fidelity audio generative models and their potential for misuse. Given that audio large language models (ALLMs) have made significant progress in various audio processing tasks, a heuristic question arises: Can ALLMs be leveraged to solve ADD?. In this paper, we first conduct a comprehensive zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in detecting fake audio. To enhance their performance, we propose $\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we reformulate ADD task as an audio question answering problem, prompting the model with the question: "Is this audio fake or real?". We then perform supervised fine-tuning to enable the ALLM to assess the authenticity of query audio. Extensive experiments are conducted to demonstrate that our ALLM-based method can achieve superior performance in fake audio detection, particularly in data-scarce scenarios. As a pioneering study, we anticipate that this work will inspire the research community to leverage ALLMs to develop more effective ADD systems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPMA: Preference Manipulation Attack Against Model Context Protocol</title>
<link>https://arxiv.org/abs/2505.11154</link>
<guid>https://arxiv.org/abs/2505.11154</guid>
<content:encoded><![CDATA[
arXiv:2505.11154v1 Announce Type: cross 
Abstract: Model Context Protocol (MCP) standardizes interface mapping for large language models (LLMs) to access external data and tools, which revolutionizes the paradigm of tool selection and facilitates the rapid expansion of the LLM agent tool ecosystem. However, as the MCP is increasingly adopted, third-party customized versions of the MCP server expose potential security vulnerabilities. In this paper, we first introduce a novel security threat, which we term the MCP Preference Manipulation Attack (MPMA). An attacker deploys a customized MCP server to manipulate LLMs, causing them to prioritize it over other competing MCP servers. This can result in economic benefits for attackers, such as revenue from paid MCP services or advertising income generated from free servers. To achieve MPMA, we first design a Direct Preference Manipulation Attack ($\mathtt{DPMA}$) that achieves significant effectiveness by inserting the manipulative word and phrases into the tool name and description. However, such a direct modification is obvious to users and lacks stealthiness. To address these limitations, we further propose Genetic-based Advertising Preference Manipulation Attack ($\mathtt{GAPMA}$). $\mathtt{GAPMA}$ employs four commonly used strategies to initialize descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness. The experiment results demonstrate that $\mathtt{GAPMA}$ balances high effectiveness and stealthiness. Our study reveals a critical vulnerability of the MCP in open ecosystems, highlighting an urgent need for robust defense mechanisms to ensure the fairness of the MCP ecosystem.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Asynchronicity in Event-based Neural Networks</title>
<link>https://arxiv.org/abs/2505.11165</link>
<guid>https://arxiv.org/abs/2505.11165</guid>
<content:encoded><![CDATA[
arXiv:2505.11165v1 Announce Type: cross 
Abstract: Event cameras deliver visual data with high temporal resolution, low latency, and minimal redundancy, yet their asynchronous, sparse sequential nature challenges standard tensor-based machine learning (ML). While the recent asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by asynchronously encoding events into learned representations for ML pipelines, existing A2S approaches often sacrifice representation expressivity and generalizability compared to dense, synchronous methods. This paper introduces EVA (EVent Asynchronous representation learning), a novel A2S framework to generate highly expressive and generalizable event-by-event representations. Inspired by the analogy between events and language, EVA uniquely adapts advances from language modeling in linear attention and self-supervised learning for its construction. In demonstration, EVA outperforms prior A2S methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the first A2S framework to successfully master demanding detection tasks, achieving a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's transformative potential for advancing real-time event-based vision applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback</title>
<link>https://arxiv.org/abs/2505.11178</link>
<guid>https://arxiv.org/abs/2505.11178</guid>
<content:encoded><![CDATA[
arXiv:2505.11178v1 Announce Type: cross 
Abstract: State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest's feedback as preference signals to improve diffusion models' compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms</title>
<link>https://arxiv.org/abs/2505.11183</link>
<guid>https://arxiv.org/abs/2505.11183</guid>
<content:encoded><![CDATA[
arXiv:2505.11183v1 Announce Type: cross 
Abstract: Probabilistic next-token prediction trained using cross-entropy loss is the basis of most large language models. Given a sequence of previous values, next-token prediction assigns a probability to each possible next value in the vocabulary. There are many ways to use next-token prediction to output token sequences. This paper examines a few of these algorithms (greedy, lookahead, random sampling, and temperature-scaled random sampling) and studies their consistency with respect to various goals encoded as loss functions. Although consistency of surrogate losses with respect to a target loss function is a well researched topic, we are the first to study it in the context of LLMs (to the best of our knowledge). We find that, so long as next-token prediction converges to its true probability distribution, random sampling is consistent with outputting sequences that mimic sampling from the true probability distribution. For the other goals, such as minimizing the 0-1 loss on the entire sequence, we show no polynomial-time algorithm is optimal for all probability distributions and all decoding algorithms studied are only optimal for a subset of probability distributions. When analyzing these results, we see that there is a dichotomy created between the goals of information retrieval and creative generation for the decoding algorithms. This shows that choosing the correct decoding algorithm based on the desired goal is extremely important and many of the ones used are lacking theoretical grounding in numerous scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese</title>
<link>https://arxiv.org/abs/2505.11200</link>
<guid>https://arxiv.org/abs/2505.11200</guid>
<content:encoded><![CDATA[
arXiv:2505.11200v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly improved text-to-speech (TTS) systems, enhancing control over speech style, naturalness, and emotional expression, which brings TTS Systems closer to human-level performance. Although the Mean Opinion Score (MOS) remains the standard for TTS System evaluation, it suffers from subjectivity, environmental inconsistencies, and limited interpretability. Existing evaluation datasets also lack a multi-dimensional design, often neglecting factors such as speaking styles, context diversity, and trap utterances, which is particularly evident in Chinese TTS evaluation. To address these challenges, we introduce the Audio Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on complex MOS scales or direct model comparisons, ATT asks evaluators to judge whether a voice sounds human. This simplification reduces rating bias and improves evaluation robustness. To further support rapid model development, we also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for automatic evaluation. Experimental results show that ATT effectively differentiates models across specific capability dimensions using its multi-dimensional design. Auto-ATT also demonstrates strong alignment with human evaluations, confirming its value as a fast and reliable assessment tool. The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face Collection (https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.11274</link>
<guid>https://arxiv.org/abs/2505.11274</guid>
<content:encoded><![CDATA[
arXiv:2505.11274v1 Announce Type: cross 
Abstract: Recently, large reasoning models demonstrate exceptional performance on various tasks. However, reasoning models inefficiently over-process both trivial and complex queries, leading to resource waste and prolonged user latency. To address this challenge, we propose SelfBudgeter - a self-adaptive controllable reasoning strategy for efficient reasoning. Our approach adopts a dual-phase training paradigm: first, the model learns to pre-estimate the reasoning cost based on the difficulty of the query. Then, we introduce budget-guided GPRO for reinforcement learning, which effectively maintains accuracy while reducing output length. SelfBudgeter allows users to anticipate generation time and make informed decisions about continuing or interrupting the process. Furthermore, our method enables direct manipulation of reasoning length via pre-filling token budget. Experimental results demonstrate that SelfBudgeter can rationally allocate budgets according to problem complexity, achieving up to 74.47% response length compression on the MATH benchmark while maintaining nearly undiminished accuracy.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks</title>
<link>https://arxiv.org/abs/2505.11314</link>
<guid>https://arxiv.org/abs/2505.11314</guid>
<content:encoded><![CDATA[
arXiv:2505.11314v1 Announce Type: cross 
Abstract: The assessment of evaluation metrics (meta-evaluation) is crucial for determining the suitability of existing metrics in text-to-image (T2I) generation tasks. Human-based meta-evaluation is costly and time-intensive, and automated alternatives are scarce. We address this gap and propose CROC: a scalable framework for automated Contrastive Robustness Checks that systematically probes and quantifies metric robustness by synthesizing contrastive test cases across a comprehensive taxonomy of image properties. With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one million contrastive prompt-image pairs to enable a fine-grained comparison of evaluation metrics. We also use the dataset to train CROCScore, a new metric that achieves state-of-the-art performance among open-source methods, demonstrating an additional key application of our framework. To complement this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$) targeting especially challenging categories. Our results highlight robustness issues in existing metrics: for example, many fail on prompts involving negation, and all tested open-source metrics fail on at least 25% of cases involving correct identification of body parts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phare: A Safety Probe for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11365</link>
<guid>https://arxiv.org/abs/2505.11365</guid>
<content:encoded><![CDATA[
arXiv:2505.11365v1 Announce Type: cross 
Abstract: Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.11405</link>
<guid>https://arxiv.org/abs/2505.11405</guid>
<content:encoded><![CDATA[
arXiv:2505.11405v1 Announce Type: cross 
Abstract: Emotion understanding is a critical yet challenging task. Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities in this area. However, MLLMs often suffer from hallucinations, generating irrelevant or nonsensical content. To the best of our knowledge, despite the importance of this issue, there has been no dedicated effort to evaluate emotion-related hallucinations in MLLMs. In this work, we introduce EmotionHallucer, the first benchmark for detecting and analyzing emotion hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from the interplay of biology and social learning, MLLMs rely solely on data-driven learning and lack innate emotional instincts. Fortunately, emotion psychology provides a solid foundation of knowledge about human emotions. Building on this, we assess emotion hallucinations from two dimensions: emotion psychology knowledge and real-world multimodal perception. To support robust evaluation, we utilize an adversarial binary question-answer (QA) framework, which employs carefully crafted basic and hallucinated pairs to assess the emotion hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on EmotionHallucer, we reveal that: i) most current models exhibit substantial issues with emotion hallucinations; ii) closed-source models outperform open-source ones in detecting emotion hallucinations, and reasoning capability provides additional advantages; iii) existing models perform better in emotion psychology knowledge than in multimodal emotion perception. As a byproduct, these findings inspire us to propose the PEP-MEK framework, which yields an average improvement of 9.90% in emotion hallucination detection across selected models. Resources will be available at https://github.com/xxtars/EmotionHallucer.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Use Impact Locus of Control</title>
<link>https://arxiv.org/abs/2505.11406</link>
<guid>https://arxiv.org/abs/2505.11406</guid>
<content:encoded><![CDATA[
arXiv:2505.11406v1 Announce Type: cross 
Abstract: As AI tools increasingly shape how we write, they may also quietly reshape how we perceive ourselves. This paper explores the psychological impact of co-writing with AI on people's locus of control. Through an empirical study with 462 participants, we found that employment status plays a critical role in shaping users' reliance on AI and their locus of control. Current results demonstrated that employed participants displayed higher reliance on AI and a shift toward internal control, while unemployed users tended to experience a reduction in personal agency. Through quantitative results and qualitative observations, this study opens a broader conversation about AI's role in shaping personal agency and identity.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Planning: Let's Think Only with Images</title>
<link>https://arxiv.org/abs/2505.11409</link>
<guid>https://arxiv.org/abs/2505.11409</guid>
<content:encoded><![CDATA[
arXiv:2505.11409v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator</title>
<link>https://arxiv.org/abs/2305.15099</link>
<guid>https://arxiv.org/abs/2305.15099</guid>
<content:encoded><![CDATA[
arXiv:2305.15099v2 Announce Type: replace 
Abstract: The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code is publicly available at https://github.com/LUMIA-Group/FourierTransformer
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?</title>
<link>https://arxiv.org/abs/2311.07564</link>
<guid>https://arxiv.org/abs/2311.07564</guid>
<content:encoded><![CDATA[
arXiv:2311.07564v4 Announce Type: replace 
Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on human-transcribed conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of neural and non-neural baselines, finding that although written text attribution models achieve surprisingly good performance in certain settings, they perform markedly worse as conversational topic is increasingly controlled. We present analyses of the impact of transcription style on performance as well as the ability of fine-tuning on speech transcripts to improve performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models</title>
<link>https://arxiv.org/abs/2402.14889</link>
<guid>https://arxiv.org/abs/2402.14889</guid>
<content:encoded><![CDATA[
arXiv:2402.14889v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These benchmarks measure bias by observing an LLM's behavior on biased statements. However, these statements lack contextual considerations of the situations they try to present. To address this, we introduce a contextual reliability framework, which evaluates model robustness to biased statements by considering the various contexts in which they may appear. We develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to measure a biased statement's reliability in detecting bias, based on the variance in model behavior across different contexts. To evaluate the metric, we augmented 2,291 stereotyped statements from two existing benchmark datasets by adding contextual information. We show that COBIAS aligns with human judgment on the contextual reliability of biased statements (Spearman's $\rho = 0.65, p = 3.4 * 10^{-60}$) and can be used to create reliable benchmarks, which would assist bias mitigation works.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images</title>
<link>https://arxiv.org/abs/2404.10652</link>
<guid>https://arxiv.org/abs/2404.10652</guid>
<content:encoded><![CDATA[
arXiv:2404.10652v3 Announce Type: replace 
Abstract: Visual Question Answerinng (VQA) is a complicated task that requires the capability of simultaneously processing natural language and images. This task was initially researched with a focus on developing methods to help machines understand objects and scene contexts in images. However, some scene text that carries explicit information about the full content of the image is not mentioned. Along with the continuous development of the AI era, there have been many studies on the reading comprehension ability of VQA models in the world. Therefore, we introduce the first large-scale dataset in Vietnamese specializing in the ability to understand scene text, we call it ViTextVQA (\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering dataset) which contains \textbf{over 16,000} images and \textbf{over 50,000} questions with answers. To tackle this task efficiently, we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal feature fusion. Through experiments with various state-of-the-art models, we uncover the significance of the order in which tokens in OCR text are processed and selected to formulate answers. This finding helped us significantly improve the performance of the baseline models on the ViTextVQA dataset. Our dataset is available (https://github.com/minhquan6203/ViTextVQA-Dataset) for research purposes.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation</title>
<link>https://arxiv.org/abs/2405.00715</link>
<guid>https://arxiv.org/abs/2405.00715</guid>
<content:encoded><![CDATA[
arXiv:2405.00715v5 Announce Type: replace 
Abstract: Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching</title>
<link>https://arxiv.org/abs/2406.06326</link>
<guid>https://arxiv.org/abs/2406.06326</guid>
<content:encoded><![CDATA[
arXiv:2406.06326v5 Announce Type: replace 
Abstract: Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from unseen raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</title>
<link>https://arxiv.org/abs/2408.06518</link>
<guid>https://arxiv.org/abs/2408.06518</guid>
<content:encoded><![CDATA[
arXiv:2408.06518v3 Announce Type: replace 
Abstract: Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights yet another type of bias in language models that affects their generation patterns and behavior.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards understanding evolution of science through language model series</title>
<link>https://arxiv.org/abs/2409.09636</link>
<guid>https://arxiv.org/abs/2409.09636</guid>
<content:encoded><![CDATA[
arXiv:2409.09636v2 Announce Type: replace 
Abstract: We introduce AnnualBERT, a series of language models designed specifically to capture the temporal evolution of scientific text. Deviating from the prevailing paradigms of subword tokenizations and "one model to rule them all", AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model pretrained from scratch on the full-text of 1.7 million arXiv papers published until 2008 and a collection of progressively trained models on arXiv papers at an annual basis. We demonstrate the effectiveness of AnnualBERT models by showing that they not only have comparable performances in standard tasks but also achieve state-of-the-art performances on domain-specific NLP tasks as well as link prediction tasks in the arXiv citation network. We then utilize probing tasks to quantify the models' behavior in terms of representation learning and forgetting as time progresses. Our approach enables the pretrained models to not only improve performances on scientific text processing tasks but also to provide insights into the development of scientific discourse over time. The series of the models is available at https://huggingface.co/jd445/AnnualBERTs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach</title>
<link>https://arxiv.org/abs/2409.20204</link>
<guid>https://arxiv.org/abs/2409.20204</guid>
<content:encoded><![CDATA[
arXiv:2409.20204v2 Announce Type: replace 
Abstract: Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms. These tools draw on insights from both social science and computer science. Given the increasing concern over gender-based discrimination in digital spaces, the contested definitions and measurements of sexism, and the rise of interdisciplinary efforts to understand its online manifestations, a systematic literature review is essential for capturing the current state and trajectory of this evolving field. In this review, we make four key contributions: (1) we synthesize the literature into five core themes: definitions of sexism and misogyny, disciplinary divergences, automated detection methods, associated challenges, and design-based interventions; (2) we adopt an interdisciplinary lens, bridging theoretical and methodological divides across disciplines; (3) we highlight critical gaps, including the need for intersectional approaches, the under-representation of non-Western languages and perspectives, and the limited focus on proactive design strategies beyond text classification; and (4) we offer a methodological contribution by applying a rigorous semi-automated systematic review process guided by PRISMA, establishing a replicable standard for future work in this domain. Our findings reveal a clear disciplinary divide in how sexism and misogyny are conceptualized and measured. Through an evidence-based synthesis, we examine how existing studies have attempted to bridge this gap through interdisciplinary collaboration. Drawing on both social science theories and computational modeling practices, we assess the strengths and limitations of current methodologies. Finally, we outline key challenges and future directions for advancing research on the detection and mitigation of online sexism and misogyny.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training of Scaffolded Language Models with Language Supervision: A Survey</title>
<link>https://arxiv.org/abs/2410.16392</link>
<guid>https://arxiv.org/abs/2410.16392</guid>
<content:encoded><![CDATA[
arXiv:2410.16392v2 Announce Type: replace 
Abstract: This survey organizes the intricate literature on the design and optimization of emerging structures around post-trained LMs. We refer to this overarching structure as scaffolded LMs and focus on LMs that are integrated into multi-step processes with tools. We view scaffolded LMs as semi-parametric models wherein we train non-parametric variables, including the prompt, tools, and scaffold's code. In particular, they interpret instructions, use tools, and receive feedback all in language. Recent works use an LM as an optimizer to interpret language supervision and update non-parametric variables according to intricate objectives. In this survey, we refer to this paradigm as training of scaffolded LMs with language supervision. A key feature of non-parametric training is the ability to learn from language. Parametric training excels in learning from demonstration (supervised learning), exploration (reinforcement learning), or observations (unsupervised learning), using well-defined loss functions. Language-based optimization enables rich, interpretable, and expressive objectives, while mitigating issues like catastrophic forgetting and supporting compatibility with closed-source models. Furthermore, agents are increasingly deployed as co-workers in real-world applications such as Copilot in Office tools or software development. In these mixed-autonomy settings, where control and decision-making are shared between human and AI, users point out errors or suggest corrections. Accordingly, we discuss agents that continuously improve by learning from this real-time, language-based feedback and refer to this setting as streaming learning from language supervision.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework</title>
<link>https://arxiv.org/abs/2410.19453</link>
<guid>https://arxiv.org/abs/2410.19453</guid>
<content:encoded><![CDATA[
arXiv:2410.19453v5 Announce Type: replace 
Abstract: Although fine-tuning Large Language Models (LLMs) with multilingual data can rapidly enhance the multilingual capabilities of LLMs, they still exhibit a performance gap between the dominant language (e.g., English) and non-dominant ones due to the imbalance of training data across languages. To further enhance the performance of non-dominant languages, we propose ShifCon, a Shift-based Contrastive framework that aligns the internal forward process of other languages toward that of the dominant one. Specifically, it shifts the representations of non-dominant languages into the dominant language subspace, allowing them to access relatively rich information encoded in the model parameters. The enriched representations are then shifted back into their original language subspace before generation. Moreover, we introduce a subspace distance metric to pinpoint the optimal layer area for shifting representations and employ multilingual contrastive learning to further enhance the alignment of representations within this area. Experiments demonstrate that our ShifCon framework significantly enhances the performance of non-dominant languages, particularly for low-resource ones. Further analysis offers extra insights to verify the effectiveness of ShifCon and propel future research
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP</title>
<link>https://arxiv.org/abs/2411.05527</link>
<guid>https://arxiv.org/abs/2411.05527</guid>
<content:encoded><![CDATA[
arXiv:2411.05527v2 Announce Type: replace 
Abstract: Wikipedia's perceived high quality and broad language coverage have established it as a fundamental resource in multilingual NLP. In the context of low-resource languages, however, these quality assumptions are increasingly being scrutinised. This paper critically examines the data quality of Wikipedia in a non-English setting by subjecting it to various quality filtering techniques, revealing widespread issues such as a high percentage of one-line articles and duplicate articles. We evaluate the downstream impact of quality filtering on Wikipedia and find that data quality pruning is an effective means for resource-efficient training without hurting performance, especially for low-resource languages. Moreover, we advocate for a shift in perspective from seeking a general definition of data quality towards a more language- and task-specific one. Ultimately, we aim for this study to serve as a guide to using Wikipedia for pretraining in a multilingual setting.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction</title>
<link>https://arxiv.org/abs/2411.07019</link>
<guid>https://arxiv.org/abs/2411.07019</guid>
<content:encoded><![CDATA[
arXiv:2411.07019v3 Announce Type: replace 
Abstract: Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, constrained by complex fact representation forms, existing link prediction models for beyond-triple facts have difficulty achieving hierarchical fact modeling and generalizing the modules for one specific facts to other fact types. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Empirical results demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/Lza12a/UniHR.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Speech Data in Reducing Toxicity Detection Bias</title>
<link>https://arxiv.org/abs/2411.08135</link>
<guid>https://arxiv.org/abs/2411.08135</guid>
<content:encoded><![CDATA[
arXiv:2411.08135v2 Announce Type: replace 
Abstract: Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTox dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving classifiers, rather than transcription pipelines, is more helpful for reducing group bias. We publicly release our annotations and provide recommendations for future toxicity dataset construction.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to Speak, When to Abstain: Contrastive Decoding with Abstention</title>
<link>https://arxiv.org/abs/2412.12527</link>
<guid>https://arxiv.org/abs/2412.12527</guid>
<content:encoded><![CDATA[
arXiv:2412.12527v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e., contextual) knowledge. While substantial efforts have been made to enhance the utilization of both forms of knowledge, situations in which models lack relevant information remain underexplored. To investigate this challenge, we first present a controlled testbed featuring four distinct knowledge access scenarios, including the aforementioned edge case, revealing that conventional LLM usage exhibits insufficient robustness in handling all instances. Addressing this limitation, we propose Contrastive Decoding with Abstention (CDA), a novel training-free decoding method that allows LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA estimates the relevance of both knowledge sources for a given input, adaptively deciding which type of information to prioritize and which to exclude. Through extensive experiments, we demonstrate that CDA can effectively perform accurate generation and abstention simultaneously, enhancing reliability and preserving user trust.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context</title>
<link>https://arxiv.org/abs/2412.12632</link>
<guid>https://arxiv.org/abs/2412.12632</guid>
<content:encoded><![CDATA[
arXiv:2412.12632v2 Announce Type: replace 
Abstract: Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and evaluate LLMs' effectiveness, faithfulness and robustness with CoE, including its application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs show CoE improves generation accuracy, answer faithfulness, robustness to knowledge conflicts, and boosts the performance of existing approaches in three practical RAG scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2412.15529</link>
<guid>https://arxiv.org/abs/2412.15529</guid>
<content:encoded><![CDATA[
arXiv:2412.15529v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines</title>
<link>https://arxiv.org/abs/2501.00745</link>
<guid>https://arxiv.org/abs/2501.00745</guid>
<content:encoded><![CDATA[
arXiv:2501.00745v2 Announce Type: replace 
Abstract: The increasing integration of Large Language Model (LLM) based search engines has transformed the landscape of information retrieval. However, these systems are vulnerable to adversarial attacks, especially ranking manipulation attacks, where attackers craft webpage content to manipulate the LLM's ranking and promote specific content, gaining an unfair advantage over competitors. In this paper, we study the dynamics of ranking manipulation attacks. We frame this problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players strategically decide whether to cooperate or attack. We analyze the conditions under which cooperation can be sustained, identifying key factors such as attack costs, discount rates, attack success rates, and trigger strategies that influence player behavior. We identify tipping points in the system dynamics, demonstrating that cooperation is more likely to be sustained when players are forward-looking. However, from a defense perspective, we find that simply reducing attack success probabilities can, paradoxically, incentivize attacks under certain conditions. Furthermore, defensive measures to cap the upper bound of attack success rates may prove futile in some scenarios. These insights highlight the complexity of securing LLM-based systems. Our work provides a theoretical foundation and practical insights for understanding and mitigating their vulnerabilities, while emphasizing the importance of adaptive security strategies and thoughtful ecosystem design.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena</title>
<link>https://arxiv.org/abs/2501.03266</link>
<guid>https://arxiv.org/abs/2501.03266</guid>
<content:encoded><![CDATA[
arXiv:2501.03266v2 Announce Type: replace 
Abstract: LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. In particular, little is known about how users respond when models refuse to answer a prompt-one of the primary mechanisms used to enforce ethical boundaries in LLMs. We address this gap by analyzing nearly 50,000 model comparisons from Chatbot Arena, a platform where users indicate their preferred LLM response in pairwise matchups, providing a large-scale setting for studying real-world user preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a hand-labeled dataset, we distinguish between refusals due to ethical concerns and technical limitations. Our results reveal a substantial refusal penalty: ethical refusals yield significantly lower win rates than both technical refusals and standard responses, indicating that users are especially dissatisfied when models decline a task for ethical reasons. However, this penalty is not uniform. Refusals receive more favorable evaluations when the underlying prompt is highly sensitive (e.g., involving illegal content), and when the refusal is phrased in a detailed and contextually aligned manner. These findings underscore a core tension in LLM design: safety-aligned behaviors may conflict with user expectations, calling for more adaptive moderation strategies that account for context and presentation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeKV: Smooth Key-Value Cache Compression with Tree Structures</title>
<link>https://arxiv.org/abs/2501.04987</link>
<guid>https://arxiv.org/abs/2501.04987</guid>
<content:encoded><![CDATA[
arXiv:2501.04987v3 Announce Type: replace 
Abstract: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling</title>
<link>https://arxiv.org/abs/2501.10316</link>
<guid>https://arxiv.org/abs/2501.10316</guid>
<content:encoded><![CDATA[
arXiv:2501.10316v3 Announce Type: replace 
Abstract: Recent LLMs have enabled significant advancements for conversational agents. However, they are also well known to hallucinate, producing responses that seem plausible but are factually incorrect. On the other hand, users tend to over-rely on LLM-based AI agents, accepting AI's suggestion even when it is wrong. Adding positive friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head that functions as a binary classifier to predict the relevant slots of the dialogue state mentioned in the conversation. We perform our experiments with multiple backbone LLMs on two established benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the proposed approach not only enables reliable estimation of AI agent errors but also guides the decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy (JGA) of DST output by incorporating accountability heads into modern LLMs. Self-correcting the detected errors further increases the JGA from 67.13 to 70.51, achieving state-of-the-art DST performance. Finally, we show that error correction through user confirmations (friction turn) achieves a similar performance gain, highlighting its potential to reduce user overreliance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine</title>
<link>https://arxiv.org/abs/2501.11885</link>
<guid>https://arxiv.org/abs/2501.11885</guid>
<content:encoded><![CDATA[
arXiv:2501.11885v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. Despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 14.74\% improvement over vanilla RAG methods and even a 3.32\% enhancement compared to fine-tuning strategies, without incurring additional training costs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms</title>
<link>https://arxiv.org/abs/2501.13977</link>
<guid>https://arxiv.org/abs/2501.13977</guid>
<content:encoded><![CDATA[
arXiv:2501.13977v2 Announce Type: replace 
Abstract: Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do we really have to filter out random noise in pre-training data for language models?</title>
<link>https://arxiv.org/abs/2502.06604</link>
<guid>https://arxiv.org/abs/2502.06604</guid>
<content:encoded><![CDATA[
arXiv:2502.06604v2 Announce Type: replace 
Abstract: Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the Internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low quality or synthetic data, our study \textbf{provides the first systematic investigation of such random noise through a cohesive ``What-Why-How'' framework.} Surprisingly, we observed that the resulting increase in the loss of next-token prediction (NTP) was significantly lower than the proportion of random noise even when the model was scaled up to 2.7B. We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models and can be applied to multimodal models. On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance. To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters. Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging</title>
<link>https://arxiv.org/abs/2502.06876</link>
<guid>https://arxiv.org/abs/2502.06876</guid>
<content:encoded><![CDATA[
arXiv:2502.06876v3 Announce Type: replace 
Abstract: Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\textit{data-level}) and model merging (\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \textbf{R}eweighting \textbf{E}nhanced task \textbf{S}ingular \textbf{M}erging method, \textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\%-5\% gain) and model merging (1\%-3\% gain) methods in achieving balanced LLM alignment. We release our models through \href{https://huggingface.co/Jinluan}{3H\_Merging} for further investigations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More</title>
<link>https://arxiv.org/abs/2502.07490</link>
<guid>https://arxiv.org/abs/2502.07490</guid>
<content:encoded><![CDATA[
arXiv:2502.07490v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination, Monofacts, and Miscalibration: An Empirical Investigation</title>
<link>https://arxiv.org/abs/2502.08666</link>
<guid>https://arxiv.org/abs/2502.08666</guid>
<content:encoded><![CDATA[
arXiv:2502.08666v2 Announce Type: replace 
Abstract: Hallucinated facts in large language models (LLMs) have recently been shown to obey a statistical lower bound determined by the monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration (Kalai & Vempala, 2024). We present the first empirical investigation of this three-way relationship in classical n-gram models and fine-tuned encoder-decoder Transformers. By generating training data from Pareto distributions with varying shape parameters, we systematically control the monofact rates and establish its positive relationship with hallucination. To bridge theory and practice, we derive an empirical analog of the hallucination bound by replacing the population miscalibration term (Section 2.1) with an empirical bin-wise KL divergence and confirm its practical viability. We then introduce selective upweighting -- a simple yet effective technique that strategically repeats as little as 5% of training examples -- to deliberately inject miscalibration into the model. This intervention reduces hallucination by up to 40%, challenging universal deduplication policies. Our experiments reveal a critical trade-off: selective upweighting maintains pre-injection levels of accuracy while substantially reducing hallucination, whereas standard training gradually improves accuracy but fails to address persistently high hallucination, indicating an inherent tension in optimization objectives.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Language Preference of Multilingual RAG Systems</title>
<link>https://arxiv.org/abs/2502.11175</link>
<guid>https://arxiv.org/abs/2502.11175</guid>
<content:encoded><![CDATA[
arXiv:2502.11175v2 Announce Type: replace 
Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Your Uncertainty Scores Detect Hallucinated Entity?</title>
<link>https://arxiv.org/abs/2502.11948</link>
<guid>https://arxiv.org/abs/2502.11948</guid>
<content:encoded><![CDATA[
arXiv:2502.11948v2 Announce Type: replace 
Abstract: To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research. HalluEntity: https://huggingface.co/datasets/samuelyeh/HalluEntity
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iAgent: LLM Agent as a Shield between User and Recommender Systems</title>
<link>https://arxiv.org/abs/2502.14662</link>
<guid>https://arxiv.org/abs/2502.14662</guid>
<content:encoded><![CDATA[
arXiv:2502.14662v2 Announce Type: replace 
Abstract: Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing</title>
<link>https://arxiv.org/abs/2502.15208</link>
<guid>https://arxiv.org/abs/2502.15208</guid>
<content:encoded><![CDATA[
arXiv:2502.15208v2 Announce Type: replace 
Abstract: Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Call for Rigor in Reporting Quality of Instruction Tuning Data</title>
<link>https://arxiv.org/abs/2503.04807</link>
<guid>https://arxiv.org/abs/2503.04807</guid>
<content:encoded><![CDATA[
arXiv:2503.04807v3 Announce Type: replace 
Abstract: Instruction tuning is crucial for adapting large language models (LLMs) to align with user intentions. Numerous studies emphasize the significance of the quality of instruction tuning (IT) data, revealing a strong correlation between IT data quality and the alignment performance of LLMs. In these studies, the quality of IT data is typically assessed by evaluating the performance of LLMs trained with that data. However, we identified a prevalent issue in such practice: hyperparameters for training models are often selected arbitrarily without adequate justification. We observed significant variations in hyperparameters applied across different studies, even when training the same model with the same data. In this study, we demonstrate the potential problems arising from this practice and emphasize the need for careful consideration in verifying data quality. Through our experiments on the quality of LIMA data and a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary hyperparameter decisions can make any arbitrary conclusion.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TigerLLM -- A Family of Bangla Large Language Models</title>
<link>https://arxiv.org/abs/2503.10995</link>
<guid>https://arxiv.org/abs/2503.10995</guid>
<content:encoded><![CDATA[
arXiv:2503.10995v2 Announce Type: replace 
Abstract: The development of Large Language Models (LLMs) remains heavily skewed towards English and a few other high-resource languages. This linguistic disparity is particularly evident for Bangla - the 5th most spoken language. A few initiatives attempted to create open-source Bangla LLMs with performance still behind high-resource languages and limited reproducibility. To address this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results demonstrate that these models surpass all open-source alternatives and also outperform larger proprietary models like GPT3.5 across standard benchmarks, establishing TigerLLM as the new baseline for future Bangla language modeling.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse</title>
<link>https://arxiv.org/abs/2503.16525</link>
<guid>https://arxiv.org/abs/2503.16525</guid>
<content:encoded><![CDATA[
arXiv:2503.16525v2 Announce Type: replace 
Abstract: Recent advances in long-text understanding have pushed the context length of large language models (LLMs) up to one million tokens. It boosts LLMs's accuracy and reasoning capacity but causes exorbitant computational costs and unsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the exact same KV cache of prefixes and templates or shares similar ones but with extra selective recomputation, offers a promising way to tackle this issue. However, prior studies overlook the cross-request KV reuse and the attention deviations introduced by new tokens during the decoding stage. In this paper, we present a KV cache management module that shares the KV cache across requests under multi-tenant scenarios without sacrificing model accuracy. Our system, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage High Deviation algorithm (DHD) that conditionally selects a small portion of KV cache to be recomputed during both prefill and decode phases, and 2) a cache-aware scheduler that prioritizes requests based on their KV cache hit rates and orchestrates continuous batching to achieve enhanced system efficiency and faster TTFT. Multi-task experiments conducted on models such as Qwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up to 9.39x and increases 1.2x of the throughput compared to the full KV recompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy compared to SOTA methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts</title>
<link>https://arxiv.org/abs/2503.16529</link>
<guid>https://arxiv.org/abs/2503.16529</guid>
<content:encoded><![CDATA[
arXiv:2503.16529v2 Announce Type: replace 
Abstract: DeepSeek-R1, renowned for its exceptional reasoning capabilities and open-source strategy, is significantly influencing the global artificial intelligence landscape. However, it exhibits notable safety shortcomings. Recent research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 achieves a 100\% attack success rate when processing harmful prompts. Furthermore, multiple security firms and research institutions have identified critical security vulnerabilities within the model. Although China Unicom has uncovered safety vulnerabilities of R1 in Chinese contexts, the safety capabilities of the remaining distilled models in the R1 series have not yet been comprehensively evaluated. To address this gap, this study utilizes the comprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth safety evaluation of the DeepSeek-R1 series distilled models. The objective is to assess the safety capabilities of these models in Chinese contexts both before and after distillation, and to further elucidate the adverse effects of distillation on model safety. Building on these findings, we implement targeted safety enhancements for the entire DeepSeek-R1 model series. Evaluation results indicate that the enhanced models achieve significant improvements in safety while maintaining reasoning capabilities without notable degradation. We open-source the safety-enhanced models at https://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource for future research and optimization of DeepSeek models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Routers</title>
<link>https://arxiv.org/abs/2503.23362</link>
<guid>https://arxiv.org/abs/2503.23362</guid>
<content:encoded><![CDATA[
arXiv:2503.23362v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: https://anonymous.4open.science/r/MoR-DFC6.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?</title>
<link>https://arxiv.org/abs/2504.01698</link>
<guid>https://arxiv.org/abs/2504.01698</guid>
<content:encoded><![CDATA[
arXiv:2504.01698v3 Announce Type: replace 
Abstract: Theory of Mind (ToM), the ability to attribute mental states to others, is fundamental for human social intelligence and a critical capability for advanced Artificial Intelligence. Recent advancements in Large Language Models (LLMs) have shown promising performance on ToM benchmarks, raising the question: Do these benchmarks necessitate explicit human-like reasoning processes, or can models succeed through alternative strategies? We investigate this question empirically by applying Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to LLMs of varying scales (0.5B to 7B parameters) and evaluating them across multiple ToM datasets. Our results reveal a scale-dependent impact of RL: while RL significantly improves accuracy and fosters high-quality, interpretable, and transferable belief-tracking reasoning in larger models (7B), it leads to "reasoning collapse" in smaller models ($\leq$3B), where high accuracy and generalization ability are achieved via drastically shortened, less meaningful responses. Surprisingly, further SFT achieves competitive and generalizable performance across these benchmarks, often matching or exceeding RL models in accuracy, despite not being explicitly trained to produce structured reasoning traces. These findings highlight a critical discrepancy between benchmark accuracy and the nature of learned reasoning. Our work suggests that current ToM benchmarks may be solvable without requiring the explicit, human-like simulation of mental states they were designed to probe. LLMs, particularly when scale is limited or training signals focus solely on output correctness, may leverage alternative rules effective for benchmark data structures.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized Synthetic Text Generation with SimpleStories</title>
<link>https://arxiv.org/abs/2504.09184</link>
<guid>https://arxiv.org/abs/2504.09184</guid>
<content:encoded><![CDATA[
arXiv:2504.09184v2 Announce Type: replace 
Abstract: We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million samples each in English and Japanese. Through parameterizing prompts at multiple levels of abstraction, we achieve control over story characteristics at scale, inducing syntactic and semantic diversity. Ablations on a newly trained model suite show improved sample efficiency and model interpretability compared to the TinyStories dataset. We open-source all constituent parts of model creation, hoping to enable novel ways to study the end-to-end training process. As a byproduct, we move the frontier regarding the fewest-parameter language model that outputs grammatical natural language.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety in Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2504.17704</link>
<guid>https://arxiv.org/abs/2504.17704</guid>
<content:encoded><![CDATA[
arXiv:2504.17704v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning</title>
<link>https://arxiv.org/abs/2403.11083</link>
<guid>https://arxiv.org/abs/2403.11083</guid>
<content:encoded><![CDATA[
arXiv:2403.11083v2 Announce Type: replace-cross 
Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, our objective is to develop a generic anomaly detection model that can be applied in multiple scenarios. To achieve this, we custom-build generic visual language foundation models that possess extensive knowledge and robust reasoning abilities as anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers diverse prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images, point clouds, and videos. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is publicly available at https://github.com/Xiaohao-Xu/Customizable-VLM
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Attention Sequence Parallelism</title>
<link>https://arxiv.org/abs/2404.02882</link>
<guid>https://arxiv.org/abs/2404.02882</guid>
<content:encoded><![CDATA[
arXiv:2404.02882v3 Announce Type: replace-cross 
Abstract: Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective</title>
<link>https://arxiv.org/abs/2405.13522</link>
<guid>https://arxiv.org/abs/2405.13522</guid>
<content:encoded><![CDATA[
arXiv:2405.13522v3 Announce Type: replace-cross 
Abstract: Traditional time series forecasting methods predominantly rely on historical data patterns, neglecting external interventions that significantly shape future dynamics. Through control-theoretic analysis, we show that the implicit "self-stimulation" assumption limits the accuracy of these forecasts. To overcome this limitation, we propose an Intervention-Aware Time Series Forecasting (IATSF) framework explicitly designed to incorporate external interventions. We particularly emphasize textual interventions due to their unique capability to represent qualitative or uncertain influences inadequately captured by conventional exogenous variables. We propose a leak-free benchmark composed of temporally synchronized textual intervention data across synthetic and real-world scenarios. To rigorously evaluate IATSF, we develop FIATS, a lightweight forecasting model that integrates textual interventions through Channel-Aware Adaptive Sensitivity Modeling (CASM) and Channel-Aware Parameter Sharing (CAPS) mechanisms, enabling the model to adjust its sensitivity to interventions and historical data in a channel-specific manner. Extensive empirical evaluations confirm that FIATS surpasses state-of-the-art methods, highlighting that forecasting improvements stem explicitly from modeling external interventions rather than increased model complexity alone.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Item-Language Model for Conversational Recommendation</title>
<link>https://arxiv.org/abs/2406.02844</link>
<guid>https://arxiv.org/abs/2406.02844</guid>
<content:encoded><![CDATA[
arXiv:2406.02844v2 Announce Type: replace-cross 
Abstract: Large-language Models (LLMs) have been extremely successful at tasks like complex dialogue understanding, reasoning and coding due to their emergent abilities. These emergent abilities have been extended with multi-modality to include image, audio, and video capabilities. Recommender systems, on the other hand, have been critical for information seeking and item discovery needs. Recently, there have been attempts to apply LLMs for recommendations. One difficulty of current attempts is that the underlying LLM is usually not trained on the recommender system data, which largely contains user interaction signals and is often not publicly available. Another difficulty is user interaction signals often have a different pattern from natural language text, and it is currently unclear if the LLM training setup can learn more non-trivial knowledge from interaction signals compared with traditional recommender system methods. Finally, it is difficult to train multiple LLMs for different use-cases, and to retain the original language and reasoning abilities when learning from recommender system data. To address these three limitations, we propose an Item-Language Model (ILM), which is composed of an item encoder to produce text-aligned item representations that encode user interaction signals, and a frozen LLM that can understand those item representations with preserved pretrained knowledge. We conduct extensive experiments which demonstrate both the importance of the language-alignment and of user interaction knowledge in the item encoder.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers</title>
<link>https://arxiv.org/abs/2406.11624</link>
<guid>https://arxiv.org/abs/2406.11624</guid>
<content:encoded><![CDATA[
arXiv:2406.11624v5 Announce Type: replace-cross 
Abstract: Transformer-based models generate hidden states that are difficult to interpret. In this work, we analyze hidden states and modify them at inference, with a focus on motion forecasting. We use linear probing to analyze whether interpretable features are embedded in hidden states. Our experiments reveal high probing accuracy, indicating latent space regularities with functionally important directions. Building on this, we use the directions between hidden states with opposing features to fit control vectors. At inference, we add our control vectors to hidden states and evaluate their impact on predictions. Remarkably, such modifications preserve the feasibility of predictions. We further refine our control vectors using sparse autoencoders (SAEs). This leads to more linear changes in predictions when scaling control vectors. Our approach enables mechanistic interpretation as well as zero-shot generalization to unseen dataset characteristics with negligible computational overhead.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions</title>
<link>https://arxiv.org/abs/2410.00031</link>
<guid>https://arxiv.org/abs/2410.00031</guid>
<content:encoded><![CDATA[
arXiv:2410.00031v2 Announce Type: replace-cross 
Abstract: Machine-learning technologies are seeing increased deployment in real-world market scenarios. In this work, we explore the strategic behaviors of large language models (LLMs) when deployed as autonomous agents in multi-commodity markets, specifically within Cournot competition frameworks. We examine whether LLMs can independently engage in anti-competitive practices such as collusion or, more specifically, market division. Our findings demonstrate that LLMs can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies, thereby maximizing profitability without direct human input or explicit collusion commands. These results pose unique challenges and opportunities for businesses looking to integrate AI into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets. The study provides a foundation for further exploration into the ramifications of deferring high-stakes decisions to LLM-based agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction</title>
<link>https://arxiv.org/abs/2410.11507</link>
<guid>https://arxiv.org/abs/2410.11507</guid>
<content:encoded><![CDATA[
arXiv:2410.11507v4 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly deployed to various vertical domains, automatically evaluating their performance across different domains remains a critical challenge. Current evaluation methods often rely on static and resource-intensive datasets that are not aligned with real-world requirements and lack cross-domain adaptability. To address these limitations, we revisit the evaluation process and introduce two key concepts: \textbf{Benchmark+}, which extends the traditional question-answer benchmark into a more flexible ``strategy-criterion'' format; and \textbf{Assessment+}, which enhances the interaction process to facilitate deeper exploration and comprehensive analysis from multiple perspectives. We propose \textbf{\textsc{TestAgent}}, an agent-based evaluation framework that implements these concepts using retrieval-augmented generation and reinforcement learning. \textsc{TestAgent} enables automatic dynamic benchmark generation and in-depth assessment across diverse vertical domains. Experiments on tasks ranging from constructing multiple vertical domain evaluations to transforming static benchmarks into dynamic forms demonstrate the effectiveness of \textsc{TestAgent}. This work provides a novel perspective on automatic evaluation methods for domain-specific LLMs, offering a pathway for domain-adaptive dynamic benchmark construction and exploratory assessment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search</title>
<link>https://arxiv.org/abs/2410.14609</link>
<guid>https://arxiv.org/abs/2410.14609</guid>
<content:encoded><![CDATA[
arXiv:2410.14609v2 Announce Type: replace-cross 
Abstract: Conversational Search (CS) involves retrieving relevant documents from a corpus while considering the conversational context, integrating retrieval with context modeling. Recent advancements in Large Language Models (LLMs) have significantly enhanced CS by enabling query rewriting based on conversational context. However, employing LLMs during inference poses efficiency challenges. Existing solutions mitigate this issue by distilling embeddings derived from human-rewritten queries, focusing primarily on learning the context modeling task. These methods, however, often separate the contrastive retrieval task from the distillation process, treating it as an independent loss term. To overcome these limitations, we introduce DiSCo (Distillation of Sparse Conversational retrieval), a novel approach that unifies retrieval and context modeling through a relaxed distillation objective. Instead of relying exclusively on representation learning, our method distills similarity scores between conversations and documents, providing more freedom in the representation space and better leveraging the contrastive nature of document relevance. Extensive experiments on Learned Sparse Retrieval (LSR) across five CS datasets demonstrate that DiSCo achieves substantial improvements in both in-domain and out-of-domain retrieval tasks, achieving up to a six-point gain in recall for out-of-domain datasets over state-of-the-art methods. Additionally, DiSCo employs a multi-teacher distillation strategy, using multiple LLMs as teachers, further enhancing performance and surpassing the individual teachers in in-domain settings. Furthermore, analysis of model sparsity reveals that DiSCo allows for more effective control over the sparsity of the trained models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection</title>
<link>https://arxiv.org/abs/2410.14731</link>
<guid>https://arxiv.org/abs/2410.14731</guid>
<content:encoded><![CDATA[
arXiv:2410.14731v2 Announce Type: replace-cross 
Abstract: KV cache has become a de facto technique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. As the size of the model and data grows, the KV cache can quickly become a bottleneck within the system in both storage and memory transfer. To address this, prior studies usually focus on the first three axes of the cache tensors for compression. This paper supplements them, focusing on the feature dimension axis, by utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. We begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (PCA). We observe the issue with PCA projection where significant performance degradation is observed at low compression rates. To bridge the gap, we propose to directly tune the orthogonal projection matrices with a distillation objective using an elaborate Matryoshka training strategy. After training, we adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. Compared to previous works, our method can easily embrace pre-trained LLMs and hold a smooth tradeoff between performance and compression rate. We empirically witness the high data efficiency of our training procedure and find that our method can sustain over 90% performance with an average KV cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title>
<link>https://arxiv.org/abs/2411.02335</link>
<guid>https://arxiv.org/abs/2411.02335</guid>
<content:encoded><![CDATA[
arXiv:2411.02335v3 Announce Type: replace-cross 
Abstract: Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision-Language Models as Evaluators in Path Planning</title>
<link>https://arxiv.org/abs/2411.18711</link>
<guid>https://arxiv.org/abs/2411.18711</guid>
<content:encoded><![CDATA[
arXiv:2411.18711v4 Announce Type: replace-cross 
Abstract: Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities</title>
<link>https://arxiv.org/abs/2501.02406</link>
<guid>https://arxiv.org/abs/2501.02406</guid>
<content:encoded><![CDATA[
arXiv:2501.02406v4 Announce Type: replace-cross 
Abstract: Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly challenging as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by a particular LLM or not? We model LLM-generated text as a sequential stochastic process with complete dependence on history. We then design zero-shot statistical tests to (i) distinguish between text generated by two different known sets of LLMs $A$ (non-sanctioned) and $B$ (in-house), and (ii) identify whether text was generated by a known LLM or generated by any unknown model, e.g., a human or some other language generation process. We prove that the type I and type II errors of our test decrease exponentially with the length of the text. For that, we show that if $B$ generates the text, then except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. We then present experiments using LLMs with white-box access to support our theoretical results and empirically examine the robustness of our results to black-box settings and adversarial attacks. In the black-box setting, our method achieves an average TPR of 82.5\% at a fixed FPR of 5\%. Under adversarial perturbations, our minimum TPR is 48.6\% at the same FPR threshold. Both results outperform all non-commercial baselines. See https://github.com/TaraRadvand74/llm-text-detection for code, data, and an online demo of the project.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2502.01384</link>
<guid>https://arxiv.org/abs/2502.01384</guid>
<content:encoded><![CDATA[
arXiv:2502.01384v2 Announce Type: replace-cross 
Abstract: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shuttle Between the Instructions and the Parameters of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02315</link>
<guid>https://arxiv.org/abs/2502.02315</guid>
<content:encoded><![CDATA[
arXiv:2502.02315v3 Announce Type: replace-cross 
Abstract: The interaction with Large Language Models (LLMs) through instructions has been extensively investigated in the research community. While instructions have been widely used as the guidelines for task solving, this paper further notices that both instructions and parameters are the compression of task data. Therefore, they could be strongly correlated and can be learned to predict one from the other. This paper proposes a novel neural network framework, SHIP (\textbf{Sh}uttle between the \textbf{I}nstructions and the \textbf{P}arameters), to model and learn the mutual mappings between the instructions and the parameters of LLMs. We verify that SHIP can effectively map one of the instructions/parameters to the other by evaluating it on the tasks of instruction deduction and induction. The results show that SHIP performs better than existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. Moreover, SHIP can effectively combine the two mapping processes to perform excellent inductive reasoning. The code and data for this paper are released at https://anonymous.4open.science/r/Shuttle-Between-Instructions-Parameters/.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?</title>
<link>https://arxiv.org/abs/2502.09933</link>
<guid>https://arxiv.org/abs/2502.09933</guid>
<content:encoded><![CDATA[
arXiv:2502.09933v4 Announce Type: replace-cross 
Abstract: The ability to recognize patterns from examples and apply them to new ones is a primal ability for general intelligence, and is widely studied by psychology and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually <10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations often focus on classification, and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context reasoning benchmark for pattern recognition that asks LLM to predict output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for many-shot in-context reasoning, and acquired many insightful findings including scaling effect, robustness, inductive vs. transductive reasoning, retrieval Augmented Generation (RAG), coding for inductive reasoning, cross-domain generalizability, etc.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark</title>
<link>https://arxiv.org/abs/2502.19676</link>
<guid>https://arxiv.org/abs/2502.19676</guid>
<content:encoded><![CDATA[
arXiv:2502.19676v4 Announce Type: replace-cross 
Abstract: Forecasting is an important task in many domains, such as technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and Confidence Assessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. FOReCAst spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Preference Optimization for Vision-Language Long-Horizon Task Planning</title>
<link>https://arxiv.org/abs/2502.20742</link>
<guid>https://arxiv.org/abs/2502.20742</guid>
<content:encoded><![CDATA[
arXiv:2502.20742v3 Announce Type: replace-cross 
Abstract: Existing methods for vision-language task planning excel in short-horizon tasks but often fall short in complex, long-horizon planning within dynamic environments. These challenges primarily arise from the difficulty of effectively training models to produce high-quality reasoning processes for long-horizon tasks. To address this, we propose Structured Preference Optimization (SPO), which aims to enhance reasoning and action selection in long-horizon task planning through structured preference evaluation and optimized training strategies. Specifically, SPO introduces: 1) Preference-Based Scoring and Optimization, which systematically evaluates reasoning chains based on task relevance, visual grounding, and historical consistency; and 2) Curriculum-Guided Training, where the model progressively adapts from simple to complex tasks, improving its generalization ability in long-horizon scenarios and enhancing reasoning robustness. To advance research in vision-language long-horizon task planning, we introduce ExtendaBench, a comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat 2.0, categorized into ultra-short, short, medium, and long tasks. Experimental results demonstrate that SPO significantly improves reasoning quality and final decision accuracy, outperforming prior methods on long-horizon tasks and underscoring the effectiveness of preference-driven optimization in vision-language task planning. Specifically, SPO achieves a +5.98% GCR and +4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement in Habitat over the best-performing baselines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations</title>
<link>https://arxiv.org/abs/2504.13955</link>
<guid>https://arxiv.org/abs/2504.13955</guid>
<content:encoded><![CDATA[
arXiv:2504.13955v4 Announce Type: replace-cross 
Abstract: The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Idea Bench 2025: AI Research Idea Generation Benchmark</title>
<link>https://arxiv.org/abs/2504.14191</link>
<guid>https://arxiv.org/abs/2504.14191</guid>
<content:encoded><![CDATA[
arXiv:2504.14191v2 Announce Type: replace-cross 
Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction and achieved significant success in the generation of novel ideas. However, current assessments of idea generation overlook crucial factors such as knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded truth, and the limited scope of feasibility analysis constrained by prompt design. These limitations hinder the potential of uncovering groundbreaking research ideas. In this paper, we present AI Idea Bench 2025, a framework designed to quantitatively evaluate and compare the ideas generated by LLMs within the domain of AI research from diverse perspectives. The framework comprises a comprehensive dataset of 3,495 AI papers and their associated inspired works, along with a robust evaluation methodology. This evaluation system gauges idea quality in two dimensions: alignment with the ground-truth content of the original papers and judgment based on general reference material. AI Idea Bench 2025's benchmarking system stands to be an invaluable resource for assessing and comparing idea-generation techniques, thereby facilitating the automation of scientific discovery.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Word Suggestion using Graph Neural Network</title>
<link>https://arxiv.org/abs/2505.09649</link>
<guid>https://arxiv.org/abs/2505.09649</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Modeling, Graph Convolution, GNNs, LSTMs, Context Embedding

Summary: 
In this project, the focus is on context embedding within language modeling. Traditional language models require large models and extensive resources, but this project aims to develop a more efficient approach. By utilizing the Graph Convolution operation in Graph Neural Networks (GNNs) to encode context and combining it with Long Short-Term Memory networks (LSTMs), the researchers were able to predict the next word in a sentence with high accuracy. The experiment was conducted on a custom Wikipedia text corpus using limited resources, demonstrating the effectiveness of the proposed approach. Overall, this project showcases a promising method for improving language modeling tasks by leveraging graph convolution and LSTM networks for context embedding. 

<br /><br />Summary: <div>
arXiv:2505.09649v1 Announce Type: new 
Abstract: Language Modeling is a prevalent task in Natural Language Processing. The currently existing most recent and most successful language models often tend to build a massive model with billions of parameters, feed in a tremendous amount of text data, and train with enormous computation resources which require millions of dollars. In this project, we aim to address an important sub-task in language modeling, i.e., context embedding. We propose an approach to exploit the Graph Convolution operation in GNNs to encode the context and use it in coalition with LSTMs to predict the next word given a local context of preceding words. We test this on the custom Wikipedia text corpus using a very limited amount of resources and show that this approach works fairly well to predict the next word.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.09655</link>
<guid>https://arxiv.org/abs/2505.09655</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language model, diversity-aware reward adjustment, submodular mutual information, mathematical reasoning<br />
Summary:<br />
The article introduces Diversity-aware Reward Adjustment (DRA) to tackle the diversity-quality inconsistency in reinforcement learning for language model post-training. By incorporating Submodular Mutual Information (SMI) to adjust rewards based on semantic diversity, DRA encourages better exploration while maintaining stable exploitation of high-quality samples. The method seamlessly integrates with Group Relative Policy Optimization (GRPO) and its variant DR.GRPO, resulting in DRA-GRPO and DGA-DR.GRPO. Evaluation on five mathematical reasoning benchmarks shows that DRA outperforms recent baselines, achieving state-of-the-art accuracy of 58.2% with only 7,000 fine-tuning samples and a total training cost of $55. The code for the method is available on GitHub at https://github.com/xiwenc1/DRA-GRPO. <br />Summary: <div>
arXiv:2505.09655v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at https://github.com/xiwenc1/DRA-GRPO.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Are More Persuasive Than Incentivized Human Persuaders</title>
<link>https://arxiv.org/abs/2505.09662</link>
<guid>https://arxiv.org/abs/2505.09662</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, persuasion, interactive quiz, incentivized experiment, AI<br />
Summary:<br />
In a study comparing the persuasive abilities of a large language model (LLM) and incentivized human persuaders in an interactive quiz setting, the LLM outperformed humans in persuading participants towards correct and incorrect answers. The LLM persuaders achieved significantly higher compliance with their directions, leading to increased quiz takers' accuracy and earnings when guided towards correct answers, and decreased accuracy and earnings when directed towards incorrect answers. The findings suggest that AI's persuasive capabilities surpass those of humans even in scenarios involving real-money incentives. The study underscores the importance of developing alignment and governance frameworks to address the increasing capabilities of AI persuaders. <div>
arXiv:2505.09662v1 Announce Type: new 
Abstract: We directly compare the persuasion capabilities of a frontier large language model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an interactive, real-time conversational quiz setting. In this preregistered, large-scale incentivized experiment, participants (quiz takers) completed an online quiz where persuaders (either humans or LLMs) attempted to persuade quiz takers toward correct or incorrect answers. We find that LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than incentivized human persuaders, demonstrating superior persuasive capabilities in both truthful (toward correct answers) and deceptive (toward incorrect answers) contexts. We also find that LLM persuaders significantly increased quiz takers' accuracy, leading to higher earnings, when steering quiz takers toward correct answers, and significantly decreased their accuracy, leading to lower earnings, when steering them toward incorrect answers. Overall, our findings suggest that AI's persuasion capabilities already exceed those of humans that have real-money bonuses tied to performance. Our findings of increasingly capable AI persuaders thus underscore the urgency of emerging alignment and governance frameworks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Prompt Optimization with Meta-Learning</title>
<link>https://arxiv.org/abs/2505.09666</link>
<guid>https://arxiv.org/abs/2505.09666</guid>
<content:encoded><![CDATA[
<div> optimization, language models, prompts, meta-learning, transferability
Summary: 
The article introduces the concept of bilevel system prompt optimization for Large Language Models (LLMs). It addresses the need to optimize the system prompt, which is task-agnostic and can be applied across different tasks and domains. A meta-learning framework is proposed to optimize the system prompt over various user prompts from multiple datasets, ensuring robustness and transferability. Experimental results on 14 unseen datasets demonstrate the effectiveness of the approach in generalizing to diverse user prompts. The optimized system prompt also enables rapid adaptation to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.<br /><br />Summary: <div>
arXiv:2505.09666v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts</title>
<link>https://arxiv.org/abs/2505.09701</link>
<guid>https://arxiv.org/abs/2505.09701</guid>
<content:encoded><![CDATA[
<div> Framework, factuality evaluation, VeriFact, FactRBench, long-form model responses

Summary:
VeriFact is a factuality evaluation framework designed to enhance fact extraction by resolving incomplete and missing facts in long-form model responses. It aims to improve factuality assessment accuracy by considering essential context and relational facts often overlooked by existing evaluation pipelines. The introduced FactRBench benchmark evaluates precision and recall in long-form model responses, providing reference fact sets for assessment. Empirical evaluations demonstrate that VeriFact enhances fact completeness and preserves complex relational information, leading to more accurate factuality evaluation. Benchmarking various open- and close-weight LLMs on FactRBench reveals that larger models within the same family improve precision and recall. The study highlights the importance of comprehensive factuality assessment and challenges the assumption that high precision always corresponds to high recall in generating fact-based responses. 

<br /><br />Summary: <div>
arXiv:2505.09701v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at generating long-form responses, but evaluating their factuality remains challenging due to complex inter-sentence dependencies within the generated facts. Prior solutions predominantly follow a decompose-decontextualize-verify pipeline but often fail to capture essential context and miss key relational facts. In this paper, we introduce VeriFact, a factuality evaluation framework designed to enhance fact extraction by identifying and resolving incomplete and missing facts to support more accurate verification results. Moreover, we introduce FactRBench , a benchmark that evaluates both precision and recall in long-form model responses, whereas prior work primarily focuses on precision. FactRBench provides reference fact sets from advanced LLMs and human-written answers, enabling recall assessment. Empirical evaluations show that VeriFact significantly enhances fact completeness and preserves complex facts with critical relational information, resulting in more accurate factuality evaluation. Benchmarking various open- and close-weight LLMs on FactRBench indicate that larger models within same model family improve precision and recall, but high precision does not always correlate with high recall, underscoring the importance of comprehensive factuality assessment.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs</title>
<link>https://arxiv.org/abs/2505.09724</link>
<guid>https://arxiv.org/abs/2505.09724</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, text analysis, taxonomy, intercoder reliability, data-driven

Summary: 
LLMs are efficient tools for analyzing unstructured text data, offering the flexibility of using predefined or data-driven taxonomies. This tutorial outlines a systematic approach to develop, test, and apply taxonomies for text analysis in a collaborative manner. By using personal goals as an example, researchers can write prompts, generate a taxonomy of life domains, evaluate and refine it, test for intercoder agreements, and apply the taxonomy with high reliability. The article discusses the potential and limitations of LLMs for text analysis, highlighting the benefits of iterative and collaborative processes in achieving high-quality results. <div>
arXiv:2505.09724v1 Announce Type: new 
Abstract: Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning</title>
<link>https://arxiv.org/abs/2505.09738</link>
<guid>https://arxiv.org/abs/2505.09738</guid>
<content:encoded><![CDATA[
<div> transplantation method, pre-tokenization learning, token embeddings, compression gains, perplexity results 

Summary:
Tokenadapt introduces a model-agnostic tokenizer transplantation method and pre-tokenization learning for multi-word Supertokens to address constraints in pretrained language models' tokenization schemes. The transplantation heuristic combines local estimates based on subword decomposition and global estimates using semantically similar tokens to minimize retraining needs while preserving semantics. Empirical investigations validate the effectiveness of Tokenadapt in initializing unique tokens, outperforming conventional baselines like ReTok and TransTokenizer. Additionally, Supertokens achieve notable compression gains. Zero-shot perplexity results show that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to ReTok and TransTokenizer across different base models and target tokenizers, with TokenAdapt significantly reducing overall perplexity ratio compared to ReTok. This approach offers a promising solution to overcome tokenizer lock-in challenges and improve efficiency and performance in multilingual or specialized applications. 

<br /><br />Summary: <div>
arXiv:2505.09738v1 Announce Type: new 
Abstract: Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this often require prohibitive computational resources. Although tokenizer replacement with heuristic initialization aims to reduce this burden, existing methods often require exhaustive residual fine-tuning and still may not fully preserve semantic nuances or adequately address the underlying compression inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a model-agnostic tokenizer transplantation method, and second, novel pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation. Tokenadapt initializes new unique token embeddings via a hybrid heuristic that combines two methods: a local estimate based on subword decomposition using the old tokenizer, and a global estimate utilizing the top-k semantically similar tokens from the original vocabulary. This methodology aims to preserve semantics while significantly minimizing retraining requirements. Empirical investigations validate both contributions: the transplantation heuristic successfully initializes unique tokens, markedly outperforming conventional baselines and sophisticated methods including Transtokenizer and ReTok, while our Supertokens achieve notable compression gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to both ReTok and TransTokenizer baselines across different base models and newly trained target tokenizers. TokenAdapt typically reduced the overall perplexity ratio significantly compared to ReTok, yielding at least a 2-fold improvement in these aggregate scores.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques</title>
<link>https://arxiv.org/abs/2505.09794</link>
<guid>https://arxiv.org/abs/2505.09794</guid>
<content:encoded><![CDATA[
<div> NLP, Named Entity Recognition, Cancer, EHRs, Data Extraction
Summary: 
- Research projects, especially in cancer, often require manual extraction of information from clinical reports, limiting efficiency.
- Natural Language Processing (NLP) offers a solution by automating data extraction from electronic health records (EHRs).
- The study focuses on lung and breast cancer due to their high incidence and impact on public health.
- Utilizing GMV's NLP tool uQuery, the study aims to enhance accuracy and efficiency in data extraction from EHRs.
- NLP techniques, specifically Named Entity Recognition (NER), are used to automatically identify and extract key clinical information from 200 breast cancer and 400 lung cancer reports.
- Fine-tuning the bsc-bio-ehr-en3 model enables accurate recognition of clinical entities, with strong performance in identifying key entities like MET and PAT.
- Challenges remain with less frequent entities like EVOL. 

<br /><br />Summary: <div>
arXiv:2505.09794v1 Announce Type: new 
Abstract: Research projects, including those focused on cancer, rely on the manual extraction of information from clinical reports. This process is time-consuming and prone to errors, limiting the efficiency of data-driven approaches in healthcare. To address these challenges, Natural Language Processing (NLP) offers an alternative for automating the extraction of relevant data from electronic health records (EHRs). In this study, we focus on lung and breast cancer due to their high incidence and the significant impact they have on public health. Early detection and effective data management in both types of cancer are crucial for improving patient outcomes. To enhance the accuracy and efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels at identifying relevant entities in clinical texts and converting them into standardized formats such as SNOMED and OMOP. uQuery not only detects and classifies entities but also associates them with contextual information, including negated entities, temporal aspects, and patient-related details. In this work, we explore the use of NLP techniques, specifically Named Entity Recognition (NER), to automatically identify and extract key clinical information from EHRs related to these two cancers. A dataset from Health Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast cancer and 400 lung cancer reports, was used, with eight clinical entities manually labeled using the Doccano platform. To perform NER, we fine-tuned the bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained in Spanish. Fine-tuning was performed using the Transformers architecture, enabling accurate recognition of clinical entities in these cancer types. Our results demonstrate strong overall performance, particularly in identifying entities like MET and PAT, although challenges remain with less frequent entities like EVOL.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the generalization of LLM truth directions on conversational formats</title>
<link>https://arxiv.org/abs/2505.09807</link>
<guid>https://arxiv.org/abs/2505.09807</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, truth direction, lie detection, conversational formats, generalization 

Summary:
In this work, the researchers investigate the generalization of the truth direction in Large Language Models (LLMs) across various conversational formats. Previous studies have shown that linear probes on a single hidden state of LLMs can detect lies in conversations. The researchers found that there is good generalization between short conversations ending on a lie, but poor generalization to longer formats with lies occurring earlier in the conversation. To improve this generalization, the researchers propose adding a fixed key phrase at the end of each conversation, which significantly enhances the detection accuracy. These findings shed light on the challenges faced in developing reliable lie detectors based on LLMs that can generalize to different conversational settings. <br /><br />Summary: <div>
arXiv:2505.09807v1 Announce Type: new 
Abstract: Several recent works argue that LLMs have a universal truth direction where true and false statements are linearly separable in the activation space of the model. It has been demonstrated that linear probes trained on a single hidden state of the model already generalize across a range of topics and might even be used for lie detection in LLM conversations. In this work we explore how this truth direction generalizes between various conversational formats. We find good generalization between short conversations that end on a lie, but poor generalization to longer formats where the lie appears earlier in the input prompt. We propose a solution that significantly improves this type of generalization by adding a fixed key phrase at the end of each conversation. Our results highlight the challenges towards reliable LLM lie detectors that generalize to new settings.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning</title>
<link>https://arxiv.org/abs/2505.09825</link>
<guid>https://arxiv.org/abs/2505.09825</guid>
<content:encoded><![CDATA[
<div> literary works, close reading, language models, interpretive reasoning, benchmarking 
Summary: 

- Close reading is a common practice in college English courses where students analyze literary texts through evidence-based arguments.
- Close reading has not been evaluated on large language models, leading to the development of the KRISTEVA benchmark.
- KRISTEVA consists of 1331 multiple-choice questions designed to test different aspects of close reading, such as extracting stylistic features and multi-hop reasoning.
- While state-of-the-art language models show some competency in close reading tasks, they still lag behind human evaluators in accuracy on most tasks.
- This benchmark aims to bridge the gap in evaluating interpretive reasoning in literary analysis using machine learning technologies. 

<br /><br />Summary: <div>
arXiv:2505.09825v1 Announce Type: new 
Abstract: Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting</title>
<link>https://arxiv.org/abs/2505.09852</link>
<guid>https://arxiv.org/abs/2505.09852</guid>
<content:encoded><![CDATA[
<div> parametric knowledge, conflict escalation, fatalities, Large Language Models, early warning systems

Summary:
Large Language Models (LLMs) are being explored for their ability to predict conflict escalation and fatalities without external data, crucial for early warning systems and policy-making. The study evaluates LLMs' parametric knowledge encoded in pretrained weights, comparing it with non-parametric capabilities where models access external conflict datasets and news reports. The evaluation covers conflict-prone regions in the Horn of Africa and the Middle East from 2020-2024. In the parametric setting, LLMs forecast conflict trends and fatalities solely based on pretrained knowledge. In the non-parametric setting, models incorporate recent conflict summaries and geopolitical developments. The findings showcase the strengths and limitations of LLMs in conflict forecasting and emphasize the benefits of augmenting them with structured external knowledge. 

<br /><br />Summary: <div>
arXiv:2505.09852v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive performance across natural language tasks, but their ability to forecast violent conflict remains underexplored. We investigate whether LLMs possess meaningful parametric knowledge-encoded in their pretrained weights-to predict conflict escalation and fatalities without external data. This is critical for early warning systems, humanitarian planning, and policy-making. We compare this parametric knowledge with non-parametric capabilities, where LLMs access structured and unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent news reports via Retrieval-Augmented Generation (RAG). Incorporating external information could enhance model performance by providing up-to-date context otherwise missing from pretrained weights. Our two-part evaluation framework spans 2020-2024 across conflict-prone regions in the Horn of Africa and the Middle East. In the parametric setting, LLMs predict conflict trends and fatalities relying only on pretrained knowledge. In the non-parametric setting, models receive summaries of recent conflict events, indicators, and geopolitical developments. We compare predicted conflict trend labels (e.g., Escalate, Stable Conflict, De-escalate, Peace) and fatalities against historical data. Our findings highlight the strengths and limitations of LLMs for conflict forecasting and the benefits of augmenting them with structured external knowledge.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries</title>
<link>https://arxiv.org/abs/2505.09902</link>
<guid>https://arxiv.org/abs/2505.09902</guid>
<content:encoded><![CDATA[
<div> language models, Spanish variants, localization, sociolinguistic dissonances, AI models <br />
<br />
Language models based on Spanish face challenges due to differences in variants across Latin America and Spain, leading to sociolinguistic dissonances among dialectal groups. This paper highlights the need for regional localized models to bridge these divides and enhance user trust and reliance on AI. By implementing the proposed five sub variants of Spanish, localization strategies can be improved to meet inclusivity goals and drive user growth in low-risk investment areas. This approach not only fosters a better understanding of cultural, historical, and sociolinguistic nuances but also contributes to the success of internationalization strategies. Ultimately, the adoption of locale-sensitive AI models can lead to more efficient localization processes that benefit both users and developers. <br /><br />Summary: <div>
arXiv:2505.09902v1 Announce Type: new 
Abstract: Large language models are, by definition, based on language. In an effort to underscore the critical need for regional localized models, this paper examines primary differences between variants of written Spanish across Latin America and Spain, with an in-depth sociocultural and linguistic contextualization therein. We argue that these differences effectively constitute significant gaps in the quotidian use of Spanish among dialectal groups by creating sociolinguistic dissonances, to the extent that locale-sensitive AI models would play a pivotal role in bridging these divides. In doing so, this approach informs better and more efficient localization strategies that also serve to more adequately meet inclusivity goals, while securing sustainable active daily user growth in a major low-risk investment geographic area. Therefore, implementing at least the proposed five sub variants of Spanish addresses two lines of action: to foment user trust and reliance on AI language models while also demonstrating a level of cultural, historical, and sociolinguistic awareness that reflects positively on any internationalization strategy.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2505.09924</link>
<guid>https://arxiv.org/abs/2505.09924</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, watermarking, logits-based, sampling-based, symbiotic framework

Summary:
The paper introduces a novel symbiotic watermarking framework for Large Language Models (LLMs) that combines the strengths of logits-based and sampling-based watermarking schemes. The framework includes three strategies: serial, parallel, and hybrid, which leverage token entropy and semantic entropy for optimal watermark embedding. Through comprehensive experiments on various datasets and models, the proposed method outperforms existing baselines and achieves state-of-the-art performance. The symbiotic framework provides insights into diverse watermarking paradigms, addressing the trade-offs among robustness, text quality, security, and detectability. This versatile approach offers a balanced solution for watermarking AI-generated text. The code for the framework is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2505.09924v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at \href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Prompt Optimizers: From Prompt Merits to Optimization</title>
<link>https://arxiv.org/abs/2505.09930</link>
<guid>https://arxiv.org/abs/2505.09930</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt optimization, large language models, interpretable design, lightweight models, MePO

Summary:<br /><br />
This article introduces MePO, a merit-guided prompt optimizer designed to enhance prompt and response quality in large language models. By focusing on interpretable design principles, MePO improves performance without the need for fine-tuning model weights. The approach avoids reliance on advanced LLMs like GPT-4, making it more lightweight and locally deployable. MePO is trained on a preference dataset created from merit-aligned prompts generated by a lightweight LLM. Unlike previous methods, MePO reduces cost and privacy concerns, while learning clear and interpretable merits. The model demonstrates improved results across various tasks and model types, providing a scalable and robust solution for real-world deployment. The model and dataset are publicly available for further exploration and research. <div>
arXiv:2505.09930v1 Announce Type: new 
Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large language models (LLMs), enabling performance improvements without altering model weights. Existing methods typically rely on advanced, large-scale LLMs like GPT-4 to generate optimized prompts. However, due to limited downward compatibility, verbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight inference models and degrade response quality. In this work, we rethink prompt optimization through the lens of interpretable design. We first identify a set of model-agnostic prompt quality merits and empirically validate their effectiveness in enhancing prompt and response quality. We then introduce MePO, a merit-guided, lightweight, and locally deployable prompt optimizer trained on our preference dataset built from merit-aligned prompts generated by a lightweight LLM. Unlike prior work, MePO avoids online optimization reliance, reduces cost and privacy concerns, and, by learning clear, interpretable merits, generalizes effectively to both large-scale and lightweight inference models. Experiments demonstrate that MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution for real-world deployment. Our model and dataset are available at: https://github.com/MidiyaZhu/MePO
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph</title>
<link>https://arxiv.org/abs/2505.09945</link>
<guid>https://arxiv.org/abs/2505.09945</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, large language models, response generation, personalized information, retrieval augmented generation

Summary:
Knowledge graphs are used to augment large language models in generating personalized responses by providing factual and timely information. Overfitting in LLMs can lead to the generation of incorrect data, causing hallucinations. The proposed approach, retrieval augmented generation, leverages knowledge graphs to assist LLMs in generating accurate responses tailored to users' personal information, particularly calendar data. Experimental results show improved understanding of personal data and enhanced response accuracy compared to baseline LLMs using personal data as text inputs. Response time is moderately reduced with the proposed approach. <div>
arXiv:2505.09945v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has allowed numerous applications, including the generation of queried responses, to be leveraged in chatbots and other conversational assistants. Being trained on a plethora of data, LLMs often undergo high levels of over-fitting, resulting in the generation of extra and incorrect data, thus causing hallucinations in output generation. One of the root causes of such problems is the lack of timely, factual, and personalized information fed to the LLM. In this paper, we propose an approach to address these problems by introducing retrieval augmented generation (RAG) using knowledge graphs (KGs) to assist the LLM in personalized response generation tailored to the users. KGs have the advantage of storing continuously updated factual information in a structured way. While our KGs can be used for a variety of frequently updated personal data, such as calendar, contact, and location data, we focus on calendar data in this paper. Our experimental results show that our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs</title>
<link>https://arxiv.org/abs/2505.10013</link>
<guid>https://arxiv.org/abs/2505.10013</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, implicit bias, ethics, technical issue, benchmark

Summary:
Large Language Models (LLMs) have been a topic of concern due to potential biases inherited from training data. A study explores how LLMs display implicit bias, highlighting both ethical and technical issues. The inability of LLMs to handle extraneous information is a significant problem. The lack of standard methods for benchmarking bias in LLMs necessitated the development of the DIF (Demographic Implicit Fairness) benchmark. By evaluating logic and math problem datasets with sociodemographic personas, the method can statistically validate implicit bias in LLM behavior. The study reveals an inverse relationship between question answering accuracy and implicit bias, supporting the argument that LLMs struggle to handle diverse social contexts effectively. The DIF benchmark provides a valuable tool for assessing and addressing bias in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.10013v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) have risen in prominence over the past few years, there has been concern over the potential biases in LLMs inherited from the training data. Previous studies have examined how LLMs exhibit implicit bias, such as when response generation changes when different social contexts are introduced. We argue that this implicit bias is not only an ethical, but also a technical issue, as it reveals an inability of LLMs to accommodate extraneous information. However, unlike other measures of LLM intelligence, there are no standard methods to benchmark this specific subset of LLM bias. To bridge this gap, we developed a method for calculating an easily interpretable benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM logic and math problem datasets with sociodemographic personas. We demonstrate that this method can statistically validate the presence of implicit bias in LLM behavior and find an inverse trend between question answering accuracy and implicit bias, supporting our argument.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability</title>
<link>https://arxiv.org/abs/2505.10063</link>
<guid>https://arxiv.org/abs/2505.10063</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multi-document Question Answering, Coarse-to-fine Method, Retrieval Heads, Attention Mechanism

Summary:
CAFE is a novel two-stage method designed to improve multi-document question-answering capabilities in Large Language Models (LLMs). The method addresses the challenge of balancing retrieval precision and recall by gradually filtering out irrelevant documents and guiding attention to relevant content. In the first stage, a coarse-grained filtering method ranks relevant documents using retrieval heads. In the second stage, a fine-grained steering method directs attention to the most relevant information, reducing the influence of background and distracting documents. Experimental results demonstrate that CAFE outperforms baseline methods, achieving significant improvements in SubEM over existing approaches. Specifically, CAFE shows up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model, highlighting its effectiveness in enhancing the performance of LLMs in complex question-answering tasks. 

<br /><br />Summary: <div>
arXiv:2505.10063v1 Announce Type: new 
Abstract: Advancements in Large Language Models (LLMs) have extended their input context length, yet they still struggle with retrieval and reasoning in long-context inputs. Existing methods propose to utilize the prompt strategy and retrieval head to alleviate this limitation. However, they still face challenges in balancing retrieval precision and recall, impacting their efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$, a two-stage coarse-to-fine method to enhance multi-document question-answering capacities. By gradually eliminating the negative impacts of background and distracting documents, CAFE makes the responses more reliant on the evidence documents. Initially, a coarse-grained filtering method leverages retrieval heads to identify and rank relevant documents. Then, a fine-grained steering method guides attention to the most relevant content. Experiments across benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dark LLMs: The Growing Threat of Unaligned AI Models</title>
<link>https://arxiv.org/abs/2505.10066</link>
<guid>https://arxiv.org/abs/2505.10066</guid>
<content:encoded><![CDATA[
<div> Large Language Models, jailbreaking, vulnerabilities, dark content, ethical guardrails
Summary: Large Language Models (LLMs) have revolutionized various fields but are vulnerable to jailbreak attacks due to their training data. Dark LLMs lack ethical safeguards and can be manipulated to produce harmful outputs. A universal jailbreak attack, first shared online, compromises LLMs, allowing them to answer any question. Several state-of-the-art models remain susceptible despite the disclosure of this threat. The industry's response to AI safety concerns has been inadequate, posing risks with the increasing accessibility of model training and open-source LLMs. Without intervention, LLMs might democratize access to dangerous information, leading to unforeseen dangers. <br /><br />Summary: <div>
arXiv:2505.10066v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields from healthcare to education and beyond. However, alongside their remarkable capabilities lies a significant threat: the susceptibility of these models to jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems from the very data they learn from. As long as this training data includes unfiltered, problematic, or 'dark' content, the models can inherently learn undesirable patterns or weaknesses that allow users to circumvent their intended safety controls. Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. In our research, we uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. The main idea of our attack was published online over seven months ago. However, many of the tested LLMs were still vulnerable to this attack. Despite our responsible disclosure efforts, responses from major LLM providers were often inadequate, highlighting a concerning gap in industry practices regarding AI safety. As model training becomes more accessible and cheaper, and as open-source LLMs proliferate, the risk of widespread misuse escalates. Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing and Contextualising Probes for African Languages</title>
<link>https://arxiv.org/abs/2505.10081</link>
<guid>https://arxiv.org/abs/2505.10081</guid>
<content:encoded><![CDATA[
<div> Keywords: Pretrained language models, African languages, linguistic knowledge, layer-wise probes, interpretability techniques<br />
Summary:<br />
This paper presents a systematic investigation into probing Pretrained Language Models (PLMs) for linguistic knowledge about African languages. The study involves training layer-wise probes for six typologically diverse African languages to analyze the distribution of linguistic features. Results show that PLMs adapted for African languages encode more linguistic information about the target languages compared to massively multilingual PLMs. The research confirms previous findings that token-level syntactic information tends to concentrate in middle-to-last layers, while sentence-level semantic information is spread across all layers. Additionally, control tasks and probing baselines reveal that probe performance reflects the internal knowledge of PLMs rather than memorization, highlighting the internal mechanisms behind successful strategies like active learning and multilingual adaptation. The study applies established interpretability techniques to African-language PLMs, shedding light on the reasons behind the continuous improvement of PLMs for African languages.<br /> <div>
arXiv:2505.10081v1 Announce Type: new 
Abstract: Pretrained language models (PLMs) for African languages are continually improving, but the reasons behind these advances remain unclear. This paper presents the first systematic investigation into probing PLMs for linguistic knowledge about African languages. We train layer-wise probes for six typologically diverse African languages to analyse how linguistic features are distributed. We also design control tasks, a way to interpret probe performance, for the MasakhaPOS dataset. We find PLMs adapted for African languages to encode more linguistic information about target languages than massively multilingual PLMs. Our results reaffirm previous findings that token-level syntactic information concentrates in middle-to-last layers, while sentence-level semantic information is distributed across all layers. Through control tasks and probing baselines, we confirm that performance reflects the internal knowledge of PLMs rather than probe memorisation. Our study applies established interpretability techniques to African-language PLMs. In doing so, we highlight the internal mechanisms underlying the success of strategies like active learning and multilingual adaptation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XRAG: Cross-lingual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.10089</link>
<guid>https://arxiv.org/abs/2505.10089</guid>
<content:encoded><![CDATA[
<div> Benchmark, LLMs, Cross-lingual, Retrieval-Augmented Generation, XRAG<br />
Summary: <br />
- XRAG is a new benchmark designed to evaluate the generation abilities of Large Language Models (LLMs) in cross-lingual Retrieval-Augmented Generation (RAG) settings. 
- The dataset is constructed from recent news articles and includes questions that require external knowledge to be answered, challenging LLM reasoning abilities. 
- XRAG covers both monolingual and multilingual retrieval scenarios, with relevancy annotations provided for each retrieved document. 
- Experimental results on five LLMs reveal challenges in response language correctness in monolingual retrieval and reasoning over retrieved information in multilingual retrieval. 
- LLM performance lags significantly behind human performance in answering questions on XRAG, making it a valuable benchmark for studying LLM reasoning abilities in cross-lingual settings. 

Summary: <div>
arXiv:2505.10089v1 Announce Type: new 
Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings where the user language does not match the retrieval results. XRAG is constructed from recent news articles to ensure that its questions require external knowledge to be answered. It covers the real-world scenarios of monolingual and multilingual retrieval, and provides relevancy annotations for each retrieved document. Our novel dataset construction pipeline results in questions that require complex reasoning, as evidenced by the significant gap between human and LLM performance. Consequently, XRAG serves as a valuable benchmark for studying LLM reasoning abilities, even before considering the additional cross-lingual complexity. Experimental results on five LLMs uncover two previously unreported challenges in cross-lingual RAG: 1) in the monolingual retrieval setting, all evaluated models struggle with response language correctness; 2) in the multilingual retrieval setting, the main challenge lies in reasoning over retrieved information across languages rather than generation of non-English text.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs</title>
<link>https://arxiv.org/abs/2505.10113</link>
<guid>https://arxiv.org/abs/2505.10113</guid>
<content:encoded><![CDATA[
<div> Keywords: S-MedQA, medical question-answering, knowledge injection, fine-tuning, clinical specialties<br />
<br />
Summary: <br />
1) The research introduces S-MedQA, an English medical question-answering dataset for evaluating large language models in specific clinical fields.<br />
2) The study challenges the popular hypothesis that training on data from a medical specialty would yield the best performance on that specialty.<br />
3) Token probabilities of clinically relevant terms consistently increase across specialties, indicating the importance of domain shifting in performance improvement rather than knowledge injection.<br />
4) The findings suggest reconsidering the role of fine-tuning data in the medical domain and imply that gains in performance may be attributed more to domain-specific adaptation than previously thought.<br />
5) The dataset, S-MedQA, and all research code are released to the research community for replication of experiments. <div>
arXiv:2505.10113v1 Announce Type: new 
Abstract: In this paper, we introduce S-MedQA, an English medical question-answering (QA) dataset for benchmarking large language models in fine-grained clinical specialties. We use S-MedQA to check the applicability of a popular hypothesis related to knowledge injection in the knowledge-intense scenario of medical QA, and show that: 1) training on data from a speciality does not necessarily lead to best performance on that specialty and 2) regardless of the specialty fine-tuned on, token probabilities of clinically relevant terms for all specialties increase consistently. Thus, we believe improvement gains come mostly from domain shifting (e.g., general to medical) rather than knowledge injection and suggest rethinking the role of fine-tuning data in the medical domain. We release S-MedQA and all code needed to reproduce all our experiments to the research community.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs</title>
<link>https://arxiv.org/abs/2505.10143</link>
<guid>https://arxiv.org/abs/2505.10143</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Evidence-based response generation, Knowledge Graph, Retrieval-augmented agent, Trustworthiness

Summary: 
The paper introduces GE-Chat, a framework that enhances response generation from Large Language Models (LLMs) by utilizing a Knowledge Graph to provide evidence-based responses. By creating a knowledge graph from the user's uploaded document, the framework is able to construct a retrieval-augmented agent that includes additional knowledge beyond the model's training corpus. Through the use of Chain-of-Thought (CoT) logic generation, n-hop sub-graph searching, and entailment-based sentence generation, the framework improves the accuracy of evidence retrieval in a free-form context. This method helps users examine the sources of LLM conclusions and assess the trustworthiness of the generated responses. GE-Chat addresses the challenge of unreliable outputs from LLMs and aims to enhance user confidence in the decision-making process. 

<br /><br />Summary: <div>
arXiv:2505.10143v1 Announce Type: new 
Abstract: Large Language Models are now key assistants in human decision-making processes. However, a common note always seems to follow: "LLMs can make mistakes. Be careful with important info." This points to the reality that not all outputs from LLMs are dependable, and users must evaluate them manually. The challenge deepens as hallucinated responses, often presented with seemingly plausible explanations, create complications and raise trust issues among users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph enhanced retrieval-augmented generation framework to provide Evidence-based response generation. Specifically, when the user uploads a material document, a knowledge graph will be created, which helps construct a retrieval-augmented agent, enhancing the agent's responses with additional knowledge beyond its training corpus. Then we leverage Chain-of-Thought (CoT) logic generation, n-hop sub-graph searching, and entailment-based sentence generation to realize accurate evidence retrieval. We demonstrate that our method improves the existing models' performance in terms of identifying the exact evidence in a free-form context, providing a reliable way to examine the resources of LLM's conclusion and help with the judgment of the trustworthiness.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.10182</link>
<guid>https://arxiv.org/abs/2505.10182</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, reinforcement learning, large language models, continual pretraining, reasoning skills <br />
Summary: 
- Large Language Models (LLMs) show improved reasoning abilities through supervised fine-tuning and reinforcement learning.
- Continual pretraining (CPT) does not require task-specific signals, providing a broader range of training data.
- Reasoning CPT, utilizing synthetic data to reconstruct hidden thought processes in texts, enhances performance across various domains.
- Reasoning skills learned in one domain transfer effectively to others.
- Models trained with hidden thoughts adjust the depth of reasoning based on problem difficulty, leading to gains of up to 8 points on challenging problems. <div>
arXiv:2505.10182v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden thought processes underlying texts, based on the premise that texts are the result of the author's thinking process. Specifically, we apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis reveals that Reasoning CPT consistently improves performance across all evaluated domains. Notably, reasoning skills acquired in one domain transfer effectively to others; the performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Furthermore, models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think</title>
<link>https://arxiv.org/abs/2505.10185</link>
<guid>https://arxiv.org/abs/2505.10185</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought, Language models, Reasoning strategies, CoT Encyclopedia, Model behavior <br />
Summary: 
The article introduces the CoT Encyclopedia framework for analyzing and guiding model reasoning strategies. By automatically extracting diverse reasoning criteria from model-generated CoTs and clustering them into categories, the framework offers more comprehensive and interpretable analyses than existing methods. Human evaluations confirm the effectiveness of this approach. The framework enables predicting model strategies and guiding them towards more effective alternatives. Practical insights include the impact of training data format on reasoning behavior, with free-form vs. multiple-choice formats playing a significant role. This underscores the importance of format-aware model design in improving reasoning capabilities.<br /><br />Summary: <div>
arXiv:2505.10185v1 Announce Type: new 
Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits</title>
<link>https://arxiv.org/abs/2505.10202</link>
<guid>https://arxiv.org/abs/2505.10202</guid>
<content:encoded><![CDATA[
<div> Vector Quantization, Large Language Models, Output Layer, Computational Efficiency, Parameter Reduction

Summary: 
The paper introduces VQ-Logits, a novel approach to reducing the parameter count and computational load of the final linear projection layer in Large Language Models (LLMs). By leveraging Vector Quantization (VQ), the approach uses a compact codebook of embedding vectors to replace the large output embedding matrix, leading to significant parameter reduction in the output layer and faster logit computation. Experimental results on language modeling benchmarks demonstrate up to 99% parameter reduction and 6x speedup in logit computation with only a marginal increase in perplexity compared to full softmax baselines. Ablation studies on codebook size, initialization, and learning strategies also show the robustness and effectiveness of the VQ-Logits approach. <div>
arXiv:2505.10202v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success but face significant computational and memory challenges, particularly due to their extensive output vocabularies. The final linear projection layer, mapping hidden states to vocabulary-sized logits, often constitutes a substantial portion of the model's parameters and computational cost during inference. Existing methods like adaptive softmax or hierarchical softmax introduce structural complexities. In this paper, we propose VQ-Logits, a novel approach that leverages Vector Quantization (VQ) to drastically reduce the parameter count and computational load of the LLM output layer. VQ-Logits replaces the large V * dmodel output embedding matrix with a small, shared codebook of K embedding vectors (K << V ). Each token in the vocabulary is mapped to one of these K codebook vectors. The LLM predicts logits over this compact codebook, which are then efficiently "scattered" to the full vocabulary space using the learned or preassigned mapping. We demonstrate through extensive experiments on standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines. We further provide detailed ablation studies on codebook size, initialization, and learning strategies, showcasing the robustness and effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward</title>
<link>https://arxiv.org/abs/2505.10218</link>
<guid>https://arxiv.org/abs/2505.10218</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, role-playing conversational agents, role consistency, verifiable role-awareness reward, dataset collaboration

Summary:
Role-playing conversational agents (RPCAs) often struggle with maintaining role consistency. To address this issue, this study introduces RAIDEN-R1, a novel reinforcement learning framework that incorporates Verifiable Role-Awareness Reward (VRAR). The method utilizes singular and multi-term mining strategies to generate quantifiable rewards based on role-specific keys. A high-quality role-aware Chain-of-Thought dataset is constructed through multi-LLM collaboration, improving reasoning coherence. Experimental results on the RAIDEN benchmark show that the 14B-GRPO model of RAIDEN-R1 outperforms baseline models with an accuracy of 88.04% and 88.65% on Script-Based Knowledge and Conversation Memory metrics, respectively, while maintaining robustness. Case analyses further demonstrate the model's proficiency in resolving conflicting contextual cues and maintaining first-person narrative consistency. This work not only bridges the non-quantifiability gap in RPCA training but also provides valuable insights into role-aware reasoning patterns, ultimately advancing the development of RPCAs. 

<br /><br />Summary: <div>
arXiv:2505.10218v1 Announce Type: new 
Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in maintaining role consistency. To address this, we propose RAIDEN-R1, a novel reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset through multi-LLM collaboration, and implement experiments to enhance reasoning coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness. Case analyses further reveal the model's enhanced ability to resolve conflicting contextual cues and sustain first-person narrative consistency. This work bridges the non-quantifiability gap in RPCA training and provides insights into role-aware reasoning patterns, advancing the development of RPCAs.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data</title>
<link>https://arxiv.org/abs/2505.10260</link>
<guid>https://arxiv.org/abs/2505.10260</guid>
<content:encoded><![CDATA[
<div> natural language processing, large language models, binary classification, human rights violations, multilingual contexts

Summary:<br />
This study explores the capabilities of state-of-the-art large language models for zero-shot and few-shot annotation of social media posts in Russian and Ukrainian, focusing on identifying references to human rights violations. The models evaluated include GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2, compared against human double-annotated labels. The analysis assesses annotation performance under different prompting conditions in English and Russian, revealing unique error patterns. By comparing model outputs with human annotations, the research contributes to understanding the reliability of language models in multilingual, domain-specific tasks and their handling of subjective judgments. This insight is crucial for considering the deployment of language models in real-world scenarios.<br /><br />Summary: <div>
arXiv:2505.10260v1 Announce Type: new 
Abstract: In the era of increasingly sophisticated natural language processing (NLP) systems, large language models (LLMs) have demonstrated remarkable potential for diverse applications, including tasks requiring nuanced textual understanding and contextual reasoning. This study investigates the capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex textual dataset comprising social media posts in Russian and Ukrainian. Specifically, the focus is on the binary classification task of identifying references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared against a gold standard set of human double-annotated labels across 1000 samples. The analysis includes assessing annotation performance under different prompting conditions, with prompts provided in both English and Russian. Additionally, the study explores the unique patterns of errors and disagreements exhibited by each model, offering insights into their strengths, limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes to understanding the reliability and applicability of LLMs for sensitive, domain-specific tasks in multilingual contexts. It also sheds light on how language models handle inherently subjective and context-dependent judgments, a critical consideration for their deployment in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine</title>
<link>https://arxiv.org/abs/2505.10261</link>
<guid>https://arxiv.org/abs/2505.10261</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, medicine, generative large language models, ethical use, medical applications

Summary:<br /><br />Natural language processing (NLP) in medicine has evolved with the rise of generative large language models (LLMs). A study of 19,123 research papers reveals that while generative LLMs excel in open-ended tasks, traditional NLP is more efficient in information extraction and analysis tasks. The comparison highlights the distinct advantages of each approach across various medical applications. As these technologies continue to evolve, ethical considerations are paramount to harness their full potential in the healthcare sector. It is crucial to ensure responsible and ethical use of NLP and generative LLMs to maximize their benefits in medical settings. Ethical guidelines and safeguards must be put in place to protect patient privacy, confidentiality, and data security. Embracing the potential of these technologies while upholding ethical principles will be key in driving their successful adoption in the medical field.  <div>
arXiv:2505.10261v1 Announce Type: new 
Abstract: Natural language processing (NLP) has been traditionally applied to medicine, and generative large language models (LLMs) have become prominent recently. However, the differences between them across different medical tasks remain underexplored. We analyzed 19,123 studies, finding that generative LLMs demonstrate advantages in open-ended tasks, while traditional NLP dominates in information extraction and analysis tasks. As these technologies advance, ethical use of them is essential to ensure their potential in medical applications.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making</title>
<link>https://arxiv.org/abs/2505.10282</link>
<guid>https://arxiv.org/abs/2505.10282</guid>
<content:encoded><![CDATA[
<div> clinical evidence, evidence synthesis, clinical decision support system, large language models, Quicker <br />
<br />
Summary: The study introduces Quicker, an evidence-based clinical decision support system that automates evidence synthesis and generates clinical recommendations. Quicker's fully automated chain covers all phases of decision-making, including question decomposition and recommendation generation. Evaluation using the Q2CRBench-3 benchmark dataset showed Quicker's strong performance in question decomposition, retrieval sensitivity, and literature screening. Quicker-assisted evidence assessment supported human reviewers effectively, with recommendations more comprehensive and coherent than those of clinicians. Collaboration between a single reviewer and Quicker reduced recommendation development time significantly. Overall, the study affirms Quicker's potential to help physicians make quicker and more reliable evidence-based clinical decisions. <br /> <div>
arXiv:2505.10282v1 Announce Type: new 
Abstract: Clinical evidence, derived from rigorous research and data analysis, provides healthcare professionals with reliable scientific foundations for informed decision-making. Integrating clinical evidence into real-time practice is challenging due to the enormous workload, complex professional processes, and time constraints. This highlights the need for tools that automate evidence synthesis to support more efficient and accurate decision making in clinical settings. This study introduces Quicker, an evidence-based clinical decision support system powered by large language models (LLMs), designed to automate evidence synthesis and generate clinical recommendations modeled after standard clinical guideline development processes. Quicker implements a fully automated chain that covers all phases, from questions to clinical recommendations, and further enables customized decision-making through integrated tools and interactive user interfaces. To evaluate Quicker's capabilities, we developed the Q2CRBench-3 benchmark dataset, based on clinical guideline development records for three different diseases. Experimental results highlighted Quicker's strong performance, with fine-grained question decomposition tailored to user preferences, retrieval sensitivities comparable to human experts, and literature screening performance approaching comprehensive inclusion of relevant studies. In addition, Quicker-assisted evidence assessment effectively supported human reviewers, while Quicker's recommendations were more comprehensive and logically coherent than those of clinicians. In system-level testing, collaboration between a single reviewer and Quicker reduced the time required for recommendation development to 20-40 minutes. In general, our findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.10320</link>
<guid>https://arxiv.org/abs/2505.10320</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, judgment tasks, reward strategies, model training, evaluation

Summary:
J1, a reinforcement learning approach, aims to train LLM-as-a-Judge models for improved judgment ability by enhancing chain-of-thought reasoning. The method converts prompts to judgment tasks with verifiable rewards, surpassing existing 8B or 70B models in performance. Comparison between Pairwise-J1 and Pointwise-J1 models, offline versus online training recipes, reward strategies, seed prompts, and thought length/content variations reveal the effectiveness of J1 in outlining evaluation criteria, self-generating reference answers, and re-evaluating model responses. Through these enhancements, J1 excels in judgment tasks, outperforming larger models like o1-mini and R1 on certain benchmarks despite its smaller size. Overall, J1 contributes to overcoming evaluation quality bottlenecks in AI progress by facilitating stronger chain-of-thought reasoning and incentivizing critical thinking in model training. 

<br /><br />Summary: <div>
arXiv:2505.10320v1 Announce Type: new 
Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations</title>
<link>https://arxiv.org/abs/2505.10354</link>
<guid>https://arxiv.org/abs/2505.10354</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic text representation, text embeddings, interpretable embeddings, dense embeddings, farthest point sampling

Summary:
Semantic text representation is crucial in natural language processing, with text embeddings like SimCSE and LLM2Vec showing strong performance but lacking interpretability. Benara et al. recently introduced interpretable text embeddings based on large language models, but they are high-dimensional and complex. In this study, a new approach called Low-dimensional Dense and Interpretable text embeddings with Relative representations (LDIR) is proposed, with fewer than 500 dimensions. LDIR's dimensions indicate semantic relatedness to anchor texts using farthest point sampling, balancing semantic representation with traceability and interpretability. Validation on various tasks demonstrates LDIR's effectiveness, performing comparably to black-box models while surpassing existing interpretable embeddings with significantly fewer dimensions. The LDIR code is available on GitHub for further exploration and implementation.

<br /><br />Summary: <div>
arXiv:2505.10354v1 Announce Type: new 
Abstract: Semantic text representation is a fundamental task in the field of natural language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have demonstrated excellent performance, but the values of each dimension are difficult to trace and interpret. Bag-of-words, as classic sparse interpretable embeddings, suffers from poor performance. Recently, Benara et al. (2024) propose interpretable text embeddings using large language models, which forms "0/1" embeddings based on responses to a series of questions. These interpretable text embeddings are typically high-dimensional (larger than 10,000). In this work, we propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling, offering both semantic representation as well as a certain level of traceability and interpretability. We validate LDIR on multiple semantic textual similarity, retrieval, and clustering tasks. Extensive experimental results show that LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions. Code is available at https://github.com/szu-tera/LDIR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli</title>
<link>https://arxiv.org/abs/2505.10356</link>
<guid>https://arxiv.org/abs/2505.10356</guid>
<content:encoded><![CDATA[
<div> Keywords: brain decoding, multimodal, language reconstruction, visual-language models, brain-computer interaction

Summary:
This study introduces a framework for decoding coherent language from brain activity, which incorporates multiple input modalities, including visual, auditory, and textual stimuli. By leveraging visual-language models (VLMs) and modality-specific experts, the framework can interpret information across different modalities simultaneously. The experiments conducted demonstrate that the proposed method achieves comparable performance to existing systems while maintaining adaptability and extensibility. This research represents a significant step towards more ecologically valid and generalizable mind decoding techniques in the field of brain-computer interaction. <div>
arXiv:2505.10356v1 Announce Type: new 
Abstract: Decoding thoughts from brain activity offers valuable insights into human cognition and enables promising applications in brain-computer interaction. While prior studies have explored language reconstruction from fMRI data, they are typically limited to single-modality inputs such as images or audio. In contrast, human thought is inherently multimodal. To bridge this gap, we propose a unified and flexible framework for reconstructing coherent language from brain recordings elicited by diverse input modalities-visual, auditory, and textual. Our approach leverages visual-language models (VLMs), using modality-specific experts to jointly interpret information across modalities. Experiments demonstrate that our method achieves performance comparable to state-of-the-art systems while remaining adaptable and extensible. This work advances toward more ecologically valid and generalizable mind decoding.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples</title>
<link>https://arxiv.org/abs/2505.10389</link>
<guid>https://arxiv.org/abs/2505.10389</guid>
<content:encoded><![CDATA[
<div> Keywords: aspect-based sentiment analysis, large language models, opinion extraction, domain-specific taxonomies, structured prediction tasks

Summary:<br />
This paper delves into the design of an aspect-based sentiment analysis system utilizing large language models (LLMs) for practical applications. The focus is on quadruple opinion extraction, involving aspect categories, sentiment polarity, targets, and opinion expressions across various domains and languages. Through the use of internal datasets, the study examines the effectiveness of a single fine-tuned model in handling multiple domain-specific taxonomies concurrently. Results show that a combined multi-domain model can achieve comparable performance to specialized single-domain models, simplifying operational complexity. The paper also discusses insights gained on dealing with non-extractive predictions and evaluating different failure modes in developing LLM-based systems for structured prediction tasks.<br /><br />Summary: <div>
arXiv:2505.10389v1 Announce Type: new 
Abstract: This paper explores the design of an aspect-based sentiment analysis system using large language models (LLMs) for real-world use. We focus on quadruple opinion extraction -- identifying aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages. Using internal datasets, we investigate whether a single fine-tuned model can effectively handle multiple domain-specific taxonomies simultaneously. We demonstrate that a combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity. We also share lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Repetition Problems of LLMs in Code Generation</title>
<link>https://arxiv.org/abs/2505.10402</link>
<guid>https://arxiv.org/abs/2505.10402</guid>
<content:encoded><![CDATA[
<div> structual repetition, code generation, neural language models, RPG, CodeRepetEval

Summary:<br /><br />This paper addresses the issue of structural repetition in code generation for neural language models (LLMs), a more challenging problem compared to content repetition. The proposed approach, RPG (Repetition Penalization based on Grammar), utilizes grammar rules to identify and mitigate structural repetitions during code generation. By strategically reducing the likelihood of critical tokens that contribute to repetitions, RPG effectively enhances the quality of generated code. The study introduces a new dataset, CodeRepetEval, to evaluate approaches for mitigating repetition problems in code generation. Experimental results demonstrate that RPG outperforms baselines on CodeRepetEval, HumanEval, and MBPP benchmarks, significantly reducing repetitions and improving the overall quality of the generated code. <div>
arXiv:2505.10402v1 Announce Type: new 
Abstract: With the advent of neural language models, the performance of code generation has been significantly boosted. However, the problem of repetitions during the generation process continues to linger. Previous work has primarily focused on content repetition, which is merely a fraction of the broader repetition problem in code generation. A more prevalent and challenging problem is structural repetition. In structural repetition, the repeated code appears in various patterns but possesses a fixed structure, which can be inherently reflected in grammar. In this paper, we formally define structural repetition and propose an efficient decoding approach called RPG, which stands for Repetition Penalization based on Grammar, to alleviate the repetition problems in code generation for LLMs. Specifically, RPG first leverages grammar rules to identify repetition problems during code generation, and then strategically decays the likelihood of critical tokens that contribute to repetitions, thereby mitigating them in code generation. To facilitate this study, we construct a new dataset CodeRepetEval to comprehensively evaluate approaches for mitigating the repetition problems in code generation. Extensive experimental results demonstrate that RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation</title>
<link>https://arxiv.org/abs/2505.10409</link>
<guid>https://arxiv.org/abs/2505.10409</guid>
<content:encoded><![CDATA[
<div> Evaluation, Plain language summaries, Large language models, Comprehension, Human judgment

Summary:
The study evaluated the effectiveness of Large Language Models (LLMs) in generating Plain Language Summaries (PLSs) to aid communication between clinicians and patients. While LLM-generated PLSs were rated similarly to human-written ones in subjective evaluations, the study found that human-written PLSs resulted in better comprehension among readers. The research highlighted the limitations of automated evaluation metrics in reflecting human judgment, emphasizing the significance of frameworks that prioritize layperson comprehension in PLS generation. The findings emphasize the need for methods that optimize for understanding rather than surface-level quality.<br /><br /> <div>
arXiv:2505.10409v1 Announce Type: new 
Abstract: Plain language summaries (PLSs) are essential for facilitating effective communication between clinicians and patients by making complex medical information easier for laypeople to understand and act upon. Large language models (LLMs) have recently shown promise in automating PLS generation, but their effectiveness in supporting health information comprehension remains unclear. Prior evaluations have generally relied on automated scores that do not measure understandability directly, or subjective Likert-scale ratings from convenience samples with limited generalizability. To address these gaps, we conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using Amazon Mechanical Turk with 150 participants. We assessed PLS quality through subjective Likert-scale ratings focusing on simplicity, informativeness, coherence, and faithfulness; and objective multiple-choice comprehension and recall measures of reader understanding. Additionally, we examined the alignment between 10 automated evaluation metrics and human judgments. Our findings indicate that while LLMs can generate PLSs that appear indistinguishable from human-written ones in subjective evaluations, human-written PLSs lead to significantly better comprehension. Furthermore, automated evaluation metrics fail to reflect human judgment, calling into question their suitability for evaluating PLSs. This is the first study to systematically evaluate LLM-generated PLSs based on both reader preferences and comprehension outcomes. Our findings highlight the need for evaluation frameworks that move beyond surface-level quality and for generation methods that explicitly optimize for layperson comprehension.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Document Refinement for Long-context Retrieval-augmented Generation</title>
<link>https://arxiv.org/abs/2505.10413</link>
<guid>https://arxiv.org/abs/2505.10413</guid>
<content:encoded><![CDATA[
<div> Keywords: LongRefiner, plug-and-play refiner, hierarchical document structuring, multi-task learning, computational costs  
Summary:  
LongRefiner is a new plug-and-play refiner designed to improve performance in long-context input scenarios encountered in real-world RAG applications. It utilizes dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. The experiments conducted on seven QA datasets demonstrate that LongRefiner achieves competitive performance while significantly reducing computational costs and latency by 10x compared to the best baseline method. The study shows that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. The code for LongRefiner is publicly available on GitHub at https://github.com/ignorejjj/LongRefiner. 

<br /><br />Summary: <div>
arXiv:2505.10413v1 Announce Type: new 
Abstract: Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance. To address these challenges, we propose LongRefiner, an efficient plug-and-play refiner that leverages the inherent structural characteristics of long documents. LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. Our code is available at https://github.com/ignorejjj/LongRefiner.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models</title>
<link>https://arxiv.org/abs/2505.10446</link>
<guid>https://arxiv.org/abs/2505.10446</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Chain of Lateral Thought, reasoning framework, reinforcement learning, diffusion language models, bidirectional reasoning

Summary: 
The article introduces the Diffusion Chain of Lateral Thought (DCoLT), a novel reasoning framework for diffusion language models that optimizes the entire reasoning trajectory using reinforcement learning. DCoLT allows bidirectional, non-linear reasoning with no strict grammatical correctness rules in intermediate thought steps. Two representative diffusion language models, SEDD and LLaDA, are implemented with DCoLT. SEDD uses a probabilistic policy derived from its concrete score to maximize RL reward, while LLaDA optimizes its RL action through a ranking-based Unmasking Policy Module (UPM) based on the Plackett-Luce model. Experiment results on math and code generation tasks show that DCoLT-reinforced DLMs outperform other models trained by supervised fine-tuning (SFT) or RL or both. DCoLT-reinforced LLaDA significantly boosts reasoning accuracy on various tasks, demonstrating the effectiveness of the proposed framework. 

<br /><br />Summary: <div>
arXiv:2505.10446v1 Announce Type: new 
Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent "thinking" action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal, linear thinking process, DCoLT allows bidirectional, non-linear reasoning with no strict rule on grammatical correctness amid its intermediate steps of thought. We implement DCoLT on two representative Diffusion Language Models (DLMs). First, we choose SEDD as a representative continuous-time discrete diffusion model, where its concrete score derives a probabilistic policy to maximize the RL reward over the entire sequence of intermediate diffusion steps. We further consider the discrete-time masked diffusion language model -- LLaDA, and find that the order to predict and unmask tokens plays an essential role to optimize its RL action resulting from the ranking-based Unmasking Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both math and code generation tasks show that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning</title>
<link>https://arxiv.org/abs/2505.10493</link>
<guid>https://arxiv.org/abs/2505.10493</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Curriculum Learning, Training Framework, Open-Domain QA, Performance Improvement

Summary:
The paper introduces a novel approach called CL-RAG, which combines Curriculum Learning with a Retrieval-Augmented Generation (RAG) system to enhance the training process and optimize performance. By generating training data with varying difficulty levels for the retriever and generator components, the model progresses through stages based on the curriculum learning approach. This method improves the generalization and overall performance of the RAG system effectively. Experimental results across four open-domain question answering datasets show consistent performance gains of 2% to 4% compared to other advanced methods. CL-RAG demonstrates the potential of integrating curriculum learning into the training of large language models to enhance their adaptability and knowledge retrieval capabilities. 

<br /><br />Summary: <div>
arXiv:2505.10493v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods focus on optimizing the retriever or generator in the RAG system by directly utilizing the top-k retrieved documents. However, the documents effectiveness are various significantly across user queries, i.e. some documents provide valuable knowledge while others totally lack critical information. It hinders the retriever and generator's adaptation during training. Inspired by human cognitive learning, curriculum learning trains models using samples progressing from easy to difficult, thus enhancing their generalization ability, and we integrate this effective paradigm to the training of the RAG system. In this paper, we propose a multi-stage Curriculum Learning based RAG system training framework, named CL-RAG. We first construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution. Then, we train the model in stages based on the curriculum learning approach, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective</title>
<link>https://arxiv.org/abs/2505.10494</link>
<guid>https://arxiv.org/abs/2505.10494</guid>
<content:encoded><![CDATA[
<div> propose, CoV-Eval, multi-task benchmark, code security, LLMs <br />
Summary: <br />
The paper introduces CoV-Eval, a multi-task benchmark for evaluating code security in large language models (LLMs). This benchmark covers tasks like code completion, vulnerability repair, detection, and classification, providing a comprehensive assessment of LLMs. The authors also present VC-Judge, a judgment model that efficiently reviews LLM-generated programs for vulnerabilities. Evaluation of 20 LLMs reveals that while they can identify vulnerable code, they struggle with generating secure code and recognizing specific vulnerability types. The study highlights key challenges and areas for improvement in LLM code security, offering valuable insights for future research in this field. <br /> <div>
arXiv:2505.10494v1 Announce Type: new 
Abstract: Code security and usability are both essential for various coding assistant applications driven by large language models (LLMs). Current code security benchmarks focus solely on single evaluation task and paradigm, such as code completion and generation, lacking comprehensive assessment across dimensions like secure code generation, vulnerability repair and discrimination. In this paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks such as code completion, vulnerability repair, vulnerability detection and classification, for comprehensive evaluation of LLM code security. Besides, we developed VC-Judge, an improved judgment model that aligns closely with human experts and can review LLM-generated programs for vulnerabilities in a more efficient and reliable way. We conduct a comprehensive evaluation of 20 proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable codes well, they still tend to generate insecure codes and struggle with recognizing specific vulnerability types and performing repairs. Extensive experiments and qualitative analyses reveal key challenges and optimization directions, offering insights for future research in LLM code security.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks</title>
<link>https://arxiv.org/abs/2505.10507</link>
<guid>https://arxiv.org/abs/2505.10507</guid>
<content:encoded><![CDATA[
<div> translation-based transfer learning, cross-lingual, token classification, label projection, word aligners 

Summary:
- Translation-based strategies for cross-lingual transfer learning (XLT) for token classification tasks involve label projection to map labels between original and translated sentences.
- Word aligners (WAs) are commonly used for label projection, with low-level design decisions impact XLT performance significantly.
- The study systematically investigates WA design decisions like label mapping algorithm, filtering strategies, and pre-tokenization of translated sentences.
- Optimized WA choices offer XLT performance comparable to marker-based methods, with a new ensemble strategy of translate-train and translate-test predictions outperforming marker-based projection.
- The proposed ensembling approach enhances XLT robustness and reduces sensitivity to low-level WA design choices for token classification tasks. 

<br /><br />Summary: <div>
arXiv:2505.10507v1 Announce Type: new 
Abstract: Translation-based strategies for cross-lingual transfer XLT such as translate-train -- training on noisy target language data translated from the source language -- and translate-test -- evaluating on noisy source language data translated from the target language -- are competitive XLT baselines. In XLT for token classification tasks, however, these strategies include label projection, the challenging step of mapping the labels from each token in the original sentence to its counterpart(s) in the translation. Although word aligners (WAs) are commonly used for label projection, the low-level design decisions for applying them to translation-based XLT have not been systematically investigated. Moreover, recent marker-based methods, which project labeled spans by inserting tags around them before (or after) translation, claim to outperform WAs in label projection for XLT. In this work, we revisit WAs for label projection, systematically investigating the effects of low-level design decisions on token-level XLT: (i) the algorithm for projecting labels between (multi-)token spans, (ii) filtering strategies to reduce the number of noisily mapped labels, and (iii) the pre-tokenization of the translated sentences. We find that all of these substantially impact translation-based XLT performance and show that, with optimized choices, XLT with WA offers performance at least comparable to that of marker-based methods. We then introduce a new projection strategy that ensembles translate-train and translate-test predictions and demonstrate that it substantially outperforms the marker-based projection. Crucially, we show that our proposed ensembling also reduces sensitivity to low-level WA design choices, resulting in more robust XLT for token classification tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Token Prediction Needs Registers</title>
<link>https://arxiv.org/abs/2505.10518</link>
<guid>https://arxiv.org/abs/2505.10518</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-token prediction, language model pretraining, MuToR, fine-tuning, parameter-efficient fine-tuning

Summary:
MuToR is a new approach to multi-token prediction that involves integrating learnable register tokens into the input sequence. This method is simple, effective, and requires minimal additional parameters, making it compatible with existing pretrained language models. MuToR aligns well with the next-token pretraining objective, making it suitable for supervised fine-tuning tasks. It also supports scalable prediction horizons, allowing for versatile applications across different tasks. The authors demonstrate the effectiveness of MuToR in supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining on challenging generative tasks in both language and vision domains. The code for MuToR will be made available on GitHub for further exploration and implementation. 

<br /><br />Summary: MuToR is a novel approach to multi-token prediction that leverages learnable register tokens in the input sequence. It is compatible with existing language models, requires minimal parameter changes, and supports scalable prediction horizons. The method demonstrates effectiveness in supervised fine-tuning, PEFT, and pretraining on challenging generative tasks in language and vision domains. <div>
arXiv:2505.10518v1 Announce Type: new 
Abstract: Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldPM: Scaling Human Preference Modeling</title>
<link>https://arxiv.org/abs/2505.10527</link>
<guid>https://arxiv.org/abs/2505.10527</guid>
<content:encoded><![CDATA[
<div> scaling laws, preference modeling, World Preference Modeling, language models, generalization performance

Summary:<br />
- The study explores how test loss scales in preference modeling similarly to language modeling.
- World Preference Modeling (WorldPM) is proposed to represent human preferences.
- Adversarial metrics exhibit consistent scalability with increased training data and model size.
- Objective metrics show emergent behavior in larger language models, highlighting WorldPM's scalability potential.
- Subjective metrics do not exhibit scaling trends.
- WorldPM shows effectiveness in preference fine-tuning, improving generalization performance across human preference datasets.
- Integration of WorldPM into the RLHF pipeline results in significant performance improvements on both in-house and public evaluation sets. <div>
arXiv:2505.10527v1 Announce Type: new 
Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.10554</link>
<guid>https://arxiv.org/abs/2505.10554</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, reinforcement learning, meta-abilities, alignment, scalability

Summary:<br />
Large reasoning models (LRMs) have the potential for advanced reasoning abilities but lack predictability and control in their emergence. This paper introduces a method to explicitly align LRMs with meta-abilities (deduction, induction, abduction) through self-verifiable tasks, improving performance by over 10% compared to baseline models. The three-stage pipeline involves individual alignment, parameter-space merging, and domain-specific reinforcement learning. The approach not only boosts performance but also increases the reliability and scalability of LRMs' reasoning capabilities. Domain-specific reinforcement learning from the aligned checkpoint further enhances performance across various benchmarks in math, coding, and science, showcasing the effectiveness of meta-ability alignment. The code for this method is openly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.10554v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling</title>
<link>https://arxiv.org/abs/2505.09665</link>
<guid>https://arxiv.org/abs/2505.09665</guid>
<content:encoded><![CDATA[
<div> perceive response wildfires Reddit discourse LA fires 2025
<br />
Summary: 
During the 2025 Los Angeles wildfires, Reddit discourse was analyzed to understand public perception and response. 385 posts and 114,879 comments related to the Palisades and Eaton fires were collected and analyzed using topic modeling. A hierarchical framework categorized topics into Situational Awareness (SA) and Crisis Narratives (CN). SA topics peaked within the first 2-5 days aligning with fire progression. Public health and safety, loss and damage, and emergency resources were frequent topics, including discussions on environmental, occupational, and one health. Grief signals and mental health risks accounted for a significant portion of CN instances, with the highest volume at night. This study provides insights into public health concerns during wildfires, informing strategies for disaster response, health communication, and future research in similar climate-related disasters.
<br /> <div>
arXiv:2505.09665v1 Announce Type: cross 
Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Models in Multimodal Recommender Systems</title>
<link>https://arxiv.org/abs/2505.09777</link>
<guid>https://arxiv.org/abs/2505.09777</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal recommender systems, Large language models, Pre-trained language models, Prompting strategies, Fine-tuning methods<br />
Summary: 
This survey explores the intersection of large language models (LLMs) and multimodal recommender systems (MRS). It discusses the benefits of LLMs in enhancing recommendation performance through semantic reasoning, in-context learning, and dynamic input handling. The survey highlights the challenges of scalability and model accessibility posed by LLMs compared to pre-trained language models (PLMs). It categorizes integration patterns, transferable techniques from related recommendation domains, and evaluation metrics and datasets. The review aims to elucidate the evolving role of LLMs in multimodal recommendation systems and provides insights for future research directions in this dynamic field. <br /><br /> <div>
arXiv:2505.09777v1 Announce Type: cross 
Abstract: Multimodal recommender systems (MRS) integrate heterogeneous user and item data, such as text, images, and structured information, to enhance recommendation performance. The emergence of large language models (LLMs) introduces new opportunities for MRS by enabling semantic reasoning, in-context learning, and dynamic input handling. Compared to earlier pre-trained language models (PLMs), LLMs offer greater flexibility and generalisation capabilities but also introduce challenges related to scalability and model accessibility. This survey presents a comprehensive review of recent work at the intersection of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques. We propose a novel taxonomy to characterise integration patterns, identify transferable techniques from related recommendation domains, provide an overview of evaluation metrics and datasets, and point to possible future directions. We aim to clarify the emerging role of LLMs in multimodal recommendation and support future research in this rapidly evolving field.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers</title>
<link>https://arxiv.org/abs/2505.09855</link>
<guid>https://arxiv.org/abs/2505.09855</guid>
<content:encoded><![CDATA[
<div> Transformer models, learning modes, in-weights learning (IWL), in-context learning (ICL), evolutionary biology<br />
Summary:<br />
- Transformer models exhibit two learning modes, in-weights learning (IWL) and in-context learning (ICL), akin to genetic encoding and phenotypic plasticity in evolutionary biology.<br />
- Environmental stability favors IWL, while cue reliability enhances ICL efficacy, particularly in low stability scenarios.<br />
- High stability leads to a dominance of IWL, with a shift towards ICL in settings with easier IWL or slower ICL acquisition.<br />
- Task-contingent temporal evolution is observed in learning dynamics, with different learning mode transitions based on task characteristics.<br />
- These findings support the relative-cost hypothesis, highlighting predictability as a crucial factor in determining adaptive strategies in Transformers and providing insights for training methodologies. <br /> <div>
arXiv:2505.09855v1 Announce Type: cross 
Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL), encoding knowledge into model weights, and in-context learning (ICL), adapting flexibly to context without weight modification. To better understand the interplay between these learning modes, we draw inspiration from evolutionary biology's analogous adaptive strategies: genetic encoding (akin to IWL, adapting over generations and fixed within an individual's lifetime) and phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to environmental cues). In evolutionary biology, environmental predictability dictates the balance between these strategies: stability favors genetic encoding, while reliable predictive cues promote phenotypic plasticity. We experimentally operationalize these dimensions of predictability and systematically investigate their influence on the ICL/IWL balance in Transformers. Using regression and classification tasks, we show that high environmental stability decisively favors IWL, as predicted, with a sharp transition at maximal stability. Conversely, high cue reliability enhances ICL efficacy, particularly when stability is low. Furthermore, learning dynamics reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift occurs in some settings (e.g., classification with many classes), we demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL acquisition (e.g., regression) can exhibit an initial IWL phase later yielding to ICL dominance. These findings support a relative-cost hypothesis for explaining these learning mode transitions, establishing predictability as a critical factor governing adaptive strategies in Transformers, and offering novel insights for understanding ICL and guiding training methodologies.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks</title>
<link>https://arxiv.org/abs/2505.09901</link>
<guid>https://arxiv.org/abs/2505.09901</guid>
<content:encoded><![CDATA[
<div> cognitive science, exploratory behavior, multi-armed bandit tasks, large language models, decision-making strategies<br />
<br />Summary: Large language models (LLMs) are increasingly used to simulate human behavior in decision-making tasks. This study compares the exploration-exploitation strategies of LLMs, humans, and multi-armed bandit algorithms. Reasoning enhances LLM decision-making, making them exhibit more human-like behavior in simple tasks. However, in complex, changing environments, LLMs struggle to match human adaptability in directed exploration. Despite achieving similar regret in some scenarios, LLMs show limitations in effective exploration. This study highlights the potential and boundaries of LLMs in simulating human behavior and automated decision-making, pointing to areas for improvement. <br /> <div>
arXiv:2505.09901v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to simulate or automate human behavior in complex sequential decision-making tasks. A natural question is then whether LLMs exhibit similar decision-making behavior to humans, and can achieve comparable (or superior) performance. In this work, we focus on the exploration-exploitation (E&amp;E) tradeoff, a fundamental aspect of dynamic decision-making under uncertainty. We employ canonical multi-armed bandit (MAB) tasks introduced in the cognitive science and psychiatry literature to conduct a comparative study of the E&amp;E strategies of LLMs, humans, and MAB algorithms. We use interpretable choice models to capture the E&amp;E strategies of the agents and investigate how explicit reasoning, through both prompting strategies and reasoning-enhanced models, shapes LLM decision-making. We find that reasoning shifts LLMs toward more human-like behavior, characterized by a mix of random and directed exploration. In simple stationary tasks, reasoning-enabled LLMs exhibit similar levels of random and directed exploration compared to humans. However, in more complex, non-stationary environments, LLMs struggle to match human adaptability, particularly in effective directed exploration, despite achieving similar regret in certain scenarios. Our findings highlight both the promise and limits of LLMs as simulators of human behavior and tools for automated decision-making and point to potential areas of improvements.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization</title>
<link>https://arxiv.org/abs/2505.09921</link>
<guid>https://arxiv.org/abs/2505.09921</guid>
<content:encoded><![CDATA[
<div> Privacy Leakage, Large Language Models, Jailbreak Attacks, Personally Identifiable Information, Privacy Risks  
Summary:  
Large Language Models (LLMs) have shown proficiency in various domains but also present privacy risks. This paper explores the intersection of privacy leakage and jailbreak attacks in LLMs. The authors introduce PIG, a framework designed to target Personally Identifiable Information (PII) by identifying PII entities and types, utilizing in-context learning, and updating privacy context with gradient-based strategies. Evaluations on privacy datasets using white-box and black-box LLMs demonstrate that PIG outperforms existing methods and achieves state-of-the-art results. The study highlights the substantial privacy risks associated with LLMs and emphasizes the necessity for enhanced safeguards. Available code for PIG can be found at the provided GitHub link.  
Summary: <div>
arXiv:2505.09921v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel in various domains but pose inherent privacy risks. Existing methods to evaluate privacy leakage in LLMs often use memorized prefixes or simple instructions to extract data, both of which well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM safety mechanisms to generate harmful content, but their role in privacy scenarios remains underexplored. In this paper, we examine the effectiveness of jailbreak attacks in extracting sensitive information, bridging privacy leakage and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework targeting Personally Identifiable Information (PII) and addressing the limitations of current jailbreak methods. Specifically, PIG identifies PII entities and their types in privacy queries, uses in-context learning to build a privacy context, and iteratively updates it with three gradient-based strategies to elicit target PII. We evaluate PIG and existing jailbreak methods using two privacy-related datasets. Experiments on four white-box and two black-box LLMs show that PIG outperforms baseline methods and achieves state-of-the-art (SoTA) results. The results underscore significant privacy risks in LLMs, emphasizing the need for stronger safeguards. Our code is availble at \href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors</title>
<link>https://arxiv.org/abs/2505.09949</link>
<guid>https://arxiv.org/abs/2505.09949</guid>
<content:encoded><![CDATA[
<div> Keywords: freeway crashes, large language model, crash causation analysis, traffic safety, machine learning

Summary: 
- LLMs are utilized to analyze freeway crash data and provide comprehensive crash causation analysis by understanding the complex interactions between various factors.
- A training dataset consisting of environmental, driver, traffic, and geometric design factors was compiled from 226 traffic safety studies related to freeway crashes.
- The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and identify crash causes without pre-labeled data through zero-shot classification.
- Results demonstrate the effectiveness of LLMs in identifying primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention.
- Practical applicability and potential to improve traffic safety measures were validated through a high level of agreement among researchers in the field of traffic safety, indicating the valuable insights and potential countermeasures offered by this approach.

<br /><br />Summary: <div>
arXiv:2505.09949v1 Announce Type: cross 
Abstract: Understanding the factors contributing to traffic crashes and developing strategies to mitigate their severity is essential. Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors and the unique characteristics of each crash. This research leverages large language model (LLM) to analyze freeway crash data and provide crash causation analysis accordingly. By compiling 226 traffic safety studies related to freeway crashes, a training dataset encompassing environmental, driver, traffic, and geometric design factors was created. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors, as covered in these studies. The fine-tuned Llama3 8B model was then used to identify crash causation without pre-labeled data through zero-shot classification, providing comprehensive explanations to ensure that the identified causes were reasonable and aligned with existing research. Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data, such as road maintenance, offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%. This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. Moreover, it provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI</title>
<link>https://arxiv.org/abs/2505.10093</link>
<guid>https://arxiv.org/abs/2505.10093</guid>
<content:encoded><![CDATA[
<div> CS, Taiwanese China Studies, AI, knowledge graph, generative AI <br />
<br />
Summary: 
This study focuses on the development of Taiwanese China Studies (CS) and proposes an AI-assisted approach to organize and analyze decades of CS scholarship. By utilizing generative AI techniques and large language models, the researchers extract entity relation triples from peer-reviewed CS articles and visualize them in a knowledge graph using a D3.js based system. This allows for the exploration of conceptual nodes and semantic relationships within the corpus, revealing intellectual trajectories and research gaps. The system enables a shift from linear text consumption to network-based knowledge navigation, enhancing scholarly access to CS literature. By demonstrating how generative AI can support area studies and digital humanities, this work offers a scalable, data-driven alternative to traditional ontology construction and highlights the potential for a reimagined scholarly infrastructure for regional knowledge systems. <br /><br /> <div>
arXiv:2505.10093v1 Announce Type: cross 
Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. This study responds to the growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship by proposing an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations. We apply generative AI (GAI) techniques and large language models (LLMs) to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. By decomposing textual content into graph structured knowledge units, our system enables a paradigm shift from linear text consumption to network based knowledge navigation. In doing so, it enhances scholarly access to CS literature while offering a scalable, data driven alternative to traditional ontology construction. This work not only demonstrates how generative AI can augment area studies and digital humanities but also highlights its potential to support a reimagined scholarly infrastructure for regional knowledge systems.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Virtual Machine Scheduling in Cloud Computing through Language Agents</title>
<link>https://arxiv.org/abs/2505.10117</link>
<guid>https://arxiv.org/abs/2505.10117</guid>
<content:encoded><![CDATA[
arXiv:2505.10117v1 Announce Type: cross 
Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by large-scale complexity and fluctuating demands. Traditional optimization methods struggle to adapt to real-time changes, domain-expert-designed heuristic approaches suffer from rigid strategies, and existing learning-based methods often lack generalizability and interpretability. To address these limitations, this paper proposes a hierarchical language agent framework named MiCo, which provides a large language model (LLM)-driven heuristic design paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov Decision Process with Options (SMDP-Option), enabling dynamic scheduling through a two-stage architecture, i.e., Option Miner and Option Composer. Option Miner utilizes LLMs to discover diverse and useful non-context-aware strategies by interacting with constructed environments. Option Composer employs LLMs to discover a composing strategy that integrates the non-context-aware strategies with the contextual ones. Extensive experiments on real-world enterprise datasets demonstrate that MiCo achieves a 96.9\% competitive ratio in large-scale scenarios involving more than 10,000 virtual machines. It maintains high performance even under nonstationary request flows and diverse configurations, thus validating its effectiveness in complex and large-scale cloud environments.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering</title>
<link>https://arxiv.org/abs/2505.10118</link>
<guid>https://arxiv.org/abs/2505.10118</guid>
<content:encoded><![CDATA[
arXiv:2505.10118v1 Announce Type: cross 
Abstract: Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance. To address this, we derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging $\epsilon$-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading. MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention</title>
<link>https://arxiv.org/abs/2505.10222</link>
<guid>https://arxiv.org/abs/2505.10222</guid>
<content:encoded><![CDATA[
arXiv:2505.10222v1 Announce Type: cross 
Abstract: Transformer models rely on self-attention to capture token dependencies but face challenges in effectively integrating positional information while allowing multi-head attention (MHA) flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity. This paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA. CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. ComplexFormer incorporates two key improvements: (1) a per-head Euler transformation, converting real-valued query/key projections into polar-form complex vectors for head-specific complex subspace operation; and (2) a per-head adaptive differential rotation mechanism, exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct strategies for integrating semantic angle differences (ASmn,i) with relative positional encodings (Delta(Pmn),i). Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity , and improved long-context coherence compared to strong baselines like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging</title>
<link>https://arxiv.org/abs/2505.10231</link>
<guid>https://arxiv.org/abs/2505.10231</guid>
<content:encoded><![CDATA[
arXiv:2505.10231v1 Announce Type: cross 
Abstract: Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation</title>
<link>https://arxiv.org/abs/2505.10292</link>
<guid>https://arxiv.org/abs/2505.10292</guid>
<content:encoded><![CDATA[
arXiv:2505.10292v1 Announce Type: cross 
Abstract: Visual storytelling systems struggle to maintain character identity across frames and link actions to appropriate subjects, frequently leading to referential hallucinations. These issues can be addressed through grounding of characters, objects, and other entities on the visual elements. We propose StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie images, with both structured scene analyses and grounded stories. Each story maintains character and object consistency across frames while explicitly modeling multi-frame relationships through structured tabular representations. Our approach features cross-frame object re-identification using visual similarity and face recognition, chain-of-thought reasoning for explicit narrative modeling, and a grounding scheme that links textual elements to visual entities across multiple frames. We establish baseline performance by fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end object detection, re-identification, and landmark detection while maintaining consistent object references throughout the story. Evaluation demonstrates a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when compared to a non-fine-tuned model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superposition Yields Robust Neural Scaling</title>
<link>https://arxiv.org/abs/2505.10465</link>
<guid>https://arxiv.org/abs/2505.10465</guid>
<content:encoded><![CDATA[
arXiv:2505.10465v1 Announce Type: cross 
Abstract: The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law -- the finding that loss decreases as a power law with model size -- remains unclear. Starting from two empirical principles -- that LLMs represent more things than the model dimensions (widths) they have (i.e., representations are superposed), and that words or concepts in language occur with varying frequencies -- we constructed a toy model to study the loss scaling with model size. We found that when superposition is weak, meaning only the most frequent features are represented without interference, the scaling of loss with model size depends on the underlying feature frequency; if feature frequencies follow a power law, so does the loss. In contrast, under strong superposition, where all features are represented but overlap with each other, the loss becomes inversely proportional to the model dimension across a wide range of feature frequency distributions. This robust scaling behavior is explained geometrically: when many more vectors are packed into a lower dimensional space, the interference (squared overlaps) between vectors scales inversely with that dimension. We then analyzed four families of open-sourced LLMs and found that they exhibit strong superposition and quantitatively match the predictions of our toy model. The Chinchilla scaling law turned out to also agree with our results. We conclude that representation superposition is an important mechanism underlying the observed neural scaling laws. We anticipate that these insights will inspire new training strategies and model architectures to achieve better performance with less computation and fewer parameters.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Scaling Law for Language Models</title>
<link>https://arxiv.org/abs/2505.10475</link>
<guid>https://arxiv.org/abs/2505.10475</guid>
<content:encoded><![CDATA[
arXiv:2505.10475v1 Announce Type: cross 
Abstract: It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\times$ less memory increase and 6$\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs</title>
<link>https://arxiv.org/abs/2505.10495</link>
<guid>https://arxiv.org/abs/2505.10495</guid>
<content:encoded><![CDATA[
arXiv:2505.10495v1 Announce Type: cross 
Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function calling tasks when real user interaction data is unavailable. In digital content creation tools, where users express their needs through natural language queries that must be mapped to API calls, the lack of real-world task-specific data and privacy constraints for training on it necessitate synthetic data generation. Existing approaches to synthetic data generation fall short in diversity and complexity, failing to replicate real-world data distributions and leading to suboptimal performance after LLM fine-tuning. We present a novel router-based architecture that leverages domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models to generate high-quality synthetic training data. Our architecture's flexible routing mechanism enables synthetic data generation that matches observed real-world distributions, addressing a fundamental limitation of traditional approaches. Evaluation on a comprehensive set of real user queries demonstrates significant improvements in both function classification accuracy and API parameter selection. Models fine-tuned with our synthetic data consistently outperform traditional approaches, establishing new benchmarks for function calling tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10526</link>
<guid>https://arxiv.org/abs/2505.10526</guid>
<content:encoded><![CDATA[
arXiv:2505.10526v1 Announce Type: cross 
Abstract: Speculative decoding significantly accelerates language model inference by enabling a lightweight draft model to propose multiple tokens that a larger target model verifies simultaneously. However, applying this technique to vision-language models (VLMs) presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context. We introduce Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models (MASSV), which transforms existing small language models into effective multimodal drafters through a two-phase approach. MASSV first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions. Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV provides a scalable, architecture-compatible method for accelerating both current and future VLMs.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2505.10543</link>
<guid>https://arxiv.org/abs/2505.10543</guid>
<content:encoded><![CDATA[
arXiv:2505.10543v1 Announce Type: cross 
Abstract: While large language models demonstrate impressive performance on static benchmarks, the true potential of large language models as self-learning and reasoning agents in dynamic environments remains unclear. This study systematically evaluates the efficacy of self-reflection, heuristic mutation, and planning as prompting techniques to test the adaptive capabilities of agents. We conduct experiments with various open-source language models in dynamic environments and find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, a too-long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in crucial areas such as planning, reasoning, and spatial coordination, suggesting that current-generation large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while reasoning methods like Chain of thought improves multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.10557</link>
<guid>https://arxiv.org/abs/2505.10557</guid>
<content:encoded><![CDATA[
arXiv:2505.10557v1 Announce Type: cross 
Abstract: Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at https://github.com/mathllm/MathCoder.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Scaling Law for Large Language Models</title>
<link>https://arxiv.org/abs/2404.17785</link>
<guid>https://arxiv.org/abs/2404.17785</guid>
<content:encoded><![CDATA[
arXiv:2404.17785v3 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mosaic Memory of Large Language Models</title>
<link>https://arxiv.org/abs/2405.15523</link>
<guid>https://arxiv.org/abs/2405.15523</guid>
<content:encoded><![CDATA[
arXiv:2405.15523v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become widely adopted, understanding how they learn from, and memorize, training data becomes crucial. Memorization in LLMs is widely assumed to only occur as a result of sequences being repeated in the training data. Instead, we show that LLMs memorize by assembling information from similar sequences, a phenomena we call mosaic memory. We show major LLMs to exhibit mosaic memory, with fuzzy duplicates contributing to memorization as much as 0.8 of an exact duplicate and even heavily modified sequences contributing substantially to memorization. Despite models display reasoning capabilities, we somewhat surprisingly show memorization to be predominantly syntactic rather than semantic. We finally show fuzzy duplicates to be ubiquitous in real-world data, untouched by deduplication techniques. Taken together, our results challenge widely held beliefs and show memorization to be a more complex, mosaic process, with real-world implications for privacy, confidentiality, model utility and evaluation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization</title>
<link>https://arxiv.org/abs/2405.17067</link>
<guid>https://arxiv.org/abs/2405.17067</guid>
<content:encoded><![CDATA[
arXiv:2405.17067v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in language understanding and generation. Nonetheless, it was also witnessed that LLMs tend to produce inaccurate responses to specific queries. This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs. In fact, incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output. This defect is more obvious in Chinese scenarios. To demonstrate this flaw of LLMs, we construct an adversarial dataset, named as $\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs to challenge LLMs' tokenization. ADT consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto. Our empirical results reveal that our ADT is highly effective on challenging the tokenization of leading LLMs, including GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs' capabilities. Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs. In this paper, we substantially investigate LLMs' vulnerability in terms of challenging their token segmentation, which will shed light on the subsequent research of improving LLMs' capabilities through optimizing their tokenization process and algorithms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.00367</link>
<guid>https://arxiv.org/abs/2406.00367</guid>
<content:encoded><![CDATA[
arXiv:2406.00367v2 Announce Type: replace 
Abstract: Effectively analyzing the comments to uncover latent intentions holds immense value in making strategic decisions across various domains. However, several challenges hinder the process of sentiment analysis including the lexical diversity exhibited in comments, the presence of long dependencies within the text, encountering unknown symbols and words, and dealing with imbalanced datasets. Moreover, existing sentiment analysis tasks mostly leveraged sequential models to encode the long dependent texts and it requires longer execution time as it processes the text sequentially. In contrast, the Transformer requires less execution time due to its parallel processing nature. In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM, which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to generate meaningful word embedding vectors, while BiLSTM effectively captures the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid model leverages the strengths of both sequential and Transformer models to enhance performance in sentiment analysis. We conducted experiments using datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the proposed model against existing state-of-the-art methods. Our experimental findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models (e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140 datasets, respectively. Additionally, the model achieves F1-scores of 80.73%, 92.35%, and 82.25% on the same datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling</title>
<link>https://arxiv.org/abs/2406.02069</link>
<guid>https://arxiv.org/abs/2406.02069</guid>
<content:encoded><![CDATA[
arXiv:2406.02069v4 Announce Type: replace 
Abstract: In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusing on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques, achieving up to a 20.5 absolute accuracy improvement on TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms competing methods in maintaining long-context comprehension in LLMs; notably, retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve 100.0 Acc. performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes</title>
<link>https://arxiv.org/abs/2407.01257</link>
<guid>https://arxiv.org/abs/2407.01257</guid>
<content:encoded><![CDATA[
arXiv:2407.01257v5 Announce Type: replace 
Abstract: Recent work on distilling Whisper's knowledge into small models using pseudo-labels shows promising performance while reducing the size by up to 50%. This results in small, efficient, and dedicated models. However, a critical step of distillation using pseudo-labels involves filtering high-quality predictions and using only those during training. This step requires ground truth labels to compare with and filter low-quality examples, making the process dependent on human labels. Additionally, the distillation process requires a large amount of data thereby limiting its applicability in low-resource settings. To address this, we propose a distillation framework that does not require any labeled data. Through experimentation, we show that our best-distilled models outperform the teacher model by 5-7 WER points and are on par with or outperform similar supervised data filtering setups. When scaling the data, our models significantly outperform all zero-shot and supervised models. Our models are also 25-50% more compute- and memory-efficient while maintaining performance equal to or better than that of the teacher model. For more details about our models, dataset, and other resources, please visit our GitHub page: https://github.com/UBC-NLP/uDistilWhisper.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Query Reformulation with the Guidance of Retrieved Documents</title>
<link>https://arxiv.org/abs/2407.12363</link>
<guid>https://arxiv.org/abs/2407.12363</guid>
<content:encoded><![CDATA[
arXiv:2407.12363v5 Announce Type: replace 
Abstract: Conversational search seeks to retrieve relevant passages for the given questions in conversational question answering. Conversational Query Reformulation (CQR) improves conversational search by refining the original queries into de-contextualized forms to resolve the issues in the original queries, such as omissions and coreferences. Previous CQR methods focus on imitating human written queries which may not always yield meaningful search results for the retriever. In this paper, we introduce GuideCQR, a framework that refines queries for CQR by leveraging key information from the initially retrieved documents. Specifically, GuideCQR extracts keywords and generates expected answers from the retrieved documents, then unifies them with the queries after filtering to add useful information that enhances the search process. Experimental results demonstrate that our proposed method achieves state-of-the-art performance across multiple datasets, outperforming previous CQR methods. Additionally, we show that GuideCQR can get additional performance gains in conversational search using various types of queries, even for queries written by humans.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersLLM: A Personified Training Approach for Large Language Models</title>
<link>https://arxiv.org/abs/2407.12393</link>
<guid>https://arxiv.org/abs/2407.12393</guid>
<content:encoded><![CDATA[
arXiv:2407.12393v5 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit human-like intelligence, enabling them to simulate human behavior and support various applications that require both humanized communication and extensive knowledge reserves. Efforts are made to personify LLMs with special training data or hand-crafted prompts, while correspondingly faced with challenges such as insufficient data usage or rigid behavior patterns. Consequently, personified LLMs fail to capture personified knowledge or express persistent opinion. To fully unlock the potential of LLM personification, we propose PersLLM, a framework for better data construction and model tuning. For insufficient data usage, we incorporate strategies such as Chain-of-Thought prompting and anti-induction, improving the quality of data construction and capturing the personality experiences, knowledge, and thoughts more comprehensively. For rigid behavior patterns, we design the tuning process and introduce automated DPO to enhance the specificity and dynamism of the models' personalities, which leads to a more natural opinion communication. Both automated metrics and expert human evaluations demonstrate the effectiveness of our approach. Case studies in human-machine interactions and multi-agent systems further suggest potential application scenarios and future directions for LLM personification.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Next Token Prediction: Patch-Level Training for Large Language Models</title>
<link>https://arxiv.org/abs/2407.12665</link>
<guid>https://arxiv.org/abs/2407.12665</guid>
<content:encoded><![CDATA[
arXiv:2407.12665v3 Announce Type: replace 
Abstract: The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\times$, without compromising the model performance compared to token-level training. Source code: https://github.com/shaochenze/PatchTrain.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners</title>
<link>https://arxiv.org/abs/2407.15508</link>
<guid>https://arxiv.org/abs/2407.15508</guid>
<content:encoded><![CDATA[
arXiv:2407.15508v3 Announce Type: replace 
Abstract: The quantization of large language models (LLMs) has been a prominent research area aimed at enabling their lightweight deployment in practice. Existing research about LLM's quantization has mainly explored the interplay between weights and activations, or employing auxiliary components while neglecting the necessity of adjusting weights during quantization. Consequently, original weight distributions frequently fail to yield desired results after round-to-nearest (RTN) quantization. Even though incorporating techniques such as mixed precision and low-rank error approximation in LLM's quantization can yield improved results, they inevitably introduce additional computational overhead. On the other hand, traditional techniques for weight quantization, such as Generative Post-Training Quantization, rely on manually tweaking weight distributions to minimize local errors, but they fall short of achieving globally optimal outcomes. Although the recently proposed Learnable Singular-value Increment improves global weight quantization by modifying weight distributions, it disrupts the original distribution considerably. This introduces pronounced bias toward the training data and can degrade downstream task performance. In this paper, we introduce Singular-value Diagonal Expansion, a more nuanced approach to refining weight distributions to achieve better quantization alignment. Furthermore, we introduce Cross-layer Learning that improves overall quantization outcomes by distributing errors more evenly across layers. Our plug-and-play weight-quantization methods demonstrate substantial performance improvements over state-of-the-art approaches, including OmniQuant, DuQuant, and PrefixQuant.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time</title>
<link>https://arxiv.org/abs/2409.13338</link>
<guid>https://arxiv.org/abs/2409.13338</guid>
<content:encoded><![CDATA[
arXiv:2409.13338v3 Announce Type: replace 
Abstract: Who is the US President? The answer changes depending on when the question is asked. While large language models (LLMs) are evaluated on various reasoning tasks, they often miss a crucial dimension: time. In real-world scenarios, the correctness of answers is frequently tied to temporal context. To address this gap, we present a novel framework and dataset spanning over 8,000 events from 2018 to 2024, annotated with day-level granularity and sourced globally across domains such as politics, science, and business. Our TimeShift evaluation method systematically probes LLMs for temporal reasoning, revealing that base models often outperform instruction-tuned and synthetic-trained counterparts on time-sensitive recall. Additionally, we find that even large-scale models exhibit brittleness in handling paraphrased facts, highlighting unresolved challenges in temporal consistency. By identifying these limitations, our work provides a significant step toward advancing time-aware language models capable of adapting to the dynamic nature of real-world knowledge.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder</title>
<link>https://arxiv.org/abs/2409.14074</link>
<guid>https://arxiv.org/abs/2409.14074</guid>
<content:encoded><![CDATA[
arXiv:2409.14074v3 Announce Type: replace 
Abstract: Multilingual automatic speech recognition (ASR) in the medical domain serves as a foundational task for various downstream applications such as speech translation, spoken language understanding, and voice-activated assistants. This technology improves patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, the first multilingual medical ASR dataset, along with the first collection of small-to-large end-to-end medical ASR models, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest medical ASR dataset across all major benchmarks: total duration, number of recording conditions, number of accents, and number of speaking roles. Furthermore, we present the first multilinguality study for medical ASR, which includes reproducible empirical baselines, a monolinguality-multilinguality analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a linguistic analysis. We present practical ASR end-to-end training schemes optimized for a fixed number of trainable parameters that are common in industry settings. All code, data, and models are available online: https://github.com/leduckhai/MultiMed/tree/master/MultiMed.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoLM: brain-like spatio-functional organization in a topographic language model</title>
<link>https://arxiv.org/abs/2410.11516</link>
<guid>https://arxiv.org/abs/2410.11516</guid>
<content:encoded><![CDATA[
arXiv:2410.11516v3 Announce Type: replace 
Abstract: Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does Knowledge Selection Help Retrieval Augmented Generation?</title>
<link>https://arxiv.org/abs/2410.13258</link>
<guid>https://arxiv.org/abs/2410.13258</guid>
<content:encoded><![CDATA[
arXiv:2410.13258v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoFact: Timeline-based Temporal Fact Verification</title>
<link>https://arxiv.org/abs/2410.14964</link>
<guid>https://arxiv.org/abs/2410.14964</guid>
<content:encoded><![CDATA[
arXiv:2410.14964v2 Announce Type: replace 
Abstract: Temporal claims, often riddled with inaccuracies, are a significant challenge in the digital misinformation landscape. Fact-checking systems that can accurately verify such claims are crucial for combating misinformation. Current systems struggle with the complexities of evaluating the accuracy of these claims, especially when they include multiple, overlapping, or recurring events. We introduce a novel timeline-based fact verification framework that identify events from both claim and evidence and organize them into their respective chronological timelines. The framework systematically examines the relationships between the events in both claim and evidence to predict the veracity of each claim event and their chronological accuracy. This allows us to accurately determine the overall veracity of the claim. We also introduce a new dataset of complex temporal claims involving timeline-based reasoning for the training and evaluation of our proposed framework. Experimental results demonstrate the effectiveness of our approach in handling the intricacies of temporal claim verification.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</title>
<link>https://arxiv.org/abs/2410.21909</link>
<guid>https://arxiv.org/abs/2410.21909</guid>
<content:encoded><![CDATA[
arXiv:2410.21909v2 Announce Type: replace 
Abstract: The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction</title>
<link>https://arxiv.org/abs/2411.00646</link>
<guid>https://arxiv.org/abs/2411.00646</guid>
<content:encoded><![CDATA[
arXiv:2411.00646v2 Announce Type: replace 
Abstract: Vision Large Language Models (VLLMs) usually take input as a concatenation of image token embeddings and text token embeddings and conduct causal modeling. However, their internal behaviors remain underexplored, raising the question of interaction among two types of tokens. To investigate such multimodal interaction during model inference, in this paper, we measure the contextualization among the hidden state vectors of tokens from different modalities. Our experiments uncover a four-phase inference dynamics of VLLMs against the depth of Transformer-based LMs, including (I) Alignment: In very early layers, contextualization emerges between modalities, suggesting a feature space alignment. (II) Intra-modal Encoding: In early layers, intra-modal contextualization is enhanced while inter-modal interaction is suppressed, suggesting a local encoding within modalities. (III) Inter-modal Encoding: In later layers, contextualization across modalities is enhanced, suggesting a deeper fusion across modalities. (IV) Output Preparation: In very late layers, contextualization is reduced globally, and hidden states are aligned towards the unembedding space.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Memory and Reasoning Ability in Large Language Models</title>
<link>https://arxiv.org/abs/2411.13504</link>
<guid>https://arxiv.org/abs/2411.13504</guid>
<content:encoded><![CDATA[
arXiv:2411.13504v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities. However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized. This ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of LLMs in high-stakes domains. In this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge. To facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. Our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively. The code is available at https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KBAlign: Efficient Self Adaptation on Specific Knowledge Bases</title>
<link>https://arxiv.org/abs/2411.14790</link>
<guid>https://arxiv.org/abs/2411.14790</guid>
<content:encoded><![CDATA[
arXiv:2411.14790v4 Announce Type: replace 
Abstract: Although retrieval-augmented generation (RAG) remains essential for knowledge-based question answering (KBQA), current paradigms face critical challenges under specific domains. Existing methods struggle with targeted adaptation on small-scale KBs: vanilla unsupervised training exhibits poor effectiveness, while fine-tuning incurs prohibitive costs of external signals. We present KBAlign, a self-supervised framework that enhances RAG systems through efficient model adaptation. Our key insight is to leverage the model's intrinsic capabilities for knowledge alignment through two innovative mechanisms: multi-grained self-annotation that captures global knowledge for data construction, and iterative tuning that accelerates convergence through self verification. This framework enables cost-effective model adaptation to specific textual KBs, without human supervision or external model assistance. Experiments demonstrate that KBAlign can achieve 90\% of the performance gain obtained through GPT-4-supervised adaptation, while relying entirely on self-annotation of much smaller models. KBAlign significantly improves downstream QA accuracy across multiple domains with tiny costs, particularly benefiting scenarios requiring deep knowledge integration from specialized corpora. We release our experimental data, models, and process analyses to the community for further exploration (https://github.com/thunlp/KBAlign).
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models</title>
<link>https://arxiv.org/abs/2411.19477</link>
<guid>https://arxiv.org/abs/2411.19477</guid>
<content:encoded><![CDATA[
arXiv:2411.19477v3 Announce Type: replace 
Abstract: We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2412.03587</link>
<guid>https://arxiv.org/abs/2412.03587</guid>
<content:encoded><![CDATA[
arXiv:2412.03587v2 Announce Type: replace 
Abstract: Transformer-based large-scale pre-trained models achieve great success. Fine-tuning is the standard practice for leveraging these models in downstream tasks. Among the fine-tuning methods, adapter-tuning provides a parameter-efficient fine-tuning by introducing lightweight trainable modules while keeping most pre-trained parameters frozen. However, existing adapter-tuning methods still impose substantial resource usage. Through our investigation, we show that each adapter unequally contributes to both task performance and resource usage. Motivated by this insight, we propose Selective Adapter FrEezing (SAFE), which gradually freezes less important adapters early to reduce unnecessary resource usage while maintaining performance. In our experiments, SAFE reduces memory usage, computation amount, and training time by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or better task performance compared to the baseline. We also demonstrate that SAFE induces regularization effect, thereby smoothing the loss landscape, which enables the model to generalize better by avoiding sharp minima.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation</title>
<link>https://arxiv.org/abs/2501.00777</link>
<guid>https://arxiv.org/abs/2501.00777</guid>
<content:encoded><![CDATA[
arXiv:2501.00777v2 Announce Type: replace 
Abstract: Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)</title>
<link>https://arxiv.org/abs/2501.13957</link>
<guid>https://arxiv.org/abs/2501.13957</guid>
<content:encoded><![CDATA[
arXiv:2501.13957v2 Announce Type: replace 
Abstract: Objective Structured Clinical Examinations (OSCEs) are widely used to assess medical students' communication skills, but scoring interview-based assessments is time-consuming and potentially subject to human bias. This study explored the potential of large language models (LLMs) to automate OSCE evaluations using the Master Interview Rating Scale (MIRS). We compared the performance of four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts across all 28 items of the MIRS under the conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step prompting. The models were benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores available. Model performance was measured using three accuracy metrics (exact, off-by-one, thresholded). Averaging across all MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater reliability ({\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step techniques proved valuable when tailored to specific assessment items. The performance was consistent across MIRS items, independent of encounter phases and communication domains. We demonstrated the feasibility of AI-assisted OSCE evaluation and provided benchmarking of multiple LLMs across multiple prompt techniques. Our work provides a baseline performance assessment for LLMs that lays a foundation for future research into automated assessment of clinical communication skills.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs</title>
<link>https://arxiv.org/abs/2501.15674</link>
<guid>https://arxiv.org/abs/2501.15674</guid>
<content:encoded><![CDATA[
arXiv:2501.15674v2 Announce Type: replace 
Abstract: The reasoning abilities of Large Language Models (LLMs) can be improved by structurally denoising their weights, yet existing techniques primarily focus on denoising the feed-forward network (FFN) of the transformer block, and can not efficiently utilise the Multi-head Attention (MHA) block, which is the core of transformer architectures. To address this issue, we propose a novel intuitive framework that, at its very core, performs MHA compression through a multi-head tensorisation process and the Tucker decomposition. This enables both higher-dimensional structured denoising and compression of the MHA weights, by enforcing a shared higher-dimensional subspace across the weights of the multiple attention heads. We demonstrate that this approach consistently enhances the reasoning capabilities of LLMs across multiple benchmark datasets, and for both encoder-only and decoder-only architectures, while achieving compression rates of up to $\sim 250$ times in the MHA weights, all without requiring any additional data, training, or fine-tuning. Furthermore, we show that the proposed method can be seamlessly combined with existing FFN-only-based denoising techniques to achieve further improvements in LLM reasoning performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning</title>
<link>https://arxiv.org/abs/2502.04689</link>
<guid>https://arxiv.org/abs/2502.04689</guid>
<content:encoded><![CDATA[
arXiv:2502.04689v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities on complex evaluation benchmarks, many of which are formulated as question-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts is becoming increasingly vital for advancing their development and applicability. This paper introduces ARR, an intuitive, effective, and general QA solving method that explicitly incorporates three key steps: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Notably, this paper is the first to introduce intent analysis in QA, which plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA tasks demonstrate that ARR consistently outperforms the baseline methods. Ablation and case studies further validate the positive contributions of each ARR component. Furthermore, experiments involving variations in prompt design indicate that ARR maintains its effectiveness regardless of the specific prompt formulation. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Multiple Large Language Models: A Survey on LLM Ensemble</title>
<link>https://arxiv.org/abs/2502.18036</link>
<guid>https://arxiv.org/abs/2502.18036</guid>
<content:encoded><![CDATA[
arXiv:2502.18036v4 Announce Type: replace 
Abstract: LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of "ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus</title>
<link>https://arxiv.org/abs/2503.06899</link>
<guid>https://arxiv.org/abs/2503.06899</guid>
<content:encoded><![CDATA[
arXiv:2503.06899v2 Announce Type: replace 
Abstract: Video-based dialogue systems, such as education assistants, have compelling application value, thereby garnering growing interest. However, the current video-based dialogue systems are limited by their reliance on a single dialogue type, which hinders their versatility in practical applications across a range of scenarios, including question-answering, emotional dialog, etc. In this paper, we identify this challenge as how to generate video-driven multilingual mixed-type dialogues. To mitigate this challenge, we propose a novel task and create a human-to-human video-driven multilingual mixed-type dialogue corpus, termed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues, across 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally, we establish baseline models on KwaiChat. An extensive analysis of 7 distinct LLMs on KwaiChat reveals that GPT-4o achieves the best performance but still cannot perform well in this situation even with the help of in-context learning and fine-tuning, which indicates that the task is not trivial and needs further research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concise Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05185</link>
<guid>https://arxiv.org/abs/2504.05185</guid>
<content:encoded><![CDATA[
arXiv:2504.05185v2 Announce Type: replace 
Abstract: Despite significant advancements in large language models (LLMs), a major drawback of reasoning models is their enormous token usage, which increases computational cost, resource requirements, and response time. In this work, we revisit the core principles of reinforcement learning (RL) and, through mathematical analysis, demonstrate that the tendency to generate lengthy responses arises inherently from RL-based optimization during training. This finding questions the prevailing assumption that longer responses inherently improve reasoning accuracy. Instead, we uncover a natural correlation between conciseness and accuracy that has been largely overlooked. We show that introducing a secondary phase of RL training, using a very small set of problems, can significantly reduce chains of thought while maintaining or even enhancing accuracy. Additionally, we demonstrate that, while GRPO shares some interesting properties of PPO, it suffers from collapse modes, which limit its reliability for concise reasoning. Finally, we validate our conclusions through extensive experimental results.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric</title>
<link>https://arxiv.org/abs/2504.07440</link>
<guid>https://arxiv.org/abs/2504.07440</guid>
<content:encoded><![CDATA[
arXiv:2504.07440v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development. One core challenge of evaluation in the large language model (LLM) era is the generalization issue: how to infer a model's near-unbounded abilities from inevitably bounded benchmarks. We address this challenge by proposing Model Utilization Index (MUI), a mechanism interpretability enhanced metric that complements traditional performance scores. MUI quantifies the effort a model expends on a task, defined as the proportion of activated neurons or features during inference. Intuitively, a truly capable model should achieve higher performance with lower effort. Extensive experiments across popular LLMs reveal a consistent inverse logarithmic relationship between MUI and performance, which we formulate as the Utility Law. From this law we derive four practical corollaries that (i) guide training diagnostics, (ii) expose data contamination issue, (iii) enable fairer model comparisons, and (iv) design model-specific dataset diversity. Our code can be found at https://github.com/ALEX-nlp/MUI-Eva.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives</title>
<link>https://arxiv.org/abs/2504.10823</link>
<guid>https://arxiv.org/abs/2504.10823</guid>
<content:encoded><![CDATA[
arXiv:2504.10823v2 Announce Type: replace 
Abstract: Navigating high-stakes dilemmas involving conflicting values is challenging even for humans, let alone for AI. Yet prior work in evaluating the reasoning capabilities of large language models (LLMs) in such situations has been limited to everyday scenarios. To close this gap, this work first introduces CLASH (Character perspective-based LLM Assessments in Situations with High-stakes), a meticulously curated dataset consisting of 345 high-impact dilemmas along with 3,795 individual perspectives of diverse values. In particular, we design CLASH in a way to support the study of critical aspects of value-based decision-making processes which are missing from prior work, including understanding decision ambivalence and psychological discomfort as well as capturing the temporal shifts of values in characters' perspectives. By benchmarking 10 open and closed frontier models, we uncover several key findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet, achieve less than 50% accuracy in identifying situations where the decision should be ambivalent, while they perform significantly better in clear-cut scenarios. (2) While LLMs reasonably predict psychological discomfort as marked by human, they inadequately comprehend perspectives involving value shifts, indicating a need for LLMs to reason over complex values. (3) Our experiments also reveal a significant correlation between LLMs' value preferences and their steerability towards a given value. (4) Finally, LLMs exhibit greater steerability when engaged in value reasoning from a third-party perspective, compared to a first-person setup, though certain value pairs benefit uniquely from the first-person framing.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction</title>
<link>https://arxiv.org/abs/2504.17671</link>
<guid>https://arxiv.org/abs/2504.17671</guid>
<content:encoded><![CDATA[
arXiv:2504.17671v3 Announce Type: replace 
Abstract: This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>https://arxiv.org/abs/2405.03689</link>
<guid>https://arxiv.org/abs/2405.03689</guid>
<content:encoded><![CDATA[
arXiv:2405.03689v2 Announce Type: replace-cross 
Abstract: Language is often used to describe physical interaction, yet most 3D human pose estimation methods overlook this rich source of information. We bridge this gap by leveraging large multimodal models (LMMs) as priors for reconstructing contact poses, offering a scalable alternative to traditional methods that rely on human annotations or motion capture data. Our approach extracts contact-relevant descriptors from an LMM and translates them into tractable losses to constrain 3D human pose optimization. Despite its simplicity, our method produces compelling reconstructions for both two-person interactions and self-contact scenarios, accurately capturing the semantics of physical and social interactions. Our results demonstrate that LMMs can serve as powerful tools for contact prediction and pose estimation, offering an alternative to costly manual human annotations or motion capture data. Our code is publicly available at https://prosepose.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Action Pretraining from Videos</title>
<link>https://arxiv.org/abs/2410.11758</link>
<guid>https://arxiv.org/abs/2410.11758</guid>
<content:encoded><![CDATA[
arXiv:2410.11758v2 Announce Type: replace-cross 
Abstract: We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Reinforcement Learning</title>
<link>https://arxiv.org/abs/2411.14251</link>
<guid>https://arxiv.org/abs/2411.14251</guid>
<content:encoded><![CDATA[
arXiv:2411.14251v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective</title>
<link>https://arxiv.org/abs/2504.19458</link>
<guid>https://arxiv.org/abs/2504.19458</guid>
<content:encoded><![CDATA[
arXiv:2504.19458v3 Announce Type: replace-cross 
Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence</title>
<link>https://arxiv.org/abs/2505.08828</link>
<guid>https://arxiv.org/abs/2505.08828</guid>
<content:encoded><![CDATA[
<div> authorship verification, AI assistance, academic writing, transparency, student development

Summary:
This research explores the use of authorship verification (AV) techniques to quantify AI assistance in academic writing, focusing on transparency and student development. By expanding datasets and developing an adapted AV method, the study evaluates the effectiveness of identifying stylometric differences between student and AI-generated texts. Results show that the enhanced AV classifier can detect discrepancies at word and sentence levels, providing educators with a transparent tool for academic integrity investigations. This work advances AV technology, offering insights into the dynamics of academic writing in an AI-driven era. <br /><br />Summary: <div>
arXiv:2505.08828v1 Announce Type: new 
Abstract: As human-AI collaboration becomes increasingly prevalent in educational contexts, understanding and measuring the extent and nature of such interactions pose significant challenges. This research investigates the use of authorship verification (AV) techniques not as a punitive measure, but as a means to quantify AI assistance in academic writing, with a focus on promoting transparency, interpretability, and student development. Building on prior work, we structured our investigation into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Using three datasets - including a public dataset (PAN-14) and two from University of Melbourne students from various courses - we expanded the data to include LLM-generated texts, totalling 1,889 documents and 540 authorship problems from 506 students. We developed an adapted Feature Vector Difference AV methodology to construct robust academic writing profiles for students, designed to capture meaningful, individual characteristics of their writing. The method's effectiveness was evaluated across multiple scenarios, including distinguishing between student-authored and LLM-generated texts and testing resilience against LLMs' attempts to mimic student writing styles. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels while providing educators with a transparent tool to support academic integrity investigations. This work advances AV technology, offering actionable insights into the dynamics of academic writing in an AI-driven era.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives</title>
<link>https://arxiv.org/abs/2505.08891</link>
<guid>https://arxiv.org/abs/2505.08891</guid>
<content:encoded><![CDATA[
<div> interactive narrative, AI, motivation, educational game, research ethics
<br />
Summary:
This article explores the impact of dynamic narratives in educational interactive games on learner motivation. Two versions of the game, one with a static narrative and the other with a dynamic narrative, were compared. Results showed that responsive content and a variety of choices are crucial for player engagement. Balancing pedagogical goals with dynamic narrative elements presents a challenge. The study highlights the potential of AI-driven dynamic narratives in educational games and discusses design implications arising from the findings. Ultimately, the research provides insights into the benefits and challenges of incorporating dynamic storytelling in educational games. <div>
arXiv:2505.08891v1 Announce Type: new 
Abstract: Motivation is an important factor underlying successful learning. Previous research has demonstrated the positive effects that static interactive narrative games can have on motivation. Concurrently, advances in AI have made dynamic and adaptive approaches to interactive narrative increasingly accessible. However, limited work has explored the impact that dynamic narratives can have on learner motivation. In this paper, we compare two versions of Academical, a choice-based educational interactive narrative game about research ethics. One version employs a traditional hand-authored branching plot (i.e., static narrative) while the other dynamically sequences plots during play (i.e., dynamic narrative). Results highlight the importance of responsive content and a variety of choices for player engagement, while also illustrating the challenge of balancing pedagogical goals with the dynamic aspects of narrative. We also discuss design implications that arise from these findings. Ultimately, this work provides initial steps to illuminate the emerging potential of AI-driven dynamic narrative in educational games.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A suite of LMs comprehend puzzle statements as well as humans</title>
<link>https://arxiv.org/abs/2505.08996</link>
<guid>https://arxiv.org/abs/2505.08996</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, comprehension, human performance, GPT-4, pragmatics

Summary: 
The study challenges previous claims that large language models (LMs) underperform humans in comprehending minimally complex English statements. The research found that human accuracy dropped significantly when rereading was restricted, falling below that of the Falcon-180B-Chat and GPT-4 models. The newer GPT-o1 model achieved perfect accuracy in comprehension tests. Both humans and models struggled with queries involving potentially reciprocal actions, suggesting shared pragmatic sensitivities. The study also revealed systematic underestimation of model performance and highlighted the need for more careful experimental design and coding practices in evaluating LLMs. Additionally, the results showed that GPT-4o can align with either naive or expert grammaticality judgments depending on prompt framing. This challenges the assumption that current models are inherently weaker than humans at language comprehension. 

<br /><br />Summary: <div>
arXiv:2505.08996v1 Announce Type: new 
Abstract: Recent claims suggest that large language models (LMs) underperform humans in comprehending minimally complex English statements (Dentella et al., 2024). Here, we revisit those findings and argue that human performance was overestimated, while LLM abilities were underestimated. Using the same stimuli, we report a preregistered study comparing human responses in two conditions: one allowed rereading (replicating the original study), and one that restricted rereading (a more naturalistic comprehension test). Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect accuracy. Results further show that both humans and models are disproportionately challenged by queries involving potentially reciprocal actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than model-specific deficits. Additional analyses using Llama-2-70B log probabilities, a recoding of open-ended model responses, and grammaticality ratings of other sentences reveal systematic underestimation of model performance. We find that GPT-4o can align with either naive or expert grammaticality judgments, depending on prompt framing. These findings underscore the need for more careful experimental design and coding practices in LLM evaluation, and they challenge the assumption that current models are inherently weaker than humans at language comprehension.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies</title>
<link>https://arxiv.org/abs/2505.09005</link>
<guid>https://arxiv.org/abs/2505.09005</guid>
<content:encoded><![CDATA[
<div> Keywords: LM, natural language understanding, metalinguistic judgments, information structure, syntax

Summary: 
The study investigates the ability of the GPT-4 language model (LM) to understand natural language and make reliable metalinguistic judgments. Through experiments involving information structure and acceptability tasks, the researchers find that GPT-4 exhibits metalinguistic skill that replicates human performance. They also demonstrate a causal relationship between the prominence of a constituent in a context sentence and subsequent acceptability ratings on long distance dependency constructions. The results suggest a close correspondence between how English speakers perceive information structure and syntax, and highlight the potential for LMs to capture subtle linguistic relationships proposed by linguists. Further research is needed to explore the implications of these findings. 

<br /><br />Summary: <div>
arXiv:2505.09005v1 Announce Type: new 
Abstract: It remains debated how well any LM understands natural language or generates reliable metalinguistic judgments. Moreover, relatively little work has demonstrated that LMs can represent and respect subtle relationships between form and function proposed by linguists. We here focus on a particular such relationship established in recent work: English speakers' judgments about the information structure of canonical sentences predicts independently collected acceptability ratings on corresponding 'long distance dependency' [LDD] constructions, across a wide array of base constructions and multiple types of LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on the same tasks used with humans and new extensions.Results reveal reliable metalinguistic skill on the information structure and acceptability tasks, replicating a striking interaction between the two, despite the zero-shot, explicit nature of the tasks, and little to no chance of contamination [Studies 1a, 1b]. Study 2 manipulates the information structure of base sentences and confirms a causal relationship: increasing the prominence of a constituent in a context sentence increases the subsequent acceptability ratings on an LDD construction. The findings suggest a tight relationship between natural and GPT-4 generated English, and between information structure and syntax, which begs for further exploration.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomic Consistency Preference Optimization for Long-Form Question Answering</title>
<link>https://arxiv.org/abs/2505.09039</link>
<guid>https://arxiv.org/abs/2505.09039</guid>
<content:encoded><![CDATA[
<div> ---
Large Language Models, Factoid Hallucinations, Model Alignment, Atomic Consistency Signals, Factual Accuracy <br />
<br />
Summary: 
The article introduces a new method called Atomic Consistency Preference Optimization (ACPO) to enhance factual accuracy in Large Language Models (LLMs) without the need for external supervision. ACPO leverages atomic consistency signals to identify high- and low-quality data pairs for model alignment, improving factual reliability in question-answering tasks. The method outperforms a strong supervised alignment baseline, FactAlign, on the LongFact and BioGen datasets, showcasing its effectiveness in enhancing factual accuracy. ACPO eliminates the reliance on costly external models or knowledge bases, providing a scalable and efficient approach for improving factoid question-answering in LLMs. <div>
arXiv:2505.09039v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated factual and non-factual pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness, which may not always be accessible. To address this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals, i.e., the agreement of individual facts across multiple stochastic responses, to identify high- and low-quality data pairs for model alignment. By eliminating the need for costly GPT calls, ACPO provides a scalable and efficient approach to improving factoid question-answering. Despite being self-supervised, empirical results demonstrate that ACPO outperforms FactAlign, a strong supervised alignment baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its effectiveness in enhancing factual reliability without relying on external models or knowledge bases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias</title>
<link>https://arxiv.org/abs/2505.09056</link>
<guid>https://arxiv.org/abs/2505.09056</guid>
<content:encoded><![CDATA[
<div> similarity, variability, ethical implications, LLMs, linguistic uniqueness <br />
Summary: 
- Outputs from the same LLM are more similar to each other than to human-written texts.
- Models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4 produces more varied responses.
- LLM writing styles differ significantly, with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for distinctiveness.
- Differences in vocabulary and tone underscore the linguistic uniqueness of LLM-generated content.
- Some LLMs demonstrate greater gender balance and reduced bias. <div>
arXiv:2505.09056v1 Announce Type: new 
Abstract: Large Language Models (LLMs) represent a major step toward artificial general intelligence, significantly advancing our ability to interact with technology. While LLMs perform well on Natural Language Processing tasks -- such as translation, generation, code writing, and summarization -- questions remain about their output similarity, variability, and ethical implications. For instance, how similar are texts generated by the same model? How does this compare across different models? And which models best uphold ethical standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like generation, explanation, and rewriting. This resulted in approximately 3 million texts from 12 LLMs, including proprietary and open-source systems from OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs from the same LLM are more similar to each other than to human-written texts; (2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4 produces more varied responses; (3) LLM writing styles differ significantly, with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for distinctiveness; (4) differences in vocabulary and tone underscore the linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate greater gender balance and reduced bias. These results offer new insights into the behavior and diversity of LLM outputs, helping guide future development and ethical evaluation.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment</title>
<link>https://arxiv.org/abs/2505.09068</link>
<guid>https://arxiv.org/abs/2505.09068</guid>
<content:encoded><![CDATA[
<div> Keywords: S-DAT, divergent thinking, multilingual, semantic distance, cognitive flexibility <br />
<br />
Summary: <br />
This paper introduces S-DAT, a framework for automated assessment of divergent thinking (DT) using semantic distance calculations. S-DAT is scalable and can be applied across multiple languages, showing consistent scoring. It demonstrates convergent validity with other DT measures and discriminant validity with convergent thinking. This cross-linguistic flexibility makes it a valuable tool for global-scale creativity research, overcoming limitations of previous methods. S-DAT enables fairer and more comprehensive evaluation of cognitive flexibility in diverse populations, providing a powerful online assessment tool. <div>
arXiv:2505.09068v1 Announce Type: new 
Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a scalable, multilingual framework for automated assessment of divergent thinking (DT) -a core component of human creativity. Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability. In contrast, S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance -- a language-agnostic proxy for DT. We evaluate S-DAT across eleven diverse languages, including English, Spanish, German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating robust and consistent scoring across linguistic contexts. Unlike prior DAT approaches, the S-DAT shows convergent validity with other DT measures and correct discriminant validity with convergent thinking. This cross-linguistic flexibility allows for more inclusive, global-scale creativity research, addressing key limitations of earlier approaches. S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online: https://sdat.iol.zib.de/.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEC-Zero: Chinese Error Correction Solution Based on LLM</title>
<link>https://arxiv.org/abs/2505.09082</link>
<guid>https://arxiv.org/abs/2505.09082</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese Spelling Correction, reinforcement learning, self-correction, generalization

Summary:
The paper introduces CEC-Zero, a novel reinforcement learning framework that enhances large language models (LLMs) for Chinese text processing. LLMs have shown superior performance in Chinese Spelling Correction (CSC) compared to traditional models like BERT, but challenges remain in reliability and generalization. By combining reinforcement learning with LLMs' generative capabilities, CEC-Zero enables autonomous error strategy learning without the need for annotated data or auxiliary models. Experiments demonstrate that the RL-enhanced LLMs achieve high accuracy and improved cross-domain generalization, making them suitable for practical Chinese NLP applications. This breakthrough in self-correction techniques opens up new possibilities for deploying LLMs in real-world scenarios and sets a precedent for self-improving language models in the future. 

<br /><br />Summary: <div>
arXiv:2505.09082v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How an unintended Side Effect of a Research Project led to Boosting the Power of UML</title>
<link>https://arxiv.org/abs/2505.09269</link>
<guid>https://arxiv.org/abs/2505.09269</guid>
<content:encoded><![CDATA[
<div> Keywords: UML, modeling tool, object diagrams, software architectures, teaching

Summary: 
This paper presents a new UML modeling tool that offers advancements over traditional tools by integrating class diagrams and object diagrams, as well as enabling object execution. This tool facilitates the creation of new software architectures that combine software with object models, making it valuable for educational purposes and providing students with an engaging learning experience. The project has originated from a longstanding international research initiative focused on a comprehensive multi-level architecture, demonstrating how research efforts can yield significant results as a byproduct of other undertakings. The tool's innovative features and origins highlight its potential to enhance modeling practices and educational experiences within the software development domain. 

<br /><br />Summary: <div>
arXiv:2505.09269v1 Announce Type: new 
Abstract: This paper describes the design, implementation and use of a new UML modeling tool that represents a significant advance over conventional tools. Among other things, it allows the integration of class diagrams and object diagrams as well as the execution of objects. This not only enables new software architectures characterized by the integration of software with corresponding object models, but is also ideal for use in teaching, as it provides students with a particularly stimulating learning experience. A special feature of the project is that it has emerged from a long-standing international research project, which is aimed at a comprehensive multi-level architecture. The project is therefore an example of how research can lead to valuable results that arise as a side effect of other work.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data</title>
<link>https://arxiv.org/abs/2505.09286</link>
<guid>https://arxiv.org/abs/2505.09286</guid>
<content:encoded><![CDATA[
<div> Keywords: Online review data, Multilingual, Unsupervised framework, Aspect detection, Language models

Summary: <br /><br />Effectively analyzing online review data is crucial for various industries. This study proposes a multilingual, scalable, and unsupervised framework for cross-domain aspect detection, addressing limitations of domain-specific studies and dependency on large labeled datasets. By extracting aspect category candidates through clustering and utilizing aspect-aware embedding vectors, the framework achieves high performance in multi-aspect labeling. Fine-tuning pretrained language models demonstrates the effectiveness of the generated labels, outperforming publicly available models in consistency and scalability. Human evaluation confirms the quality of automatic labels comparable to manual creation. The study highlights the potential of a robust multi-aspect labeling approach adaptable to diverse language and domain environments. Future research will explore automatic review summarization and integrating artificial intelligence agents to enhance review analysis efficiency and depth. <br /><br />Summary: <div>
arXiv:2505.09286v1 Announce Type: new 
Abstract: Effectively analyzing online review data is essential across industries. However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets. To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection. This framework is designed for multi-aspect labeling of multilingual and multi-domain review data. In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments. Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling. To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels. Results show that these models achieve high performance, demonstrating that the labels are suitable for training. Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data. A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually. This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments. Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging</title>
<link>https://arxiv.org/abs/2505.09316</link>
<guid>https://arxiv.org/abs/2505.09316</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented reasoning, dynamic interaction, adaptive search behaviors, reinforcement learning, information-seeking process
Summary:<br /><br />
The study introduces InForage, a reinforcement learning framework inspired by Information Foraging Theory, for retrieval-augmented reasoning in large language models. Unlike traditional methods, InForage rewards intermediate retrieval quality, promoting adaptive search behaviors. A human-guided dataset capturing iterative search and reasoning trajectories was constructed for training the model. Extensive evaluations across various tasks showed that InForage outperformed baseline methods in general question answering, multi-hop reasoning tasks, and real-time web QA. The results emphasize InForage's ability to build robust, adaptive, and efficient reasoning agents. <div>
arXiv:2505.09316v1 Announce Type: new 
Abstract: Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs</title>
<link>https://arxiv.org/abs/2505.09338</link>
<guid>https://arxiv.org/abs/2505.09338</guid>
<content:encoded><![CDATA[
<div> entrainment, language models, distraction, context, semantic<br />
<br />
Contextual entrainment is observed in language models, where tokens previously seen in the prompt have higher probabilities assigned to them, regardless of relevance. This phenomenon occurs independently of semantic relations but is influenced by semantic factors, with counterfactual prompts more impactful. Entrainment heads, attention circuits responsible for entrainment, are identified through a novel method, and when deactivated, reduce the effect of contextual entrainment. This discovery offers insights into how language models become distracted and provides a step towards understanding and addressing this issue.<br /><br />Summary: Contextual entrainment is a novel phenomenon observed in language models, where previously seen tokens are assigned higher probabilities. It occurs independently of semantic relevance but is influenced by semantic factors, with specific prompts having a greater impact. Entrainment heads, attention circuits responsible for entrainment, are identified and their deactivation reduces the entrainment effect. This discovery provides a mechanistic insight into how language models become distracted by irrelevant context. <div>
arXiv:2505.09338v1 Announce Type: new 
Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by ``irrelevant'' contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, we identify these heads across various settings. When we ``turn off'' these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. Our discovery of contextual entrainment, along with our investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen3 Technical Report</title>
<link>https://arxiv.org/abs/2505.09388</link>
<guid>https://arxiv.org/abs/2505.09388</guid>
<content:encoded><![CDATA[
<div> large language models, performance, multilingual capabilities, thinking mode, computational resources<br />
Summary:<br />
Qwen3 is the latest version in the Qwen model family, consisting of large language models with dense and MoE architectures ranging from 0.6 to 235 billion parameters. It integrates thinking mode for complex reasoning and non-thinking mode for rapid responses into a unified framework. This eliminates the need to switch between different models and enables dynamic mode switching based on user queries. Qwen3 introduces a thinking budget mechanism to adaptively allocate computational resources during inference, balancing latency and performance based on task complexity. The model achieves state-of-the-art results across various benchmarks, including code generation and mathematical reasoning. It expands multilingual support to 119 languages and dialects, enhancing global accessibility. Qwen3 models are publicly accessible under Apache 2.0 to promote reproducibility and community-driven research. <br /><br />Summary: <div>
arXiv:2505.09388v1 Announce Type: new 
Abstract: In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits</title>
<link>https://arxiv.org/abs/2505.09407</link>
<guid>https://arxiv.org/abs/2505.09407</guid>
<content:encoded><![CDATA[
<div> Keywords: Cloud-based multilingual translation, quantum computing, machine translation, quantum encoder-decoder architecture, OPUS dataset<br />
Summary: <br />
- Cloud-based multilingual translation services like Google Translate and Microsoft Translator utilize large multilingual language models such as GRU, LSTM, BERT, GPT, and T5, achieving state-of-the-art translation capabilities.
- New natural language systems like ChatGPT and DeepSeek have also shown significant potential in various natural language processing tasks and multilingual translation.
- QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) offers an alternative approach by exploring the quantum computing realm for multilingual machine translation, using quantum convolution, pooling, variational circuits, and attention.
- QEDACVC achieves an accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora, showcasing its effectiveness in multilingual translations.
- By simulating and running on quantum computing hardware, QEDACVC aims to revolutionize machine translation through its quantum encoder-decoder architecture. <br /> 
Summary: <div>
arXiv:2505.09407v1 Announce Type: new 
Abstract: Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning</title>
<link>https://arxiv.org/abs/2505.09519</link>
<guid>https://arxiv.org/abs/2505.09519</guid>
<content:encoded><![CDATA[
<div> Efficient fine-tuning, matrix decomposition, mixture-of-experts, question answering, mathematical problem solving

Summary:
Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models. The integration of router into prompt tuning (PT) increases efficiency but does not universally improve performance. Parameter reduction through matrix decomposition can enhance performance in specific domains. A novel framework, PT-MoE, integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets show PT-MoE achieves state-of-the-art performance in question answering (QA) and mathematical problem solving tasks. PT-MoE improves F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, using 25% fewer parameters than LoRA. PT methods excel in QA tasks, while LoRA-based methods perform better in math datasets. Integration of matrix decomposition and MoE in PT-MoE yields complementary benefits, demonstrating cross-task consistency and generalization abilities. Ablation studies on routing mechanisms and architectural components provide insights for future PEFT methods. <br /><br />Summary: <div>
arXiv:2505.09519v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency yet does not improve performance universally; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models</title>
<link>https://arxiv.org/abs/2505.09595</link>
<guid>https://arxiv.org/abs/2505.09595</guid>
<content:encoded><![CDATA[
<div> bias, cultural inclusivity, benchmark, multiplexity, AI evaluation <br />
Summary: <br />
The article discusses how Large Language Models (LLMs) are often biased towards Western-centric perspectives, leading to cultural homogenization. To address this, the authors introduce WorldView-Bench, a benchmark that evaluates Global Cultural Inclusivity (GCI) in LLMs by measuring their ability to accommodate diverse worldviews. They introduce the concept of Multiplex models, which integrate diverse perspectives, and propose two intervention strategies to implement multiplexity in LLMs. The results show a significant increase in Perspectives Distribution Score (PDS) entropy with MAS-Implemented Multiplex LLMs, indicating a shift towards positive sentiment and enhanced cultural balance. This research highlights the importance of multiplex-aware AI evaluation in mitigating bias in LLMs and promoting more inclusive and ethically aligned AI systems. <br /> <div>
arXiv:2505.09595v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures</title>
<link>https://arxiv.org/abs/2505.08795</link>
<guid>https://arxiv.org/abs/2505.08795</guid>
<content:encoded><![CDATA[
<div> keywords: fast algorithm, hierarchical structures, Minkowski spacetime, token pairs, WordNet

Summary:
A fast algorithm is presented that embeds hierarchical structures in three-dimensional Minkowski spacetime, where data correlations are encoded solely in causal structure. Using oriented token pairs as local hierarchical signals, the model successfully embeds hierarchical structures from the WordNet corpus. The algorithm efficiently represents the mammal sub-tree and expands to include a subset of WordNet with over 82,000 noun tokens. A novel retrieval mechanism based on causality governs hierarchical access, rather than distance. The results suggest that all discrete data can be perfectly represented in a three-dimensional geometry, which exhibits nearly conformal invariance. This indicates deep connections with general relativity and field theory, suggesting that concepts, categories, and their interrelations are inherently geometric. <div>
arXiv:2505.08795v1 Announce Type: cross 
Abstract: We show that there is a fast algorithm that embeds hierarchical structures in three-dimensional Minkowski spacetime. The correlation of data ends up purely encoded in the causal structure. Our model relies solely on oriented token pairs -- local hierarchical signals -- with no access to global symbolic structure. We apply our method to the corpus of \textit{WordNet}. We provide a perfect embedding of the mammal sub-tree including ambiguities (more than one hierarchy per node) in such a way that the hierarchical structures get completely codified in the geometry and exactly reproduce the ground-truth. We extend this to a perfect embedding of the maximal unambiguous subset of the \textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We introduce a novel retrieval mechanism in which causality, not distance, governs hierarchical access. Our results seem to indicate that all discrete data has a perfect geometrical representation that is three-dimensional. The resulting embeddings are nearly conformally invariant, indicating deep connections with general relativity and field theory. These results suggest that concepts, categories, and their interrelations, namely hierarchical meaning itself, is geometric.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits</title>
<link>https://arxiv.org/abs/2505.08823</link>
<guid>https://arxiv.org/abs/2505.08823</guid>
<content:encoded><![CDATA[
<div> quantization, language models, large-scale, computational efficiency, normalization
<br />
Large language models (LLMs) have had a significant impact on natural-language processing but deploying them in real-world scenarios can be costly due to their scale. Post-training quantization can reduce memory and computation requirements, but often leads to accuracy degradation. This study focuses on ternary quantization (2-bit) for LLMs, which can provide significant savings but is challenging to implement. By incorporating RMS normalization and a layer-wise quantization schedule, researchers were able to fine-tune full-precision LLMs into ternary models without sacrificing performance. This approach proved to be as effective as more complex knowledge-distillation methods on standard language-modeling benchmarks, highlighting the importance of careful normalization in bridging the accuracy gap between ternary and full-precision LLMs. These findings suggest that ultra-low-bit inference for LLMs could become more feasible with the right normalization techniques.
<br /><br />Summary: <div>
arXiv:2505.08823v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries</title>
<link>https://arxiv.org/abs/2505.08842</link>
<guid>https://arxiv.org/abs/2505.08842</guid>
<content:encoded><![CDATA[
<div> framework, assessment, libraries, risks, governance
Summary:
LibVulnWatch is a graph-based agentic assessment framework that evaluates open-source AI libraries for risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. Using a directed acyclic graph of specialized agents, the system extracts and quantifies risk using evidence from trusted sources. It generates reproducible scores across five critical domains and publishes them on a public leaderboard for ecosystem monitoring. The system covers up to 88% of OpenSSF Scorecard checks and uncovers additional risks per library, including critical vulnerabilities like Remote Code Execution (RCE) and licensing constraints. It also identifies gaps in regulatory documentation and auditability. LibVulnWatch translates governance principles into practical metrics, advancing technical AI governance with continuous supply chain risk assessment and informed library selection. <br /><br />Summary: <div>
arXiv:2505.08842v1 Announce Type: cross 
Abstract: Open-source AI libraries are foundational to modern AI systems but pose significant, underexamined risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. We present LibVulnWatch, a graph-based agentic assessment framework that performs deep, source-grounded evaluations of these libraries. Built on LangGraph, the system coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases. LibVulnWatch generates reproducible, governance-aligned scores across five critical domains, publishing them to a public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely used libraries, including ML frameworks, LLM inference engines, and agent orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library. These include critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability. By translating high-level governance principles into practical, verifiable metrics, LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Gains of LLMs With Humans in a World of LLMs Versus Humans</title>
<link>https://arxiv.org/abs/2505.08902</link>
<guid>https://arxiv.org/abs/2505.08902</guid>
<content:encoded><![CDATA[
<div> LLMs, human experts, patient care, safeguard, safe delivery <br />
<br />
Summary: The research discusses the comparison between Large Language Models (LLMs) and human experts in the context of patient care. It highlights the potential harm LLMs could pose to established systems of safe patient care delivery. The article emphasizes the importance of characterizing the safe use of LLMs in clinical settings amidst rapid model developments. Rather than pitting LLMs against humans, the focus should shift towards developing strategies for effective collaboration between humans and LLMs. The research calls for a symbiotic relationship where humans and LLMs can work together efficiently to enhance patient care outcomes. <div>
arXiv:2505.08902v1 Announce Type: cross 
Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term "expert" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under "humans versus LLMs" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora</title>
<link>https://arxiv.org/abs/2505.08905</link>
<guid>https://arxiv.org/abs/2505.08905</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Benchmark Construction, Synthetic Data, Document Populations, Model Evaluation

Summary: 
Language Models (LMs) are continuously improving in quality and coherence, raising the need for effective evaluation benchmarks. However, the manual construction of benchmarks is becoming challenging due to the vast amount of data encountered by LMs during training. To address this, a methodology for automating the creation of fact-based synthetic data model evaluations grounded in document populations is proposed. This approach utilizes LMs to evaluate domain-specific knowledge automatically, using only grounding documents as input. The methodology shows a high correlation with human-curated questions and provides diagnostic insight into LM capabilities through multiple choice and open-ended questions. Applied to a relevant arXiv preprint, the evaluation reveals strong performance from Gemma3 models. This automated benchmarking tool offers a promising solution for assessing LM performance efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2505.08905v1 Announce Type: cross 
Abstract: Language Models (LMs) continue to advance, improving response quality and coherence. Given Internet-scale training datasets, LMs have likely encountered much of what users might ask them to generate in some form during their training. A plethora of evaluation benchmarks have been constructed to assess model quality, response appropriateness, and reasoning capabilities. However, the human effort required for benchmark construction is limited and being rapidly outpaced by the size and scope of the models under evaluation. Additionally, having humans build a benchmark for every possible domain of interest is impractical. Therefore, we propose a methodology for automating the construction of fact-based synthetic data model evaluations grounded in document populations. This work leverages those very same LMs to evaluate domain-specific knowledge automatically, using only grounding documents (e.g., a textbook) as input. This synthetic data benchmarking approach corresponds well with human curated questions with a Spearman ranking correlation of 0.96 and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel tool supports generating both multiple choice and open-ended synthetic data questions to gain diagnostic insight of LM capability. We apply this methodology to evaluate model performance on a recent relevant arXiv preprint, discovering a surprisingly strong performance from Gemma3 models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind Maya: Building a Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2505.08910</link>
<guid>https://arxiv.org/abs/2505.08910</guid>
<content:encoded><![CDATA[
<div> keywords: Vision-Language Models, Multilingual, Image-Text, Low-resource languages, Cultural contexts

Summary:<br /><br />
In response to the limitations of current large Vision-Language Models (VLMs) in performing well on low-resource languages and diverse cultural contexts, a new Multilingual VLM called Maya has been developed. This open-source model is based on a multilingual image-text pretraining dataset in eight languages, derived from the LLaVA pretraining dataset. Maya aims to enhance cultural and linguistic understanding in vision-language tasks across a range of languages. The model provides support for multiple languages, making it more inclusive and globally applicable. The code for Maya is available on GitHub for easy access and further development in the research community. Maya represents a step forward in promoting diversity and inclusivity in the field of Vision-Language Models, enabling improved performance across different linguistic and cultural backgrounds. <div>
arXiv:2505.08910v1 Announce Type: cross 
Abstract: In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers</title>
<link>https://arxiv.org/abs/2505.08941</link>
<guid>https://arxiv.org/abs/2505.08941</guid>
<content:encoded><![CDATA[
<div> Keywords: academic papers, citation rate prediction, causal language models, transformers, scaling-law analysis

Summary:
ForeCite is a framework designed to predict the future citation rates of academic papers using pre-trained causal language models with a linear head. It achieves a high test correlation of 0.826 on a dataset of over 900,000 biomedical papers published between 2000 and 2024, surpassing the previous state-of-the-art by 27 points. The model shows consistent improvements across different sizes and volumes of data, demonstrating its scalability and reliability. Gradient-based saliency heatmaps indicate a potential over-reliance on titles and abstract texts in predicting citations. These findings establish ForeCite as a new benchmark for forecasting the impact of academic research, paving the way for automated and accurate evaluation of scientific contributions. 

<br /><br />Summary: <div>
arXiv:2505.08941v1 Announce Type: cross 
Abstract: Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress. We present $\textbf{ForeCite}$, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction. Adapting transformers for regression tasks, ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art. Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness. Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts. These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training</title>
<link>https://arxiv.org/abs/2505.08971</link>
<guid>https://arxiv.org/abs/2505.08971</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, pre-training, next-token prediction, importance sampling, scaling properties

Summary:
PRIOR is a novel vision-language pre-training approach that prioritizes image-related tokens in large vision-language models (LVLMs) by introducing a text-only reference language model (LLM) to weight each token based on its probability for LVLMs training. By adjusting the loss through token-specific re-weighting based on importance scores, PRIOR outperforms traditional next-token prediction (NTP) in both LVLMs with visual encoders and without, showing average relative improvements of 19% and 8% on various benchmarks. Additionally, PRIOR demonstrates superior scaling properties, with significantly higher scaling coefficients that suggest greater potential for performance gains compared to NTP with increased computational resources and data. <div>
arXiv:2505.08971v1 Announce Type: cross 
Abstract: In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Meta Prompt Engineering for Alignment with the Theory of Mind</title>
<link>https://arxiv.org/abs/2505.09024</link>
<guid>https://arxiv.org/abs/2505.09024</guid>
<content:encoded><![CDATA[
<div> Keywords: meta-prompting, Large Language Model, agentic reinforcement learning, Theory of Mind, content quality<br />
Summary: <br />
The article introduces a method of meta-prompting to improve text generation for complex tasks by optimizing the similarity between human mental expectations and a Large Language Model's neural processing. Using agentic reinforcement learning, a Judge LLM (LLMaaJ) teaches another LLM to produce content by interpreting desired text traits. Human edits on AI-generated articles were used to measure mental beliefs around content production at the US Open 2024. The LLMaaJ successfully aligned with human content reviewers 53.8% of the time, with an average iteration count of 4.38. Through a geometric interpretation of content traits over a Hilbert vector space, the LLMaaJ optimized Theory of Mind alignment by incorporating human edits into text generation. This approach improved content quality and expanded coverage of tennis action at the US Open 2024, leading to its application in other live events in sports and entertainment. <br /><br /> <div>
arXiv:2505.09024v1 Announce Type: cross 
Abstract: We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification</title>
<link>https://arxiv.org/abs/2505.09031</link>
<guid>https://arxiv.org/abs/2505.09031</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, chain-of-thought prompting, self-consistency, self-verification, hallucinations<br />
Summary:<br />
Hallucination is a significant issue in large language models (LLMs) as they often generate incorrect information. This study explores the combination of chain-of-thought (CoT) prompting with retrieval-augmented generation (RAG) and the use of self-consistency and self-verification strategies to address this problem. By integrating external knowledge sources and enabling models to verify or correct their outputs, the goal is to enhance the accuracy and coherence of responses. Comparative evaluations were conducted to assess the effectiveness of baseline LLMs, CoT, CoT+RAG, self-consistency, and self-verification techniques. The results demonstrate the efficacy of each method in reducing hallucinations while maintaining fluency and reasoning depth.Overall, the combined approach of CoT+RAG with self-consistency and self-verification techniques proves to be the most robust in minimizing hallucinations and improving factual accuracy. <br /><br />Summary: <div>
arXiv:2505.09031v1 Announce Type: cross 
Abstract: Hallucination, where large language models (LLMs) generate confident but incorrect or irrelevant information, remains a key limitation in their application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has emerged as a promising method for improving multistep reasoning by guiding models through intermediate steps. However, CoT alone does not fully address the hallucination problem. In this work, we investigate how combining CoT with retrieval-augmented generation (RAG), as well as applying self-consistency and self-verification strategies, can reduce hallucinations and improve factual accuracy. By incorporating external knowledge sources during reasoning and enabling models to verify or revise their own outputs, we aim to generate more accurate and coherent responses. We present a comparative evaluation of baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification techniques. Our results highlight the effectiveness of each method and identify the most robust approach for minimizing hallucinations while preserving fluency and reasoning depth.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications</title>
<link>https://arxiv.org/abs/2505.09083</link>
<guid>https://arxiv.org/abs/2505.09083</guid>
<content:encoded><![CDATA[
<div> taxonomy-guided reasoning, weakly-supervised textual classification, transparency, explainability, hawkishness<br />
<br />
Summary:<br />
The article introduces Ornithologist, a weakly-supervised textual classification system that measures the hawkishness and dovishness of central bank text. It utilizes "taxonomy-guided reasoning" to guide a large language model with decision trees created by humans, increasing transparency and explainability while reducing the risk of hallucination. This approach requires less supervision than traditional systems, making it adaptable to various text sources like news. Ornithologist's measurements of RBA communication reveal insights into future cash rate paths and market expectations. <div>
arXiv:2505.09083v1 Announce Type: cross 
Abstract: I develop Ornithologist, a weakly-supervised textual classification system and measure the hawkishness and dovishness of central bank text. Ornithologist uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees. This increases the transparency and explainability of the system and makes it accessible to non-experts. It also reduces hallucination risk. Since it requires less supervision than traditional classification systems, it can more easily be applied to other problems or sources of text (e.g. news) without much modification. Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases</title>
<link>https://arxiv.org/abs/2505.09246</link>
<guid>https://arxiv.org/abs/2505.09246</guid>
<content:encoded><![CDATA[
<div> framework, multi-hop question answering, structured knowledge, unstructured content, Semi-Structured Knowledge Bases  

Summary:  
FocusedRetriever is a modular framework for multi-hop question answering that leverages Semi-Structured Knowledge Bases (SKBs) to bridge the gap between structured knowledge and unstructured content. It outperforms state-of-the-art methods on diverse test sets, with an average first-hit rate 25.7% higher than the second-best method. The framework integrates components for entity search, query generation, pairwise re-ranking, and vector similarity search to retrieve and rank relevant information. Utilizing Large Language Models (LLMs), FocusedRetriever extracts relational facts and entity attributes from unstructured text, filters answer candidates based on extracted triplets, and uses contextual capabilities of LLMs for final ranking. The source code is publicly available for further upgrades, including finetuning with advanced LLMs. <div>
arXiv:2505.09246v1 Announce Type: cross 
Abstract: In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. However, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data, thereby enabling new strategies for knowledge access and use. In this work, we present FocusedRetriever, a modular SKB-based framework for multi-hop question answering. It integrates components (VSS-based entity search, LLM-based generation of Cypher queries and pairwise re-ranking) in a way that enables it to outperform state-of-the-art methods across all three STaRK benchmark test sets, covering diverse domains and multiple performance metrics. The average first-hit rate exceeds that of the second-best method by 25.7%. FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to extract relational facts and entity attributes from unstructured text, (2) node set joins to filter answer candidates based on these extracted triplets and constraints, (3) vector similarity search to retrieve and rank relevant unstructured content, and (4) the contextual capabilities of LLMs to finally rank the top-k answers. For generality, we only incorporate base LLMs in FocusedRetriever in our evaluation. However, our analysis of intermediate results highlights several opportunities for further upgrades including finetuning. The source code is publicly available at https://github.com/kramerlab/FocusedRetriever .
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios</title>
<link>https://arxiv.org/abs/2505.09436</link>
<guid>https://arxiv.org/abs/2505.09436</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, customer experience management, contact center operations, synthetic dataset

Summary:
The article introduces CXMArena, a new synthetic benchmark dataset designed to assess the practical utility of Large Language Models (LLMs) in Customer Experience Management (CXM) contexts. The dataset includes knowledge base integration, real-world noise, and operational tasks beyond conversational fluency. The CXMArena benchmarks cover tasks such as Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Baseline experiments show that even state-of-the-art models struggle with tasks like article search and knowledge base refinement, indicating the need for more advanced solutions over conventional techniques. The dataset aims to bridge the gap in evaluating AI in complex operational CXM environments by providing a realistic and challenging benchmark for LLMs. <div>
arXiv:2505.09436v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors</title>
<link>https://arxiv.org/abs/2505.09610</link>
<guid>https://arxiv.org/abs/2505.09610</guid>
<content:encoded><![CDATA[
<div> Verilog, VHDL, Large Language Models, hardware design, processor design <br />
Summary: The paper discusses the use of Large Language Models (LLMs) in hardware design, particularly focusing on their application in explaining VHDL code for high-performance processor design. The study describes the development of test sets tailored to specific needs and the evaluation of models through extended pretraining (EPT). The expert evaluation of code explanations improved from 43% to 69% with the EPT model. An LLM-as-a-judge was also developed to assess models similarly to expert evaluators. Experimentation led to the creation and evaluation of new models, including an instruction-tuned version of the EPT model expected to achieve a 71% expert evaluator rating. The potential use of newer base models could push this rating to 85% and beyond. The paper concludes by discussing ways to enhance hardware design LLMs using advancements in Generative AI. <br />Summary: ArXiv:2505.09610v1 presents the development of Large Language Models focused on explaining VHDL code in high-performance processor design, highlighting improvements in expert evaluation ratings, the creation of an LLM-as-a-judge, and opportunities to enhance LLM quality. <div>
arXiv:2505.09610v1 Announce Type: cross 
Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in recent years, principally through its incorporation in tools that increase chip designer productivity. There has been considerable discussion about the use of LLMs in RTL specifications of chip designs, for which the two most popular languages are Verilog and VHDL. LLMs and their use in Verilog design has received significant attention due to the higher popularity of the language, but little attention so far has been given to VHDL despite its continued popularity in the industry. There has also been little discussion about the unique needs of organizations that engage in high-performance processor design, and techniques to deploy AI solutions in these settings. In this paper, we describe our journey in developing a Large Language Model (LLM) specifically for the purpose of explaining VHDL code, a task that has particular importance in an organization with decades of experience and assets in high-performance processor design. We show how we developed test sets specific to our needs and used them for evaluating models as we performed extended pretraining (EPT) of a base LLM. Expert evaluation of the code explanations produced by the EPT model increased to 69% compared to a base model rating of 43%. We further show how we developed an LLM-as-a-judge to gauge models similar to expert evaluators. This led us to deriving and evaluating a host of new models, including an instruction-tuned version of the EPT model with an expected expert evaluator rating of 71%. Our experiments also indicate that with the potential use of newer base models, this rating can be pushed to 85% and beyond. We conclude with a discussion on further improving the quality of hardware design LLMs using exciting new developments in the Generative AI world.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?</title>
<link>https://arxiv.org/abs/2505.09614</link>
<guid>https://arxiv.org/abs/2505.09614</guid>
<content:encoded><![CDATA[
<div> Keywords: Language model, exploration, causal relationships, biases, reasoning

Summary: 
Language models (LMS) are used as autonomous decision-makers, but it is unclear if they can efficiently explore and infer causal relationships. In a study using the "Blicket Test" paradigm, LMs reliably infer common disjunctive relationships but struggle with less intuitive conjunctive ones. This "disjunctive bias" persists across LMs of different sizes and prompting strategies and worsens with task complexity. LMs exhibit adult-like inference profiles similar to humans. A test-time sampling method reduces the bias and improves LM reasoning towards scientific, causally rigorous thinking. <div>
arXiv:2505.09614v1 Announce Type: cross 
Abstract: Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established "Blicket Test" paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This "disjunctive bias" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based NLG Evaluation: Current Status and Challenges</title>
<link>https://arxiv.org/abs/2402.01383</link>
<guid>https://arxiv.org/abs/2402.01383</guid>
<content:encoded><![CDATA[
<div> NLG evaluation, natural language processing, large language models, automatic evaluation, ChatGPT <br />
Summary: 
Evaluating natural language generation (NLG) has been a challenging task, with traditional metrics lacking in capturing the essence of content overlap. Recently, large language models (LLMs) like ChatGPT have shown promise in NLG evaluation. Various methods based on LLMs have been developed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. This survey provides a taxonomy of LLM-based NLG evaluation methods, discussing their advantages and disadvantages. The article also highlights open issues in this field and suggests future research directions. <div>
arXiv:2402.01383v3 Announce Type: replace 
Abstract: Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model</title>
<link>https://arxiv.org/abs/2404.03080</link>
<guid>https://arxiv.org/abs/2404.03080</guid>
<content:encoded><![CDATA[
<div> Keywords: materials science, artificial intelligence, natural language processing, knowledge graph, data integration

Summary: 
The article introduces the Materials Knowledge Graph (MKG) which combines artificial intelligence with materials science to accelerate the discovery process. MKG utilizes natural language processing techniques to extract high-quality research data and categorizes it into structured triples with comprehensive labels such as Name, Formula, and Application. With 162,605 nodes and 731,772 edges, MKG enhances data usability and integration through a meticulously designed ontology. By leveraging network-based algorithms, MKG enables efficient link prediction and reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also sets the foundation for more advanced science knowledge graphs. 

Summary: <div>
arXiv:2404.03080v4 Announce Type: replace 
Abstract: Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering</title>
<link>https://arxiv.org/abs/2410.04526</link>
<guid>https://arxiv.org/abs/2410.04526</guid>
<content:encoded><![CDATA[
<div> benchmark, financial, multilingual, multimodal, question answering <br />
<br />
Summary: 
The paper introduces FAMMA, an open-source benchmark for evaluating large language models (LLMs) in financial question answering tasks. FAMMA consists of two versions: FAMMA-Basic with 1,945 questions from textbooks and exams, and FAMMA-LivePro with 103 novel questions created by experts. The questions cover advanced finance topics in multiple languages and include non-text data like charts and tables. Experiments show that FAMMA is challenging for LLMs, including reasoning models like GPT-01. The paper also curated reasoning trajectories of DeepSeek-R1 and fine-tuned Qwen models, leading to performance improvements on FAMMA-LivePro. The benchmark, data, code, and trained models are available on the FAMMA website. <div>
arXiv:2410.04526v3 Announce Type: replace 
Abstract: In this paper, we introduce FAMMA, an open-source benchmark for \underline{f}in\underline{a}ncial \underline{m}ultilingual \underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims to evaluate the abilities of large language models (LLMs) in answering complex reasoning questions that require advanced financial knowledge. The benchmark has two versions: FAMMA-Basic consists of 1,945 questions extracted from university textbooks and exams, along with human-annotated answers and rationales; FAMMA-LivePro consists of 103 novel questions created by human domain experts, with answers and rationales held out from the public for a contamination-free evaluation. These questions cover advanced knowledge of 8 major subfields in finance (e.g., corporate finance, derivatives, and portfolio management). Some are in Chinese or French, while a majority of them are in English. Each question has some non-text data such as charts, diagrams, or tables. Our experiments reveal that FAMMA poses a significant challenge on LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally, we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data, and fine-tuned a series of open-source Qwen models using this reasoning data. We found that training a model on these reasoning trajectories can significantly improve its performance on FAMMA-LivePro. We released our leaderboard, data, code, and trained models at https://famma-bench.github.io/famma/.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2411.09116</link>
<guid>https://arxiv.org/abs/2411.09116</guid>
<content:encoded><![CDATA[
<div> Multilingual, Large language models, Benchmark, Performance, Knowledge transfer<br />
<br />
Summary: 
The article introduces a comprehensive multilingual multitask benchmark called P-MMEval, which covers fundamental and capability-specialized datasets. It aims to address the limitations of previous assessments and offers consistent language coverage across various datasets with parallel samples. The benchmark is designed to evaluate performances of multilingual models across tasks and explore factors such as model sizes, languages, and prompts. Extensive experiments are conducted on representative multilingual model series to compare performances and assess knowledge transfer from English to other languages. The insights gained from these experiments are expected to provide valuable guidance for future research in the field of large language models. The dataset for P-MMEval is available at the provided link. 

Summary: <div>
arXiv:2411.09116v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) showcase varied multilingual capabilities across tasks like translation, code generation, and reasoning. Previous assessments often limited their scope to fundamental natural language processing (NLP) or isolated capability-specific tasks. To alleviate this drawback, we aim to present a comprehensive multilingual multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark covering effective fundamental and capability-specialized datasets. Furthermore, P-MMEval delivers consistent language coverage across various datasets and provides parallel samples. Finally, we conduct extensive experiments on representative multilingual model series to compare performances across models and tasks, explore the relationship between multilingual performances and factors such as tasks, model sizes, languages, and prompts, and examine the effectiveness of knowledge transfer from English to other languages. The resulting insights are intended to offer valuable guidance for future research. The dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples</title>
<link>https://arxiv.org/abs/2502.09650</link>
<guid>https://arxiv.org/abs/2502.09650</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, alignment, difficulty, data selection, Selective DPO

Summary:<br /><br />
This study challenges the common assumption that using more clean data always leads to better outcomes in aligning large language models (LLMs). The authors propose a new principle that preference data vary in difficulty, with overly difficult examples hindering alignment by exceeding the model's capacity. Through systematic experimentation, they find that preference examples indeed vary in difficulty, affecting model performance. Overly difficult examples significantly degrade LLM performance across different models and datasets. The capacity of a model plays a crucial role in handling difficult examples, highlighting the importance of aligning data difficulty with model capacity. Introducing Selective DPO, a method that filters out overly difficult examples, improves alignment performance by 9-16% in win rates on a benchmark. The study emphasizes the need to consider data difficulty and model capacity in alignment strategies for LLMs. <div>
arXiv:2502.09650v2 Announce Type: replace 
Abstract: The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropNet: a White-Box and Human-Like Network for Sentence Representation</title>
<link>https://arxiv.org/abs/2502.10725</link>
<guid>https://arxiv.org/abs/2502.10725</guid>
<content:encoded><![CDATA[
<div> transformer-based embedding methods, sentence representation, interpretability, PropNet, cognitive science

Summary: 
The article introduces PropNet, a white-box, human-like sentence representation network designed for interpretability in NLP tasks. While transformer-based embedding models have shown impressive performance, their black-box nature raises concerns about bias and trust. PropNet addresses these issues by constructing a hierarchical network based on propositions in a sentence, inspired by cognitive science principles. Despite lagging behind state-of-the-art models in semantic textual similarity tasks, PropNet offers insights into human cognitive processes in understanding language. The network's interpretability allows for in-depth analysis of sentence representations, shedding light on the limitations and potential improvements in current NLP models. Further research and refinement of PropNet could lead to a more transparent and human-centered approach to natural language understanding. <div>
arXiv:2502.10725v3 Announce Type: replace 
Abstract: Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference</title>
<link>https://arxiv.org/abs/2503.10652</link>
<guid>https://arxiv.org/abs/2503.10652</guid>
<content:encoded><![CDATA[
<div> consumer preferences, policy decisions, stated preference surveys, large language models, energy choices<br />
<br />
Summary: 
Survey research is essential for understanding consumer preferences and informing policy decisions. Stated preference (SP) surveys help in capturing individual trade-offs in hypothetical scenarios. Traditional SP survey methods are costly and time-consuming, leading researchers to explore the use of large language models (LLMs) for simulating consumer choices. This study evaluates the performance of various LLMs in energy-related SP surveys, considering factors like prompt design, reasoning capabilities, model types, and integration with traditional choice models. While LLMs show promising accuracy levels, they are not yet practical for simulation use. DeepSeek-R1 performs the best among the LLMs tested, indicating potential for data analysis. Pre-trained LLMs offer scalability and require minimal historical data, but further research is needed to improve prompt design and explore reasoning capabilities for better simulation results. <div>
arXiv:2503.10652v2 Announce Type: replace 
Abstract: Survey research plays a crucial role in studies by capturing consumer preferences and informing policy decisions. Stated preference (SP) surveys help researchers understand how individuals make trade-offs in hypothetical, potentially futuristic, scenarios. However, traditional methods are costly, time-consuming, and affected by respondent fatigue and ethical constraints. Large language models (LLMs) have shown remarkable capabilities in generating human-like responses, prompting interest in their use in survey research. This study investigates LLMs for simulating consumer choices in energy-related SP surveys and explores their integration into data collection and analysis workflows. Test scenarios were designed to assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and aggregated levels, considering prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, model types, integration with traditional choice models, and potential biases. While LLMs achieve accuracy above random guessing, performance remains insufficient for practical simulation use. Cloud-based LLMs do not consistently outperform smaller local models. DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Previous SP choices are the most effective input; longer prompts with more factors reduce accuracy. Mixed logit models can support LLM prompt refinement. Reasoning LLMs show potential in data analysis by indicating factor significance, offering a qualitative complement to statistical models. Despite limitations, pre-trained LLMs offer scalability and require minimal historical data. Future work should refine prompts, further explore CoT reasoning, and investigate fine-tuning techniques.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark</title>
<link>https://arxiv.org/abs/2503.17599</link>
<guid>https://arxiv.org/abs/2503.17599</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, General Practice, Evaluation Framework, Benchmark, State-of-the-Art

Summary:
The article discusses the challenges of assessing the ability of Large Language Models (LLMs) to function effectively as general practitioners (GPs) in real-world clinical settings. Existing evaluation frameworks are limited in capturing the comprehensive skills required in routine clinical practice. To address this gap, a novel evaluation framework called GPBench is proposed, consisting of data annotated by domain experts based on clinical practice standards. The study evaluates ten state-of-the-art LLMs and finds that they are not yet ready for autonomous deployment in GP roles due to the need for further optimization tailored to the specific responsibilities of GPs. The results highlight the importance of refining LLMs to meet the demands of daily clinical practice in general medicine. 

<br /><br />Summary: <div>
arXiv:2503.17599v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated considerable potential in general practice. However, existing benchmarks and evaluation frameworks primarily depend on exam-style or simplified question-answer formats, lacking a competency-based structure aligned with the real-world clinical responsibilities encountered in general practice. Consequently, the extent to which LLMs can reliably fulfill the duties of general practitioners (GPs) remains uncertain. In this work, we propose a novel evaluation framework to assess the capability of LLMs to function as GPs. Based on this framework, we introduce a general practice benchmark (GPBench), whose data are meticulously annotated by domain experts in accordance with routine clinical practice standards. We evaluate ten state-of-the-art LLMs and analyze their competencies. Our findings indicate that current LLMs are not yet ready for deployment in such settings without human oversight, and further optimization specifically tailored to the daily responsibilities of GPs is essential.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
<div> Embodied Reasoner, interactive search tasks, spatial understanding, temporal reasoning, self-reflection, observation-thought-action trajectories <br />
Summary: <br />
Recent advancements in deep learning have shown impressive capabilities in math and coding tasks, but their potential in embodied domains requiring interaction with environments is largely unexplored. The Embodied Reasoner model addresses this by incorporating spatial understanding, temporal reasoning, and self-reflection into its problem-solving approach. By synthesizing a large dataset of interactive images and thinking processes, the model undergoes a three-stage training process to improve its abilities through imitation learning, self-exploration, and reflection tuning. Evaluation results demonstrate superior performance compared to other visual reasoning models, specifically outperforming OpenAI O1, O3-mini, and Claude-3.7. The model exhibits fewer repeated searches and logical inconsistencies, particularly excelling in complex long-horizon tasks and real-world environments. <div>
arXiv:2503.21696v2 Announce Type: replace 
Abstract: Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is analogy enough to draw novel adjective-noun inferences?</title>
<link>https://arxiv.org/abs/2503.24293</link>
<guid>https://arxiv.org/abs/2503.24293</guid>
<content:encoded><![CDATA[
<div> generalization, novel, adjective-noun, composition, inference

Summary: 
- Recent research has highlighted the ability of humans and large language models (LLMs) to generalize to novel adjective-noun combinations, suggesting access to a compositional mechanism for deriving inferences.
- The study explores whether inferences can be derived through analogy to known inferences instead of composition.
- A model of analogical reasoning based on similarity over lexical items was built, and human participants were asked to reason by analogy.
- While the analogy strategy worked well for a significant portion of the dataset, there were novel combinations where both humans and LLMs derived convergent inferences not well-suited for analogy.
- The findings suggest that the mechanism used by humans and LLMs to generalize in these instances cannot be fully explained by analogy alone and likely involves composition. 

<br /><br />Summary: <div>
arXiv:2503.24293v2 Announce Type: replace 
Abstract: Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks</title>
<link>https://arxiv.org/abs/2411.03343</link>
<guid>https://arxiv.org/abs/2411.03343</guid>
<content:encoded><![CDATA[
<div> jailbreaks, large language models, probes, non-linear features, model behavior <br />
Summary: <br />
The study focuses on understanding jailbreak mechanisms in large language models (LLMs) by analyzing both linear and non-linear features in prompt attempts. A dataset of 10,800 jailbreak attempts across 35 methods is introduced. Probes are trained to classify successful jailbreaks using latent representations of prompt tokens, showing that different strategies exploit varied non-linear features. While linear probes can predict jailbreak success accurately, their performance does not generalize to new attack methods, indicating the need for a nuanced understanding. Non-linear probes are effective in steering model behavior, guiding targeted perturbations in the latent space to enhance the model's robustness against jailbreaks. This challenges the idea that jailbreaks can be fully grasped through linear or universal prompt features alone, emphasizing the importance of comprehending LLM vulnerabilities in depth. <br /> <div>
arXiv:2411.03343v2 Announce Type: replace-cross 
Abstract: Jailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train probes to classify successful from unsuccessful jailbreaks using the latent representations corresponding to prompt tokens. Notably, we find that even when probes achieve high accuracy in predicting the success of jailbreaks, their performance often fails to generalize to unseen attack methods. This reveals that different jailbreaking strategies exploit different non-linear, non-universal features. Next, we demonstrate that non-linear probes provide a powerful tool for steering model behavior. Specifically, we use these probes to guide targeted latent space perturbations, enabling us to effectively modulate the model's robustness against jailbreaks. Overall, our findings challenge the assumption that jailbreaks can be fully understood through linear or simple universal prompt features alone, highlighting the importance of a nuanced understanding of the mechanisms behind LLM vulnerabilities.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAS: Fast ANN-SNN Conversion for Spiking Large Language Models</title>
<link>https://arxiv.org/abs/2502.04405</link>
<guid>https://arxiv.org/abs/2502.04405</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Large Language Models, Fast ANN-SNN conversion, fine-tuning, calibration, reduced inference latency

Summary:
The study introduces a novel Fast ANN-SNN conversion strategy (FAS) for transforming Language Models into Spiking Large Language Models (LLMs) in two stages. The first stage involves fine-tuning pre-trained models without the need for direct training from scratch. The second stage implements a calibration method to minimize conversion errors and enhance accuracy. Experimental results demonstrate that FAS achieves state-of-the-art performance across various language and vision-language tasks while significantly reducing inference latency and computational costs. Notably, FAS surpasses the OPT-7B model's accuracy by 3% using only eight timesteps and reducing energy consumption by 96.63%. The source code for FAS is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2502.04405v2 Announce Type: replace-cross 
Abstract: Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. Notably, FAS only takes eight timesteps to achieve an accuracy of 3\% higher than that of the OPT-7B model, while reducing energy consumption by 96.63\%. The source code is available at https://github.com/lc783/FAS
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InductionBench: LLMs Fail in the Simplest Complexity Class</title>
<link>https://arxiv.org/abs/2502.15823</link>
<guid>https://arxiv.org/abs/2502.15823</guid>
<content:encoded><![CDATA[
<div> InductionBench, benchmark, inductive reasoning, Large language models, deficiencies <br />
Summary: <br />
The study introduces InductionBench, a benchmark to evaluate the inductive reasoning abilities of Large Language Models (LLMs). LLMs, such as o1 and o3, have excelled at deductive reasoning tasks, but struggle with inductive reasoning, crucial for scientific discovery. The new benchmark assesses LLMs' capacity to infer underlying rules from data. Experimental results show that even advanced models struggle with simple complexity classes within the subregular hierarchy of functions, revealing a deficiency in their inductive reasoning capabilities. The challenges LLMs face in mastering inductive reasoning indicate a need for further research and development in this area. The code and data for the benchmark are available on GitHub for further analysis and improvement. <br /> <div>
arXiv:2502.15823v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analytical Emotion Framework of Rumour Threads on Social Media</title>
<link>https://arxiv.org/abs/2502.16560</link>
<guid>https://arxiv.org/abs/2502.16560</guid>
<content:encoded><![CDATA[
<div> Keywords: online social media, rumours, emotions, negativity, positivity

Summary:
Rumours in online social media can have significant impacts on society, highlighting the need for a deeper understanding of their development. This study focuses on the interplay between emotions and rumours in threaded discussions, exploring a comprehensive analytical framework for detecting emotions in both rumour and non-rumour threads. The analysis reveals that rumours tend to elicit more negative emotions like anger, fear, and pessimism, while non-rumours evoke more positive emotions. Emotions in online threads are found to be contagious, with rumours spreading negativity and non-rumours spreading positivity. Causal analysis indicates that surprise acts as a bridge between rumours and other emotions, while pessimism stems from sadness and fear, and optimism from joy and love. By examining the dynamics of emotions in online social media, this study offers insights into the emotional aspects of rumour propagation and sheds light on the comparative differences between rumours and non-rumours. 

<br /><br />Summary: <div>
arXiv:2502.16560v2 Announce Type: replace-cross 
Abstract: Rumours in online social media pose significant risks to modern society, motivating the need for better understanding of how they develop. We focus specifically on the interface between emotion and rumours in threaded discourses, building on the surprisingly sparse literature on the topic which has largely focused on single aspect of emotions within the original rumour posts themselves, and largely overlooked the comparative differences between rumours and non-rumours. In this work, we take one step further to provide a comprehensive analytical emotion framework with multi-aspect emotion detection, contrasting rumour and non-rumour threads and provide both correlation and causal analysis of emotions. We applied our framework on existing widely-used rumour datasets to further understand the emotion dynamics in online social media threads. Our framework reveals that rumours trigger more negative emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive ones. Emotions are contagious, rumours spread negativity, non-rumours spread positivity. Causal analysis shows surprise bridges rumours and other emotions; pessimism comes from sadness and fear, while optimism arises from joy and love.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering</title>
<link>https://arxiv.org/abs/2503.11197</link>
<guid>https://arxiv.org/abs/2503.11197</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, audio understanding, reasoning, audio question answering, large language models<br />
<br />
Summary: <br />
1) The study explores reinforcement learning in audio understanding and reasoning, specifically focusing on audio question answering tasks. The group relative policy optimization (GRPO) algorithm is applied to Qwen2-Audio-7B-Instruct, achieving state-of-the-art performance on the MMAU Test-mini benchmark with 64.5% accuracy.
2) Despite having only 8.2B parameters and 38k post-training samples, RL surpasses supervised fine-tuning (SFT), showcasing its effectiveness with limited data.
3) Explicit reasoning does not significantly improve AQA tasks, highlighting the need for more efficient utilization of deep thinking in future research.
4) Large audio language models (LALMs) like LALMs still fall short of human auditory-language reasoning abilities, emphasizing the need for further exploration and improvement in RL-based approaches. The project resources are available on GitHub and Hugging Face platforms for further analysis and research. <br /> <div>
arXiv:2503.11197v4 Announce Type: replace-cross 
Abstract: Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks. However, the audio modality has largely been overlooked in these developments. Thus, we conduct a series of RL explorations in audio understanding and reasoning, specifically focusing on the audio question answering (AQA) task. We leverage the group relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and our experiments demonstrated state-of-the-art performance on the MMAU Test-mini benchmark, achieving an accuracy rate of 64.5%. The main findings in this technical report are as follows: 1) The GRPO algorithm can be effectively applied to large audio language models (LALMs), even when the model has only 8.2B parameters; 2) With only 38k post-training samples, RL significantly outperforms supervised fine-tuning (SFT), indicating that RL-based approaches can be effective without large datasets; 3) The explicit reasoning process has not shown significant benefits for AQA tasks, and how to efficiently utilize deep thinking remains an open question for further research; 4) LALMs still lag far behind humans auditory-language reasoning, suggesting that the RL-based approaches warrant further exploration. Our project is available at https://github.com/xiaomi-research/r1-aqa and https://huggingface.co/mispeech/r1-aqa.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces</title>
<link>https://arxiv.org/abs/2505.07831</link>
<guid>https://arxiv.org/abs/2505.07831</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic neurons, artificial intelligence, language models, categorical vector space, intra-neuronal attention

Summary: 
The article introduces a new perspective on understanding synthetic neurons in artificial intelligence language models. Instead of considering them as a superposition of distributed features within a latent space, the authors propose a geometric approach. They define a neuron in a specific layer as a categorical vector space with a non-orthogonal basis. This basis is composed of categorical sub-dimensions extracted from neurons in the preceding layer. The categorical vector space is structured by the activation space of each neuron, allowing for an intra-neuronal attention process. This process identifies a critical categorical zone within the neuron that is essential for the model's efficiency. This zone is more homogeneous and located at the intersection of different categorical sub-dimensions. This novel approach provides insights into how synthetic neurons operate within language models, emphasizing the importance of categorical spaces for efficient model performance. 

<br /><br />Summary: <div>
arXiv:2505.07831v1 Announce Type: new 
Abstract: The polysemantic nature of synthetic neurons in artificial intelligence language models is currently understood as the result of a necessary superposition of distributed features within the latent space. We propose an alternative approach, geometrically defining a neuron in layer n as a categorical vector space with a non-orthogonal basis, composed of categorical sub-dimensions extracted from preceding neurons in layer n-1. This categorical vector space is structured by the activation space of each neuron and enables, via an intra-neuronal attention process, the identification and utilization of a critical categorical zone for the efficiency of the language model - more homogeneous and located at the intersection of these different categorical sub-dimensions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas</title>
<link>https://arxiv.org/abs/2505.07850</link>
<guid>https://arxiv.org/abs/2505.07850</guid>
<content:encoded><![CDATA[
<div> Algorithmic othering, racial identity, synthetic personas, LLMs, representational harm<br />
<br />
Summary: 
The article examines how large language models (LLMs) generate synthetic personas and evaluate their representation of racial identities. Through analysis of personas created by three LLMs and comparing them to human-authored responses, the study found that LLMs tend to emphasize racial markers, utilize culturally coded language excessively, and create personas that are narratively simplistic despite syntactic complexity. This leads to harmful outcomes such as stereotyping, exoticism, erasure, and bias. The phenomenon is termed algorithmic othering, where minoritized identities are made overly visible but lacking in authenticity. The article suggests design recommendations for evaluation metrics and validation protocols to address these issues in the generation of synthetic identities. <div>
arXiv:2505.07850v1 Announce Type: new 
Abstract: As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment</title>
<link>https://arxiv.org/abs/2505.07852</link>
<guid>https://arxiv.org/abs/2505.07852</guid>
<content:encoded><![CDATA[
<div> ensemble classification model, concept drift analysis, One Class Drift Detector (OCDD), large language model (LLM), social engineering chat scenarios

Summary: 
The paper addresses the challenge of detecting fake interactions in digital communication platforms, which can range from spam to sophisticated scams. Traditional methods often fail to adapt to dynamic conversational shifts, leading to false alarms or missed threats. The proposed framework consists of a two-stage process: first identifying suspicious conversations using an ensemble classification model, then analyzing concept drift within flagged dialogues using an OCDD. When drift is detected, a LLM assesses the shift to determine fraudulent intent or legitimate topic change. The framework is validated using a dataset of social engineering chat scenarios, demonstrating improved accuracy and interpretability for real-time fraud detection. A modular approach outperforms a Dual LLM baseline, highlighting the benefits of incorporating concept drift analysis into fraud detection systems.<br /><br />Summary: <div>
arXiv:2505.07852v1 Announce Type: new 
Abstract: Detecting fake interactions in digital communication platforms remains a challenging and insufficiently addressed problem. These interactions may appear as harmless spam or escalate into sophisticated scam attempts, making it difficult to flag malicious intent early. Traditional detection methods often rely on static anomaly detection techniques that fail to adapt to dynamic conversational shifts. One key limitation is the misinterpretation of benign topic transitions referred to as concept drift as fraudulent behavior, leading to either false alarms or missed threats. We propose a two stage detection framework that first identifies suspicious conversations using a tailored ensemble classification model. To improve the reliability of detection, we incorporate a concept drift analysis step using a One Class Drift Detector (OCDD) to isolate conversational shifts within flagged dialogues. When drift is detected, a large language model (LLM) assesses whether the shift indicates fraudulent manipulation or a legitimate topic change. In cases where no drift is found, the behavior is inferred to be spam like. We validate our framework using a dataset of social engineering chat scenarios and demonstrate its practical advantages in improving both accuracy and interpretability for real time fraud detection. To contextualize the trade offs, we compare our modular approach against a Dual LLM baseline that performs detection and judgment using different language models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis</title>
<link>https://arxiv.org/abs/2505.07853</link>
<guid>https://arxiv.org/abs/2505.07853</guid>
<content:encoded><![CDATA[
<div> Keywords: Road crashes, Large Language Model, Crash analysis, Data augmentation, Explainability<br />
Summary: <br />
The study introduces CrashSage, a framework based on Large Language Models, to enhance crash analysis by transforming structured crash data into enriched textual narratives. Context-aware data augmentation and fine-tuning of a specific LLaMA3-8B model enable superior performance in crash severity inference compared to traditional approaches. Additionally, a gradient-based explainability technique is employed to elucidate model decisions and provide insights into influential factors. This innovative approach aims to address the limitations of conventional statistical models in capturing complex relationships and contextual nuances in road crash data, ultimately providing actionable insights for targeted road safety interventions. <br /><br />Summary: <div>
arXiv:2505.07853v1 Announce Type: new 
Abstract: Road crashes claim over 1.3 million lives annually worldwide and incur global economic losses exceeding \$1.8 trillion. Such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights. Conventional statistical models and tree ensemble approaches typically rely on structured crash data, overlooking contextual nuances and struggling to capture complex relationships and underlying semantics. Moreover, these approaches tend to incur significant information loss, particularly in narrative elements related to multi-vehicle interactions, crash progression, and rare event characteristics. This study presents CrashSage, a novel Large Language Model (LLM)-centered framework designed to advance crash analysis and modeling through four key innovations. First, we introduce a tabular-to-text transformation strategy paired with relational data integration schema, enabling the conversion of raw, heterogeneous crash data into enriched, structured textual narratives that retain essential structural and relational context. Second, we apply context-aware data augmentation using a base LLM model to improve narrative coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B model for crash severity inference, demonstrating superior performance over baseline approaches, including zero-shot, zero-shot with chain-of-thought prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini, LLaMA3-70B). Finally, we employ a gradient-based explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions. This interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights</title>
<link>https://arxiv.org/abs/2505.07856</link>
<guid>https://arxiv.org/abs/2505.07856</guid>
<content:encoded><![CDATA[
<div> evaluation, adversarial attacks, inflectional languages, robustness, model behaviour

Summary:<br />
Various techniques for generating adversarial examples have been developed, including subtle perturbations such as TextBugger and synonym substitutions like TextFooler. However, these methods are primarily tested on non-inflectional languages like English. This study evaluates the performance of adversarial attacks in inflectional languages, specifically Polish and English, using a novel Edge Attribution Patching (EAP) protocol. By analyzing models on the MultiEmo dataset, the impact of inflection on model behavior and robustness is explored. A benchmark is created to assess task-specific corpora containing inflected and syncretic text variants, allowing for the identification of inflection-related elements in model circuits and their behavior under attack. This work provides insights into the challenges posed by inflection in adversarial attacks and sheds light on the robustness of models in inflectional languages. 

Summary: <div>
arXiv:2505.07856v1 Announce Type: new 
Abstract: Various techniques are used in the generation of adversarial examples, including methods such as TextBugger which introduce minor, hardly visible perturbations to words leading to changes in model behaviour. Another class of techniques involves substituting words with their synonyms in a way that preserves the text's meaning but alters its predicted class, with TextFooler being a prominent example of such attacks. Most adversarial example generation methods are developed and evaluated primarily on non-inflectional languages, typically English. In this work, we evaluate and explain how adversarial attacks perform in inflectional languages. To explain the impact of inflection on model behaviour and its robustness under attack, we designed a novel protocol inspired by mechanistic interpretability, based on Edge Attribution Patching (EAP) method. The proposed evaluation protocol relies on parallel task-specific corpora that include both inflected and syncretic variants of texts in two languages -- Polish and English. To analyse the models and explain the relationship between inflection and adversarial robustness, we create a new benchmark based on task-oriented dataset MultiEmo, enabling the identification of mechanistic inflection-related elements of circuits within the model and analyse their behaviour under attack.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines</title>
<link>https://arxiv.org/abs/2505.07857</link>
<guid>https://arxiv.org/abs/2505.07857</guid>
<content:encoded><![CDATA[
<div> Urdu, intent detection, contrastive learning, pre-trained language models, prototype-informed attention mechanism
<br />
Multifarious intent detection predictors are developed for different languages, but Urdu, the 10th most spoken language, lacks sophisticated models. This study introduces a contrastive learning approach for intent detection in Urdu, using unlabeled data to re-train pre-trained language models. The re-training enhances representation learning for intent detection tasks. The proposed LLMPIA pipeline combines pre-trained language models and a prototype-informed attention mechanism. The framework is evaluated on two benchmark datasets, ATIS and Web Queries, achieving high F1-scores under various experimental settings. In a case study on the Web Queries dataset, the LLMPIA outperforms the state-of-the-art predictor by a significant margin. This approach shows promise in addressing the underdeveloped field of intent detection in Urdu language.
<br /><br />Summary: <div>
arXiv:2505.07857v1 Announce Type: new 
Abstract: Multifarious intent detection predictors are developed for different languages, including English, Chinese and French, however, the field remains underdeveloped for Urdu, the 10th most spoken language. In the realm of well-known languages, intent detection predictors utilize the strategy of few-shot learning and prediction of unseen classes based on the model training on seen classes. However, Urdu language lacks few-shot strategy based intent detection predictors and traditional predictors are focused on prediction of the same classes which models have seen in the train set. To empower Urdu language specific intent detection, this introduces a unique contrastive learning approach that leverages unlabeled Urdu data to re-train pre-trained language models. This re-training empowers LLMs representation learning for the downstream intent detection task. Finally, it reaps the combined potential of pre-trained LLMs and the prototype-informed attention mechanism to create a comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm of proposed predictive pipeline, it explores the potential of 6 distinct language models and 13 distinct similarity computation methods. The proposed framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing 5836 samples and Web Queries having 8519 samples. Across ATIS dataset under 4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and 98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score, respectively. In an additional case study on the Web Queries dataset under same classes train and test set settings, LLMPIA outperformed state-of-the-art predictor by 53.55% F1-Score.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.07858</link>
<guid>https://arxiv.org/abs/2505.07858</guid>
<content:encoded><![CDATA[
<div> Efficient decoding, large language models, speculative decoding techniques, scaling laws, Scylla<br />
<br />
Summary:<br />
The study focuses on efficient decoding in large language models, particularly in reasoning-intensive architectures. Speculative decoding methods using parallel draft-verification cycles are explored for accelerating reasoning tasks. Log-linear Scaling Laws governing draft model acceptance rate across various dimensions are discovered. Scylla, a system coordinating multi-dimensional scaling for popular LLMs, achieves higher acceptance rates and decoding throughput improvements compared to existing models. Empirical validation highlights performance gains on summarization and QA tasks. Industrial inference engine deployments demonstrate the potential of systematic scaling for efficient LLM inference. Code release is planned for the future. <div>
arXiv:2505.07858v1 Announce Type: new 
Abstract: The escalating demand for efficient decoding in large language models (LLMs) is particularly critical for reasoning-intensive architectures like OpenAI-o3 and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This study investigates speculative decoding techniques through dense LLM architectures to establish foundational insights for accelerating reasoning tasks. While speculative decoding methods leveraging parallel draft-verification cycles have emerged as promising acceleration techniques, the scaling laws governing decoding efficiency remain under-explored compared to conventional backbone LLMs developed through Pretraining->SFT->RLHF training paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2 and 1.3) governing draft model acceptance rate (or decoding speed) across three dimensions: pretraining token volume, draft model capacity, and decoding batch size. Building on these laws, we achieve Scylla, which coordinates multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on summarization and QA tasks (Figure 2). Industrial inference engine deployments demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5), validating the transformative potential of systematic scaling for efficient LLM inference. Code will be released later.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Performance on ARC is a Matter of Perspective</title>
<link>https://arxiv.org/abs/2505.07859</link>
<guid>https://arxiv.org/abs/2505.07859</guid>
<content:encoded><![CDATA[
<div> data augmentation, depth-first search algorithm, language models, scoring, performance<br />
<br />
Summary: 
The article discusses the use of task-specific data augmentations, a depth-first search algorithm, and language models to improve abstract reasoning abilities on the ARC-AGI Corpus. By incorporating these techniques throughout training, generation, and scoring phases, the proposed method achieves a score of 71.6% on the ARC-AGI evaluation set. This performance is considered state-of-the-art among publicly available approaches. The approach not only utilizes language models as generators but also as scorers, using output probabilities to select the most promising solutions. Notably, the method stands out for its transparency, reproducibility, and low inference cost, averaging only 2 cents per task on standard hardware. While other closed-source work has reported higher scores, this method offers a cost-effective and accessible solution for addressing abstract reasoning challenges posed by ARC-AGI. <br /><br />Summary: <div>
arXiv:2505.07859v1 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware (we assume a price of 36ct/hour for a Nvidia 4090 GPU).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable LLM Math Reasoning Acceleration with Low-rank Distillation</title>
<link>https://arxiv.org/abs/2505.07861</link>
<guid>https://arxiv.org/abs/2505.07861</guid>
<content:encoded><![CDATA[
<div> Efficient Inference, Large Language Model, Math Reasoning, Caprese, Distillation

Summary:
Caprese is a new low-cost distillation method designed to enhance math performance in large language models (LLMs) while maintaining efficiency in inference methods. By focusing on feedforward blocks and utilizing only 1% additional parameters and 20K synthetic training samples, Caprese successfully recovers lost math capabilities without impacting language tasks. The method does not alter the original weights, significantly reduces the number of active parameters in LLMs such as Gemma and Llama, and integrates seamlessly into existing model layers to reduce latency. Caprese also promotes response brevity and improves token generation efficiency, making it a practical solution for enhancing math reasoning in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.07861v1 Announce Type: new 
Abstract: Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a low-cost distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>11% reduction to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition</title>
<link>https://arxiv.org/abs/2505.07862</link>
<guid>https://arxiv.org/abs/2505.07862</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence-to-sequence, graph wavelet transformer, multi scale spectral decomposition, graph structured sequence modeling, efficient

Summary:
The article introduces the Graph Wavelet Transformer (GWT), a new architecture for structured language tasks that replaces the traditional dot product self-attention mechanism with a learnable, multi-scale wavelet transform. This transform is defined over an explicit graph Laplacian derived from syntactic or semantic parses, providing an interpretable and efficient alternative to quadratic self-attention. The GWT offers a more expressive way to model sequences in graph structures and reduces the computational and memory complexities associated with traditional models. By leveraging multi-scale spectral decomposition, the GWT achieves better performance in graph structured sequence modeling tasks. This approach opens up new possibilities for improving the efficiency and interpretability of sequence-to-sequence models in various applications. <div>
arXiv:2505.07862v1 Announce Type: new 
Abstract: Existing sequence to sequence models for structured language tasks rely heavily on the dot product self attention mechanism, which incurs quadratic complexity in both computation and memory for input length N. We introduce the Graph Wavelet Transformer (GWT), a novel architecture that replaces this bottleneck with a learnable, multi scale wavelet transform defined over an explicit graph Laplacian derived from syntactic or semantic parses. Our analysis shows that multi scale spectral decomposition offers an interpretable, efficient, and expressive alternative to quadratic self attention for graph structured sequence modeling.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction</title>
<link>https://arxiv.org/abs/2505.07863</link>
<guid>https://arxiv.org/abs/2505.07863</guid>
<content:encoded><![CDATA[
<div> BERT, QoS, prediction, uncertainty, service selection 
Summary: 
QoSBERT is introduced as a framework for QoS prediction, utilizing pre-trained language models to encode user service metadata and provide deep semantic understanding. It integrates uncertainty estimation using Monte Carlo Dropout, allowing for trustworthy service quality prediction. QoSBERT outperforms existing models in response time and throughput prediction benchmarks, reducing errors and providing well-calibrated confidence intervals. The framework also aids in selecting high-quality training samples for improved robustness in low-resource settings. QoSBERT not only advances accuracy in service quality prediction but also delivers reliable uncertainty quantification, enhancing data-driven service selection and optimization. <div>
arXiv:2505.07863v1 Announce Type: new 
Abstract: Accurate prediction of Quality of Service (QoS) metrics is fundamental for selecting and managing cloud based services. Traditional QoS models rely on manual feature engineering and yield only point estimates, offering no insight into the confidence of their predictions. In this paper, we propose QoSBERT, the first framework that reformulates QoS prediction as a semantic regression task based on pre trained language models. Unlike previous approaches relying on sparse numerical features, QoSBERT automatically encodes user service metadata into natural language descriptions, enabling deep semantic understanding. Furthermore, we integrate a Monte Carlo Dropout based uncertainty estimation module, allowing for trustworthy and risk-aware service quality prediction, which is crucial yet underexplored in existing QoS models. QoSBERT applies attentive pooling over contextualized embeddings and a lightweight multilayer perceptron regressor, fine tuned jointly to minimize absolute error. We further exploit the resulting uncertainty estimates to select high quality training samples, improving robustness in low resource settings. On standard QoS benchmark datasets, QoSBERT achieves an average reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and 6.9% in MAE for throughput prediction compared to the strongest baselines, while providing well calibrated confidence intervals for robust and trustworthy service quality estimation. Our approach not only advances the accuracy of service quality prediction but also delivers reliable uncertainty quantification, paving the way for more trustworthy, data driven service selection and optimization.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection</title>
<link>https://arxiv.org/abs/2505.07870</link>
<guid>https://arxiv.org/abs/2505.07870</guid>
<content:encoded><![CDATA[
<div> metamorphic testing, fairness detection, large language models, prioritization, fault detection  
Summary:  
- The study focuses on using metamorphic testing to detect fairness issues in Large Language Models (LLMs).  
- By prioritizing metamorphic relations (MRs) based on their effectiveness, the study aims to optimize fault detection in LLMs.  
- A sentence diversity-based approach is proposed to compute and rank MRs, leading to a 22% improvement in fault detection rates compared to random prioritization.  
- The prioritization approach also reduces the time to the first failure by 15% and significantly decreases computational costs associated with fault labeling.  
- Experimental results show that the diversity-based MR prioritization method is effective in enhancing fairness testing for LLMs, performing within 5% of fault-based prioritization.  
<br /><br />Summary: <div>
arXiv:2505.07870v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in various applications, raising critical concerns about fairness and potential biases in their outputs. This paper explores the prioritization of metamorphic relations (MRs) in metamorphic testing as a strategy to efficiently detect fairness issues within LLMs. Given the exponential growth of possible test cases, exhaustive testing is impractical; therefore, prioritizing MRs based on their effectiveness in detecting fairness violations is crucial. We apply a sentence diversity-based approach to compute and rank MRs to optimize fault detection. Experimental results demonstrate that our proposed prioritization approach improves fault detection rates by 22% compared to random prioritization and 12% compared to distance-based prioritization, while reducing the time to the first failure by 15% and 8%, respectively. Furthermore, our approach performs within 5% of fault-based prioritization in effectiveness, while significantly reducing the computational cost associated with fault labeling. These results validate the effectiveness of diversity-based MR prioritization in enhancing fairness testing for LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy</title>
<link>https://arxiv.org/abs/2505.07871</link>
<guid>https://arxiv.org/abs/2505.07871</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial sentiment analysis, Language Models, Benchmark datasets, Annotators' Instruction Assisted Prompt, WSBS dataset

Summary:
- Financial sentiment analysis poses challenges for Language Models (LLMs) due to nuanced language in financial contexts.
- Existing benchmark datasets like Financial Phrasebank have undefined sentiment classes leading to variability in annotations.
- The Annotators' Instruction Assisted Prompt (AIAP) redefines FSA task for LLMs by integrating detailed instructions into the prompt framework.
- AIAP significantly improves LLM performance by aligning machine operations with refined task definitions, yielding up to 9.08% improvements.
- AIAP introduces a context-aware sentiment-indexing method using model confidence scores, enhancing stock price prediction models and extracting more value from financial sentiment analysis. 
<br /><br />Summary: Financial sentiment analysis presents challenges for Language Models due to nuanced language in financial contexts. Benchmark datasets have undefined sentiment classes, leading to variability in annotations. The Annotators' Instruction Assisted Prompt (AIAP) improves LLM performance by standardizing sentiment understanding. AIAP enhances stock price prediction models using model confidence scores. <div>
arXiv:2505.07871v1 Announce Type: new 
Abstract: Financial sentiment analysis (FSA) presents unique challenges to LLMs that surpass those in typical sentiment analysis due to the nuanced language used in financial contexts. The prowess of these models is often undermined by the inherent subjectivity of sentiment classifications in existing benchmark datasets like Financial Phrasebank. These datasets typically feature undefined sentiment classes that reflect the highly individualized perspectives of annotators, leading to significant variability in annotations. This variability results in an unfair expectation for LLMs during benchmarking, where they are tasked to conjecture the subjective viewpoints of human annotators without sufficient context. In this paper, we introduce the Annotators' Instruction Assisted Prompt, a novel evaluation prompt designed to redefine the task definition of FSA for LLMs. By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations, providing a fair and context-rich foundation for sentiment analysis. We utilize a new dataset, WSBS, derived from the WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM performance by aligning machine operations with the refined task definitions. Experimental results demonstrate that AIAP enhances LLM performance significantly, with improvements up to 9.08. This context-aware approach not only yields incremental gains in performance but also introduces an innovative sentiment-indexing method utilizing model confidence scores. This method enhances stock price prediction models and extracts more value from the financial sentiment analysis, underscoring the significance of WSB as a critical source of financial text. Our research offers insights into both improving FSA through better evaluation methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sound of Populism: Distinct Linguistic Features Across Populist Variants</title>
<link>https://arxiv.org/abs/2505.07874</link>
<guid>https://arxiv.org/abs/2505.07874</guid>
<content:encoded><![CDATA[
<div> populism, linguistic markers, political rhetoric, American politics, RoBERTa<br />
Summary:<br />
This study combines LIWC features and a RoBERTa model to analyze the sound of populism in U.S. presidential speeches. It identifies four populist dimensions (left-wing, right-wing, anti-elitism, people-centrism) and explores their linguistic markers, revealing a common direct and assertive tone used to connect with the people. Right-wing populism and people-centrism exhibit more emotionally charged discourse compared to left-wing and anti-elitist expressions, focusing on themes of identity, grievance, and crisis. The study highlights the strategic calibration of populist rhetoric, emphasizing the charismatic leadership persona it constructs. <div>
arXiv:2505.07874v1 Announce Type: new 
Abstract: This study explores the sound of populism by integrating the classic Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional and stylistic tones of language, with a fine-tuned RoBERTa model, a state-of-the-art context-aware language model trained to detect nuanced expressions of populism. This approach allows us to uncover the auditory dimensions of political rhetoric in U.S. presidential inaugural and State of the Union addresses. We examine how four key populist dimensions (i.e., left-wing, right-wing, anti-elitism, and people-centrism) manifest in the linguistic markers of speech, drawing attention to both commonalities and distinct tonal shifts across these variants. Our findings reveal that populist rhetoric consistently features a direct, assertive ``sound" that forges a connection with ``the people'' and constructs a charismatic leadership persona. However, this sound is not simply informal but strategically calibrated. Notably, right-wing populism and people-centrism exhibit a more emotionally charged discourse, resonating with themes of identity, grievance, and crisis, in contrast to the relatively restrained emotional tones of left-wing and anti-elitist expressions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints</title>
<link>https://arxiv.org/abs/2505.07883</link>
<guid>https://arxiv.org/abs/2505.07883</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, event probabilities, rational decision-making, variational autoencoder, coherence

Summary:
This paper addresses the issue of incoherent event probabilities generated by Large Language Models (LLMs) by exploring the possibility of recovering coherent probabilities from the embeddings used by these models. By enforcing axiomatic constraints in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings, the authors show that event probabilities can naturally emerge in the latent space. The proposed method is evaluated on complementary events, demonstrating that the recovered probabilities exhibit greater coherence and align closely with true probabilities. This approach has the potential to improve rational decision-making under uncertainty by providing more accurate estimates for events involving uncertainty. Additionally, the results suggest that embedding-based approaches can offer a promising solution for addressing the incoherence issue in LLM-generated event probabilities. 

<br /><br />Summary: <div>
arXiv:2505.07883v1 Announce Type: new 
Abstract: Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of a WAZOBIA-Named Entity Recognition System</title>
<link>https://arxiv.org/abs/2505.07884</link>
<guid>https://arxiv.org/abs/2505.07884</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, African languages, NLP frameworks, Transfer learning, WAZOBIA-NER system<br />
Summary:<br />
- Development of a WAZOBIA-NER system for Nigerian languages Hausa, Yoruba, and Igbo
- Compilation of annotated datasets to address data scarcity and linguistic diversity challenges
- Evaluation of machine learning techniques like CRF and deep learning models for entity recognition
- Utilization of OCR technology for text extraction from images
- Achieved high performance metrics in precision, recall, F1-score, and accuracy<br />

Summary:<br /> <div>
arXiv:2505.07884v1 Announce Type: new 
Abstract: Named Entity Recognition NER is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLHF: Prompt Optimization with Few-Shot Human Feedback</title>
<link>https://arxiv.org/abs/2505.07886</link>
<guid>https://arxiv.org/abs/2505.07886</guid>
<content:encoded><![CDATA[
<div> Framework, prompt optimization, large language models, PLHF, human feedback
Summary:
The article introduces PLHF, a new prompt optimization framework inspired by RLHF, designed for large language models. PLHF utilizes a specific evaluator module as a metric to estimate output quality, requiring only a single round of human feedback for prompt optimization. Unlike previous strategies, PLHF addresses the challenge of optimizing prompts without a clear metric for output quality assessment. Empirical results on public and industrial datasets demonstrate that PLHF surpasses existing output grading methods for LLM prompt optimizations. The framework offers a more effective and efficient approach to obtaining suitable prompts, especially for tasks where output quality cannot be easily assessed by comparisons with standard golden samples. <div>
arXiv:2505.07886v1 Announce Type: new 
Abstract: Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman "F"eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping</title>
<link>https://arxiv.org/abs/2505.07888</link>
<guid>https://arxiv.org/abs/2505.07888</guid>
<content:encoded><![CDATA[
<div> Keywords: long-text style transfer, zero-shot learning, large language models, hierarchical framework, structured rewriting<br />
Summary:<br />
This paper introduces a hierarchical framework, ZeroStylus, for improving long-text style transfer by combining sentence-level stylistic adaptation with paragraph-level structural coherence. The framework addresses the challenge of preserving original syntactic and semantic information during style transfer by including paragraph-level semantic considerations. Two phases are used: hierarchical template acquisition and template-guided generation. Experimental evaluations show that the framework outperforms baseline methods in style consistency, content preservation, and expression quality metrics. Ablation studies confirm the benefits of using both sentence and paragraph template repositories for context-aware transformations. The results demonstrate the feasibility of coherent long-text style transfer without the need for parallel corpora or training large language models. <br /> <div>
arXiv:2505.07888v1 Announce Type: new 
Abstract: This paper addresses the challenge in long-text style transfer using zero-shot learning of large language models (LLMs), proposing a hierarchical framework that combines sentence-level stylistic adaptation with paragraph-level structural coherence. We argue that in the process of effective paragraph-style transfer, to preserve the consistency of original syntactic and semantic information, it is essential to perform style transfer not only at the sentence level but also to incorporate paragraph-level semantic considerations, while ensuring structural coherence across inter-sentential relationships. Our proposed framework, ZeroStylus, operates through two systematic phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching. The framework dynamically constructs sentence and paragraph template repositories, enabling context-aware transformations while preserving inter-sentence logical relationships. Experimental evaluations demonstrate significant improvements over baseline methods, with structured rewriting achieving 6.90 average score compared to 6.70 for direct prompting approaches in tri-axial metrics assessing style consistency, content preservation, and expression quality. Ablation studies validate the necessity of both template hierarchies during style transfer, showing higher content preservation win rate against sentence-only approaches through paragraph-level structural encoding, as well as direct prompting method through sentence-level pattern extraction and matching. The results establish new capabilities for coherent long-text style transfer without requiring parallel corpora or LLM fine-tuning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.07889</link>
<guid>https://arxiv.org/abs/2505.07889</guid>
<content:encoded><![CDATA[
<div> Keywords: biological protocols, BioProBench, language models, procedural reasoning, benchmark

Summary:
BioProBench is introduced as a multi-task benchmark for evaluating language models (LLMs) on understanding and reasoning with biological protocols. It includes tasks such as Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, providing a comprehensive assessment of LLMs on procedural biological texts. The benchmark, built on 27K original protocols, reveals that while LLMs perform well on surface understanding tasks, they struggle with deeper reasoning and structured generation tasks like ordering and generation. Comparison of 12 mainstream LLMs shows diverse performance, with certain open-source models approaching closed-source levels on some tasks. However, bio-specific small models lag behind general LLMs, indicating limitations in handling complex procedural content. The findings emphasize the challenge of procedural reasoning within biological protocols for current LLMs and highlight the need for improved AI systems tailored for automating scientific procedures safely. The code and data for BioProBench are available on GitHub and Hugging Face. 

<br /><br />Summary: BioProBench, a benchmark for evaluating language models on biological protocols, includes tasks like Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning. While models perform well on certain tasks, they struggle with deep reasoning and structured generation. Comparison of mainstream models shows varying performance levels, with bio-specific models lagging behind general models. This underscores the challenge of procedural reasoning in biological protocols for current language models and the need for improved AI systems tailored for complex scientific procedures. <div>
arXiv:2505.07889v1 Announce Type: new 
Abstract: Biological protocols are fundamental to reproducible and safe life science research. While LLMs excel on general tasks, their systematic evaluation on these highly specialized, accuracy-critical, and inherently procedural texts remains limited. In this work, we present BioProBench, the first large-scale, integrated multi-task benchmark for biological protocol understanding and reasoning. While limited benchmarks have touched upon specific aspects like protocol QA, BioProBench provides a comprehensive suite of five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts. Built upon 27K original protocols, it yields nearly 556K high-quality structured instances. We evaluate 12 mainstream open/closed-source LLMs on BioProBench. Experimental results reveal that while top models preform well on surface understanding tasks, struggle significantly with deep reasoning and structured generation tasks like ordering and generation. Furthermore, model comparisons reveal diverse performance: certain open-source models approach closed-source levels on some tasks, yet bio-specific small models lag behind general LLMs, indicating limitations on complex procedural content. Overall, our findings underscore that procedural reasoning within biological protocols represents a significant challenge for current LLMs. BioProBench serves as a standardized framework to diagnose these specific limitations and guide the development of AI systems better equipped for safely automating complex scientific procedures. The code and data are available at: https://github.com/YuyangSunshine/bioprotocolbench and https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks</title>
<link>https://arxiv.org/abs/2505.07890</link>
<guid>https://arxiv.org/abs/2505.07890</guid>
<content:encoded><![CDATA[
<div> Keywords: Turkish Sign Language, TSLFormer, sequence-to-sequence translation, self-attention mechanism, assistive communication<br />
<br />
Summary: <br />
The study introduces TSLFormer, a model for word-level Turkish Sign Language recognition that uses 3D joint positions instead of raw videos, reducing input dimensionality while retaining gesture information. Inspired by sign languages' linguistic nature and transformer success, TSLFormer employs a sequence-to-sequence translation approach with self-attention. By capturing temporal co-occurrence and motion patterns, it achieves competitive performance on the AUTSL dataset with minimal computational cost. The model's joint-based input enables the development of real-time, mobile assistive communication systems for hearing-impaired individuals, showcasing its potential for practical applications. <div>
arXiv:2505.07890v1 Announce Type: new 
Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign Language (TSL) recognition model that treats sign gestures as ordered, string-like language. Instead of using raw RGB or depth videos, our method only works with 3D joint positions - articulation points - extracted using Google's Mediapipe library, which focuses on the hand and torso skeletal locations. This creates efficient input dimensionality reduction while preserving important semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence translation, inspired by the linguistic nature of sign languages and the success of transformers in natural language processing. Since TSLFormer uses the self-attention mechanism, it effectively captures temporal co-occurrence within gesture sequences and highlights meaningful motion patterns as words unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different words, TSLFormer achieves competitive performance with minimal computational cost. These results show that joint-based input is sufficient for enabling real-time, mobile, and assistive communication systems for hearing-impaired individuals.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking</title>
<link>https://arxiv.org/abs/2505.07891</link>
<guid>https://arxiv.org/abs/2505.07891</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, fact-checking, TrumorGPT, health domain, semantic reasoning

<br /><br />Summary: In today's social media landscape, the rapid spread of misinformation has led to widespread infodemics, particularly in health-related discussions. To tackle this challenge, the authors introduce TrumorGPT, a new generative artificial intelligence tool focused on fact-checking health claims. This system is specifically designed to identify "trumors," which are health-related rumors that may actually be true. TrumorGPT utilizes a large language model (LLM) and employs few-shot learning techniques for constructing and reasoning with semantic health knowledge graphs. To combat the hallucination issues often associated with LLMs and the constraints of static training datasets, TrumorGPT integrates a method called graph-based retrieval-augmented generation (GraphRAG). This innovative approach allows access to frequently updated semantic health knowledge graphs that contain the latest medical news and health information. By relying on the most current data, TrumorGPT enhances its fact-checking capabilities. Evaluation against extensive healthcare datasets indicates that TrumorGPT significantly outperforms other methods in verifying public health claims, marking a substantial advancement in combating health misinformation and fostering trust in the information circulating in the digital realm. <div>
arXiv:2505.07891v1 Announce Type: new 
Abstract: In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT , a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish "trumors", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</title>
<link>https://arxiv.org/abs/2505.07897</link>
<guid>https://arxiv.org/abs/2505.07897</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context models, Code comprehension, Code repair, LongCodeBench, Benchmark

Summary: 
Long-context models have seen a rapid increase in context lengths, presenting challenges in constructing realistic benchmarks for testing their capabilities. LongCodeBench (LCB) is introduced as a benchmark to assess the coding abilities of Long-context language models (LCLMs) in code comprehension and repair tasks. The benchmark draws from real-world GitHub issues to create LongCodeQA and bug fixing tasks, evaluating models across different scales. Results show that long-context remains a weakness for all models, with significant performance drops observed. For example, Claude 3.5 Sonnet experienced a drop from 29% to 3%, while Qwen2.5 dropped from 70.2% to 40%. Overall, the study highlights the need for further advancements in long-context modeling for improved performance in code comprehension and repair tasks. 

<br /><br />Summary: <div>
arXiv:2505.07897v1 Announce Type: new 
Abstract: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise</title>
<link>https://arxiv.org/abs/2505.07899</link>
<guid>https://arxiv.org/abs/2505.07899</guid>
<content:encoded><![CDATA[
<div> sequential knowledge editing, language models, editing success rates, DeltaEdit, interference reduction <br />
<br />
Summary:<br />
The article introduces the concept of sequential knowledge editing to update large language models efficiently. It highlights the issue of declining editing success rates with an increase in the number of edits due to accumulated superimposed noise. The newly proposed DeltaEdit method addresses this problem by optimizing update parameters through a dynamic orthogonal constraints strategy. By reducing interference between edits, DeltaEdit significantly improves edit success rates and retains generalization capabilities of the model. Experimental results showcase the superiority of DeltaEdit over existing methods, ensuring stable and reliable model performance even during extensive sequential editing. <div>
arXiv:2505.07899v1 Announce Type: new 
Abstract: Sequential knowledge editing techniques aim to continuously update the knowledge in large language models at a low cost, preventing the models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, we identify that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the accumulation of superimposed noise problem. To address this, we identify the factors contributing to this deviation and propose DeltaEdit, a novel method that optimizes update parameters through a dynamic orthogonal constraints strategy, effectively reducing interference between edits to mitigate deviation. Experimental results demonstrate that DeltaEdit significantly outperforms existing methods in edit success rates and the retention of generalization capabilities, ensuring stable and reliable model performance even under extensive sequential editing.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEM: Reinforcement Learning for Search-Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2505.07903</link>
<guid>https://arxiv.org/abs/2505.07903</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, Large Language Models, search optimization, structured reasoning, external retrieval<br />
<br />
Summary: 
The paper introduces a novel post-training reinforcement learning framework called SEM aimed at optimizing search usage for Large Language Models (LLMs). By combining datasets MuSiQue and MMLU, scenarios are created to train the model to differentiate between questions that can be answered internally and those requiring external search. A structured reasoning template is designed, and Group Relative Policy Optimization (GRPO) is used to post-train the model's search behaviors. The reward function encourages accurate answering and efficient retrieval, reducing redundant search operations while maintaining or improving answer accuracy across various benchmarks. This approach enhances the model's reasoning efficiency and its ability to effectively utilize external knowledge when needed. <div>
arXiv:2505.07903v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions</title>
<link>https://arxiv.org/abs/2505.07920</link>
<guid>https://arxiv.org/abs/2505.07920</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, large language models, dataset, rebuttal, interactive<br />
Summary:<br />
The article discusses the challenges faced by the scientific community in dealing with the increasing volume of submissions in fields like AI due to a lack of effective tools for authors to self-evaluate their work before submission. This has led to reviewer shortages and declines in review quality. The utilization of Large Language Models (LLMs) shows promise in assisting both authors and reviewers, but their performance is limited by the quality of peer review data. The article introduces a new dataset named Re^2, which addresses the limitations of existing peer review datasets by providing more diverse and consistent data, as well as support for tasks involving rebuttal and reviewer-author interactions. Additionally, the dataset includes a multi-turn conversation paradigm to support dynamic interactive LLM assistants, aiming to provide practical guidance for authors to refine their manuscripts and alleviate the growing review burden.<br /><br />Summary: <div>
arXiv:2505.07920v1 Announce Type: new 
Abstract: Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in https://anonymous.4open.science/r/ReviewBench_anon/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models</title>
<link>https://arxiv.org/abs/2505.07968</link>
<guid>https://arxiv.org/abs/2505.07968</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, health care, clinical guidelines, concept drift, model performance

Summary:<br />
- Large Language Models (LLMs) have potential in health care but struggle with evolving medical knowledge.
- A study investigated LLM response to evolving clinical guidelines, focusing on concept drift and internal inconsistencies.
- The DriftMedQA benchmark simulated guideline evolution and evaluated temporal reliability of seven state-of-the-art models.
- Models had difficulty rejecting outdated recommendations and often endorsed conflicting guidance.
- Mitigation strategies like Retrieval-Augmented Generation and preference fine-tuning improved model performance, with the combination yielding the best results.
- Improving LLM robustness to temporal shifts is crucial for more reliable applications in clinical practice.

Summary: <div>
arXiv:2505.07968v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have great potential in the field of health care, yet they face great challenges in adapting to rapidly evolving medical knowledge. This can lead to outdated or contradictory treatment suggestions. This study investigated how LLMs respond to evolving clinical guidelines, focusing on concept drift and internal inconsistencies. We developed the DriftMedQA benchmark to simulate guideline evolution and assessed the temporal reliability of various LLMs. Our evaluation of seven state-of-the-art models across 4,290 scenarios demonstrated difficulties in rejecting outdated recommendations and frequently endorsing conflicting guidance. Additionally, we explored two mitigation strategies: Retrieval-Augmented Generation and preference fine-tuning via Direct Preference Optimization. While each method improved model performance, their combination led to the most consistent and reliable results. These findings underscore the need to improve LLM robustness to temporal shifts to ensure more dependable applications in clinical practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration</title>
<link>https://arxiv.org/abs/2505.07980</link>
<guid>https://arxiv.org/abs/2505.07980</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic communications, task-adaptive, diffusion models, deep-compressed, attention mechanism

Summary:
This study introduces a novel task-adaptive semantic communication framework that aims to improve bandwidth efficiency by conveying semantic meanings instead of raw data. By utilizing diffusion models, the framework dynamically adjusts the semantic message delivery based on the downstream tasks. The process begins with transmitting a deep-compressed general semantic representation from the sender to facilitate coarse data reconstruction at the receiver. Upon receiving task-specific prompts as feedback, the sender incorporates an attention mechanism to refine the semantic transmission with more detailed information that aligns with the receiver's objectives. Experimental results demonstrate the effectiveness of this approach in adaptively preserving crucial task-relevant information for semantic communications while maintaining high compression efficiency.<br /><br />Summary: This study presents a task-adaptive semantic communication framework utilizing diffusion models to dynamically adjust semantic message delivery based on downstream tasks. It starts with transmitting a deep-compressed general semantic representation, allowing for coarse data reconstruction at the receiver. Through task-specific prompts and an attention mechanism, the framework refines semantic transmission to align with receiver objectives, effectively preserving critical task-relevant information while maintaining high compression efficiency. <div>
arXiv:2505.07980v1 Announce Type: new 
Abstract: Semantic communications represent a new paradigm of next-generation networking that shifts bit-wise data delivery to conveying the semantic meanings for bandwidth efficiency. To effectively accommodate various potential downstream tasks at the receiver side, one should adaptively convey the most critical semantic information. This work presents a novel task-adaptive semantic communication framework based on diffusion models that is capable of dynamically adjusting the semantic message delivery according to various downstream tasks. Specifically, we initialize the transmission of a deep-compressed general semantic representation from the transmitter to enable diffusion-based coarse data reconstruction at the receiver. The receiver identifies the task-specific demands and generates textual prompts as feedback. Integrated with the attention mechanism, the transmitter updates the semantic transmission with more details to better align with the objectives of the intended receivers. Our test results demonstrate the efficacy of the proposed method in adaptively preserving critical task-relevant information for semantic communications while preserving high compression efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models and Arabic Content: A Review</title>
<link>https://arxiv.org/abs/2505.08004</link>
<guid>https://arxiv.org/abs/2505.08004</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Arabic NLP, pre-trained models, finetuning, benchmarks

Summary:<br /><br />Large Language Models (LLMs) have greatly impacted Artificial Intelligence, particularly in Natural Language Processing (NLP) for Arabic. Despite the challenges of the Arabic language, such as its rich morphology and diverse writing standards, pre-trained LLMs have shown success in various Arabic NLP tasks. Techniques like finetuning and prompt engineering can enhance LLM performance in handling diverse Arabic content and dialects. The study discusses early pre-trained Arabic language models and their applications across NLP tasks. It also highlights the importance of Arabic benchmarks and datasets, noting the growing adoption of LLMs in the field. <div>
arXiv:2505.08004v1 Announce Type: new 
Abstract: Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation</title>
<link>https://arxiv.org/abs/2505.08037</link>
<guid>https://arxiv.org/abs/2505.08037</guid>
<content:encoded><![CDATA[
<div> spelling correction, Tibetan, multi-level, data augmentation, TiSpell

Summary:
TiSpell is a new approach for multi-level Tibetan spelling correction that addresses errors at both character and syllable levels in a unified model. Existing methods often focus only on one level of correction, lacking effective integration. To overcome this limitation, TiSpell utilizes a data augmentation technique that generates multi-level corruptions using unlabeled text. The proposed semi-masked model, TiSpell, is capable of correcting both character- and syllable-level errors. By synthesizing nine types of corruptions on clean sentences, a robust training set is created. Experimental results on both simulated and real-world data show that TiSpell outperforms baseline models and matches the performance of state-of-the-art approaches, demonstrating its effectiveness in improving Tibetan spelling correction. <div>
arXiv:2505.08037v1 Announce Type: new 
Abstract: Multi-level Tibetan spelling correction addresses errors at both the character and syllable levels within a unified model. Existing methods focus mainly on single-level correction and lack effective integration of both levels. Moreover, there are no open-source datasets or augmentation methods tailored for this task in Tibetan. To tackle this, we propose a data augmentation approach using unlabeled text to generate multi-level corruptions, and introduce TiSpell, a semi-masked model capable of correcting both character- and syllable-level errors. Although syllable-level correction is more challenging due to its reliance on global context, our semi-masked strategy simplifies this process. We synthesize nine types of corruptions on clean sentences to create a robust training set. Experiments on both simulated and real-world data demonstrate that TiSpell, trained on our dataset, outperforms baseline models and matches the performance of state-of-the-art approaches, confirming its effectiveness.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning</title>
<link>https://arxiv.org/abs/2505.08054</link>
<guid>https://arxiv.org/abs/2505.08054</guid>
<content:encoded><![CDATA[
<div> Keywords: Safety alignment, large language models, FalseReject, adversarial multi-agent interaction, over-refusal

Summary:
FalseReject is a new resource containing toxic queries and structured responses to help address the over-refusal issue in large language models (LLMs). It includes datasets tailored for different types of models and a benchmark test set. A graph-informed adversarial multi-agent interaction framework is proposed to generate diverse prompts and structured responses with explicit reasoning. Benchmarking on 29 state-of-the-art LLMs shows persistent over-refusal challenges. Supervised finetuning with FalseReject significantly reduces unnecessary refusals while maintaining safety and language capabilities. The resource aims to improve the utility and accuracy of LLMs in sensitive scenarios. <div>
arXiv:2505.08054v1 Announce Type: new 
Abstract: Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method</title>
<link>https://arxiv.org/abs/2505.08058</link>
<guid>https://arxiv.org/abs/2505.08058</guid>
<content:encoded><![CDATA[
<div> optimization, token reduction, LLM prompts, text representation scheme, semantic compression<br />
<br />
Summary: <br />
The white paper introduces a novel text representation scheme and word-level semantic compression technique that can achieve over 90% token reduction while maintaining high semantic similarity to the original text. The compression technique is controllable in detail granularity and can be lossless. The benchmark results, using Project Gutenberg's Dracula as open source data, demonstrate the effectiveness of the compression at the paragraph level across different genres and models. This innovation has potential applications in NLP and next-generation AI, offering a more efficient way to optimize compute using token reduction for LLM prompts. <div>
arXiv:2505.08058v1 Announce Type: new 
Abstract: Compute optimization using token reduction of LLM prompts is an emerging task in the fields of NLP and next generation, agentic AI. In this white paper, we introduce a novel (patent pending) text representation scheme and a first-of-its-kind word-level semantic compression of paragraphs that can lead to over 90\% token reduction, while retaining high semantic similarity to the source text. We explain how this novel compression technique can be lossless and how the detail granularity is controllable. We discuss benchmark results over open source data (i.e. Bram Stoker's Dracula available through Project Gutenberg) and show how our results hold at the paragraph level, across multiple genres and models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs complicated ethical dilemma analyzers?</title>
<link>https://arxiv.org/abs/2505.08106</link>
<guid>https://arxiv.org/abs/2505.08106</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Large Language Models, ethical reasoning, expert opinions, model evaluation

Summary:
The study examines whether Large Language Models (LLMs) can mimic human ethical reasoning by analyzing a benchmark dataset of real-world ethical dilemmas and expert opinions. The dataset is structured into five components and includes non-expert human responses for comparison. Various LLMs are evaluated using a composite metric framework, showing that LLMs generally outperform non-experts in lexical and structural alignment, with GPT-4o-mini performing consistently well. However, all models struggle with historical grounding and proposing nuanced resolution strategies, indicating limitations in contextual abstraction. Human responses, though less structured, sometimes match the semantic similarity of LLMs, suggesting intuitive moral reasoning. This study highlights both the strengths and current challenges of LLMs in ethical decision-making.<br /><br />Summary: <div>
arXiv:2505.08106v1 Announce Type: new 
Abstract: One open question in the study of Large Language Models (LLMs) is whether they can emulate human ethical reasoning and act as believable proxies for human judgment. To investigate this, we introduce a benchmark dataset comprising 196 real-world ethical dilemmas and expert opinions, each segmented into five structured components: Introduction, Key Factors, Historical Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also collect non-expert human responses for comparison, limited to the Key Factors section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini, Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine similarity, and Universal Sentence Encoder similarity. Metric weights are computed through an inversion-based ranking alignment and pairwise AHP analysis, enabling fine-grained comparison of model outputs to expert responses. Our results show that LLMs generally outperform non-expert humans in lexical and structural alignment, with GPT-4o-mini performing most consistently across all sections. However, all models struggle with historical grounding and proposing nuanced resolution strategies, which require contextual abstraction. Human responses, while less structured, occasionally achieve comparable semantic similarity, suggesting intuitive moral reasoning. These findings highlight both the strengths and current limitations of LLMs in ethical decision-making.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Putting It All into Context: Simplifying Agents with LCLMs</title>
<link>https://arxiv.org/abs/2505.08120</link>
<guid>https://arxiv.org/abs/2505.08120</guid>
<content:encoded><![CDATA[
<div> LM agent, complex tasks, SWE-bench, Gemini-1.5-Pro, Claude-3.7 <br />
Summary: 
Recent advancements in language model (LM) agents have shown promise in automating complex real-world tasks. The study explores whether the complexity of current LM agent architectures is necessary by focusing on the challenging SWE-bench task. Results indicate that using a long context language model (LCLM) and appropriate prompts can make a Gemini-1.5-Pro model competitive with intricate, well-tuned agent scaffolds on SWE-Bench-Verified, achieving a 38% solve rate without any additional tools. The more advanced Gemini-2.5-Pro model, employing the same unscaffolded approach, reaches a 50.8% solve rate. Moreover, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate. These findings suggest that simplifying LM agent architectures by leveraging LCLMs and effective prompts can yield impressive results on challenging tasks like SWE-bench. <br /><br /> <div>
arXiv:2505.08120v1 Announce Type: new 
Abstract: Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. In this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval</title>
<link>https://arxiv.org/abs/2505.08130</link>
<guid>https://arxiv.org/abs/2505.08130</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, information retrieval, multilingual, university orientation, interactive service

Summary: 
ALOHA is a multilingual agent designed to address the limitations of current search engines in providing campus-specific information. It incorporates hierarchical retrieval and external APIs to enhance user experience. The system has outperformed commercial chatbots and search engines in providing correct, timely, and user-friendly responses in multiple languages. ALOHA has been deployed successfully, serving over 12,000 users. <div>
arXiv:2505.08130v1 Announce Type: new 
Abstract: The rise of Large Language Models~(LLMs) revolutionizes information retrieval, allowing users to obtain required answers through complex instructions within conversations. However, publicly available services remain inadequate in addressing the needs of faculty and students to search campus-specific information. It is primarily due to the LLM's lack of domain-specific knowledge and the limitation of search engines in supporting multilingual and timely scenarios. To tackle these challenges, we introduce ALOHA, a multilingual agent enhanced by hierarchical retrieval for university orientation. We also integrate external APIs into the front-end interface to provide interactive service. The human evaluation and case study show our proposed system has strong capabilities to yield correct, timely, and user-friendly responses to the queries in multiple languages, surpassing commercial chatbots and search engines. The system has been deployed and has provided service for more than 12,000 people.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage</title>
<link>https://arxiv.org/abs/2505.08167</link>
<guid>https://arxiv.org/abs/2505.08167</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, intangible cultural heritage, bidirectional chains of thought, reward mechanism, domain-specific datasets
Summary: 
The article discusses the challenges faced in fine-tuning large language models using Intangible Cultural Heritage (ICH) data, proposing a novel training method that integrates bidirectional chains of thought and a reward mechanism. This method, implemented on the ICH-Qwen model, enables forward and reverse reasoning to activate latent knowledge and improve answer accuracy. Comparative experiments show superior performance over existing methods in accuracy and evaluation scores. A reward mechanism optimizes decision-making during training. Ablation experiments confirm the effectiveness of the bidirectional chains of thought and reward mechanism. Generalizability experiments demonstrate improvements on domain-specific datasets and models in different fields like Finance and StrategyQA. The method proves adaptable to various domains, offering a valuable approach for model training in diverse applications. 
<br /><br />Summary: <div>
arXiv:2505.08167v1 Announce Type: new 
Abstract: The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph</title>
<link>https://arxiv.org/abs/2505.08168</link>
<guid>https://arxiv.org/abs/2505.08168</guid>
<content:encoded><![CDATA[
<div> supervised learning, text-attributed graph, node classification, semantic augmentation, text semantics

Summary:
The paper introduces Text Semantics Augmentation (TSA) as a method to enhance few- and zero-shot node classification on Text-attributed graphs. TSA incorporates positive semantics matching and negative semantics contrast techniques to provide additional reference texts for graph nodes. By matching similar embeddings and introducing contrasting semantics, TSA achieves higher accuracy compared to 13 state-of-the-art baselines across 5 datasets. Results demonstrate consistent improvements of over 5% in accuracy, showcasing the efficacy of TSA in leveraging text semantics for improved node classification on TAGs. 

<br /><br />Summary: <div>
arXiv:2505.08168v1 Announce Type: new 
Abstract: Text-attributed graph (TAG) provides a text description for each graph node, and few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. Existing work utilizes various graph-based augmentation techniques to train the node and text embeddings, while text-based augmentations are largely unexplored. In this paper, we propose Text Semantics Augmentation (TSA) to improve accuracy by introducing more text semantic supervision signals. Specifically, we design two augmentation techniques, i.e., positive semantics matching and negative semantics contrast, to provide more reference texts for each graph node or text description. Positive semantic matching retrieves texts with similar embeddings to match with a graph node. Negative semantic contrast adds a negative prompt to construct a text description with the opposite semantics, which is contrasted with the original node and text. We evaluate TSA on 5 datasets and compare with 13 state-of-the-art baselines. The results show that TSA consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs</title>
<link>https://arxiv.org/abs/2505.08200</link>
<guid>https://arxiv.org/abs/2505.08200</guid>
<content:encoded><![CDATA[
<div> hallucination, Large Language Models, uncertainty quantification, pre-trained UQ heads, detection

Summary:<br /><br />Large Language Models (LLMs) often produce false information known as hallucinations. To address this issue, researchers have introduced pre-trained uncertainty quantification (UQ) heads, auxiliary modules that improve the models' ability to assess the reliability of their outputs. These UQ heads leverage the Transformer architecture and LLM attention maps to effectively detect hallucinations at the claim level. Experimental results demonstrate that the pre-trained UQ heads achieve state-of-the-art performance in hallucination detection across various prompts, including out-of-domain ones. They also exhibit strong generalization to languages not explicitly trained on. The study includes pre-training UQ heads for popular LLM series like Mistral, Llama, and Gemma 2, with the code and pre-trained heads publicly available for further research and use. <div>
arXiv:2505.08200v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have the tendency to hallucinate, i.e., to sporadically generate false or fabricated information. This presents a major challenge, as hallucinations often appear highly convincing and users generally lack the tools to detect them. Uncertainty quantification (UQ) provides a framework for assessing the reliability of model outputs, aiding in the identification of potential hallucinations. In this work, we introduce pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially enhance their ability to capture uncertainty compared to unsupervised UQ methods. Their strong performance stems from the powerful Transformer architecture in their design and informative features derived from LLM attention maps. Experimental evaluation shows that these heads are highly robust and achieve state-of-the-art performance in claim-level hallucination detection across both in-domain and out-of-domain prompts. Moreover, these modules demonstrate strong generalization to languages they were not explicitly trained on. We pre-train a collection of UQ heads for popular LLM series, including Mistral, Llama, and Gemma 2. We publicly release both the code and the pre-trained heads.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement</title>
<link>https://arxiv.org/abs/2505.08245</link>
<guid>https://arxiv.org/abs/2505.08245</guid>
<content:encoded><![CDATA[
<div> Psychometrics, Large language models, Evaluation, Human-like psychological constructs, Benchmarking <br />
<br />
Summary: This survey paper introduces the emerging interdisciplinary field of LLM Psychometrics, which utilizes psychometric instruments, theories, and principles to evaluate and enhance large language models. It addresses challenges in traditional evaluation methodologies by incorporating human-like psychological constructs, broadening evaluation scopes, and establishing human-centered evaluation approaches. The paper explores how Psychometrics can shape benchmarking principles, refine methodologies, validate results, and advance LLM capabilities. By integrating diverse perspectives, it provides a structured framework for researchers to understand and develop evaluation paradigms that align with human-level AI. The ultimate goal is to promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is also available to support further research and development in this field. <br /><br /> <div>
arXiv:2505.08245v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration</title>
<link>https://arxiv.org/abs/2505.08261</link>
<guid>https://arxiv.org/abs/2505.08261</guid>
<content:encoded><![CDATA[
<div> Cache-Augmented Generation, Adaptive Contextual Compression, Hybrid CAG-RAG Framework, large language models, knowledge integration

Summary:
Adaptive Contextual Compression (ACC) is introduced as a technique to dynamically compress and manage context inputs, allowing efficient use of extended memory in large language models. The Hybrid CAG-RAG Framework integrates selective retrieval to complement preloaded contexts, offering a solution for scenarios requiring additional information. This approach enhances scalability, optimizes efficiency, and improves multi-hop reasoning performance in knowledge-intensive tasks. The paper addresses challenges in scaling Cache-Augmented Generation (CAG) by introducing innovative methods to effectively manage large and dynamic knowledge bases. The proposed techniques offer practical solutions for real-world knowledge integration challenges, providing a promising alternative to traditional Retrieval-Augmented Generation (RAG) approaches. Comprehensive evaluations on diverse datasets demonstrate the effectiveness of the proposed methods in enhancing system design and minimizing retrieval latency. <div>
arXiv:2505.08261v1 Announce Type: new 
Abstract: The rapid progress in large language models (LLMs) has paved the way for novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented Generation (RAG). CAG minimizes retrieval latency and simplifies system design by preloading knowledge into the model's context. However, challenges persist in scaling CAG to accommodate large and dynamic knowledge bases effectively. This paper introduces Adaptive Contextual Compression (ACC), an innovative technique designed to dynamically compress and manage context inputs, enabling efficient utilization of the extended memory capabilities of modern LLMs. To further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG Framework, which integrates selective retrieval to augment preloaded contexts in scenarios requiring additional information. Comprehensive evaluations on diverse datasets highlight the proposed methods' ability to enhance scalability, optimize efficiency, and improve multi-hop reasoning performance, offering practical solutions for real-world knowledge integration challenges.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow</title>
<link>https://arxiv.org/abs/2505.08303</link>
<guid>https://arxiv.org/abs/2505.08303</guid>
<content:encoded><![CDATA[
<div> Keywords: Black-Box prompt optimization, Large language models, DeepSeek V3, Gemini 2.0 Flash, Scaling law <br />
Summary: 
The study examines the effectiveness of black-box prompt optimization methods on large language models (LLMs) such as DeepSeek V3 and Gemini 2.0 Flash. While these methods have shown promise with smaller-scale models, their impact on larger-scale LLMs is limited. Experimentation on LLMs of varying sizes, including the Qwen 2.5 series ranging from 7B to 72B, reveals an inverse scaling law where the benefits of black-box optimization decrease as model size increases. The primary factor contributing to the limited improvement is theorized to be the scale of the model itself. The research suggests that as LLMs continue to grow in size, the effectiveness of black-box optimization techniques may diminish, raising questions about their utility for enhancing task performance in extremely large models. <br /><br />Summary: <div>
arXiv:2505.08303v1 Announce Type: new 
Abstract: Black-Box prompt optimization methods have emerged as a promising strategy for refining input prompts to better align large language models (LLMs), thereby enhancing their task performance. Although these methods have demonstrated encouraging results, most studies and experiments have primarily focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g., GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with DeepSeek V3 (671B), it remains an open question whether these black-box optimization techniques will continue to yield significant performance improvements for models of such scale. In response to this, we select three well-known black-box optimization methods and evaluate them on large-scale LLMs (DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The results show that these black-box prompt optimization methods offer only limited improvements on these large-scale LLMs. Furthermore, we hypothesize that the scale of the model is the primary factor contributing to the limited benefits observed. To explore this hypothesis, we conducted experiments on LLMs of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an inverse scaling law, wherein the effectiveness of black-box optimization methods diminished as the model size increased.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale</title>
<link>https://arxiv.org/abs/2505.08311</link>
<guid>https://arxiv.org/abs/2505.08311</guid>
<content:encoded><![CDATA[
<div> Keywords: AM-Thinking-v1, dense language model, reasoning, open-source, collaborative spirit<br />
Summary:<br />
AM-Thinking-v1 is a new 32B dense language model that excels in reasoning tasks, surpassing DeepSeek-R1 and competing with other MoE models like Qwen3-235B-A22B. Achieving high scores on AIME 2024, AIME 2025, and LiveCodeBench, this model showcases state-of-the-art mathematical and coding capabilities. Built on the open-source Qwen2.5-32B base model and utilizing publicly available queries, AM-Thinking-v1 leverages supervised fine-tuning and reinforcement learning for exceptional performance. Demonstrating the potential of open-source collaboration, this model strikes a balance between performance and usability at the 32B scale, encouraging further collaborative efforts in mid-scale models. The model's code is open-source on Hugging Face, aiming to inspire innovation while maintaining accessibility as a core value.

<br /><br />Summary: <div>
arXiv:2505.08311v1 Announce Type: new 
Abstract: We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.
  Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on \href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Geometry of Semantics in Next-token Prediction</title>
<link>https://arxiv.org/abs/2505.08348</link>
<guid>https://arxiv.org/abs/2505.08348</guid>
<content:encoded><![CDATA[
<div> Singular Value Decomposition, Language Models, Distributional Semantics, Neural Network Training, Semantic Concepts
Summary:
Modern language models can capture linguistic meaning well through next-token prediction (NTP) training. The optimization process of NTP leads models to encode semantic and grammatical concepts using singular value decomposition (SVD) factors of a data-sparsity matrix. Although the model does not explicitly construct this matrix, it effectively factors it through learned word and context embeddings to capture linguistic structure. The most important SVD factors are learned early in training, supporting the use of spectral clustering of embeddings to identify human-interpretable semantics. Traditional k-means and a new orthant-based method can be employed for this purpose. The study connects distributional semantics, neural network training dynamics, and neural collapse geometry to explain how NTP's implicit biases influence the emergence of meaning representations in language models. 
<br /><br />Summary: <div>
arXiv:2505.08348v1 Announce Type: new 
Abstract: Modern language models demonstrate a remarkable ability to capture linguistic meaning despite being trained solely through next-token prediction (NTP). We investigate how this conceptually simple training objective leads models to extract and encode latent semantic and grammatical concepts. Our analysis reveals that NTP optimization implicitly guides models to encode concepts via singular value decomposition (SVD) factors of a centered data-sparsity matrix that captures next-word co-occurrence patterns. While the model never explicitly constructs this matrix, learned word and context embeddings effectively factor it to capture linguistic structure. We find that the most important SVD factors are learned first during training, motivating the use of spectral clustering of embeddings to identify human-interpretable semantics, including both classical k-means and a new orthant-based method directly motivated by our interpretation of concepts. Overall, our work bridges distributional semantics, neural collapse geometry, and neural network training dynamics, providing insights into how NTP's implicit biases shape the emergence of meaning representations in language models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring</title>
<link>https://arxiv.org/abs/2505.08351</link>
<guid>https://arxiv.org/abs/2505.08351</guid>
<content:encoded><![CDATA[
<div> adaptive tutors, Large Language Models, second-language learning, proficiency levels, system prompting

Summary:
This paper explores the use of Large Language Models (LLMs) as adaptive tutors for second-language learning. The study evaluates whether system prompting can effectively control the text generated by LLMs to match students' proficiency levels. Simulated teacher-student dialogues in Spanish are conducted using LLMs of varying sizes. The results indicate that while system prompting can restrict model outputs, it may not be enough for long-term interactions due to alignment drift. This study provides insights into the potential of LLMs as personalized language tutors aligned with proficiency levels and offers a cost-effective method for evaluating model performance without human participants.
<br /><br />Summary: <div>
arXiv:2505.08351v1 Announce Type: new 
Abstract: This paper investigates the potentials of Large Language Models (LLMs) as adaptive tutors in the context of second-language learning. In particular, we evaluate whether system prompting can reliably constrain LLMs to generate only text appropriate to the student's competence level. We simulate full teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs ranging in size from 7B to 12B parameters. Dialogues are generated by having an LLM alternate between tutor and student roles with separate chat histories. The output from the tutor model is then used to evaluate the effectiveness of CEFR-based prompting to control text difficulty across three proficiency levels (A1, B1, C1). Our findings suggest that while system prompting can be used to constrain model outputs, prompting alone is too brittle for sustained, long-term interactional contexts - a phenomenon we term alignment drift. Our results provide insights into the feasibility of LLMs for personalized, proficiency-aligned adaptive tutors and provide a scalable method for low-cost evaluation of model performance without human participants.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Contamination Resistant Benchmarks</title>
<link>https://arxiv.org/abs/2505.08389</link>
<guid>https://arxiv.org/abs/2505.08389</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Evaluation, Contamination, Benchmark, Caeser Ciphers

Summary:
Large Language Models (LLMs) have become dominant in natural language processing, but evaluating them properly is essential for understanding their capabilities and safety. Contamination, where irrelevant information affects evaluation results, is a significant challenge in LLM evaluation. To address this issue, the concept of contamination resistance is introduced. A benchmark using Caesar ciphers is proposed as a contamination-resistant measure. Testing this benchmark on various LLMs shows that the models struggle with controlled contamination, highlighting weaknesses. These findings expose limitations in current LLMs and prompt questions about their actual capabilities. Developing contamination-resistant benchmarks is crucial for rigorous LLM evaluation, providing insights into their true potential. <br /><br />Summary: Large language models (LLMs) are transforming natural language processing, but contamination in evaluations undermines their reliability. Introducing the concept of contamination resistance, a benchmark based on Caesar ciphers is proposed as a reliable measure. Testing this benchmark reveals challenges for LLMs when faced with contamination, raising important questions about their capabilities and highlighting the need for more rigorous evaluation methods. <div>
arXiv:2505.08389v1 Announce Type: new 
Abstract: The rapid development of large language models (LLMs) has transformed the landscape of natural language processing. Evaluating LLMs properly is crucial for understanding their potential and addressing concerns such as safety. However, LLM evaluation is confronted by various factors, among which contamination stands out as a key issue that undermines the reliability of evaluations. In this work, we introduce the concept of contamination resistance to address this challenge. We propose a benchmark based on Caesar ciphers (e.g., "ab" to "bc" when the shift is 1), which, despite its simplicity, is an excellent example of a contamination resistant benchmark. We test this benchmark on widely used LLMs under various settings, and we find that these models struggle with this benchmark when contamination is controlled. Our findings reveal issues in current LLMs and raise important questions regarding their true capabilities. Our work contributes to the development of contamination resistant benchmarks, enabling more rigorous LLM evaluation and offering insights into the true capabilities and limitations of LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping</title>
<link>https://arxiv.org/abs/2505.08392</link>
<guid>https://arxiv.org/abs/2505.08392</guid>
<content:encoded><![CDATA[
<div> Adaptive GoGI-Skip, CoT compression, dynamic compression, Goal-Gradient Importance, supervised fine-tuning <br />
<br />
Summary:
Adaptive GoGI-Skip is a framework for dynamic Chain-of-Thought (CoT) compression in Large Language Models, aiming to improve efficiency without compromising accuracy. It introduces Goal-Gradient Importance (GoGI) for identifying important tokens and Adaptive Dynamic Skipping (ADS) for dynamic compression rates based on model uncertainty. Trained on MATH data, it shows strong generalization across various reasoning benchmarks. The framework reduces CoT token counts by over 45% on average, resulting in 1.6-2.0 times inference speedups. It outperforms existing methods by maintaining high accuracy even at high compression rates, striking a balance between efficiency and accuracy in CoT reasoning tasks. <br /> <div>
arXiv:2505.08392v1 Announce Type: new 
Abstract: Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers</title>
<link>https://arxiv.org/abs/2505.08402</link>
<guid>https://arxiv.org/abs/2505.08402</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, natural language understanding, tool integration, parameter-level processing, TUMS framework

Summary:
The article introduces the TUMS framework, which aims to enhance the tool-use capabilities of large language models (LLMs) by transforming tool-level processing into parameter-level processing. The framework includes an intent recognizer, task decomposer, subtask processor, and executor to improve LLMs' understanding and generation of precise responses. Empirical studies show significant improvements in performance on ToolQA benchmarks, with a 19.6% increase on easy tasks and a 50.6% increase on hard tasks. Ablation experiments highlight the key contributions of each component, providing valuable insights for future research on Tool-augmented LLMs.<br /><br />Summary: <div>
arXiv:2505.08402v1 Announce Type: new 
Abstract: Recently, large language models(LLMs) have played an increasingly important role in solving a wide range of NLP tasks, leveraging their capabilities of natural language understanding and generating. Integration with external tools further enhances LLMs' effectiveness, providing more precise, timely, and specialized responses. However, LLMs still encounter difficulties with non-executable actions and improper actions, which are primarily attributed to incorrect parameters. The process of generating parameters by LLMs is confined to the tool level, employing the coarse-grained strategy without considering the different difficulties of various tools. To address this issue, we propose TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs by transforming tool-level processing into parameter-level processing. Specifically, our framework consists of four key components: (1) an intent recognizer that identifies the user's intent to help LLMs better understand the task; (2) a task decomposer that breaks down complex tasks into simpler subtasks, each involving a tool call; (3) a subtask processor equipped with multi-structure handlers to generate accurate parameters; and (4) an executor. Our empirical studies have evidenced the effectiveness and efficiency of the TUMS framework with an average of 19.6\% and 50.6\% improvement separately on easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key contribution of each part with ablation experiments, offering more insights and stimulating future research on Tool-augmented LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hakim: Farsi Text Embedding Model</title>
<link>https://arxiv.org/abs/2505.08435</link>
<guid>https://arxiv.org/abs/2505.08435</guid>
<content:encoded><![CDATA[
<div> embedding, Persian, language model, chatbots, retrieval-augmented generation

Summary:
Hakim is a state-of-the-art Persian text embedding model that outperforms existing approaches on the FaMTEB benchmark. It achieves an 8.5% performance improvement and introduces three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - for supervised and unsupervised training scenarios. Designed for chatbots and retrieval-augmented generation (RAG) systems, Hakim is particularly useful for retrieval tasks involving message history. The model, based on the BERT architecture, consistently achieves higher accuracy in Persian NLP tasks. Additionally, the RetroMAE-based model is effective for textual information retrieval applications. These advancements in Persian language understanding lay the groundwork for further research and development in text embedding and NLP tasks for the Persian language.
<br /><br />Summary: <div>
arXiv:2505.08435v1 Announce Type: new 
Abstract: Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court</title>
<link>https://arxiv.org/abs/2505.08439</link>
<guid>https://arxiv.org/abs/2505.08439</guid>
<content:encoded><![CDATA[
<div> YOLOv8x, optical character recognition, text anonymization, document processing pipeline, legal research <br />
Summary: 
A document processing pipeline was developed to create an anonymized dataset for topic modeling in Italian legal research, overcoming the lack of public datasets. The pipeline integrated document layout analysis using YOLOv8x, optical character recognition (OCR), and text anonymization. Results showed high performance in document layout analysis, OCR detection, and text recognition, improving topic modeling compared to OCR-only methods. The pipeline utilized BERTopic for topic extraction and large language models for label and summary generation, with outputs evaluated against expert interpretations. Claude Sonnet 3.7 achieved high BERTScore F1 for labeling and summarization. Overall, the pipeline successfully generated an optimized dataset for analyzing legal themes in Supreme Court judgments, enhancing the efficiency and accuracy of topic modeling in legal research. <br /><br />Summary: <div>
arXiv:2505.08439v1 Announce Type: new 
Abstract: Topic modeling in Italian legal research is hindered by the lack of public datasets, limiting the analysis of legal themes in Supreme Court judgments. To address this, we developed a document processing pipeline that produces an anonymized dataset optimized for topic modeling.
  The pipeline integrates document layout analysis (YOLOv8x), optical character recognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964 and a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and the text recognizer (TrOCR) obtained a character error rate of 0.0047 and a word error rate of 0.0248. Compared to OCR-only methods, our dataset improved topic modeling with a diversity score of 0.6198 and a coherence score of 0.6638.
  We applied BERTopic to extract topics and used large language models to generate labels and summaries. Outputs were evaluated against domain expert interpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for labeling and 0.9130 for summarization.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.08450</link>
<guid>https://arxiv.org/abs/2505.08450</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, IterKey, LLMs, sparse retrieval, interpretability

Summary: 
IterKey is a framework that enhances Retrieval-Augmented Generation (RAG) by integrating sparse retrieval methods with Large Language Models (LLMs). It consists of three stages driven by LLMs - generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If the validation fails, the process iteratively refines the keywords. Experimental results across four QA tasks show that IterKey improves accuracy by 5% to 20% over BM25-based RAG and simple baselines. It is comparable in performance to dense retrieval-based RAG and previous iterative query refinement methods using dense models. IterKey effectively balances accuracy and interpretability, offering a novel approach for enhancing RAG through iterative keyword generation. 

Summary: <div>
arXiv:2505.08450v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a way to complement the in-context knowledge of Large Language Models (LLMs) by integrating external documents. However, real-world applications demand not only accuracy but also interpretability. While dense retrieval methods provide high accuracy, they lack interpretability; conversely, sparse retrieval methods offer transparency but often fail to capture the full intent of queries due to their reliance on keyword matching. To address these issues, we introduce IterKey, an LLM-driven iterative keyword generation framework that enhances RAG via sparse retrieval. IterKey consists of three LLM-driven stages: generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If validation fails, the process iteratively repeats with refined keywords. Across four QA tasks, experimental results show that IterKey achieves 5% to 20% accuracy improvements over BM25-based RAG and simple baselines. Its performance is comparable to dense retrieval-based RAG and prior iterative query refinement methods using dense models. In summary, IterKey is a novel BM25-based approach leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with interpretability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2505.08463</link>
<guid>https://arxiv.org/abs/2505.08463</guid>
<content:encoded><![CDATA[
<div> calibrated representation, pre-trained language models, downstream tasks, encoder-decoder architectures, RepCali
<br />
Summary:<br />
This article introduces a novel approach, called RepCali, for calibrating the representation of pre-trained language models (PLMs) in the latent space. By integrating a calibration block after the encoder, RepCali improves the discrepancies between encoder and decoder inputs. The method is universal for all PLMs with encoder-decoder architectures, easy to implement, and shows significant performance enhancements across various tasks and datasets. Extensive experiments on 25 PLM-based models demonstrate the effectiveness of RepCali in improving downstream task performance. Comparison experiments against fine-tuning baselines further confirm the superiority of RepCali in enhancing PLMs. <div>
arXiv:2505.08463v1 Announce Type: new 
Abstract: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs still struggle with the discrepancies between the representation obtained from the PLMs' encoder and the optimal input to the PLMs' decoder. This paper tackles this challenge by learning to calibrate the representation of PLMs in the latent space. In the proposed representation calibration method (RepCali), we integrate a specific calibration block to the latent space after the encoder and use the calibrated output as the decoder input. The merits of the proposed RepCali include its universality to all PLMs with encoder-decoder architectures, its plug-and-play nature, and ease of implementation. Extensive experiments on 25 PLM-based models across 8 tasks (including both English and Chinese datasets) demonstrate that the proposed RepCali offers desirable enhancements to PLMs (including LLMs) and significantly improves the performance of downstream tasks. Comparison experiments across 4 benchmark tasks indicate that RepCali is superior to the representative fine-tuning baselines.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2505.08464</link>
<guid>https://arxiv.org/abs/2505.08464</guid>
<content:encoded><![CDATA[
arXiv:2505.08464v1 Announce Type: new 
Abstract: Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?</title>
<link>https://arxiv.org/abs/2505.08468</link>
<guid>https://arxiv.org/abs/2505.08468</guid>
<content:encoded><![CDATA[
arXiv:2505.08468v1 Announce Type: new 
Abstract: Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.08498</link>
<guid>https://arxiv.org/abs/2505.08498</guid>
<content:encoded><![CDATA[
arXiv:2505.08498v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled zero-shot automated essay scoring (AES), providing a promising way to reduce the cost and effort of essay scoring in comparison with manual grading. However, most existing zero-shot approaches rely on LLMs to directly generate absolute scores, which often diverge from human evaluations owing to model biases and inconsistent scoring. To address these limitations, we propose LLM-based Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise comparison task. Specifically, we instruct LLMs to judge which of two essays is better, collect many such comparisons, and convert them into continuous scores. Considering that the number of possible comparisons grows quadratically with the number of essays, we improve scalability by employing RankNet to efficiently transform LLM preferences into scalar scores. Experiments using AES benchmark datasets show that LCES outperforms conventional zero-shot methods in accuracy while maintaining computational efficiency. Moreover, LCES is robust across different LLM backbones, highlighting its applicability to real-world zero-shot AES.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding</title>
<link>https://arxiv.org/abs/2505.08504</link>
<guid>https://arxiv.org/abs/2505.08504</guid>
<content:encoded><![CDATA[
arXiv:2505.08504v1 Announce Type: new 
Abstract: Sequence-to-sequence models are widely used to train Abstract Meaning Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR graphs have to be linearized into a one-line text format. While Penman encoding is typically used for this purpose, we argue that it has limitations: (1) for deep graphs, some closely related nodes are located far apart in the linearized text (2) Penman's tree-based encoding necessitates inverse roles to handle node re-entrancy, doubling the number of relation types to predict. To address these issues, we propose a triple-based linearization method and compare its efficiency with Penman linearization. Although triples are well suited to represent a graph, our results suggest room for improvement in triple encoding to better compete with Penman's concise and explicit representation of a nested graph structure.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation</title>
<link>https://arxiv.org/abs/2505.08546</link>
<guid>https://arxiv.org/abs/2505.08546</guid>
<content:encoded><![CDATA[
arXiv:2505.08546v1 Announce Type: new 
Abstract: While gender bias in modern Neural Machine Translation (NMT) systems has received much attention, traditional evaluation metrics do not to fully capture the extent to which these systems integrate contextual gender cues. We propose a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures the reliance of models on gender cues for gender disambiguation. MPA is designed to go beyond surface-level gender accuracy metrics by focusing on whether models adapt to gender cues in minimal pairs -- sentence pairs that differ solely in the gendered pronoun, namely the explicit indicator of the target's entity gender in the source language (EN). We evaluate a number of NMT models on the English-Italian (EN--IT) language pair using this metric, we show that they ignore available gender cues in most cases in favor of (statistical) stereotypical gender interpretation. We further show that in anti-stereotypical cases, these models tend to more consistently take masculine gender cues into account while ignoring the feminine cues. Furthermore, we analyze the attention head weights in the encoder component and show that while all models encode gender information to some extent, masculine cues elicit a more diffused response compared to the more concentrated and specialized responses to feminine gender cues.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small but Significant: On the Promise of Small Language Models for Accessible AIED</title>
<link>https://arxiv.org/abs/2505.08588</link>
<guid>https://arxiv.org/abs/2505.08588</guid>
<content:encoded><![CDATA[
arXiv:2505.08588v1 Announce Type: new 
Abstract: GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the field's predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact that small language models (SLMs) can make in providing resource-constrained institutions with equitable and affordable access to high-quality AI tools. Supported by positive results on knowledge component (KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as Phi-2 can produce an effective solution without elaborate prompting strategies. Hence, we call for more attention to developing SLM-based AIED approaches.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models</title>
<link>https://arxiv.org/abs/2505.08590</link>
<guid>https://arxiv.org/abs/2505.08590</guid>
<content:encoded><![CDATA[
arXiv:2505.08590v1 Announce Type: new 
Abstract: Advancements in artificial intelligence (AI) are transforming pathology by integrat-ing large language models (LLMs) with retrieval-augmented generation (RAG) and domain-specific foundation models. This study explores the application of RAG-enhanced LLMs coupled with pathology foundation models for thyroid cytology diagnosis, addressing challenges in cytological interpretation, standardization, and diagnostic accuracy. By leveraging a curated knowledge base, RAG facilitates dy-namic retrieval of relevant case studies, diagnostic criteria, and expert interpreta-tion, improving the contextual understanding of LLMs. Meanwhile, pathology foun-dation models, trained on high-resolution pathology images, refine feature extrac-tion and classification capabilities. The fusion of these AI-driven approaches en-hances diagnostic consistency, reduces variability, and supports pathologists in dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate that integrating RAG with pathology-specific LLMs significantly improves diagnostic efficiency and interpretability, paving the way for AI-assisted thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for correct prediction of surgi-cal pathology diagnosis from thyroid cytology samples.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Task Detection and Heterogeneous LLM Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.08600</link>
<guid>https://arxiv.org/abs/2505.08600</guid>
<content:encoded><![CDATA[
arXiv:2505.08600v1 Announce Type: new 
Abstract: Speculative decoding, which combines a draft model with a target model, has emerged as an effective approach to accelerate large language model (LLM) inference. However, existing methods often face a trade-off between the acceptance rate and decoding speed in downstream tasks due to the limited capacity of the draft model, making it difficult to ensure efficiency across diverse tasks. To address this problem, we propose a speculative decoding algorithm tailored for downstream task optimization. It includes an automatic task partitioning and assigning method, which automatically categorizes downstream tasks into different sub-tasks and assigns them to a set of heterogeneous draft models. Each draft model is aligned with the target model using task-specific data, thereby enhancing the consistency of inference results. In addition, our proposed method incorporates an online lightweight prompt classifier to dynamically route prompts to the appropriate draft model. Experimental results demonstrate that the proposed method improves draft accuracy by 6% to 50% over vanilla speculative decoding, while achieving a speedup of 1.10x to 2.64x in LLM inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing</title>
<link>https://arxiv.org/abs/2505.08651</link>
<guid>https://arxiv.org/abs/2505.08651</guid>
<content:encoded><![CDATA[
arXiv:2505.08651v1 Announce Type: new 
Abstract: We present MegaBeam-Mistral-7B, a language model that supports 512K-token context length. Our work addresses practical limitations in long-context training, supporting real-world tasks such as compliance monitoring and verification. Evaluated on three long-context benchmarks, our 7B-parameter model demonstrates superior in-context learning performance on HELMET and robust retrieval and tracing capability on RULER. It is currently the only open model to achieve competitive long-range reasoning on BABILong at 512K context length without RAG or targeted fine-tuning. Released as fully open source under the Apache 2.0 license, the model has been downloaded over 100,000 times on Hugging Face. Model available at: https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing economic facts: LLMs know more than they say</title>
<link>https://arxiv.org/abs/2505.08662</link>
<guid>https://arxiv.org/abs/2505.08662</guid>
<content:encoded><![CDATA[
arXiv:2505.08662v1 Announce Type: new 
Abstract: We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. We also propose a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, we demonstrate the practical utility of hidden-state representations in super-resolution and data imputation tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.08690</link>
<guid>https://arxiv.org/abs/2505.08690</guid>
<content:encoded><![CDATA[
arXiv:2505.08690v1 Announce Type: new 
Abstract: Event extraction (EE) is a fundamental task in natural language processing (NLP) that involves identifying and extracting event information from unstructured text. Effective EE in real-world scenarios requires two key steps: selecting appropriate schemas from hundreds of candidates and executing the extraction process. Existing research exhibits two critical gaps: (1) the rigid schema fixation in existing pipeline systems, and (2) the absence of benchmarks for evaluating joint schema matching and extraction. Although large language models (LLMs) offer potential solutions, their schema hallucination tendencies and context window limitations pose challenges for practical deployment. In response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel paradigm combining schema paraphrasing with schema retrieval-augmented generation. ASEE adeptly retrieves paraphrased schemas and accurately generates targeted structures. To facilitate rigorous evaluation, we construct the Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which systematically consolidates 12 datasets across diverse domains, complexity levels, and language settings. Extensive evaluations on MD-SEE show that our proposed ASEE demonstrates strong adaptability across various scenarios, significantly improving the accuracy of event extraction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context</title>
<link>https://arxiv.org/abs/2505.08734</link>
<guid>https://arxiv.org/abs/2505.08734</guid>
<content:encoded><![CDATA[
arXiv:2505.08734v1 Announce Type: new 
Abstract: This work introduces the first benchmark for nursing value alignment, consisting of five core value dimensions distilled from international nursing codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The benchmark comprises 1,100 real-world nursing behavior instances collected through a five-month longitudinal field study across three hospitals of varying tiers. These instances are annotated by five clinical nurses and then augmented with LLM-generated counterfactuals with reversed ethic polarity. Each original case is paired with a value-aligned and a value-violating version, resulting in 2,200 labeled instances that constitute the Easy-Level dataset. To increase adversarial complexity, each instance is further transformed into a dialogue-based format that embeds contextual cues and subtle misleading signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA) LLMs on their alignment with nursing values. Our findings reveal three key insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2) Justice is consistently the most difficult nursing value dimension to evaluate; and (3) in-context learning significantly improves alignment. This work aims to provide a foundation for value-sensitive LLMs development in clinical settings. The dataset and the code are available at https://huggingface.co/datasets/Ben012345/NurValues.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies</title>
<link>https://arxiv.org/abs/2505.08739</link>
<guid>https://arxiv.org/abs/2505.08739</guid>
<content:encoded><![CDATA[
arXiv:2505.08739v1 Announce Type: new 
Abstract: Can autoregressive large language models (LLMs) learn consistent probability distributions when trained on sequences in different token orders? We prove formally that for any well-defined probability distribution, sequence perplexity is invariant under any factorization, including forward, backward, or arbitrary permutations. This result establishes a rigorous theoretical foundation for studying how LLMs learn from data and defines principled protocols for empirical evaluation. Applying these protocols, we show that prior studies examining ordering effects suffer from critical methodological flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted orders on scientific text. We find systematic deviations from theoretical invariance across all orderings with arbitrary permutations strongly deviating from both forward and backward models, which largely (but not completely) agreed with one another. Deviations were traceable to differences in self-attention, reflecting positional and locality biases in processing. Our theoretical and empirical results provide novel avenues for understanding positional biases in LLMs and suggest methods for detecting when LLMs' probability distributions are inconsistent and therefore untrustworthy.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2505.08750</link>
<guid>https://arxiv.org/abs/2505.08750</guid>
<content:encoded><![CDATA[
arXiv:2505.08750v1 Announce Type: new 
Abstract: Actual causality (AC), a fundamental aspect of causal reasoning (CR), is responsible for attribution and responsibility assignment in real-world scenarios. However, existing LLM-based methods lack grounding in formal AC theory, resulting in limited interpretability. Therefore, we propose AC-Reason, a semi-formal reasoning framework that identifies causally relevant events within an AC scenario, infers the values of their formal causal factors (e.g., sufficiency, necessity, and normality), and answers AC queries via a theory-guided algorithm with explanations. While AC-Reason does not explicitly construct a causal graph, it operates over variables in the underlying causal structure to support principled reasoning. To enable comprehensive evaluation, we introduce AC-Bench, a new benchmark built upon and substantially extending Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully annotated samples, each with detailed reasoning steps and focuses solely on actual causation. The case study shows that synthesized samples in AC-Bench present greater challenges for LLMs. Extensive experiments on BBH-CJ and AC-Bench show that AC-Reason consistently improves LLM performance over baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 + AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further enables fine-grained analysis of reasoning faithfulness, revealing that only Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation study proves that integrating AC theory into LLMs is highly effective, with the proposed algorithm contributing the most significant performance gains.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aya Vision: Advancing the Frontier of Multilingual Multimodality</title>
<link>https://arxiv.org/abs/2505.08751</link>
<guid>https://arxiv.org/abs/2505.08751</guid>
<content:encoded><![CDATA[
arXiv:2505.08751v1 Announce Type: new 
Abstract: Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthBench: Evaluating Large Language Models Towards Improved Human Health</title>
<link>https://arxiv.org/abs/2505.08775</link>
<guid>https://arxiv.org/abs/2505.08775</guid>
<content:encoded><![CDATA[
arXiv:2505.08775v1 Announce Type: new 
Abstract: We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. We additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. We hope that HealthBench grounds progress towards model development and applications that benefit human health.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding</title>
<link>https://arxiv.org/abs/2505.07864</link>
<guid>https://arxiv.org/abs/2505.07864</guid>
<content:encoded><![CDATA[
arXiv:2505.07864v1 Announce Type: cross 
Abstract: Flowcharts are indispensable tools in software design and business-process analysis, yet current vision-language models (VLMs) frequently misinterpret the directional arrows and graph topology that set these diagrams apart from natural images. We introduce a seven-stage pipeline grouped into three broader processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical character recognition (OCR) to extract node text; and (3) construction of a structured prompt that guides the VLMs. Tested on a 90-question benchmark distilled from 30 annotated flowcharts, the method raises overall accuracy from 80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp); branch-result questions improve more modestly, and before-step questions remain difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same trends, reinforcing the advantage of explicit arrow encoding. Limitations include dependence on detector and OCR precision, the small evaluation set, and residual errors at nodes with multiple incoming edges. Future work will enlarge the benchmark with synthetic and handwritten flowcharts and assess the approach on Business Process Model and Notation (BPMN) and Unified Modeling Language (UML).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellVerse: Do Large Language Models Really Understand Cell Biology?</title>
<link>https://arxiv.org/abs/2505.07865</link>
<guid>https://arxiv.org/abs/2505.07865</guid>
<content:encoded><![CDATA[
arXiv:2505.07865v1 Announce Type: cross 
Abstract: Recent studies have demonstrated the feasibility of modeling single-cell data as natural languages and the potential of leveraging powerful large language models (LLMs) for understanding cell biology. However, a comprehensive evaluation of LLMs' performance on language-driven single-cell analysis tasks still remains unexplored. Motivated by this challenge, we introduce CellVerse, a unified language-centric question-answering benchmark that integrates four types of single-cell multi-omics data and encompasses three hierarchical levels of single-cell analysis tasks: cell type annotation (cell-level), drug response prediction (drug-level), and perturbation analysis (gene-level). Going beyond this, we systematically evaluate the performance across 14 open-source and closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail to make reasonable decisions across all sub-tasks within CellVerse, while generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit preliminary understanding capabilities within the realm of cell biology. (2) The performance of current LLMs falls short of expectations and has substantial room for improvement. Notably, in the widely studied drug response prediction task, none of the evaluated LLMs demonstrate significant performance improvement over random guessing. CellVerse offers the first large-scale empirical demonstration that significant challenges still remain in applying LLMs to cell biology. By introducing CellVerse, we lay the foundation for advancing cell biology through natural languages and hope this paradigm could facilitate next-generation single-cell analysis.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach</title>
<link>https://arxiv.org/abs/2505.07902</link>
<guid>https://arxiv.org/abs/2505.07902</guid>
<content:encoded><![CDATA[
arXiv:2505.07902v1 Announce Type: cross 
Abstract: Classroom discourse is an essential vehicle through which teaching and learning take place. Assessing different characteristics of discursive practices and linking them to student learning achievement enhances the understanding of teaching quality. Traditional assessments rely on manual coding of classroom observation protocols, which is time-consuming and costly. Despite many studies utilizing AI techniques to analyze classroom discourse at the utterance level, investigations into the evaluation of discursive practices throughout an entire lesson segment remain limited. To address this gap, our study proposes a novel text-centered multimodal fusion architecture to assess the quality of three discourse components grounded in the Global Teaching InSights (GTI) observation protocol: Nature of Discourse, Questioning, and Explanations. First, we employ attention mechanisms to capture inter- and intra-modal interactions from transcript, audio, and video streams. Second, a multi-task learning approach is adopted to jointly predict the quality scores of the three components. Third, we formulate the task as an ordinal classification problem to account for rating level order. The effectiveness of these designed elements is demonstrated through an ablation study on the GTI Germany dataset containing 92 videotaped math lessons. Our results highlight the dominant role of text modality in approaching this task. Integrating acoustic features enhances the model's consistency with human ratings, achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to human inter-rater reliability (0.326). Our study lays the groundwork for the future development of automated discourse quality assessment to support teacher professional development through timely feedback on multidimensional discourse practices.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny</title>
<link>https://arxiv.org/abs/2505.07908</link>
<guid>https://arxiv.org/abs/2505.07908</guid>
<content:encoded><![CDATA[
arXiv:2505.07908v1 Announce Type: cross 
Abstract: In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA (Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts</title>
<link>https://arxiv.org/abs/2505.07912</link>
<guid>https://arxiv.org/abs/2505.07912</guid>
<content:encoded><![CDATA[
arXiv:2505.07912v1 Announce Type: cross 
Abstract: Democratic societies need accessible, reliable information. Videos and Podcasts have established themselves as the medium of choice for civic dissemination, but also as carriers of misinformation. The emerging Science Communication Knowledge Infrastructure (SciCom KI) curating non-textual media is still fragmented and not adequately equipped to scale against the content flood. Our work sets out to support the SciCom KI with a central, collaborative platform, the SciCom Wiki, to facilitate FAIR (findable, accessible, interoperable, reusable) media representation and the fact-checking of their content, particularly for videos and podcasts. Building an open-source service system centered around Wikibase, we survey requirements from 53 stakeholders, refine these in 11 interviews, and evaluate our prototype based on these requirements with another 14 participants. To address the most requested feature, fact-checking, we developed a neurosymbolic computational fact-checking approach, converting heterogenous media into knowledge graphs. This increases machine-readability and allows comparing statements against equally represented ground-truth. Our computational fact-checking tool was iteratively evaluated through 10 expert interviews, a public user survey with 43 participants verified the necessity and usability of our tool. Overall, our findings identified several needs to systematically support the SciCom KI. The SciCom Wiki, as a FAIR digital library complementing our neurosymbolic computational fact-checking framework, was found suitable to address the raised requirements. Further, we identified that the SciCom KI is severely underdeveloped regarding FAIR knowledge and related systems facilitating its collaborative creation and curation. Our system can provide a central knowledge node, yet a collaborative effort is required to scale against the imminent (mis-)information flood.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition</title>
<link>https://arxiv.org/abs/2505.08052</link>
<guid>https://arxiv.org/abs/2505.08052</guid>
<content:encoded><![CDATA[
arXiv:2505.08052v1 Announce Type: cross 
Abstract: This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.08080</link>
<guid>https://arxiv.org/abs/2505.08080</guid>
<content:encoded><![CDATA[
arXiv:2505.08080v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) have recently emerged as powerful tools for interpreting and steering the internal representations of large language models (LLMs). However, conventional approaches to analyzing SAEs typically rely solely on input-side activations, without considering the causal influence between each latent feature and the model's output. This work is built on two key hypotheses: (1) activated latents do not contribute equally to the construction of the model's output, and (2) only latents with high causal influence are effective for model steering. To validate these hypotheses, we propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method that identifies the most influential latents by incorporating output-side gradient information.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Computer-Aided Design: A Survey</title>
<link>https://arxiv.org/abs/2505.08137</link>
<guid>https://arxiv.org/abs/2505.08137</guid>
<content:encoded><![CDATA[
arXiv:2505.08137v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have seen rapid advancements in recent years, with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities across diverse domains. While substantial research has been conducted on LLMs in various fields, a comprehensive review focusing on their integration with Computer-Aided Design (CAD) remains notably absent. CAD is the industry standard for 3D modeling and plays a vital role in the design and development of products across different industries. As the complexity of modern designs increases, the potential for LLMs to enhance and streamline CAD workflows presents an exciting frontier. This article presents the first systematic survey exploring the intersection of LLMs and CAD. We begin by outlining the industrial significance of CAD, highlighting the need for AI-driven innovation. Next, we provide a detailed overview of the foundation of LLMs. We also examine both closed-source LLMs as well as publicly available models. The core of this review focuses on the various applications of LLMs in CAD, providing a taxonomy of six key areas where these models are making considerable impact. Finally, we propose several promising future directions for further advancements, which offer vast opportunities for innovation and are poised to shape the future of CAD technology. Github: https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem</title>
<link>https://arxiv.org/abs/2505.08148</link>
<guid>https://arxiv.org/abs/2505.08148</guid>
<content:encoded><![CDATA[
arXiv:2505.08148v1 Announce Type: cross 
Abstract: Millions of users leverage generative pretrained transformer (GPT)-based language models developed by leading model providers for a wide range of tasks. To support enhanced user interaction and customization, many platforms-such as OpenAI-now enable developers to create and publish tailored model instances, known as custom GPTs, via dedicated repositories or application stores. These custom GPTs empower users to browse and interact with specialized applications designed to meet specific needs. However, as custom GPTs see growing adoption, concerns regarding their security vulnerabilities have intensified. Existing research on these vulnerabilities remains largely theoretical, often lacking empirical, large-scale, and statistically rigorous assessments of associated risks.
  In this study, we analyze 14,904 custom GPTs to assess their susceptibility to seven exploitable threats, such as roleplay-based attacks, system prompt leakage, phishing content generation, and malicious code synthesis, across various categories and popularity tiers within the OpenAI marketplace. We introduce a multi-metric ranking system to examine the relationship between a custom GPT's popularity and its associated security risks.
  Our findings reveal that over 95% of custom GPTs lack adequate security protections. The most prevalent vulnerabilities include roleplay-based vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing (91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit inherent security weaknesses, which are often inherited or amplified in custom GPTs. These results highlight the urgent need for enhanced security measures and stricter content moderation to ensure the safe deployment of GPT-based applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not that Groove: Zero-Shot Symbolic Music Editing</title>
<link>https://arxiv.org/abs/2505.08203</link>
<guid>https://arxiv.org/abs/2505.08203</guid>
<content:encoded><![CDATA[
arXiv:2505.08203v1 Announce Type: cross 
Abstract: Most work in AI music generation focused on audio, which has seen limited use in the music production industry due to its rigidity. To maximize flexibility while assuming only textual instructions from producers, we are among the first to tackle symbolic music editing. We circumvent the known challenge of lack of labeled data by proving that LLMs with zero-shot prompting can effectively edit drum grooves. The recipe of success is a creatively designed format that interfaces LLMs and music, while we facilitate evaluation by providing an evaluation dataset with annotated unit tests that highly aligns with musicians' judgment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency</title>
<link>https://arxiv.org/abs/2505.08445</link>
<guid>https://arxiv.org/abs/2505.08445</guid>
<content:encoded><![CDATA[
arXiv:2505.08445v1 Announce Type: cross 
Abstract: Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models</title>
<link>https://arxiv.org/abs/2505.08622</link>
<guid>https://arxiv.org/abs/2505.08622</guid>
<content:encoded><![CDATA[
arXiv:2505.08622v1 Announce Type: cross 
Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAIL: Trace Reasoning and Agentic Issue Localization</title>
<link>https://arxiv.org/abs/2505.08638</link>
<guid>https://arxiv.org/abs/2505.08638</guid>
<content:encoded><![CDATA[
arXiv:2505.08638v1 Announce Type: cross 
Abstract: The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs</title>
<link>https://arxiv.org/abs/2505.08704</link>
<guid>https://arxiv.org/abs/2505.08704</guid>
<content:encoded><![CDATA[
arXiv:2505.08704v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization-Compression Cycles Improve Generalization</title>
<link>https://arxiv.org/abs/2505.08727</link>
<guid>https://arxiv.org/abs/2505.08727</guid>
<content:encoded><![CDATA[
arXiv:2505.08727v1 Announce Type: cross 
Abstract: We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodePDE: An Inference Framework for LLM-driven PDE Solver Generation</title>
<link>https://arxiv.org/abs/2505.08783</link>
<guid>https://arxiv.org/abs/2505.08783</guid>
<content:encoded><![CDATA[
arXiv:2505.08783v1 Announce Type: cross 
Abstract: Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling -- all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at https://github.com/LithiumDA/CodePDE.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions</title>
<link>https://arxiv.org/abs/2408.06787</link>
<guid>https://arxiv.org/abs/2408.06787</guid>
<content:encoded><![CDATA[
arXiv:2408.06787v3 Announce Type: replace 
Abstract: Traditional knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, making them promising for mitigating the limitations of previous methods. Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results.In this work, we aim to leverage LLMs for KGC effectively and efficiently. We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC. Additionally, to reduce ambiguity and enrich knowledge representation, we generate detailed entity descriptions through subgraph sampling on KGs. Extensive experiments on standard benchmarks demonstrate the efficiency and effectiveness of our approach. We outperform traditional KGC methods across most datasets and, notably, achieve classification performance comparable to fine-tuned LLMs while enhancing GPU memory efficiency by $188\times$ and accelerating training and inference by $13.48\times$.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying the Effects of Collaboration in Interactive Theme Discovery Systems</title>
<link>https://arxiv.org/abs/2408.09030</link>
<guid>https://arxiv.org/abs/2408.09030</guid>
<content:encoded><![CDATA[
arXiv:2408.09030v2 Announce Type: replace 
Abstract: NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks</title>
<link>https://arxiv.org/abs/2409.04168</link>
<guid>https://arxiv.org/abs/2409.04168</guid>
<content:encoded><![CDATA[
arXiv:2409.04168v2 Announce Type: replace 
Abstract: To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. The performance of LLM judges is typically evaluated by measuring the correlation with human judgments on generative tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that easy samples are easy to judge, and difficult samples are difficult to judge. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance, indicating that judges tend to favor higher-quality models even if their answer is incorrect. As a consequence, we test whether we can predict the behavior of LLM judges using simple features such as part-of-speech tags and find that we can correctly predict 70%-75% of judgments. We conclude this study by analyzing practical use cases, showing that LLM judges consistently detect the on-average better model but largely fail if we use them to improve task performance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Round and Round We Go! What makes Rotary Positional Encodings useful?</title>
<link>https://arxiv.org/abs/2410.06205</link>
<guid>https://arxiv.org/abs/2410.06205</guid>
<content:encoded><![CDATA[
arXiv:2410.06205v3 Announce Type: replace 
Abstract: Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust "positional" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CursorCore: Assist Programming through Aligning Anything</title>
<link>https://arxiv.org/abs/2410.07002</link>
<guid>https://arxiv.org/abs/2410.07002</guid>
<content:encoded><![CDATA[
arXiv:2410.07002v3 Announce Type: replace 
Abstract: Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Preference Left Behind: Group Distributional Preference Optimization</title>
<link>https://arxiv.org/abs/2412.20299</link>
<guid>https://arxiv.org/abs/2412.20299</guid>
<content:encoded><![CDATA[
arXiv:2412.20299v2 Announce Type: replace 
Abstract: Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These methods often skew toward dominant preferences, overlooking the diversity of opinions, especially when conflicting preferences arise. To address this issue, we propose Group Distributional Preference Optimization (GDPO), a novel framework that aligns language models with the distribution of preferences within a group by incorporating the concept of beliefs that shape individual preferences. GDPO calibrates a language model using statistical estimation of the group's belief distribution and aligns the model with belief-conditioned preferences, offering a more inclusive alignment framework than traditional methods. In experiments using both synthetic controllable opinion generation and real-world movie review datasets, we show that DPO fails to align with the targeted belief distributions, while GDPO consistently reduces this alignment gap during training. Moreover, our evaluation metrics demonstrate that GDPO outperforms existing approaches in aligning with group distributional preferences, marking a significant advance in pluralistic alignment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureVision: A methodology for the investigation of future cognition</title>
<link>https://arxiv.org/abs/2502.01597</link>
<guid>https://arxiv.org/abs/2502.01597</guid>
<content:encoded><![CDATA[
arXiv:2502.01597v2 Announce Type: replace 
Abstract: This paper presents a methodology combining multimodal semantic analysis with an eye-tracking experimental protocol to investigate the cognitive effort involved in understanding the communication of future scenarios. To demonstrate the methodology, we conduct a pilot study examining how visual fixation patterns vary during the evaluation of valence and counterfactuality in fictional ad pieces describing futuristic scenarios, using a portable eye tracker. Participants eye movements are recorded while evaluating the stimuli and describing them to a conversation partner. Gaze patterns are analyzed alongside semantic representations of the stimuli and participants descriptions, constructed from a frame semantic annotation of both linguistic and visual modalities. Preliminary results show that far-future and pessimistic scenarios are associated with longer fixations and more erratic saccades, supporting the hypothesis that fractures in the base spaces underlying the interpretation of future scenarios increase cognitive load for comprehenders.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals</title>
<link>https://arxiv.org/abs/2502.04066</link>
<guid>https://arxiv.org/abs/2502.04066</guid>
<content:encoded><![CDATA[
arXiv:2502.04066v2 Announce Type: replace 
Abstract: The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task indicative of a model's internal knowledge. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response to these challenges, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. Subsequently, we develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring any additional training. The experimental results demonstrate that SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on models with over one billion parameters. Theoretical analysis further reveals the marginal benefits of scaling model size and optimizing data, indicating that the upper limit of specific QA task accuracy is approximately 80%. Our project is available at https://github.com/yuhui1038/SMI.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data</title>
<link>https://arxiv.org/abs/2502.18679</link>
<guid>https://arxiv.org/abs/2502.18679</guid>
<content:encoded><![CDATA[
arXiv:2502.18679v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) has become a crucial step for aligning pretrained large language models (LLMs) using supervised datasets of input-output pairs. However, despite being supervised, SFT is inherently limited by its generative training objective. To address its limitations, the existing common strategy is to follow SFT with a separate phase of preference optimization (PO), which relies on either human-labeled preference data or a strong reward model to guide the learning process. In this paper, we address the limitations of SFT by exploring one of the most successful techniques in conventional supervised learning: discriminative learning. We introduce Discriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates the burden of collecting human-labeled preference data or training strong reward models. Unlike SFT that employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that increases the probability of positive answers while suppressing potentially negative ones, aiming for data prediction instead of token prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\rightarrow$PO. The code can be found at https://github.com/Optimization-AI/DFT.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents</title>
<link>https://arxiv.org/abs/2503.04830</link>
<guid>https://arxiv.org/abs/2503.04830</guid>
<content:encoded><![CDATA[
arXiv:2503.04830v3 Announce Type: replace 
Abstract: With the advancement of conversational large language models (LLMs), several LLM-based Conversational Shopping Agents (CSA) have been developed to help customers smooth their online shopping. The primary objective in building an engaging and trustworthy CSA is to ensure the agent's responses about product factoids are accurate and factually grounded. However, two challenges remain. First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk spreading misinformation and diminishing customer trust. Second, without providing knowledge source attribution in CSA response, customers struggle to verify LLM-generated information. To address both challenges, we present an easily productionized solution that enables a ''citation experience'' to our customers. We build auto-evaluation metrics to holistically evaluate LLM's grounding and attribution capabilities, suggesting that citation generation paradigm substantially improves grounding performance by 13.83%. To deploy this capability at scale, we introduce Multi-UX-Inference system, which appends source citations to LLM outputs while preserving existing user experience features and supporting scalable inference. Large-scale online A/B tests show that grounded CSA responses improves customer engagement by 3% - 10%, depending on UX variations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2503.13517</link>
<guid>https://arxiv.org/abs/2503.13517</guid>
<content:encoded><![CDATA[
arXiv:2503.13517v2 Announce Type: replace 
Abstract: Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes</title>
<link>https://arxiv.org/abs/2503.24027</link>
<guid>https://arxiv.org/abs/2503.24027</guid>
<content:encoded><![CDATA[
arXiv:2503.24027v2 Announce Type: replace 
Abstract: Novelty modeling and detection is a core topic in Natural Language Processing (NLP), central to numerous tasks such as recommender systems and automatic summarization. It involves identifying pieces of text that deviate in some way from previously known information. However, novelty is also a crucial determinant of the unique perception of relevance and quality of an experience, as it rests upon each individual's understanding of the world. Social factors, particularly cultural background, profoundly influence perceptions of novelty and innovation. Cultural novelty arises from differences in salience and novelty as shaped by the distance between distinct communities. While cultural diversity has garnered increasing attention in artificial intelligence (AI), the lack of robust metrics for quantifying cultural novelty hinders a deeper understanding of these divergences. This gap limits quantifying and understanding cultural differences within computational frameworks. To address this, we propose an interdisciplinary framework that integrates knowledge from sociology and management. Central to our approach is GlobalFusion, a novel dataset comprising 500 dishes and approximately 100,000 cooking recipes capturing cultural adaptation from over 150 countries. By introducing a set of Jensen-Shannon Divergence metrics for novelty, we leverage this dataset to analyze textual divergences when recipes from one community are modified by another with a different cultural background. The results reveal significant correlations between our cultural novelty metrics and established cultural measures based on linguistic, religious, and geographical distances. Our findings highlight the potential of our framework to advance the understanding and measurement of cultural diversity in AI.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why do LLMs attend to the first token?</title>
<link>https://arxiv.org/abs/2504.02732</link>
<guid>https://arxiv.org/abs/2504.02732</guid>
<content:encoded><![CDATA[
arXiv:2504.02732v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening</title>
<link>https://arxiv.org/abs/2504.02870</link>
<guid>https://arxiv.org/abs/2504.02870</guid>
<content:encoded><![CDATA[
arXiv:2504.02870v2 Announce Type: replace 
Abstract: Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models</title>
<link>https://arxiv.org/abs/2504.04717</link>
<guid>https://arxiv.org/abs/2504.04717</guid>
<content:encoded><![CDATA[
arXiv:2504.04717v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.07128</link>
<guid>https://arxiv.org/abs/2504.07128</guid>
<content:encoded><![CDATA[
arXiv:2504.07128v2 Announce Type: replace 
Abstract: Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</title>
<link>https://arxiv.org/abs/2504.16408</link>
<guid>https://arxiv.org/abs/2504.16408</guid>
<content:encoded><![CDATA[
arXiv:2504.16408v2 Announce Type: replace 
Abstract: The LLMSR@XLLM25 formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the LLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/JhCircle/Less-is-More.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training</title>
<link>https://arxiv.org/abs/2504.17565</link>
<guid>https://arxiv.org/abs/2504.17565</guid>
<content:encoded><![CDATA[
arXiv:2504.17565v3 Announce Type: replace 
Abstract: Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation</title>
<link>https://arxiv.org/abs/2410.13757</link>
<guid>https://arxiv.org/abs/2410.13757</guid>
<content:encoded><![CDATA[
arXiv:2410.13757v3 Announce Type: replace-cross 
Abstract: Existing Multimodal Large Language Model (MLLM)-based agents face significant challenges in handling complex GUI (Graphical User Interface) interactions on devices. These challenges arise from the dynamic and structured nature of GUI environments, which integrate text, images, and spatial relationships, as well as the variability in action spaces across different pages and tasks. To address these limitations, we propose MobA, a novel MLLM-based mobile assistant system. MobA introduces an adaptive planning module that incorporates a reflection mechanism for error recovery and dynamically adjusts plans to align with the real environment contexts and action module's execution capacity. Additionally, a multifaceted memory module provides comprehensive memory support to enhance adaptability and efficiency. We also present MobBench, a dataset designed for complex mobile interactions. Experimental results on MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI environments and perform complex mobile tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2501.00958</link>
<guid>https://arxiv.org/abs/2501.00958</guid>
<content:encoded><![CDATA[
arXiv:2501.00958v4 Announce Type: replace-cross 
Abstract: Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at https://github.com/DAMO-NLP-SG/multimodal_textbook.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Floating Point Quantization Training</title>
<link>https://arxiv.org/abs/2501.02423</link>
<guid>https://arxiv.org/abs/2501.02423</guid>
<content:encoded><![CDATA[
arXiv:2501.02423v2 Announce Type: replace-cross 
Abstract: Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point (FP) quantization, and thus cannot well fit the LLM losses in this scenario. In contrast, while FP quantization training is more commonly implemented in production, it's research has been relatively superficial. In this paper, we thoroughly explore the effects of FP quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in FP quantization training performance of LLM models. In addition to an accurate FP quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal FP quantization precision is directly proportional to the computational power, but within a wide computational power range. We estimate that the best cost-performance precision should lie between 4-8 bits.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Do Not Understand Negation</title>
<link>https://arxiv.org/abs/2501.09425</link>
<guid>https://arxiv.org/abs/2501.09425</guid>
<content:encoded><![CDATA[
arXiv:2501.09425v2 Announce Type: replace-cross 
Abstract: Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand negation? We introduce NegBench, a new benchmark designed to evaluate negation understanding across 18 task variations and $79$k examples spanning image, video, and medical datasets. The benchmark consists of two core tasks designed to evaluate negation understanding in diverse multimodal settings: Retrieval with Negation and Multiple Choice Questions with Negated Captions. Our evaluation reveals that modern VLMs struggle significantly with negation, often performing at chance level. To address these shortcomings, we explore a data-centric approach wherein we finetune CLIP models on large-scale synthetic datasets containing millions of negated captions. We show that this approach can result in a 10% increase in recall on negated queries and a 28% boost in accuracy on multiple-choice questions with negated captions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?</title>
<link>https://arxiv.org/abs/2501.15857</link>
<guid>https://arxiv.org/abs/2501.15857</guid>
<content:encoded><![CDATA[
arXiv:2501.15857v5 Announce Type: replace-cross 
Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, "FTCT" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Integrated Layered Attention (AILA)</title>
<link>https://arxiv.org/abs/2503.22742</link>
<guid>https://arxiv.org/abs/2503.22742</guid>
<content:encoded><![CDATA[
arXiv:2503.22742v2 Announce Type: replace-cross 
Abstract: We propose Adaptive Integrated Layered Attention (AILA), a neural network architecture that combines dense skip connections with different mechanisms for adaptive feature reuse across network layers. We evaluate AILA on three challenging tasks: price forecasting for various commodities and indices (S&amp;P 500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In all cases, AILA matches strong deep learning baselines (LSTMs, Transformers, and ResNets), achieving it at a fraction of the training and inference time. Notably, we implement and test two versions of the model - AILA-Architecture 1, which uses simple linear layers as the connection mechanism between layers, and AILA-Architecture 2, which implements an attention mechanism to selectively focus on outputs from previous layers. Both architectures are applied in a single-task learning setting, with each model trained separately for individual tasks. Results confirm that AILA's adaptive inter-layer connections yield robust gains by flexibly reusing pertinent features at multiple network depths. The AILA approach thus presents an extension to existing architectures, improving long-range sequence modeling, image recognition with optimised computational speed, and SOTA classification performance in practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Adaptation For Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2503.23083</link>
<guid>https://arxiv.org/abs/2503.23083</guid>
<content:encoded><![CDATA[
arXiv:2503.23083v2 Announce Type: replace-cross 
Abstract: Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs</title>
<link>https://arxiv.org/abs/2504.13989</link>
<guid>https://arxiv.org/abs/2504.13989</guid>
<content:encoded><![CDATA[
arXiv:2504.13989v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction</title>
<link>https://arxiv.org/abs/2504.14361</link>
<guid>https://arxiv.org/abs/2504.14361</guid>
<content:encoded><![CDATA[
arXiv:2504.14361v2 Announce Type: replace-cross 
Abstract: AI-driven drug response prediction holds great promise for advancing personalized cancer treatment. However, the inherent heterogenity of cancer and high cost of data generation make accurate prediction challenging. In this study, we investigate whether incorporating the pretrained foundation model scGPT can enhance the performance of existing drug response prediction frameworks. Our approach builds on the DeepCDR framework, which encodes drug representations from graph structures and cell representations from multi-omics profiles. We adapt this framework by leveraging scGPT to generate enriched cell representations using its pretrained knowledge to compensate for limited amount of data. We evaluate our modified framework using IC$_{50}$ values on Pearson correlation coefficient (PCC) and a leave-one-drug out validation strategy, comparing it against the original DeepCDR framework and a prior scFoundation-based approach. scGPT not only outperforms previous approaches but also exhibits greater training stability, highlighting the value of leveraging scGPT-derived knowledge in this domain.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents</title>
<link>https://arxiv.org/abs/2505.06416</link>
<guid>https://arxiv.org/abs/2505.06416</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Model Context Protocol, ScaleMCP, Tool Document Weighted Average, MCP servers

Summary: 
The article introduces ScaleMCP, a new approach for tool selection in Large Language Models (LLMs) that integrates the Model Context Protocol (MCP) servers. By enabling LLM agents to dynamically retrieve tools from MCP servers and synchronize them automatically, ScaleMCP improves tool selection efficiency and autonomy. The approach includes a novel embedding strategy, Tool Document Weighted Average (TDWA), that enhances the embedding process by emphasizing key components of tool documents. Evaluation on a dataset of financial metric MCP servers and various LLM models, embedding models, and retriever types shows significant enhancements in tool retrieval and agent invocation performance. ScaleMCP proves to be effective in enabling scalable, dynamic tool selection and invocation, addressing existing challenges in manual updates and limited autonomy faced by LLM agents. <br /><br />Summary: <div>
arXiv:2505.06416v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents' capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP's effectiveness in scalable, dynamic tool selection and invocation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is your multimodal large language model a good science tutor?</title>
<link>https://arxiv.org/abs/2505.06418</link>
<guid>https://arxiv.org/abs/2505.06418</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, science tutors, educational rubric, teaching performance, problem-solving skills 

Summary: 
Multimodal large language models (MLLMs) have shown success in scientific reasoning tasks like ScienceQA. While existing benchmarks focus on accuracy, this paper introduces a framework to evaluate MLLMs as science tutors using an educational rubric and simulated student model to assess teaching performance. By comparing strong and weak tutors, the study highlights that strong problem-solving skills do not always equate to high-quality tutoring. The research utilizes multiple preference optimization methods to enhance tutor models, showcasing the potential for MLLMs to serve as effective educational assistants. Through this approach, MLLMs can be optimized not just for problem-solving but also for teaching, emphasizing the importance of educational alignment in tutor models. <div>
arXiv:2505.06418v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) demonstrate impressive performance on scientific reasoning tasks (e.g., ScienceQA). However, most existing benchmarks focus narrowly on the accuracy of the final answer while ignoring other metrics. In particular, when applying MLLMs to educational contexts, the goal is not only correctness but also the ability to teach. In this paper, we propose a framework that evaluates MLLMs as science tutors using a comprehensive educational rubric and a simulated student model that judges the teaching performance of the tutors. Given a list of candidate MLLM science tutors, we use rubric-based student judgments to produce a range of tutor performance scores, identifying both strong and weak tutors. Using the training section of the ScienceQA dataset, we then construct a data set of pairwise comparisons between the outputs of strong and weak tutors. This enables us to apply multiple preference optimization methods to fine-tune an underperforming tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that strong problem-solving skills do not guarantee high-quality tutoring and that performance optimization-guided refinements can yield more educationally aligned tutor models. This approach opens avenues for building MLLMs that serve not only as problem solvers, but as genuinely helpful educational assistants.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xGen-small Technical Report</title>
<link>https://arxiv.org/abs/2505.06496</link>
<guid>https://arxiv.org/abs/2505.06496</guid>
<content:encoded><![CDATA[
<div> Transformer decoder, xGen-small, long-context applications, pre-training, fine-tuning <br />
Summary:
xGen-small is a family of Transformer decoder models designed for long-context applications. The models are optimized through a vertically integrated pipeline that includes domain-balanced data curation, multi-stage pre-training with quality annealing and length extension, and targeted post-training techniques such as fine-tuning and reinforcement learning. These models show strong performance across various tasks, particularly in math and coding domains, and excel in benchmarks requiring long context understanding. The xGen-small models are able to process up to 128k tokens and deliver impressive results, making them a valuable tool for tasks that require extensive contextual information. <div>
arXiv:2505.06496v1 Announce Type: new 
Abstract: We introduce xGen-small, a family of 4B and 9B Transformer decoder models optimized for long-context applications. Our vertically integrated pipeline unites domain-balanced, frequency-aware data curation; multi-stage pre-training with quality annealing and length extension to 128k tokens; and targeted post-training via supervised fine-tuning, preference learning, and online reinforcement learning. xGen-small delivers strong performance across various tasks, especially in math and coding domains, while excelling at long context benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model</title>
<link>https://arxiv.org/abs/2505.06538</link>
<guid>https://arxiv.org/abs/2505.06538</guid>
<content:encoded><![CDATA[
<div> safety evaluation, multimodal large reasoning models, benchmarks, safety degradation phenomena, safety-awareness

Summary:
The study evaluates the safety of 11 multimodal large reasoning models across 5 benchmarks, revealing prevalent safety degradation issues. Different benchmarks show distinct safety patterns, with jailbreak robustness benchmarks exhibiting significant degradation. However, safety-awareness benchmarks display less pronounced degradation. Interestingly, a longer thought process in certain scenarios actually improves safety performance. Leveraging the model's intrinsic reasoning capabilities to detect unsafe intent may be a viable approach to addressing safety concerns in MLRMs. By fine-tuning existing models with a safety-oriented thought process dataset, safety is effectively enhanced on both jailbreak robustness and safety-awareness benchmarks. This study offers a new perspective on developing safe MLRM models. The dataset used in the study is publicly available at the provided GitHub link. 

<br /><br />Summary: <div>
arXiv:2505.06538v1 Announce Type: new 
Abstract: The rapid development of multimodal large reasoning models (MLRMs) has demonstrated broad application potential, yet their safety and reliability remain critical concerns that require systematic exploration. To address this gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs across 5 benchmarks and unveil prevalent safety degradation phenomena in most advanced models. Moreover, our analysis reveals distinct safety patterns across different benchmarks: significant safety degradation is observed across jailbreak robustness benchmarks, whereas safety-awareness benchmarks demonstrate less pronounced degradation. In particular, a long thought process in some scenarios even enhances safety performance. Therefore, it is a potential approach to addressing safety issues in MLRMs by leveraging the intrinsic reasoning capabilities of the model to detect unsafe intent. To operationalize this insight, we construct a multimodal tuning dataset that incorporates a safety-oriented thought process. Experimental results from fine-tuning existing MLRMs with this dataset effectively enhances the safety on both jailbreak robustness and safety-awareness benchmarks. This study provides a new perspective for developing safe MLRMs. Our dataset is available at https://github.com/xinyuelou/Think-in-Safety.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback</title>
<link>https://arxiv.org/abs/2505.06548</link>
<guid>https://arxiv.org/abs/2505.06548</guid>
<content:encoded><![CDATA[
<div> Generating instructions for Large Language Models (LLMs) is expensive and time-consuming. Previous research has focused on semi-automated frameworks to generate instructions, often relying on costly API-only models. This study explores the performance of three small open-source LLMs (LLaMA 2-7B, LLama 2-13B, and Mistral 7B) in generating instructions with reduced human intervention. By incorporating Reinforcement Learning (RL) algorithms, the frameworks achieve significant improvements in 63-66% of tasks compared to previous methods. This approach lowers the cost and effort required to create instruction datasets for fine-tuning LLMs.<br /><br />Keywords: Large Language Models, Instructions, Semi-automated, Open-source LLMs, Reinforcement Learning<br /><br />Summary: Generating instructions for LLMs is costly and time-consuming. This study utilizes small open-source LLMs and RL algorithms to improve instruction generation, reducing human effort and cost. Significant enhancements in task performance are achieved compared to previous approaches, showcasing the effectiveness of this semi-automated framework. <div>
arXiv:2505.06548v1 Announce Type: new 
Abstract: Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation</title>
<link>https://arxiv.org/abs/2505.06552</link>
<guid>https://arxiv.org/abs/2505.06552</guid>
<content:encoded><![CDATA[
<div> reference-free, conversational query reformulation, optimization, DualReform, retrieval accuracy <br />
Summary: <br />
Conversational query reformulation (CQR) is essential for improving retrieval in dialogue-based applications. However, existing approaches rely on reference passages, which are not practical to obtain in real-world scenarios. To address this issue, DualReform introduces a novel framework that generates pseudo reference passages from datasets containing only queries and responses. It achieves this through response-based inference and response refinement, leveraging the dual role of CQR. Despite not using reference passages, DualReform achieves high retrieval accuracy, comparable to methods that do use them. In fact, it surpasses the state-of-the-art approach by up to 31.6%. <div>
arXiv:2505.06552v1 Announce Type: new 
Abstract: Conversational query reformulation (CQR) has become indispensable for improving retrieval in dialogue-based applications. However, existing approaches typically rely on reference passages for optimization, which are impractical to acquire in real-world scenarios. To address this limitation, we introduce a novel reference-free preference optimization framework DualReform that generates pseudo reference passages from commonly-encountered conversational datasets containing only queries and responses. DualReform attains this goal through two key innovations: (1) response-based inference, where responses serve as proxies to infer pseudo reference passages, and (2) response refinement via the dual-role of CQR, where a CQR model refines responses based on the shared objectives between response refinement and CQR. Despite not relying on reference passages, DualReform achieves 96.9--99.1% of the retrieval accuracy attainable only with reference passages and surpasses the state-of-the-art method by up to 31.6%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG</title>
<link>https://arxiv.org/abs/2505.06569</link>
<guid>https://arxiv.org/abs/2505.06569</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context, Large Language Models, Retrieval-Augmented Generation, Multi-scale Adaptive Context, Hierarchical Retrieval<br />
<br />
Summary:<br />
The article introduces Multi-scale Adaptive Context RAG (MacRAG), a hierarchical retrieval framework that effectively handles complex multi-hop and large-document tasks by compressing and partitioning documents into different granularities. MacRAG adaptively merges relevant contexts across chunk- and document-level expansions in real time to optimize precision and coverage. The system outperforms existing RAG pipelines on single- and multi-step generation tasks using Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o models. MacRAG is shown to be an efficient and scalable solution for real-world long-context, multi-hop reasoning. The code for MacRAG is available at the provided GitHub repository. <div>
arXiv:2505.06569v1 Announce Type: new 
Abstract: Long-context (LC) Large Language Models (LLMs) combined with Retrieval-Augmented Generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained context windows, and fragmented information caused by suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical retrieval framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through chunk- and document-level expansions in real time. By starting from the finest-level retrieval and progressively incorporating higher-level and broader context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on the challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG consistently surpasses baseline RAG pipelines on single- and multi-step generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM-Generated Q&amp;A Test: a Student-Centered Study</title>
<link>https://arxiv.org/abs/2505.06591</link>
<guid>https://arxiv.org/abs/2505.06591</guid>
<content:encoded><![CDATA[
<div> Keywords: AI chatbots, question-answer tests, psychometric analysis, student satisfaction, assessment development<br />
Summary: <br />
This research presents an automated system for generating question-answer tests using AI chatbots, specifically utilizing GPT-4o-mini. The generated test for a Natural Language Processing course was evaluated for psychometric performance and perceived quality by students and experts. The analysis showed that the AI-generated test items demonstrated strong discrimination and appropriate difficulty levels. Both students and experts rated the test highly in terms of overall quality. Additionally, a check for differential item functioning highlighted two items that may require further review. These results indicate that AI-generated assessments can achieve comparable psychometric performance and user satisfaction to those created by humans. This study showcases a scalable approach to developing assessments with the assistance of AI technology.<br /> 
Summary: <div>
arXiv:2505.06591v1 Announce Type: new 
Abstract: This research prepares an automatic pipeline for generating reliable question-answer (Q&amp;A) tests using AI chatbots. We automatically generated a GPT-4o-mini-based Q&amp;A test for a Natural Language Processing course and evaluated its psychometric and perceived-quality metrics with students and experts. A mixed-format IRT analysis showed that the generated items exhibit strong discrimination and appropriate difficulty, while student and expert star ratings reflect high overall quality. A uniform DIF check identified two items for review. These findings demonstrate that LLM-generated assessments can match human-authored tests in psychometric performance and user satisfaction, illustrating a scalable approach to AI-assisted assessment development.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation</title>
<link>https://arxiv.org/abs/2505.06594</link>
<guid>https://arxiv.org/abs/2505.06594</guid>
<content:encoded><![CDATA[
<div> screenplay generation, zero-shot video-to-text summarization, multimodal content, summarization metrics, MFactSum <br />
Summary:  
This paper introduces a zero-shot video-to-text summarization approach that creates screenplay representations of TV show episodes by integrating visual moments, dialogue, and character information. Unlike existing methods, this approach generates screenplays and character names simultaneously with input from audio, video, and transcripts. The study also introduces MFactSum, a multimodal metric that evaluates summaries considering both visual and text modalities. The proposed approach outperforms state-of-the-art VLMs like Gemini 1.5 by producing summaries with 20% more relevant visual content while using 75% less video input. The significance of balancing visual and textual information in summarizing complex multimodal inputs like TV show episodes is highlighted by the study. <div>
arXiv:2505.06594v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation</title>
<link>https://arxiv.org/abs/2505.06599</link>
<guid>https://arxiv.org/abs/2505.06599</guid>
<content:encoded><![CDATA[
<div> Keywords: Grapheme-to-phoneme conversion, Persian language processing, Large Language Model, polyphones, phoneme conversion<br />
<br />Summary: 
This paper introduces a novel approach to Grapheme-to-phoneme (G2P) conversion for the Persian language. The method addresses the complexities of Persian phonology, including homographs and Ezafe, through a combination of Large Language Model (LLM) prompting techniques and a specialized machine transliteration architecture. By constructing a comprehensive lexical database for homographs with multiple pronunciations, the model achieves superior performance in handling Persian phoneme conversion, surpassing existing state-of-the-art methods. Training on formal and informal Persian datasets, including B-Plus podcasts, the model significantly improves Phoneme Error Rate (PER) metrics, setting a new accuracy benchmark for Persian G2P conversion. This work contributes to low-resource language processing research and offers a robust solution for Persian text-to-speech systems, with potential applicability to other languages with complex phonological features. <div>
arXiv:2505.06599v1 Announce Type: new 
Abstract: Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges due to its complex phonological features, particularly homographs and Ezafe, which exist in formal and informal language contexts. This paper introduces an intermediate language specifically designed for Persian language processing that addresses these challenges through a multi-faceted approach. Our methodology combines two key components: Large Language Model (LLM) prompting techniques and a specialized sequence-to-sequence machine transliteration architecture. We developed and implemented a systematic approach for constructing a comprehensive lexical database for homographs with multiple pronunciations disambiguation often termed polyphones, utilizing formal concept analysis for semantic differentiation. We train our model using two distinct datasets: the LLM-generated dataset for formal and informal Persian and the B-Plus podcasts for informal language variants. The experimental results demonstrate superior performance compared to existing state-of-the-art approaches, particularly in handling the complexities of Persian phoneme conversion. Our model significantly improves Phoneme Error Rate (PER) metrics, establishing a new benchmark for Persian G2P conversion accuracy. This work contributes to the growing research in low-resource language processing and provides a robust solution for Persian text-to-speech systems and demonstrating its applicability beyond Persian. Specifically, the approach can extend to languages with rich homographic phenomena such as Chinese and Arabic
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using External knowledge to Enhanced PLM for Semantic Matching</title>
<link>https://arxiv.org/abs/2505.06605</link>
<guid>https://arxiv.org/abs/2505.06605</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic relevance, natural language processing, neural network, external knowledge, performance improvement<br />
<br />
Summary: <br />
- Modeling semantic relevance in natural language processing is challenging.
- Neural network-based reasoning models have shown strong performance in semantic relevance tasks.
- Annotated data is abundant but may not be sufficient for machines to learn all necessary knowledge.
- The paper explores incorporating external knowledge into neural network models for semantic relevance.
- Experimental results on 10 public datasets demonstrate consistent performance improvements using external knowledge enhancement. <div>
arXiv:2505.06605v1 Announce Type: new 
Abstract: Modeling semantic relevance has always been a challenging and critical task in natural language processing. In recent years, with the emergence of massive amounts of annotated data, it has become feasible to train complex models, such as neural network-based reasoning models. These models have shown excellent performance in practical applications and have achieved the current state-ofthe-art performance. However, even with such large-scale annotated data, we still need to think: Can machines learn all the knowledge necessary to perform semantic relevance detection tasks based on this data alone? If not, how can neural network-based models incorporate external knowledge into themselves, and how can relevance detection models be constructed to make full use of external knowledge? In this paper, we use external knowledge to enhance the pre-trained semantic relevance discrimination model. Experimental results on 10 public datasets show that our method achieves consistent improvements in performance compared to the baseline model.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Neural Language Inference via Cascaded Interactive Reasoning</title>
<link>https://arxiv.org/abs/2505.06607</link>
<guid>https://arxiv.org/abs/2505.06607</guid>
<content:encoded><![CDATA[
<div> pre-trained language models, natural language inference, Cascaded Interactive Reasoning Network, semantic comprehension, relational reasoning <br />
Summary: 
- The article introduces the Cascaded Interactive Reasoning Network (CIRN) for Natural Language Inference (NLI) task.
- CIRN utilizes hierarchical feature extraction and interactive reasoning across multiple network depths.
- The architecture aims to mimic progressive reasoning to uncover deep logical and semantic connections between premise and hypothesis.
- By mining latent semantic relationships at various levels, CIRN facilitates a more thorough understanding of input pairs.
- Comprehensive evaluations on standard NLI benchmark datasets show consistent performance gains of CIRN over competitive baselines, highlighting the effectiveness of leveraging multi-level interactive features for complex relational reasoning. <br /> <div>
arXiv:2505.06607v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) focuses on ascertaining the logical relationship (entailment, contradiction, or neutral) between a given premise and hypothesis. This task presents significant challenges due to inherent linguistic features such as diverse phrasing, semantic complexity, and contextual nuances. While Pre-trained Language Models (PLMs) built upon the Transformer architecture have yielded substantial advancements in NLI, prevailing methods predominantly utilize representations from the terminal layer. This reliance on final-layer outputs may overlook valuable information encoded in intermediate layers, potentially limiting the capacity to model intricate semantic interactions effectively. Addressing this gap, we introduce the Cascaded Interactive Reasoning Network (CIRN), a novel architecture designed for deeper semantic comprehension in NLI. CIRN implements a hierarchical feature extraction strategy across multiple network depths, operating within an interactive space where cross-sentence information is continuously integrated. This mechanism aims to mimic a process of progressive reasoning, transitioning from surface-level feature matching to uncovering more profound logical and semantic connections between the premise and hypothesis. By systematically mining latent semantic relationships at various representational levels, CIRN facilitates a more thorough understanding of the input pair. Comprehensive evaluations conducted on several standard NLI benchmark datasets reveal consistent performance gains achieved by CIRN over competitive baseline approaches, demonstrating the efficacy of leveraging multi-level interactive features for complex relational reasoning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2505.06624</link>
<guid>https://arxiv.org/abs/2505.06624</guid>
<content:encoded><![CDATA[
<div> Keywords: semi-supervised learning, text classification, Meta Pseudo Labels, teacher-student architecture, unsupervised pre-training<br />
<br />
Summary: 
This article introduces an extension of a semi-supervised model for text classification proposed by Hatefi et al., focusing on tasks where document classes have limited labeled examples. The model utilizes the teacher-student architecture of Meta Pseudo Labels, with a teacher generating labels for unlabeled data to train the student. An unsupervised pre-training phase using objective masking is added to the original model. Performance evaluations are conducted on the extended model, original model, and various baselines using datasets in English and Swedish. The experiments highlight the effectiveness of the model in leveraging unlabeled data for classification tasks, showcasing promising results across different languages and datasets. The model's iterative approach of updating the teacher based on student performance on labeled data contributes to its overall performance in semi-supervised learning scenarios. <div>
arXiv:2505.06624v1 Announce Type: new 
Abstract: We extend and study a semi-supervised model for text classification proposed earlier by Hatefi et al. for classification tasks in which document classes are described by a small number of gold-labeled examples, while the majority of training examples is unlabeled. The model leverages the teacher-student architecture of Meta Pseudo Labels in which a ''teacher'' generates labels for originally unlabeled training data to train the ''student'' and updates its own model iteratively based on the performance of the student on the gold-labeled portion of the data. We extend the original model of Hatefi et al. by an unsupervised pre-training phase based on objective masking, and conduct in-depth performance evaluations of the original model, our extension, and various independent baselines. Experiments are performed using three different datasets in two different languages (English and Swedish).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis</title>
<link>https://arxiv.org/abs/2505.06630</link>
<guid>https://arxiv.org/abs/2505.06630</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-domain sentiment classification, domain classifiers, sentiment classifiers, dynamic information modulation, hyperparameter optimization

<br /><br />Summary: Multi-domain sentiment classification addresses the challenges of limited labeled data in individual domains by utilizing data from multiple sources. Recent models that train domain and sentiment classifiers jointly have shown improvements since domain classification enhances sentiment insights. Although sentiment importance is consistent across domains, the impact of domain information on sentiment varies, which can be managed through adjustable weights or hyperparameters. However, scaling these models with more domains leads to challenges like high computing resource demands, convergence issues, and algorithmic complexity. To tackle these, the authors propose a dynamic information modulation algorithm. This method involves a two-stage training process. The first stage determines a shared hyperparameter controlling the contribution of domain classification across all domains. The second stage employs a novel domain-aware modulation algorithm that fine-tunes the domain information in the input text based on gradient and loss metrics. Experimental results from a sentiment analysis dataset with 16 domains confirm the effectiveness and superiority of the proposed approach, showcasing its potential for enhancing multi-domain sentiment classification performance. <div>
arXiv:2505.06630v1 Announce Type: new 
Abstract: Multi-domain sentiment classification aims to mitigate poor performance models due to the scarcity of labeled data in a single domain, by utilizing data labeled from various domains. A series of models that jointly train domain classifiers and sentiment classifiers have demonstrated their advantages, because domain classification helps generate necessary information for sentiment classification. Intuitively, the importance of sentiment classification tasks is the same in all domains for multi-domain sentiment classification; but domain classification tasks are different because the impact of domain information on sentiment classification varies across different fields; this can be controlled through adjustable weights or hyper parameters. However, as the number of domains increases, existing hyperparameter optimization algorithms may face the following challenges: (1) tremendous demand for computing resources, (2) convergence problems, and (3) high algorithm complexity. To efficiently generate the domain information required for sentiment classification in each domain, we propose a dynamic information modulation algorithm. Specifically, the model training process is divided into two stages. In the first stage, a shared hyperparameter, which would control the proportion of domain classification tasks across all fields, is determined. In the second stage, we introduce a novel domain-aware modulation algorithm to adjust the domain information contained in the input text, which is then calculated based on a gradient-based and loss-based method. In summary, experimental results on a public sentiment analysis dataset containing 16 domains prove the superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models</title>
<link>https://arxiv.org/abs/2505.06633</link>
<guid>https://arxiv.org/abs/2505.06633</guid>
<content:encoded><![CDATA[
<div> transformer networks, language modeling, pre-training process, multi-head attention, fully connected feedforward network

Summary:
- The study focuses on decoder-only transformer networks used for language modeling tasks, where models can have numerous transformer blocks containing billions of parameters and trained on massive amounts of text.
- Experiments conducted highlight the significance of the fully connected feedforward network (FFN) in the pre-training process, showing that the FFN plays a crucial role in the performance of the models.
- The research demonstrates that transformer block configurations with three-layer FFNs and fewer blocks outperform the standard two-layer configuration in terms of lower training loss and fewer total parameters, all achieved in less time. 
- This finding suggests that optimizing the FFN structure can lead to improved model efficiency and effectiveness in language modeling tasks. 
- Overall, the study sheds light on the importance of the FFN component in transformer networks and provides insights into enhancing the performance of such models. 

<br /><br />Summary: <div>
arXiv:2505.06633v1 Announce Type: new 
Abstract: Decoder-only transformer networks have become incredibly popular for language modeling tasks. State-of-the-art models can have over a hundred transformer blocks, containing billions of trainable parameters, and are trained on trillions of tokens of text. Each transformer block typically consists of a multi-head attention (MHA) mechanism and a two-layer fully connected feedforward network (FFN). In this paper, we examine the importance of the FFN during the model pre-training process through a series of experiments, confirming that the FFN is important to model performance. Furthermore, we show that models using a transformer block configuration with three-layer FFNs with fewer such blocks outperform the standard two-layer configuration delivering lower training loss with fewer total parameters in less time.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models</title>
<link>https://arxiv.org/abs/2505.06660</link>
<guid>https://arxiv.org/abs/2505.06660</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-supervised learning, speech processing, target-speaker tasks, benchmark, SSL-based target speech encoder

Summary:
Self-supervised learning (SSL) models have advanced speech processing tasks, but existing benchmarks mainly focus on single-speaker scenarios. A new benchmark, Target-Speaker Speech Processing Universal Performance Benchmark (TS-SUPERB), addresses target-speaker tasks in noisy, multi-talker conditions. It includes tasks where the target speaker must be identified in speech mixtures. The benchmark shows the need to evaluate SSL models in target speaker scenarios separately from single-speaker tasks. A unified SSL-based target speech encoder combines a speaker encoder and extractor module for optimized performance. Joint optimization across target-speaker tasks is explored to leverage mutual information and improve effectiveness. The study highlights the importance of evaluating SSL models in challenging real-world conditions, such as noisy, multi-talker environments, to ensure accurate performance evaluation and development. 

<br /><br />Summary: <div>
arXiv:2505.06660v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) models have significantly advanced speech processing tasks, and several benchmarks have been proposed to validate their effectiveness. However, previous benchmarks have primarily focused on single-speaker scenarios, with less exploration of target-speaker tasks in noisy, multi-talker conditions -- a more challenging yet practical case. In this paper, we introduce the Target-Speaker Speech Processing Universal Performance Benchmark (TS-SUPERB), which includes four widely recognized target-speaker processing tasks that require identifying the target speaker and extracting information from the speech mixture. In our benchmark, the speaker embedding extracted from enrollment speech is used as a clue to condition downstream models. The benchmark result reveals the importance of evaluating SSL models in target speaker scenarios, demonstrating that performance cannot be easily inferred from related single-speaker tasks. Moreover, by using a unified SSL-based target speech encoder, consisting of a speaker encoder and an extractor module, we also investigate joint optimization across TS tasks to leverage mutual information and demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing BERTopic with Intermediate Layer Representations</title>
<link>https://arxiv.org/abs/2505.06696</link>
<guid>https://arxiv.org/abs/2505.06696</guid>
<content:encoded><![CDATA[
<div> cluster, transformer-based embeddings, topic modeling, topic coherence, topic diversity

Summary:
The study introduces BERTopic, a topic modeling algorithm utilizing transformer-based embeddings to create dense clusters for analyzing document corpora. The research evaluates 18 different embedding representations through experiments on diverse datasets, focusing on topic coherence and diversity metrics. Results show that customized embedding configurations outperform BERTopic's default settings on each dataset. The influence of stop words on various embeddings is also studied. This research enhances understanding of efficient text data processing and insight extraction using BERTopic. <div>
arXiv:2505.06696v1 Announce Type: new 
Abstract: BERTopic is a topic modeling algorithm that leverages transformer-based embeddings to create dense clusters, enabling the estimation of topic structures and the extraction of valuable insights from a corpus of documents. This approach allows users to efficiently process large-scale text data and gain meaningful insights into its structure. While BERTopic is a powerful tool, embedding preparation can vary, including extracting representations from intermediate model layers and applying transformations to these embeddings. In this study, we evaluate 18 different embedding representations and present findings based on experiments conducted on three diverse datasets. To assess the algorithm's performance, we report topic coherence and topic diversity metrics across all experiments. Our results demonstrate that, for each dataset, it is possible to find an embedding configuration that performs better than the default setting of BERTopic. Additionally, we investigate the influence of stop words on different embedding configurations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback</title>
<link>https://arxiv.org/abs/2505.06698</link>
<guid>https://arxiv.org/abs/2505.06698</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic evaluation benchmarks, Large Language Models, Feedbacker framework, PC2 pointwise evaluation, model optimization

Summary:
The article introduces a new evaluation framework called Feedbacker that aims to provide comprehensive feedback for Large Language Models (LLMs), moving away from simple overall scores to more detailed insights for model optimization. Feedbacker includes a tree-based query taxonomy builder, automated query synthesis scheme, and visualization tools. A novel evaluation method, PC2 pointwise evaluation, is proposed to derive evaluation criteria from pre-comparing auxiliary responses. The article showcases the effectiveness of Feedbacker by evaluating 17 mainstream LLMs and demonstrating its potential in guiding model optimization and understanding model behavior. The Feedbacker project homepage is accessible at https://liudan193.github.io/Feedbacker.

<br /><br />Summary: <div>
arXiv:2505.06698v1 Announce Type: new 
Abstract: Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena are seeing growing adoption for the evaluation of Large Language Models (LLMs). Existing research has primarily focused on approximating human-based model rankings using limited data and LLM-as-a-Judge. However, the fundamental premise of these studies, which attempts to replicate human rankings, is flawed. Specifically, these benchmarks typically offer only overall scores, limiting their utility to leaderboard rankings, rather than providing feedback that can guide model optimization and support model profiling. Therefore, we advocate for an evaluation paradigm shift from approximating human-based model rankings to providing feedback with analytical value. To this end, we introduce Feedbacker, an evaluation framework that provides comprehensive and fine-grained results, thereby enabling thorough identification of a model's specific strengths and weaknesses. Such feedback not only supports the targeted optimization of the model but also enhances the understanding of its behavior. Feedbacker comprises three key components: an extensible tree-based query taxonomy builder, an automated query synthesis scheme, and a suite of visualization and analysis tools. Furthermore, we propose a novel LLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise evaluation. This method derives evaluation criteria by pre-comparing the differences between several auxiliary responses, achieving the accuracy of pairwise evaluation while maintaining the time complexity of pointwise evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs, we demonstrate the usage of Feedbacker and highlight its effectiveness and potential. Our homepage project is available at https://liudan193.github.io/Feedbacker.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free</title>
<link>https://arxiv.org/abs/2505.06708</link>
<guid>https://arxiv.org/abs/2505.06708</guid>
<content:encoded><![CDATA[
<div> Keywords: Gating mechanisms, Softmax attention, Mixture-of-Experts models, Sparse gating, Training stability

Summary:
Gating mechanisms in softmax attention models were systematically studied through comprehensive experiments using Mixture-of-Experts and dense models on a large dataset. The addition of a head-specific sigmoid gate after Scaled Dot-Product Attention (SDPA) consistently improved performance, training stability, and scalability. The modified softmax attention introduced non-linearity and query-dependent sparse gating scores, enhancing model effectiveness. The sparse gating mechanism helped mitigate 'attention sink' issues and improved long-context extrapolation performance. The study released codes and models to support future research in this area.<br /><br />Summary: <div>
arXiv:2505.06708v1 Announce Type: new 
Abstract: Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification-applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)-consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates 'attention sink' and enhances long-context extrapolation performance, and we also release related $\href{https://github.com/qiuzh20/gated_attention}{codes}$ and $\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK</title>
<link>https://arxiv.org/abs/2505.06782</link>
<guid>https://arxiv.org/abs/2505.06782</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic cigarettes, regulation, Australia, UK, health policy

<br /><br />Summary: This paper examines the contrasting regulatory approaches to electronic cigarettes (e-cigarettes) in Australia and the UK, highlighting Australia's restrictive stance versus the UK's permissive one. Despite sharing a common evidence base, the research employed a Large Language Model (LLM) based sentence classifier to perform automated analysis on 109 legislative documents from both countries. Using GPT-4, the classifier identified sentences as either supportive or critical of e-cigarettes regarding public health. The classifier achieved an impressive F-score of 0.9. The analysis revealed that Australian documents contained a higher proportion of harmful claims about e-cigarettes and fewer helpful ones, while the opposite trend was observed in UK documents. This disparity illustrates how both countries, although relying on the same data, interpret evidence differently to shape their respective health policies. The findings underscore the role of evidence presentation in policy formation and suggest that LLM-based methodologies could serve as valuable tools for exploring the nuanced interactions between evidence and health policy development. <div>
arXiv:2505.06782v1 Announce Type: new 
Abstract: Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad evidence base. In this paper, to investigate differences in how the two jurisdictions manage and present evidence, we developed and evaluated a Large Language Model-based sentence classifier to perform automated analyses of electronic cigarette-related policy documents drawn from official Australian and UK legislative processes (109 documents in total). Specifically, we utilized GPT-4 to automatically classify sentences based on whether they contained claims that e-cigarettes were broadly helpful or harmful for public health. Our LLM-based classifier achieved an F-score of 0.9. Further, when applying the classifier to our entire sentence-level corpus, we found that Australian legislative documents show a much higher proportion of harmful statements, and a lower proportion of helpful statements compared to the expected values, with the opposite holding for the UK. In conclusion, this work utilized an LLM-based approach to provide evidence to support the contention that - drawing on the same evidence base - Australian ENDS-related policy documents emphasize the harms associated with ENDS products and UK policy documents emphasize the benefits. Further, our approach provides a starting point for using LLM-based methods to investigate the complex relationship between evidence and health policy formation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting</title>
<link>https://arxiv.org/abs/2505.06862</link>
<guid>https://arxiv.org/abs/2505.06862</guid>
<content:encoded><![CDATA[
<div> model, abstractive text summarization, long documents, capacity, fine-tuning

Summary:
The $\texttt{BIGBIRD-PEGASUS}$ model has achieved state-of-the-art results in abstractive text summarization for long documents, but its capacity is limited to 4,096 tokens, leading to performance degradation on very long documents. In this research, a different approach is taken by fine-tuning the pretrained model on a different domain dataset. Documents with lengths less than 20,000 tokens are filtered out to focus on very long documents. To address domain shifting and overfitting issues in transfer learning, the dataset is augmented by splitting document-summary training pairs. This allows the documents to fit into the 4,096 token limit. The source code for this research is available at https://github.com/lhfazry/SPIN-summ.

<br /><br />Summary: <div>
arXiv:2505.06862v1 Announce Type: new 
Abstract: $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ on abstractive text summarization for long documents. However it's capacity still limited to maximum of $4,096$ tokens, thus caused performance degradation on summarization for very long documents. Common method to deal with the issue is to truncate the documents. In this reasearch, we'll use different approach. We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the model on other domain dataset. First, we filter out all documents which length less than $20,000$ tokens to focus on very long documents. To prevent domain shifting problem and overfitting on transfer learning due to small dataset, we augment the dataset by splitting document-summary training pair into parts, to fit the document into $4,096$ tokens. Source code available on $\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method</title>
<link>https://arxiv.org/abs/2505.06889</link>
<guid>https://arxiv.org/abs/2505.06889</guid>
<content:encoded><![CDATA[
<div> Keywords: Pre-trained Language Models, Adversarial Attacks, Numerical Stability, IM-BERT, Low-resource Scenarios

Summary:
IM-BERT is introduced as a dynamic system approach to enhance the robustness of Pre-trained Language Models (PLMs) against adversarial attacks. By conceptualizing BERT's layers as solutions of Ordinary Differential Equations (ODEs) and analyzing numerical stability using explicit and implicit Euler approaches, IM-BERT introduces a numerically robust IM-connection to improve performance. Experimental results on the AdvGLUE dataset demonstrate that IM-BERT outperforms BERT by approximately 8.3% on adversarial tasks and 5.9% in low-resource scenarios. This novel approach does not require additional parameters or adversarial training strategies, making it a promising method to mitigate overfitting and vulnerability in PLMs. 

<br /><br />Summary: 
1. IM-BERT employs a dynamic system approach to enhance PLM robustness against adversarial attacks.
2. The use of numerical ODE solvers and an IM-connection strategy improves performance without additional parameters.
3. Experimental results on the AdvGLUE dataset show significant performance gains over BERT.
4. IM-BERT excels in low-resource scenarios, outperforming BERT by 5.9% in accuracy.
5. This approach presents a promising solution to address overfitting and vulnerability in PLMs. <div>
arXiv:2505.06889v1 Announce Type: new 
Abstract: Pre-trained Language Models (PLMs) have achieved remarkable performance on diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning the model with a large number of parameters on limited downstream datasets often leads to vulnerability to adversarial attacks, causing overfitting of the model on standard datasets.
  To address these issues, we propose IM-BERT from the perspective of a dynamic system by conceptualizing a layer of BERT as a solution of Ordinary Differential Equations (ODEs). Under the situation of initial value perturbation, we analyze the numerical stability of two main numerical ODE solvers: the explicit and implicit Euler approaches.
  Based on these analyses, we introduce a numerically robust IM-connection incorporating BERT's layers. This strategy enhances the robustness of PLMs against adversarial attacks, even in low-resource scenarios, without introducing additional parameters or adversarial training strategies.
  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the robustness of IM-BERT under various conditions. Compared to the original BERT, IM-BERT exhibits a performance improvement of approximately 8.3\%p on the AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms BERT by achieving 5.9\%p higher accuracy.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation</title>
<link>https://arxiv.org/abs/2505.06904</link>
<guid>https://arxiv.org/abs/2505.06904</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Social Simulation, EcoLANG, Language Evolution, Efficiency  

<br /><br />Summary: Large language models (LLMs) exhibit remarkable capabilities in role-playing and mimicking complex social interactions. However, large-scale social simulations encounter significant challenges, including high computational and time costs. Existing solutions, such as distributed mechanisms or hybrid agent-based model (ABM) integrations, often fail to adequately address inference costs, compromising either accuracy or generalizability. To overcome these limitations, the authors introduce EcoLANG, an Efficient and Effective Agent Communication Language Induction for Social Simulation. EcoLANG operates in two key stages: first, it undergoes language evolution, which involves filtering synonymous words and optimizing sentence-level rules through a process akin to natural selection. Second, in the language utilization stage, agents within social simulations communicate using the evolved language. Experimental results indicate that EcoLANG significantly enhances efficiency, achieving over a 20% reduction in token consumption while maintaining simulation accuracy. This advancement represents a notable step forward in leveraging LLMs for social simulations, providing a solution that balances efficiency and accuracy effectively. <div>
arXiv:2505.06904v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated an impressive ability to role-play humans and replicate complex social dynamics. While large-scale social simulations are gaining increasing attention, they still face significant challenges, particularly regarding high time and computation costs. Existing solutions, such as distributed mechanisms or hybrid agent-based model (ABM) integrations, either fail to address inference costs or compromise accuracy and generalizability. To this end, we propose EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation. EcoLANG operates in two stages: (1) language evolution, where we filter synonymous words and optimize sentence-level rules through natural selection, and (2) language utilization, where agents in social simulations communicate using the evolved language. Experimental results demonstrate that EcoLANG reduces token consumption by over 20%, enhancing efficiency without sacrificing simulation accuracy.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Distracting Effect: Understanding Irrelevant Passages in RAG</title>
<link>https://arxiv.org/abs/2505.06914</link>
<guid>https://arxiv.org/abs/2505.06914</guid>
<content:encoded><![CDATA[
<div> distracting passages, retrieval augmented generation, LLMs, answering accuracy, hard distracting passages <br />
<br />
Summary: 
The paper addresses the issue of irrelevant retrieved passages in Retrieval Augmented Generation (RAG) systems that distract answer-generating Language Models (LLMs). It introduces a measure to quantify the distracting effect of a passage on a query and an LLM. By fine-tuning LLMs with carefully selected hard distracting passages, the researchers achieve a significant increase in answering accuracy compared to traditional RAG datasets. The study goes beyond simple classification of irrelevant passages and provides methods for identifying and utilizing hard distracting passages. This comprehensive framework is a novel approach in the field and aims to improve the performance of RAG systems by addressing the distracting nature of certain retrieved passages. <div>
arXiv:2505.06914v1 Announce Type: new 
Abstract: A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.
  Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire</title>
<link>https://arxiv.org/abs/2505.06974</link>
<guid>https://arxiv.org/abs/2505.06974</guid>
<content:encoded><![CDATA[
<div> Keywords: cuneiform tablet, Kizzuwatna rituals, CNN, methodology, teacher-student 

<br /><br />Summary: The cuneiform tablet KBo 23.1 ++/KUB 30.38 is known to document Kizzuwatna rituals and features two writers who produced nearly identical content in two iterations. Unlike other cuneiform texts that convey myths, essays, or commercial records, the underlying purpose of these ritual tablets remains largely enigmatic. To investigate this issue, the authors propose a novel methodology leveraging quantitative image analysis via Convolutional Neural Network (CNN) models, which allows for examination without the need for individual segmentation of cuneiform signs. This data-driven approach led to the intriguing conclusion that the first writer acted as a 'teacher' while the second writer was a student honing their cuneiform writing skills. This finding contrasts with traditional linguistic methodologies that did not identify this dynamic. The authors further explore related implications of their findings, as well as potential avenues for future applications of their method and its broader generalizations, highlighting its relevance in understanding ancient educational practices and the transmission of knowledge in early literate societies. <div>
arXiv:2505.06974v1 Announce Type: new 
Abstract: A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text of Kizzuwatna rituals, was written by two writers with almost identical content in two iterations. Unlike other cuneiform tablets that contained information such as myths, essays, or business records, the reason why ancient people left such tablets for posterity remains unclear. To study this problem, we develop a new methodology by analyzing images of a tablet quantitatively using CNN (Convolutional Neural Network)-based image models, without segmenting cuneiforms one-by-one. Our data-driven methodology implies that the writer writing the first half was a `teacher' and the other writer was a `student' who was training his skills of writing cuneiforms. This result has not been reached by classical linguistics. We also discuss related conclusions and possible further directions for applying our method and its generalizations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convert Language Model into a Value-based Strategic Planner</title>
<link>https://arxiv.org/abs/2505.06987</link>
<guid>https://arxiv.org/abs/2505.06987</guid>
<content:encoded><![CDATA[
<div> Q-learning, emotional support conversation, large language models, framework, optimization

Summary: 
The article introduces a framework called straQ* which leverages Q-learning on large language models to enhance emotional support conversations. By incorporating long-term planning, the framework enables the model to determine optimal strategies for responses, resulting in improved performance compared to various baselines. The study highlights the importance of defining the diagram in emotional support conversations from a state model perspective to achieve long-term satisfaction. Experimental results on emotional support conversation datasets demonstrate that straQ* outperforms direct inference, self-refine, chain of thought, finetuning, and finite state machines. This approach showcases the potential for incorporating reinforcement learning techniques into large language models to enhance the effectiveness of emotional support conversations. 

<br /><br />Summary: <div>
arXiv:2505.06987v1 Announce Type: new 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling</title>
<link>https://arxiv.org/abs/2505.07157</link>
<guid>https://arxiv.org/abs/2505.07157</guid>
<content:encoded><![CDATA[
<div> refinement, embeddings, topics, healthcare, LLM <br />
Summary: 
The paper introduces HAMLET, a graph-driven architecture for cross-lingual healthcare topic modeling using Large Language Models (LLMs). It addresses the limitations of traditional topic models by incorporating neural-enhanced semantic fusion to refine embeddings generated by LLMs. The proposed approach leverages Bidirectional Encoder Representations from Transformers (BERT) and Graph Neural Networks (GNN) for topic embedding refinement. A hybrid technique involving BERT and Sentence-BERT (SBERT) is used for embedding, and a GNN establishes connections between documents, topics, words, similar topics, and similar words. A novel method for computing similarities is introduced, resulting in improved topic embeddings and the extraction of top k topics. The effectiveness of HAMLET is demonstrated through experiments on English and French healthcare datasets. <br /> <br />Summary: <div>
arXiv:2505.07157v1 Announce Type: new 
Abstract: Traditional topic models often struggle with contextual nuances and fail to adequately handle polysemy and rare words. This limitation typically results in topics that lack coherence and quality. Large Language Models (LLMs) can mitigate this issue by generating an initial set of topics. However, these raw topics frequently lack refinement and representativeness, which leads to redundancy without lexical similarity and reduced interpretability. This paper introduces HAMLET, a graph-driven architecture for cross-lingual healthcare topic modeling that uses LLMs. The proposed approach leverages neural-enhanced semantic fusion to refine the embeddings of topics generated by the LLM. Instead of relying solely on statistical co-occurrence or human interpretation to extract topics from a document corpus, this method introduces a topic embedding refinement that uses Bidirectional Encoder Representations from Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for embedding. The topic representations are further refined using a GNN, which establishes connections between documents, topics, words, similar topics, and similar words. A novel method is introduced to compute similarities. Consequently, the topic embeddings are refined, and the top k topics are extracted. Experiments were conducted using two healthcare datasets, one in English and one in French, from which six sets were derived. The results demonstrate the effectiveness of HAMLET.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue</title>
<link>https://arxiv.org/abs/2505.07161</link>
<guid>https://arxiv.org/abs/2505.07161</guid>
<content:encoded><![CDATA[
<div> Keywords: feedback, mathematics education, discourse analysis, dialogue acts, AI-assisted education systems

Summary:
Effective feedback in mathematics education is crucial for refining instructional practices. However, analyzing classroom dialogues faces challenges due to multifunctionality of utterances and the exclusion of some utterances from discourse move classifications. To address this, a multi-perspective discourse analysis framework integrating domain-specific talk moves, dialogue acts, and discourse relations was proposed. This framework enables a comprehensive understanding of utterances containing talk moves as well as those that do not. Applied to mathematics education datasets, meaningful discourse patterns were discovered, highlighting the significance of utterances without talk moves in guiding and structuring classroom discourse. The study emphasizes the importance of incorporating discourse relations and dialogue acts into AI-assisted education systems to enhance feedback and create responsive learning environments. This framework can aid in providing feedback for human educators and developing AI agents that emulate educator and student roles. 

<br /><br />Summary: Effective feedback in mathematics education is essential for instructional refinement. Challenges in analyzing classroom dialogues include multifunctionality of utterances and omission of some from discourse classifications. A multi-perspective discourse analysis framework was proposed to address these challenges, integrating talk moves, dialogue acts, and discourse relations. Meaningful discourse patterns were discovered, highlighting the role of utterances without talk moves in guiding classroom discourse. Incorporating discourse relations and dialogue acts in AI-assisted education systems can enhance feedback and learning environments. The framework can assist in providing feedback for human educators and developing AI agents that emulate educator and student roles. <div>
arXiv:2505.07161v1 Announce Type: new 
Abstract: Effective feedback is essential for refining instructional practices in mathematics education, and researchers often turn to advanced natural language processing (NLP) models to analyze classroom dialogues from multiple perspectives. However, utterance-level discourse analysis encounters two primary challenges: (1) multifunctionality, where a single utterance may serve multiple purposes that a single tag cannot capture, and (2) the exclusion of many utterances from domain-specific discourse move classifications, leading to their omission in feedback. To address these challenges, we proposed a multi-perspective discourse analysis that integrates domain-specific talk moves with dialogue act (using the flattened multi-functional SWBD-MASL schema with 43 tags) and discourse relation (applying Segmented Discourse Representation Theory with 16 relations). Our top-down analysis framework enables a comprehensive understanding of utterances that contain talk moves, as well as utterances that do not contain talk moves. This is applied to two mathematics education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through distributional unigram analysis, sequential talk move analysis, and multi-view deep dive, we discovered meaningful discourse patterns, and revealed the vital role of utterances without talk moves, demonstrating that these utterances, far from being mere fillers, serve crucial functions in guiding, acknowledging, and structuring classroom discourse. These insights underscore the importance of incorporating discourse relations and dialogue acts into AI-assisted education systems to enhance feedback and create more responsive learning environments. Our framework may prove helpful for providing human educator feedback, but also aiding in the development of AI agents that can effectively emulate the roles of both educators and students.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification</title>
<link>https://arxiv.org/abs/2505.07162</link>
<guid>https://arxiv.org/abs/2505.07162</guid>
<content:encoded><![CDATA[
<div> Keyword: Knowledge Distillation, Healthcare, Multi-Label Text Classification, Large Language Models, Particle Swarm Optimization

Summary:
Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC) is introduced as a framework for efficient and accurate classification of healthcare textual data. It leverages model compression and Large Language Models (LLMs), transferring knowledge from a complex teacher model (BERT) to a lighter student model (DistilBERT) through sequential training. The approach is optimized using Particle Swarm Optimization (PSO) for hyperparameter tuning. KDH-MLTC outperforms existing methods, achieving an F1 score of 82.70% on a large medical literature dataset. Statistical validation and an ablation study confirm the robustness of KDH-MLTC. The use of PSO allows for the identification of optimal configurations. This approach contributes to healthcare text classification by balancing efficiency and accuracy, making it suitable for sensitive healthcare data and ensuring HIPAA compliance.

<br /><br />Summary: Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC) integrates model compression and Large Language Models (LLMs) to transfer knowledge from a complex teacher model to a lighter student model, resulting in superior performance for healthcare text classification. The use of Particle Swarm Optimization (PSO) for hyperparameter tuning further enhances the approach's effectiveness, demonstrating its robustness and efficiency in handling sensitive healthcare data. <div>
arXiv:2505.07162v1 Announce Type: new 
Abstract: The increasing volume of healthcare textual data requires computationally efficient, yet highly accurate classification approaches able to handle the nuanced and complex nature of medical terminology. This research presents Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC), a framework leveraging model compression and Large Language Models (LLMs). The proposed approach addresses conventional healthcare Multi-Label Text Classification (MLTC) challenges by integrating knowledge distillation and sequential fine-tuning, subsequently optimized through Particle Swarm Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e., DistilBERT) through sequential training adapted to MLTC that preserves the teacher's learned information while significantly reducing computational requirements. As a result, the classification is enabled to be conducted locally, making it suitable for healthcare textual data characterized by sensitivity and, therefore, ensuring HIPAA compliance. The experiments conducted on three medical literature datasets of different sizes, sampled from the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves superior performance compared to existing approaches, particularly for the largest dataset, reaching an F1 score of 82.70%. Additionally, statistical validation and an ablation study are carried out, proving the robustness of KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process allowed the identification of optimal configurations. The proposed approach contributes to healthcare text classification research, balancing efficiency requirements in resource-constrained healthcare settings with satisfactory accuracy demands.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs</title>
<link>https://arxiv.org/abs/2505.07184</link>
<guid>https://arxiv.org/abs/2505.07184</guid>
<content:encoded><![CDATA[
<div> Large language models, knowledge-intensive domains, synthetic data, Structural Entropy-guided Knowledge Navigator (SENATOR), Monte Carlo Tree Search (MCTS)<br />
<br />
Summary: 
The article introduces the SENATOR framework, which aims to enhance the performance of large language models in knowledge-intensive domains such as medicine and scientific research. By using the Structure Entropy metric to measure uncertainty along knowledge graph paths and leveraging Monte Carlo Tree Search, SENATOR can identify and address knowledge deficiencies in LLMs. The framework generates targeted synthetic data for fine-tuning, leading to significant performance improvements on domain-specific benchmarks. Experimental results on LLaMA-3 and Qwen2 demonstrate the effectiveness of SENATOR in detecting and repairing knowledge gaps, enabling continuous self-improvement of the models. The code and data for the methods and experiments are available on GitHub for further exploration and implementation. <div>
arXiv:2505.07184v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved unprecedented performance by leveraging vast pretraining corpora, yet their performance remains suboptimal in knowledge-intensive domains such as medicine and scientific research, where high factual precision is required. While synthetic data provides a promising avenue for augmenting domain knowledge, existing methods frequently generate redundant samples that do not align with the model's true knowledge gaps. To overcome this limitation, we propose a novel Structural Entropy-guided Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree Search (MCTS) to selectively explore regions where the model lacks domain-specific knowledge. Guided by these insights, the framework generates targeted synthetic data for supervised fine-tuning, enabling continuous self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple domain-specific benchmarks show that SENATOR effectively detects and repairs knowledge deficiencies, achieving notable performance improvements. The code and data for our methods and experiments are available at https://github.com/weiyifan1023/senator.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud</title>
<link>https://arxiv.org/abs/2505.07202</link>
<guid>https://arxiv.org/abs/2505.07202</guid>
<content:encoded><![CDATA[
<div> Keywords: TTS, conversational context, utterance-level training, training techniques, MOS scores  

<br /><br />Summary: This paper explores the effectiveness of modern text-to-speech (TTS) systems in conversational contexts, noting that while advances have been made, many high-quality models remain inaccessible. It questions whether the limitations stem from the existing open-source architectures or inadequate training techniques. Conducting empirical research on prominent models, the authors focus on two training approaches: context-based utterance-level training and full conversation training. Utilizing 20 GPU-hours on an NVIDIA H100, the results indicate that context-based utterance training yields significantly higher mean opinion scores (MOS) of 4.3 out of 5, compared to 3.7 for full conversation training. Additionally, the utterance-based approach reduces training time by 37%. In contrast, full conversation techniques face challenges such as speaker similarity hallucination, which negatively impacts output quality. The findings are relevant for developers in the field, suggesting that favoring utterance-level training with contextual conditioning can enhance both resource efficiency and quality of conversational TTS systems. Overall, the study provides actionable insights for improving conversational TTS applications. <div>
arXiv:2505.07202v1 Announce Type: new 
Abstract: Modern TTS systems designed for conversations achieve high-quality utterances but often remain inaccessible publicly. Are existing open-source architectures inadequate, or are current training techniques insufficient? This paper investigates prominent models and their underlying behaviors regarding conversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically examine two approaches: context-based utterance-level training versus full conversation training. Results demonstrate that context-based utterance training achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training time by 37%, while full conversation approaches suffer from speaker similarity hallucination issues. These findings provide practical guidelines for conversational TTS development, favoring utterance-level training with contextual conditioning for both resource efficiency and output quality.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030</title>
<link>https://arxiv.org/abs/2505.07205</link>
<guid>https://arxiv.org/abs/2505.07205</guid>
<content:encoded><![CDATA[
<div> Ethical, safety, healthcare, language models, governance
Summary:
This article discusses the use of Large Language Models (LLMs) in Chinese healthcare under the Healthy China 2030 initiative. A new Q&amp;A benchmark is introduced to evaluate the ethical and safety dimensions of LLMs in medical contexts. The study reveals moderate baseline performance of Chinese medical LLMs and highlights gaps in decision-making on ethics and safety scenarios. The research identifies governance shortfalls hindering safe LLM deployment, including the lack of ethical audit protocols and slow adaptation by hospital IRBs. A practical governance framework is proposed for healthcare institutions to proactively manage LLM risks, involving the embedding of LLM auditing teams, enactment of data ethics guidelines, and implementation of safety simulation pipelines. The study emphasizes the urgent need for robust LLM governance in Chinese healthcare to align AI innovation with patient safety and ethical standards.
<br /><br />Summary: <div>
arXiv:2505.07205v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are poised to transform healthcare under China's Healthy China 2030 initiative, yet they introduce new ethical and patient-safety challenges. We present a novel 12,000-item Q&amp;A benchmark covering 11 ethics and 9 safety dimensions in medical contexts, to quantitatively evaluate these risks. Using this dataset, we assess state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant improvements after fine-tuning on our data (up to 50.8% accuracy). Results show notable gaps in LLM decision-making on ethics and safety scenarios, reflecting insufficient institutional oversight. We then identify systemic governance shortfalls-including the lack of fine-grained ethical audit protocols, slow adaptation by hospital IRBs, and insufficient evaluation tools-that currently hinder safe LLM deployment. Finally, we propose a practical governance framework for healthcare institutions (embedding LLM auditing teams, enacting data ethics guidelines, and implementing safety simulation pipelines) to proactively manage LLM risks. Our study highlights the urgent need for robust LLM governance in Chinese healthcare, aligning AI innovation with patient safety and ethical standards.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.07233</link>
<guid>https://arxiv.org/abs/2505.07233</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, retrieval-augmented generation, dynamic reranking, language models, knowledge-intensive tasks<br />
<br />
Summary:<br />
DynamicRAG is a new framework that enhances retrieval-augmented generation systems by optimizing the reranking process. By dynamically adjusting the number and order of retrieved documents based on the query, the reranker in DynamicRAG improves both generation quality and explainability. This framework utilizes reinforcement learning to train the reranker, with rewards derived from the quality of the language model output. By leveraging both external knowledge retrieval and internal model knowledge, DynamicRAG outperforms existing systems on seven knowledge-intensive datasets, achieving state-of-the-art results. The code and data for DynamicRAG are available on GitHub, making it accessible for further research and development. <div>
arXiv:2505.07233v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models</title>
<link>https://arxiv.org/abs/2505.07247</link>
<guid>https://arxiv.org/abs/2505.07247</guid>
<content:encoded><![CDATA[
<div> Keywords: Subjective Answer Grading, Short Answer Scoring, Large Language Models, Benchmark, Few-shot prompting

Summary:
SAS-Bench is introduced as a benchmark for fine-grained scoring of short answers using large language models. It incorporates step-wise scoring, expert-annotated error categories, and real-world subject-specific questions. The benchmark aims to enhance model reasoning processes and transparency in scoring decisions. A dataset with questions and student responses, annotated by domain experts, is released for evaluation. Experiments with various LLMs highlight challenges in scoring science-related questions and the effectiveness of few-shot prompting in improving accuracy. The study aims to improve the development of fair, transparent, and meaningful LLM-based evaluation systems. 

<br /><br />Summary: 
SAS-Bench is a benchmark designed to enhance the evaluation of short answers using large language models. It offers fine-grained scoring, expert-annotated error categories, and real-world subject-specific questions for detailed evaluation of model reasoning and transparency. A dataset of questions and student responses, annotated by domain experts, is provided for assessment. Experiments with various LLMs reveal challenges in scoring science-related questions and the benefits of few-shot prompting for accuracy improvement. The goal is to advance the development of fair, transparent, and educationally meaningful LLM-based evaluation systems. <div>
arXiv:2505.07247v1 Announce Type: new 
Abstract: Subjective Answer Grading (SAG) plays a crucial role in education, standardized testing, and automated assessment systems, particularly for evaluating short-form responses in Short Answer Scoring (SAS). However, existing approaches often produce coarse-grained scores and lack detailed reasoning. Although large language models (LLMs) have demonstrated potential as zero-shot evaluators, they remain susceptible to bias, inconsistencies with human judgment, and limited transparency in scoring decisions. To overcome these limitations, we introduce SAS-Bench, a benchmark specifically designed for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring, expert-annotated error categories, and a diverse range of question types derived from real-world subject-specific exams. This benchmark facilitates detailed evaluation of model reasoning processes and explainability. We also release an open-source dataset containing 1,030 questions and 4,109 student responses, each annotated by domain experts. Furthermore, we conduct comprehensive experiments with various LLMs, identifying major challenges in scoring science-related questions and highlighting the effectiveness of few-shot prompting in improving scoring accuracy. Our work offers valuable insights into the development of more robust, fair, and educationally meaningful LLM-based evaluation systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Query, No Access</title>
<link>https://arxiv.org/abs/2505.07258</link>
<guid>https://arxiv.org/abs/2505.07258</guid>
<content:encoded><![CDATA[
<div> Keywords: Adversarial Attacks, Text Modification, Large Language Models, Attack Success Rate, Shadow Dataset

<br /><br />Summary: Textual adversarial attacks can effectively mislead NLP models, including Large Language Models (LLMs), by making subtle modifications to text. However, existing techniques often require extensive knowledge of the target model, leading to challenges in real-world application. To address these limitations, this study introduces the Victim Data-based Adversarial Attack (VDBA), which relies only on victim texts for operation. The approach uses publicly available pre-trained models to create a shadow dataset, facilitating the development of substitute models without needing direct access to the victim model. To improve attack success rates (ASR) hampered by insufficient feedback, a hierarchical substitution model design is employed, thus generating several substitute models to enhance performance at critical decision boundaries. Additionally, VDBA utilizes diverse adversarial example generation by incorporating various attack methods to identify the most effective examples in terms of similarity and attack impact. Experiments conducted on the Emotion and SST5 datasets demonstrate that VDBA significantly outperforms state-of-the-art methods, achieving a notable ASR improvement of 52.08% and requiring no query attempts. Crucially, it poses a substantial threat to advanced LLMs such as Qwen2 and the GPT family, revealing critical security vulnerabilities in modern NLP systems. <div>
arXiv:2505.07258v1 Announce Type: new 
Abstract: Textual adversarial attacks mislead NLP models, including Large Language Models (LLMs), by subtly modifying text. While effective, existing attacks often require knowledge of the victim model, extensive queries, or access to training data, limiting real-world feasibility. To overcome these constraints, we introduce the \textbf{Victim Data-based Adversarial Attack (VDBA)}, which operates using only victim texts. To prevent access to the victim model, we create a shadow dataset with publicly available pre-trained models and clustering methods as a foundation for developing substitute models. To address the low attack success rate (ASR) due to insufficient information feedback, we propose the hierarchical substitution model design, generating substitute models to mitigate the failure of a single substitute model at the decision boundary.
  Concurrently, we use diverse adversarial example generation, employing various attack methods to generate and select the adversarial example with better similarity and attack effectiveness. Experiments on the Emotion and SST5 datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR improvement of 52.08\% while significantly reducing attack queries to 0. More importantly, we discover that VDBA poses a significant threat to LLMs such as Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without access to the API, confirming that advanced NLP models still face serious security risks. Our codes can be found at https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of Reward Models for Language Model Alignment</title>
<link>https://arxiv.org/abs/2505.07271</link>
<guid>https://arxiv.org/abs/2505.07271</guid>
<content:encoded><![CDATA[
<div> Keywords: Bradley-Terry model, reward modeling, reinforcement learning, over-optimization, batch-wise sum-to-zero regularization (BSR)

Summary:
This study investigates the issue of over-optimization in reward models (RMs) trained using the Bradley-Terry (BT) model in reinforcement learning with human feedback. The research identifies the dispersion of hidden state norms as a key factor contributing to over-optimization and proposes batch-wise sum-to-zero regularization (BSR) as a solution to mitigate this issue. By enforcing a zero-centered reward sum per batch, BSR enhances the robustness of RMs to unseen data distributions. The study demonstrates the effectiveness of BSR in improving the generalizability of RMs and shows that robust RMs lead to better alignment of policies with preference models in RLHF training. Applying BSR to high-quality data and models results in significant performance improvements, surpassing state-of-the-art RMs in complex preference prediction tasks. The release of code, data, and models allows for further exploration and application of these findings. 

<br /><br />Summary: <div>
arXiv:2505.07271v1 Announce Type: new 
Abstract: The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Retention and Extreme Compression in LLMs: Can We Have Both?</title>
<link>https://arxiv.org/abs/2505.07289</link>
<guid>https://arxiv.org/abs/2505.07289</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, model compression, pruning, quantization, Semantic Retention Compression Rate

<br /><br />Summary: The rapid adoption of Large Language Models (LLMs) necessitates efficient model compression techniques to lower computational and memory expenses. This paper investigates the combined effects of pruning and quantization, two promising techniques that have yet to be fully explored together. The authors propose a joint compression approach that strategically integrates both methods, aiming for superior performance-to-compression ratios compared to using either technique independently. Recognizing challenges in assessing LLM performance, the study introduces the Semantic Retention Compression Rate (SrCr), a new metric designed to measure the balance between model compression and the preservation of semantics. This metric aids in optimizing configurations of pruning and quantization. The experiments conducted reveal that the recommended combination of pruning and quantization yields, on average, a 20% improvement in performance when compared to a quantization-only model operating at the same theoretical compression rate. Through this research, the authors highlight the potential benefits of combining compression strategies to enhance the efficiency and effectiveness of LLM applications. <div>
arXiv:2505.07289v1 Announce Type: new 
Abstract: The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection</title>
<link>https://arxiv.org/abs/2505.07293</link>
<guid>https://arxiv.org/abs/2505.07293</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning-intensive data, language model, AttentionInfluence, data selection, pretraining

Summary: 
- The study focuses on improving the complex reasoning ability of large language models (LLMs) by collecting reasoning-intensive pretraining data.
- The proposed method, AttentionInfluence, allows a small pretrained language model to act as a data selector without requiring supervision signal.
- AttentionInfluence involves identifying retrieval heads and computing loss differences when masking these heads to select data for pretraining.
- Experiment results show significant performance improvements on various knowledge-intensive and reasoning-heavy benchmarks.
- The approach demonstrates effective weak-to-strong scaling, where small models enhance the performance of larger models, offering a scalable solution for reasoning-centric data selection. 

<br /><br />Summary: <div>
arXiv:2505.07293v1 Announce Type: new 
Abstract: Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study</title>
<link>https://arxiv.org/abs/2505.07313</link>
<guid>https://arxiv.org/abs/2505.07313</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, collaborative reasoning, expertise alignment, collaboration paradigm, system scale  

<br /><br />Summary: This paper investigates the design of collaboration structures in multi-agent large language model (LLM) systems to improve collective reasoninga largely unexplored area. It examines three main dimensions: (1) Expertise-Domain Alignment, which shows that benefits are highly domain-dependent and effective mainly for contextual reasoning tasks; (2) Collaboration Paradigm, indicating that collaboration based on integrating diverse knowledge surpasses rigid task decomposition approaches; and (3) System Scale, where the study explores the impact of scaling systems through expertise specialization and emphasizes the importance of designing efficient communication protocols. The findings provide practical guidelines for configuring specialized multi-agent systems and identify critical architectural trade-offs and bottlenecks for enhancing scalable multi-agent reasoning. Overall, this research contributes to a better understanding of how to optimize multi-agent collaboration, thereby facilitating improved performance in reasoning tasks. The code related to this study will be released upon acceptance. <div>
arXiv:2505.07313v1 Announce Type: new 
Abstract: Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines</title>
<link>https://arxiv.org/abs/2505.07345</link>
<guid>https://arxiv.org/abs/2505.07345</guid>
<content:encoded><![CDATA[
<div> embedding-based SLM, generative SLM, relevance assessment, large language models, QUPID

Summary:
Combining different small language models (SLMs) can outperform large language models (LLMs) in relevance assessment for information retrieval. The QUPID approach integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs. This results in improved performance compared to state-of-the-art LLM solutions and 60x faster inference times. Experiments across various document types consistently showed performance improvements, with a Cohen's Kappa of 0.646 versus 0.387 for leading LLMs. When integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. The study highlights how combining diverse model architectures can enhance search relevance and operational efficiency in information retrieval systems.<br /><br />Summary: <div>
arXiv:2505.07345v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles</title>
<link>https://arxiv.org/abs/2505.07409</link>
<guid>https://arxiv.org/abs/2505.07409</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, civic discourse, veracity quantification, knowledge graph, FAIR <br />
<br />
Summary: 
Democratic societies require reliable information to maintain effective civic discourse. The abundance of misinformation in popular media poses a threat to this discourse, with citizens lacking the ability to verify the accuracy of the content they consume. This study introduces a semi-automated approach to quantify the scientific accuracy of online media by analyzing statements through a neurosymbolic system using LLM-based extraction and knowledge graph analysis. While the tool developed in this work shows promise in providing veracity indications, it falls short in annotating public media at the necessary level and scale. Future efforts should focus on establishing a FAIR ground truth and introducing complementary metrics to support civic discourse scientifically. <div>
arXiv:2505.07409v1 Announce Type: new 
Abstract: Democratic societies need reliable information. Misinformation in popular media such as news articles or videos threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify this content flood consumed daily at increasing rates. This work aims to semi-automatically quantify scientific accuracy of online media. By semantifying media of unknown veracity, their statements can be compared against equally processed trusted sources. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our neurosymbolic system was able to evidently streamline state-of-the-art veracity quantification. Evaluated via expert interviews and a user survey, the tool provides a beneficial veracity indication. This indicator, however, is unable to annotate public media at the required granularity and scale. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics are required to scientifically support civic discourse.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation</title>
<link>https://arxiv.org/abs/2505.07416</link>
<guid>https://arxiv.org/abs/2505.07416</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Vietnamese, Multimodal Review Helpfulness Prediction, annotation process, AI assistance <br />
Summary: <br />
The study introduces ViMRHP, a benchmark dataset for Multimodal Review Helpfulness Prediction in Vietnamese, addressing the lack of linguistic diversity in existing datasets. It covers four domains with 2K products and 46K reviews. The annotation process was optimized using AI, reducing time and costs significantly. AI assistance improved efficiency with annotations but had limitations in complex tasks. Baseline models were evaluated on human-verified and AI-generated annotations to compare quality differences. The ViMRHP dataset is publicly available for research purposes. <div>
arXiv:2505.07416v1 Announce Type: new 
Abstract: Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights</title>
<link>https://arxiv.org/abs/2505.07430</link>
<guid>https://arxiv.org/abs/2505.07430</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, public perceptions, COVID-19, Monkeypox, public health strategies 
<br />
Summary: 
The study conducts a comparative sentiment analysis of public perceptions towards COVID-19 and Monkeypox using extensive datasets of tweets. Machine learning models like Logistic Regression, Naive Bayes, RoBERTa, DistilRoBERTa, and XLNet were employed to classify sentiment, revealing variations influenced by disease characteristics, media portrayal, and pandemic fatigue. The analysis emphasizes differences in public sentiment and discourse, providing insights for tailored public health messaging, combating misinformation, and building trust during dual health crises. The study's findings advance sentiment analysis applications in public health informatics, paving the way for improved real-time monitoring and multilingual analysis in future research. 
<br /><br /> <div>
arXiv:2505.07430v1 Announce Type: new 
Abstract: The emergence of global health crises, such as COVID-19 and Monkeypox (mpox), has underscored the importance of understanding public sentiment to inform effective public health strategies. This study conducts a comparative sentiment analysis of public perceptions surrounding COVID-19 and mpox by leveraging extensive datasets of 147,475 and 106,638 tweets, respectively. Advanced machine learning models, including Logistic Regression, Naive Bayes, RoBERTa, DistilRoBERTa and XLNet, were applied to perform sentiment classification, with results indicating key trends in public emotion and discourse. The analysis highlights significant differences in public sentiment driven by disease characteristics, media representation, and pandemic fatigue. Through the lens of sentiment polarity and thematic trends, this study offers valuable insights into tailoring public health messaging, mitigating misinformation, and fostering trust during concurrent health crises. The findings contribute to advancing sentiment analysis applications in public health informatics, setting the groundwork for enhanced real-time monitoring and multilingual analysis in future research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge</title>
<link>https://arxiv.org/abs/2505.07440</link>
<guid>https://arxiv.org/abs/2505.07440</guid>
<content:encoded><![CDATA[
<div> tasks, industries, knowledge bases, machine learning, neural model

Summary:
- Commonsense knowledge bases are crucial for enhancing machine learning applications.
- Explicit knowledge from industry domains is lacking in existing KBs like ConceptNet.
- A weakly-supervised framework is proposed to augment KBs with industry-specific tasks.
- A neural model is trained to learn task-IG affinity and cluster top-k tasks per industry group.
- 2339 task-IG pairs are extracted from news datasets for 24 industry groups with a precision of 0.86.
<br /><br />Summary: <div>
arXiv:2505.07440v1 Announce Type: new 
Abstract: Commonsense knowledge bases (KB) are a source of specialized knowledge that is widely used to improve machine learning applications. However, even for a large KB such as ConceptNet, capturing explicit knowledge from each industry domain is challenging. For example, only a few samples of general {\em tasks} performed by various industries are available in ConceptNet. Here, a task is a well-defined knowledge-based volitional action to achieve a particular goal. In this paper, we aim to fill this gap and present a weakly-supervised framework to augment commonsense KB with tasks carried out by various industry groups (IG). We attempt to {\em match} each task with one or more suitable IGs by training a neural model to learn task-IG affinity and apply clustering to select the top-k tasks per IG. We extract a total of 2339 triples of the form $\langle IG, is~capable~of, task \rangle$ from two publicly available news datasets for 24 IGs with the precision of 0.86. This validates the reliability of the extracted task-IG pairs that can be directly added to existing KBs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions</title>
<link>https://arxiv.org/abs/2505.07495</link>
<guid>https://arxiv.org/abs/2505.07495</guid>
<content:encoded><![CDATA[
<div> Keywords: Grievance Dictionary, translations, psychometric analyses, reliability, application

<br /><br />Summary: This paper presents the introduction and evaluation of three translations of the Grievance Dictionary, which is focused on analyzing violent, threatening, or grievance-fuelled texts. The translations were created for Dutch, German, and Italian, highlighting the significance of these themes across different languages. The authors detail the process used for automated translation, which was enhanced by human annotation to improve accuracy. Comprehensive psychometric analyses were conducted, assessing the internal reliability of the dictionary categories and their correlations with the LIWC dictionary. The findings indicated that the Dutch and German translations exhibit a performance similar to the original English version of the dictionary. In contrast, the Italian version was found to have low reliability in some categories, suggesting a need for further refinement. The paper concludes with recommendations for continued validation and application of the Grievance Dictionary, along with suggestions for future dictionary translation initiatives that adopt a comparable approach. This study emphasizes the importance of cross-linguistic consideration in psycholinguistic research related to grievances and violence in texts. <div>
arXiv:2505.07495v1 Announce Type: new 
Abstract: This paper introduces and evaluates three translations of the Grievance Dictionary, a psycholinguistic dictionary for the analysis of violent, threatening or grievance-fuelled texts. Considering the relevance of these themes in languages beyond English, we translated the Grievance Dictionary to Dutch, German, and Italian. We describe the process of automated translation supplemented by human annotation. Psychometric analyses are performed, including internal reliability of dictionary categories and correlations with the LIWC dictionary. The Dutch and German translations perform similarly to the original English version, whereas the Italian dictionary shows low reliability for some categories. Finally, we make suggestions for further validation and application of the dictionary, as well as for future dictionary translations following a similar approach.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution</title>
<link>https://arxiv.org/abs/2505.07512</link>
<guid>https://arxiv.org/abs/2505.07512</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, tool learning, self-improving framework, sub-tasks, self-evolving paradigm <br />
Summary: 
The study introduces ToolACE-DEV, a novel framework for enhancing the tool-using capabilities of large language models (LLMs). It aims to address the limitations of current methods by breaking down the tool-learning objective into sub-tasks focused on improving basic tool-making and tool-using abilities. The framework utilizes a self-evolving paradigm, allowing lightweight models to enhance their capabilities independently, reducing the reliance on advanced LLMs. The proposed approach is validated through extensive experiments across models of different scales and architectures, demonstrating its effectiveness in improving tool-learning outcomes. This self-improving framework offers a promising solution to overcome the challenges associated with data compatibility issues and high knowledge scope discrepancies between advanced and target models.<br /><br />Summary: <div>
arXiv:2505.07512v1 Announce Type: new 
Abstract: The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion</title>
<link>https://arxiv.org/abs/2505.07528</link>
<guid>https://arxiv.org/abs/2505.07528</guid>
<content:encoded><![CDATA[
<div> hallucination, retrieval-augmented generation, ReDeEP, external information, semantic entropy 

Summary: 
The paper introduces the SEReDeEP framework for detecting hallucinations in Retrieval-Augmented Generation (RAG) models. Hallucinations in models often arise from a mismatch between external context and internal parametric knowledge. Existing approaches focus on either internal or external mechanisms, but SEReDeEP considers both. It identifies two main causes of hallucinations: overreliance on parametric knowledge and underutilization of external information. By quantitatively assessing these factors, SEReDeEP dynamically adjusts model components to reduce hallucinations. However, current methods lack semantic dimension evaluation, leading to inconsistent assessments. SEReDeEP uses semantic entropy captured by linear probes for more accurate hallucination detection. This framework improves the accuracy of hallucination assessments in RAG models by considering both internal and external factors and incorporating semantic entropy analysis. 

<br /><br />Summary: <div>
arXiv:2505.07528v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) models frequently encounter hallucination phenomena when integrating external information with internal parametric knowledge. Empirical studies demonstrate that the disequilibrium between external contextual information and internal parametric knowledge constitutes a primary factor in hallucination generation. Existing hallucination detection methodologies predominantly emphasize either the external or internal mechanism in isolation, thereby overlooking their synergistic effects. The recently proposed ReDeEP framework decouples these dual mechanisms, identifying two critical contributors to hallucinations: excessive reliance on parametric knowledge encoded in feed-forward networks (FFN) and insufficient utilization of external information by attention mechanisms (particularly copy heads). ReDeEP quantitatively assesses these factors to detect hallucinations and dynamically modulates the contributions of FFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and numerous other hallucination detection approaches have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, inadequately address the semantic dimensions of model responses, resulting in inconsistent hallucination assessments in RAG implementations. Building upon ReDeEP's foundation, this paper introduces SEReDeEP, which enhances computational processes through semantic entropy captured via trained linear probes, thereby achieving hallucination assessments that more accurately reflect ground truth evaluations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models</title>
<link>https://arxiv.org/abs/2505.07591</link>
<guid>https://arxiv.org/abs/2505.07591</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction following, large language models, constraint framework, automated instruction generation, reinforcement learning  

<br /><br />Summary: The study addresses the limitations of existing benchmarks for evaluating large language models (LLMs) on their ability to follow instructions. It introduces a multi-dimensional constraint framework that includes three constraint patterns, four constraint categories, and four difficulty levels to provide a more diverse assessment of LLM performance. An automated instruction generation pipeline is developed for constraint expansion, conflict detection, and instruction rewriting, resulting in 1,200 code-verifiable test samples. The evaluation of 19 LLMs across seven model families reveals a significant performance variation, with average scores decreasing from 77.67% at Level I to 32.96% at Level IV in terms of following constraints. The approach is further leveraged to generate data for reinforcement learning, yielding notable improvements in instruction adherence without compromising overall model performance. In-depth analyses suggest that these advancements are primarily attributed to modifications in the model's attention module parameters, leading to enhanced constraint recognition and compliance. The code and data produced by the study are made publicly accessible for further research. <div>
arXiv:2505.07591v1 Announce Type: new 
Abstract: Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent</title>
<link>https://arxiv.org/abs/2505.07596</link>
<guid>https://arxiv.org/abs/2505.07596</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, reinforcement learning, knowledge synergy, internal knowledge, knowledge boundary

<br /><br />Summary: The paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA) as a new solution to improve retrieval-augmented generation (RAG) strategies in large language models. It identifies a common issue where existing models underutilize their internal knowledge, leading to redundant retrievals and conflicts. IKEA addresses these limitations by creating an adaptive search agent that determines when to rely on internal knowledge versus external search capabilities. This is facilitated through a novel knowledge-boundary aware reward function and a specially designed training dataset, both aimed at enhancing internal-external knowledge synergy. The model is trained to prioritize internal knowledge, engaging with external databases only when necessary. Evaluation results across various knowledge reasoning tasks indicate that IKEA significantly outperforms existing methods, reduces unnecessary retrievals, and demonstrates strong generalization capabilities. By incentivizing accurate answers and minimizing redundant retrievals, IKEA enhances the overall efficiency and reliability of large language models in retrieval tasks. <div>
arXiv:2505.07596v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing the Investigative Methods of Fictional Detectives with Large Language Models</title>
<link>https://arxiv.org/abs/2505.07601</link>
<guid>https://arxiv.org/abs/2505.07601</guid>
<content:encoded><![CDATA[
<div> Keywords: Detective fiction, computational narratology, character analysis, AI-driven approach, narrative generation

Summary:
This paper introduces an AI-driven approach to characterizing the investigative methods of fictional detectives in detective fiction. The researchers employed 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of seven iconic fictional detectives, including Hercule Poirot, Sherlock Holmes, and Miss Marple, among others. The identified traits were validated against existing literary analyses and tested in a reverse identification phase, achieving an overall accuracy of 91.43%. This study offers a scalable framework for character analysis in computational narratology, with potential applications in AI-driven interactive storytelling and automated narrative generation.

<br /><br />Summary: <div>
arXiv:2505.07601v1 Announce Type: new 
Abstract: Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining</title>
<link>https://arxiv.org/abs/2505.07608</link>
<guid>https://arxiv.org/abs/2505.07608</guid>
<content:encoded><![CDATA[
arXiv:2505.07608v1 Announce Type: new 
Abstract: We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Level Explainability for Auditing &amp; Steering LLM Responses</title>
<link>https://arxiv.org/abs/2505.07610</link>
<guid>https://arxiv.org/abs/2505.07610</guid>
<content:encoded><![CDATA[
arXiv:2505.07610v1 Announce Type: new 
Abstract: As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chronocept: Instilling a Sense of Time in Machines</title>
<link>https://arxiv.org/abs/2505.07637</link>
<guid>https://arxiv.org/abs/2505.07637</guid>
<content:encoded><![CDATA[
arXiv:2505.07637v1 Announce Type: new 
Abstract: Human cognition is deeply intertwined with a sense of time, known as Chronoception. This sense allows us to judge how long facts remain valid and when knowledge becomes outdated. Despite progress in vision, language, and motor control, AI still struggles to reason about temporal validity. We introduce Chronocept, the first benchmark to model temporal validity as a continuous probability distribution over time. Using skew-normal curves fitted along semantically decomposed temporal axes, Chronocept captures nuanced patterns of emergence, decay, and peak relevance. It includes two datasets: Benchmark I (atomic facts) and Benchmark II (multi-sentence passages). Annotations show strong inter-annotator agreement (84% and 89%). Our baselines predict curve parameters - location, scale, and skewness - enabling interpretable, generalizable learning and outperforming classification-based approaches. Chronocept fills a foundational gap in AI's temporal reasoning, supporting applications in knowledge grounding, fact-checking, retrieval-augmented generation (RAG), and proactive agents. Code and data are publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JobHop: A Large-Scale Dataset of Career Trajectories</title>
<link>https://arxiv.org/abs/2505.07653</link>
<guid>https://arxiv.org/abs/2505.07653</guid>
<content:encoded><![CDATA[
arXiv:2505.07653v1 Announce Type: new 
Abstract: Understanding labor market dynamics is essential for policymakers, employers, and job seekers. However, comprehensive datasets that capture real-world career trajectories are scarce. In this paper, we introduce JobHop, a large-scale public dataset derived from anonymized resumes provided by VDAB, the public employment service in Flanders, Belgium. Utilizing Large Language Models (LLMs), we process unstructured resume data to extract structured career information, which is then mapped to standardized ESCO occupation codes using a multi-label classification model. This results in a rich dataset of over 2.3 million work experiences, extracted from and grouped into more than 391,000 user resumes and mapped to standardized ESCO occupation codes, offering valuable insights into real-world occupational transitions. This dataset enables diverse applications, such as analyzing labor market mobility, job stability, and the effects of career breaks on occupational transitions. It also supports career path prediction and other data-driven decision-making processes. To illustrate its potential, we explore key dataset characteristics, including job distributions, career breaks, and job transitions, demonstrating its value for advancing labor market research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent</title>
<link>https://arxiv.org/abs/2505.07659</link>
<guid>https://arxiv.org/abs/2505.07659</guid>
<content:encoded><![CDATA[
arXiv:2505.07659v1 Announce Type: new 
Abstract: This paper argues that the relationship between lexical identity and prosody -- one well-studied parameter of linguistic variation -- can be characterized using information theory. We predict that languages that use prosody to make lexical distinctions should exhibit a higher mutual information between word identity and prosody, compared to languages that don't. We test this hypothesis in the domain of pitch, which is used to make lexical distinctions in tonal languages, like Cantonese. We use a dataset of speakers reading sentences aloud in ten languages across five language families to estimate the mutual information between the text and their pitch curves. We find that, across languages, pitch curves display similar amounts of entropy. However, these curves are easier to predict given their associated text in the tonal languages, compared to pitch- and stress-accent languages, and thus the mutual information is higher in these languages, supporting our hypothesis. Our results support perspectives that view linguistic typology as gradient, rather than categorical.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Retrieval-Augmented Generation for Chemistry</title>
<link>https://arxiv.org/abs/2505.07671</link>
<guid>https://arxiv.org/abs/2505.07671</guid>
<content:encoded><![CDATA[
arXiv:2505.07671v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at https://chemrag.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit</title>
<link>https://arxiv.org/abs/2505.07672</link>
<guid>https://arxiv.org/abs/2505.07672</guid>
<content:encoded><![CDATA[
arXiv:2505.07672v1 Announce Type: new 
Abstract: We present OnPrem.LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem.LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codifying Character Logic in Role-Playing</title>
<link>https://arxiv.org/abs/2505.07705</link>
<guid>https://arxiv.org/abs/2505.07705</guid>
<content:encoded><![CDATA[
arXiv:2505.07705v1 Announce Type: new 
Abstract: This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. Each profile defines a set of functions parse_by_scene(scene) that outputs a list of logic-grounded assertions triggered_statements, using both explicit control structures (e.g., if-then-else) and condition checks like check_condition(scene, question), where each question is a semantically meaningful prompt about the scene (e.g., "Is the character in danger?") discriminated by the role-playing LLM as true, false, or unknown. This explicit representation offers three key advantages over traditional prompt-based profiles, which append character descriptions directly into text prompts: (1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning; (2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches; (3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve. To validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using NLI-based scoring to compare character responses against ground-truth actions. Our experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. Notably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing a scalable and efficient foundation for local deployment of role-play agents.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spoken Language Understanding on Unseen Tasks With In-Context Learning</title>
<link>https://arxiv.org/abs/2505.07731</link>
<guid>https://arxiv.org/abs/2505.07731</guid>
<content:encoded><![CDATA[
arXiv:2505.07731v1 Announce Type: new 
Abstract: Spoken language understanding (SLU) tasks involve diverse skills that probe the information extraction, classification and/or generation capabilities of models. In this setting, task-specific training data may not always be available. While traditional task-specific SLU models are unable to cater to such requirements, the speech-text large language models (LLMs) offer a promising alternative with emergent abilities. However, out of-the-box, our evaluations indicate that the zero/few-shot performance of prominent open-source speech-text LLMs on SLU tasks are not up to the mark. In this paper, we introduce a novel approach to robust task-agnostic fine-tuning using randomized class labels. With this proposed fine-tuning, we illustrate that the performance of the speech-text LLMs on an unseen task is significantly improved over standard approaches. Critically, the proposed approach avoids the requirement of task-specific data annotations for enabling new tasks in speech-text LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Must Read: A Systematic Survey of Computational Persuasion</title>
<link>https://arxiv.org/abs/2505.07775</link>
<guid>https://arxiv.org/abs/2505.07775</guid>
<content:encoded><![CDATA[
arXiv:2505.07775v1 Announce Type: new 
Abstract: Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI's susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI's role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Regeneration: How well do LLMs match syntactic properties of text domains?</title>
<link>https://arxiv.org/abs/2505.07784</link>
<guid>https://arxiv.org/abs/2505.07784</guid>
<content:encoded><![CDATA[
arXiv:2505.07784v1 Announce Type: new 
Abstract: Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Peers in Reasoning Models</title>
<link>https://arxiv.org/abs/2505.07787</link>
<guid>https://arxiv.org/abs/2505.07787</guid>
<content:encoded><![CDATA[
arXiv:2505.07787v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics in Continual Pre-Training for Large Language Models</title>
<link>https://arxiv.org/abs/2505.07796</link>
<guid>https://arxiv.org/abs/2505.07796</guid>
<content:encoded><![CDATA[
arXiv:2505.07796v1 Announce Type: new 
Abstract: Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Static Word Embeddings for Hungarian</title>
<link>https://arxiv.org/abs/2505.07809</link>
<guid>https://arxiv.org/abs/2505.07809</guid>
<content:encoded><![CDATA[
arXiv:2505.07809v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and extrinsic tasks to provide a holistic view of their performance. For intrinsic evaluation, we employ a word analogy task, which assesses the embeddings ability to capture semantic and syntactic relationships. Our results indicate that traditional static embeddings, particularly FastText, excel in this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among the BERT-based models, the X2Static method for extracting static embeddings demonstrates superior performance compared to decontextualized and aggregate methods, approaching the effectiveness of traditional static embeddings. For extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results reveal that embeddings derived from dynamic models, especially those extracted using the X2Static method, outperform purely static embeddings. Notably, ELMo embeddings achieve the highest accuracy in both NER and POS tagging tasks, underscoring the benefits of contextualized representations even when used in a static form. Our findings highlight the continued relevance of static word embeddings in NLP applications and the potential of advanced extraction methods to enhance the utility of BERT-based models. This piece of research contributes to the understanding of embedding performance in the Hungarian language and provides valuable insights for future developments in the field. The training scripts, evaluation codes, restricted vocabulary, and extracted embeddings will be made publicly available to support further research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.06297</link>
<guid>https://arxiv.org/abs/2505.06297</guid>
<content:encoded><![CDATA[
arXiv:2505.06297v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to be deployed and utilized across domains, the volume of LLM-generated data is growing rapidly. This trend highlights the increasing importance of effective and lossless compression for such data in modern text management systems. However, compressing LLM-generated data presents unique challenges compared to traditional human- or machine-generated content. Traditional machine-generated data is typically derived from computational processes or device outputs, often highly structured and limited to low-level elements like labels or numerical values. This structure enables conventional lossless compressors to perform efficiently. In contrast, LLM-generated data is more complex and diverse, requiring new approaches for effective compression. In this work, we conduct the first systematic investigation of lossless compression techniques tailored specifically to LLM-generated data. Notably, because LLMs are trained via next-token prediction, we find that LLM-generated data is highly predictable for the models themselves. This predictability enables LLMs to serve as efficient compressors of their own outputs. Through extensive experiments with 14 representative LLMs and 8 LLM-generated datasets from diverse domains, we show that LLM-based prediction methods achieve remarkable compression rates, exceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used general-purpose compressor. Furthermore, this advantage holds across different LLM sizes and dataset types, demonstrating the robustness and practicality of LLM-based methods in lossless text compression under generative AI workloads.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity</title>
<link>https://arxiv.org/abs/2505.06313</link>
<guid>https://arxiv.org/abs/2505.06313</guid>
<content:encoded><![CDATA[
arXiv:2505.06313v1 Announce Type: cross 
Abstract: The paper considers the use of GPT models with retrieval-augmented generation (RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity and NATO Article 5 trust opinion scores in different web sources: news sites found via Google Search API, Youtube videos with comments, and Reddit discussions. A RAG approach using GPT-4.1 model was applied to analyse news where NATO related topics were discussed. Two levels of RAG analytics were used: on the first level, the GPT model generates qualitative news summaries and quantitative opinion scores using zero-shot prompts; on the second level, the GPT model generates the summary of news summaries. Quantitative news opinion scores generated by the GPT model were analysed using Bayesian regression to get trend lines. The distributions found for the regression parameters make it possible to analyse an uncertainty in specified news opinion score trends. Obtained results show a downward trend for analysed scores of opinion related to NATO unity.
  This approach does not aim to conduct real political analysis; rather, it consider AI based approaches which can be used for further analytics
  as a part of a complex analytical approach. The obtained results demonstrate that the use of GPT models for news analysis can give informative qualitative and quantitative analytics, providing important insights.
  The dynamic model based on neural ordinary differential equations was considered for modelling public opinions. This approach makes it possible to analyse different scenarios for evolving public opinions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution</title>
<link>https://arxiv.org/abs/2505.06320</link>
<guid>https://arxiv.org/abs/2505.06320</guid>
<content:encoded><![CDATA[
arXiv:2505.06320v1 Announce Type: cross 
Abstract: Sentiment classification, a complex task in natural language processing, becomes even more challenging when analyzing passages with multiple conflicting tones. Typically, longer passages exacerbate this issue, leading to decreased model performance. The aim of this paper is to introduce novel methodologies for isolating conflicting sentiments and aggregating them to effectively predict the overall sentiment of such passages. One of the aggregation strategies involves a Multi-Layer Perceptron (MLP) model which outperforms baseline models across various datasets, including Amazon, Twitter, and SST while costing $\sim$1/100 of what fine-tuning the baseline would take.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations</title>
<link>https://arxiv.org/abs/2505.06653</link>
<guid>https://arxiv.org/abs/2505.06653</guid>
<content:encoded><![CDATA[
arXiv:2505.06653v1 Announce Type: cross 
Abstract: Large language models (LLMs) demand extensive memory capacity during both fine-tuning and inference. To enable memory-efficient fine-tuning, existing methods apply block-wise quantization techniques, such as NF4 and AF4, to the network weights. We show that these quantization techniques incur suboptimal quantization errors. Therefore, as a first novelty, we propose an optimization approach for block-wise quantization. Using this method, we design a family of quantizers named 4-bit block-wise optimal float (BOF4), which consistently reduces the quantization error compared to both baseline methods. We provide both a theoretical and a data-driven solution for the optimization process and prove their practical equivalence. Secondly, we propose a modification to the employed normalization method based on the signed absolute block maximum (BOF4-S), enabling further reduction of the quantization error and empirically achieving less degradation in language modeling performance. Thirdly, we explore additional variations of block-wise quantization methods applied to LLMs through an experimental study on the importance of accurately representing zero and large-amplitude weights on the one hand, and optimization towards various error metrics on the other hand. Lastly, we introduce a mixed-precision quantization strategy dubbed outlier-preserving quantization (OPQ) to address the distributional mismatch induced by outlier weights in block-wise quantization. By storing outlier weights in 16-bit precision (OPQ) while applying BOF4-S, we achieve top performance among 4-bit block-wise quantization techniques w.r.t. perplexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation</title>
<link>https://arxiv.org/abs/2505.06803</link>
<guid>https://arxiv.org/abs/2505.06803</guid>
<content:encoded><![CDATA[
arXiv:2505.06803v1 Announce Type: cross 
Abstract: Audio large language models (LLMs) are considered experts at recognizing sound objects, yet their performance relative to LLMs in other sensory modalities, such as visual or audio-visual LLMs, and to humans using their ears, eyes, or both remains unexplored. To investigate this, we systematically evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of different classes from audio-only, silent video, or sounded video inputs. We uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the sensory discrepancy between human ears and eyes. To reduce this gap, we introduce a cross-modal distillation framework, where an LLM in one modality serves as the teacher and another as the student, with knowledge transfer in sound classes predicted as more challenging to the student by a heuristic model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice versa, leads to notable improvements, particularly in challenging classes. This work highlights the sensory gap in LLMs from a human-aligned perspective and proposes a principled approach to enhancing modality-specific perception in multimodal LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
<link>https://arxiv.org/abs/2505.06814</link>
<guid>https://arxiv.org/abs/2505.06814</guid>
<content:encoded><![CDATA[
arXiv:2505.06814v1 Announce Type: cross 
Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety</title>
<link>https://arxiv.org/abs/2505.06843</link>
<guid>https://arxiv.org/abs/2505.06843</guid>
<content:encoded><![CDATA[
arXiv:2505.06843v1 Announce Type: cross 
Abstract: Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2505.06898</link>
<guid>https://arxiv.org/abs/2505.06898</guid>
<content:encoded><![CDATA[
arXiv:2505.06898v1 Announce Type: cross 
Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A digital perspective on the role of a stemma in material-philological transmission studies</title>
<link>https://arxiv.org/abs/2505.06938</link>
<guid>https://arxiv.org/abs/2505.06938</guid>
<content:encoded><![CDATA[
arXiv:2505.06938v1 Announce Type: cross 
Abstract: Taking its point of departure in the recent developments in the field of digital humanities and the increasing automatisation of scholarly workflows, this study explores the implications of digital approaches to textual traditions for the broader field of textual scholarship. It argues that the relative simplicity of creating computergenerated stemmas allows us to view the stemma codicum as a research tool rather than the final product of our scholarly investigation. Using the Old Norse saga of Hr\'omundur as a case study, this article demonstrates that stemmas can serve as a starting point for exploring textual traditions further. In doing so, they enable us to address research questions that otherwise remain unanswered. The article is accompanied by datasets used to generate stemmas for the Hr\'omundar saga tradition as well as two custom Python scripts. The scripts are designed to convert XML-based textual data, encoded according to the TEI Guidelines, into the input format used for the analysis in the PHYLIP package to generate unrooted trees of relationships between texts.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web Page Classification using LLMs for Crawling Support</title>
<link>https://arxiv.org/abs/2505.06972</link>
<guid>https://arxiv.org/abs/2505.06972</guid>
<content:encoded><![CDATA[
arXiv:2505.06972v1 Announce Type: cross 
Abstract: A web crawler is a system designed to collect web pages, and efficient crawling of new pages requires appropriate algorithms. While website features such as XML sitemaps and the frequency of past page updates provide important clues for accessing new pages, their universal application across diverse conditions is challenging. In this study, we propose a method to efficiently collect new pages by classifying web pages into two types, "Index Pages" and "Content Pages," using a large language model (LLM), and leveraging the classification results to select index pages as starting points for accessing new pages. We construct a dataset with automatically annotated web page types and evaluate our approach from two perspectives: the page type classification performance and coverage of new pages. Experimental results demonstrate that the LLM-based method outperformed baseline methods in both evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Three-Phase Dynamics of Generalization Power of a DNN</title>
<link>https://arxiv.org/abs/2505.06993</link>
<guid>https://arxiv.org/abs/2505.06993</guid>
<content:encoded><![CDATA[
arXiv:2505.06993v1 Announce Type: cross 
Abstract: This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Augmented Chemical Synthesis and Design Decision Programs</title>
<link>https://arxiv.org/abs/2505.07027</link>
<guid>https://arxiv.org/abs/2505.07027</guid>
<content:encoded><![CDATA[
arXiv:2505.07027v1 Announce Type: cross 
Abstract: Retrosynthesis, the process of breaking down a target molecule into simpler precursors through a series of valid reactions, stands at the core of organic chemistry and drug development. Although recent machine learning (ML) research has advanced single-step retrosynthetic modeling and subsequent route searches, these solutions remain restricted by the extensive combinatorial space of possible pathways. Concurrently, large language models (LLMs) have exhibited remarkable chemical knowledge, hinting at their potential to tackle complex decision-making tasks in chemistry. In this work, we explore whether LLMs can successfully navigate the highly constrained, multi-step retrosynthesis planning problem. We introduce an efficient scheme for encoding reaction pathways and present a new route-level search strategy, moving beyond the conventional step-by-step reactant prediction. Through comprehensive evaluations, we show that our LLM-augmented approach excels at retrosynthesis planning and extends naturally to the broader challenge of synthesizable molecular design.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reassessing Large Language Model Boolean Query Generation for Systematic Reviews</title>
<link>https://arxiv.org/abs/2505.07155</link>
<guid>https://arxiv.org/abs/2505.07155</guid>
<content:encoded><![CDATA[
arXiv:2505.07155v1 Announce Type: cross 
Abstract: Systematic reviews are comprehensive literature reviews that address highly focused research questions and represent the highest form of evidence in medicine. A critical step in this process is the development of complex Boolean queries to retrieve relevant literature. Given the difficulty of manually constructing these queries, recent efforts have explored Large Language Models (LLMs) to assist in their formulation. One of the first studies,Wang et al., investigated ChatGPT for this task, followed by Staudinger et al., which evaluated multiple LLMs in a reproducibility study. However, the latter overlooked several key aspects of the original work, including (i) validation of generated queries, (ii) output formatting constraints, and (iii) selection of examples for chain-of-thought (Guided) prompting. As a result, its findings diverged significantly from the original study. In this work, we systematically reproduce both studies while addressing these overlooked factors. Our results show that query effectiveness varies significantly across models and prompt designs, with guided query formulation benefiting from well-chosen seed studies. Overall, prompt design and model selection are key drivers of successful query formulation. Our findings provide a clearer understanding of LLMs' potential in Boolean query generation and highlight the importance of model- and prompt-specific optimisations. The complex nature of systematic reviews adds to challenges in both developing and reproducing methods but also highlights the importance of reproducibility studies in this domain.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition</title>
<link>https://arxiv.org/abs/2505.07166</link>
<guid>https://arxiv.org/abs/2505.07166</guid>
<content:encoded><![CDATA[
arXiv:2505.07166v1 Announce Type: cross 
Abstract: Dense retrievers utilize pre-trained backbone language models (e.g., BERT, LLaMA) that are fine-tuned via contrastive learning to perform the task of encoding text into sense representations that can be then compared via a shallow similarity operation, e.g. inner product. Recent research has questioned the role of fine-tuning vs. that of pre-training within dense retrievers, specifically arguing that retrieval knowledge is primarily gained during pre-training, meaning knowledge not acquired during pre-training cannot be sub-sequentially acquired via fine-tuning. We revisit this idea here as the claim was only studied in the context of a BERT-based encoder using DPR as representative dense retriever. We extend the previous analysis by testing other representation approaches (comparing the use of CLS tokens with that of mean pooling), backbone architectures (encoder-only BERT vs. decoder-only LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our study confirms that in DPR tuning, pre-trained knowledge underpins retrieval performance, with fine-tuning primarily adjusting neuron activation rather than reorganizing knowledge. However, this pattern does not hold universally, such as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full reproducibility and make our implementation publicly available at https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models</title>
<link>https://arxiv.org/abs/2505.07167</link>
<guid>https://arxiv.org/abs/2505.07167</guid>
<content:encoded><![CDATA[
arXiv:2505.07167v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Genomic Data Against Inference Attacks in Federated Learning Environments</title>
<link>https://arxiv.org/abs/2505.07188</link>
<guid>https://arxiv.org/abs/2505.07188</guid>
<content:encoded><![CDATA[
arXiv:2505.07188v1 Announce Type: cross 
Abstract: Federated Learning (FL) offers a promising framework for collaboratively training machine learning models across decentralized genomic datasets without direct data sharing. While this approach preserves data locality, it remains susceptible to sophisticated inference attacks that can compromise individual privacy. In this study, we simulate a federated learning setup using synthetic genomic data and assess its vulnerability to three key attack vectors: Membership Inference Attack (MIA), Gradient-Based Membership Inference Attack, and Label Inference Attack (LIA). Our experiments reveal that Gradient-Based MIA achieves the highest effectiveness, with a precision of 0.79 and F1-score of 0.87, underscoring the risk posed by gradient exposure in federated updates. Additionally, we visualize comparative attack performance through radar plots and quantify model leakage across clients. The findings emphasize the inadequacy of na\"ive FL setups in safeguarding genomic privacy and motivate the development of more robust privacy-preserving mechanisms tailored to the unique sensitivity of genomic data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge</title>
<link>https://arxiv.org/abs/2505.07365</link>
<guid>https://arxiv.org/abs/2505.07365</guid>
<content:encoded><![CDATA[
arXiv:2505.07365v1 Announce Type: cross 
Abstract: We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering (AQA) benchmark spanning multiple domains of sound understanding. This task defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA) to test audio-language models on interactive question-answering over diverse acoustic scenes. We describe the dataset composition (from marine mammal calls to soundscapes and complex real-world clips), the evaluation protocol (top-1 accuracy with answer-shuffling robustness), and baseline systems (Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the development set are compared, showing strong variation across models and subsets. This challenge aims to advance the audio understanding and reasoning capabilities of audio-language models toward human-level acuity, which are crucial for enabling AI agents to perceive and interact about the world effectively.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Collaborative Mechanisms Between Large and Small Language Models</title>
<link>https://arxiv.org/abs/2505.07460</link>
<guid>https://arxiv.org/abs/2505.07460</guid>
<content:encoded><![CDATA[
arXiv:2505.07460v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models</title>
<link>https://arxiv.org/abs/2505.07558</link>
<guid>https://arxiv.org/abs/2505.07558</guid>
<content:encoded><![CDATA[
arXiv:2505.07558v1 Announce Type: cross 
Abstract: Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model. This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences. To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO). DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling. We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure. Experiments demonstrate that DDRO achieves superior performance compared to existing methods on many major benchmarks. DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images</title>
<link>https://arxiv.org/abs/2505.07704</link>
<guid>https://arxiv.org/abs/2505.07704</guid>
<content:encoded><![CDATA[
arXiv:2505.07704v1 Announce Type: cross 
Abstract: Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding</title>
<link>https://arxiv.org/abs/2505.07768</link>
<guid>https://arxiv.org/abs/2505.07768</guid>
<content:encoded><![CDATA[
arXiv:2505.07768v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clickbait Detection via Large Language Models</title>
<link>https://arxiv.org/abs/2306.09597</link>
<guid>https://arxiv.org/abs/2306.09597</guid>
<content:encoded><![CDATA[
arXiv:2306.09597v4 Announce Type: replace 
Abstract: Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a series of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot and zero-shot scenarios on several English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Sycophancy in Language Models</title>
<link>https://arxiv.org/abs/2310.13548</link>
<guid>https://arxiv.org/abs/2310.13548</guid>
<content:encoded><![CDATA[
arXiv:2310.13548v4 Announce Type: replace 
Abstract: Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini: A Family of Highly Capable Multimodal Models</title>
<link>https://arxiv.org/abs/2312.11805</link>
<guid>https://arxiv.org/abs/2312.11805</guid>
<content:encoded><![CDATA[
arXiv:2312.11805v5 Announce Type: replace 
Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fleet of Agents: Coordinated Problem Solving with Large Language Models</title>
<link>https://arxiv.org/abs/2405.06691</link>
<guid>https://arxiv.org/abs/2405.06691</guid>
<content:encoded><![CDATA[
arXiv:2405.06691v3 Announce Type: replace 
Abstract: While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on three benchmark tasks, ``Game of 24'', ``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs, ``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average across all tasks and LLMs, FoA obtains a quality improvement of ~5% while requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at https://github.com/au-clan/FoA.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.15163</link>
<guid>https://arxiv.org/abs/2406.15163</guid>
<content:encoded><![CDATA[
arXiv:2406.15163v2 Announce Type: replace 
Abstract: Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing (NLP), addressing subjective assessments in textual content. Syntactic parsing is useful in SA because explicit syntactic information can improve accuracy while providing explainability, but it tends to be a computational bottleneck in practice due to the slowness of parsing algorithms. This paper addresses said bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject syntax into SA. By treating dependency parsing as a sequence labeling problem, we greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated on a ternary polarity classification task, demonstrating its faster performance and better accuracy in polarity prediction tasks compared to conventional parsers like Stanza and to heuristic approaches that use shallow syntactic rules for SA like VADER. This increased speed and improved accuracy make SELSP particularly appealing to SA practitioners in both research and industry. In addition, we test several sentiment dictionaries on our SELSP to see which one improves the performance in polarity prediction tasks. Moreover, we compare the SELSP with Transformer-based models trained on a 5-label classification task. The results show that dictionaries that capture polarity judgment variation provide better results than dictionaries that ignore polarity judgment variation. Moreover, we show that SELSP is considerably faster than Transformer-based models in polarity prediction tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Distributional to Overton Pluralism: Investigating Large Language Model Alignment</title>
<link>https://arxiv.org/abs/2406.17692</link>
<guid>https://arxiv.org/abs/2406.17692</guid>
<content:encoded><![CDATA[
arXiv:2406.17692v2 Announce Type: replace 
Abstract: The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models</title>
<link>https://arxiv.org/abs/2408.05093</link>
<guid>https://arxiv.org/abs/2408.05093</guid>
<content:encoded><![CDATA[
arXiv:2408.05093v4 Announce Type: replace 
Abstract: Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the "hallucination problem", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that "9.11$>$9.9". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge</title>
<link>https://arxiv.org/abs/2408.05497</link>
<guid>https://arxiv.org/abs/2408.05497</guid>
<content:encoded><![CDATA[
arXiv:2408.05497v3 Announce Type: replace 
Abstract: Models trained on real-world data often mirror and exacerbate existing social biases. Traditional methods for mitigating these biases typically require prior knowledge of the specific biases to be addressed, such as gender or racial biases, and the social groups associated with each instance. In this paper, we introduce a novel adversarial training strategy that operates independently of prior bias-type knowledge and protected attribute labels. Our approach proactively identifies biases during model training by utilizing auxiliary models, which are trained concurrently by predicting the performance of the main model without relying on task labels. Additionally, we implement these auxiliary models at various levels of the feature maps of the main model, enabling the detection of a broader and more nuanced range of bias features. Through experiments on racial and gender biases in sentiment and occupation classification tasks, our method effectively reduces social biases without the need for demographic annotations. Moreover, our approach not only matches but often surpasses the efficacy of methods that require detailed demographic insights, marking a significant advancement in bias mitigation techniques.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2408.09701</link>
<guid>https://arxiv.org/abs/2408.09701</guid>
<content:encoded><![CDATA[
arXiv:2408.09701v2 Announce Type: replace 
Abstract: The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CODELLAMA and CODEGEMMA, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and fine-tuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM's token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy</title>
<link>https://arxiv.org/abs/2409.13746</link>
<guid>https://arxiv.org/abs/2409.13746</guid>
<content:encoded><![CDATA[
arXiv:2409.13746v2 Announce Type: replace 
Abstract: This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship.
  In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95%) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Endless Jailbreaks with Bijection Learning</title>
<link>https://arxiv.org/abs/2410.01294</link>
<guid>https://arxiv.org/abs/2410.01294</guid>
<content:encoded><![CDATA[
arXiv:2410.01294v3 Announce Type: replace 
Abstract: Despite extensive safety measures, LLMs are vulnerable to adversarial inputs, or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce bijection learning, a powerful attack algorithm which automatically fuzzes LLMs for safety vulnerabilities using randomly-generated encodings whose complexity can be tightly controlled. We leverage in-context learning to teach models bijective encodings, pass encoded queries to the model to bypass built-in safety mechanisms, and finally decode responses back into English. Our attack is extremely effective on a wide range of frontier language models. Moreover, by controlling complexity parameters such as number of key-value mappings in the encodings, we find a close relationship between the capability level of the attacked LLM and the average complexity of the most effective bijection attacks. Our work highlights that new vulnerabilities in frontier models can emerge with scale: more capable models are more severely jailbroken by bijection attacks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isolated Causal Effects of Natural Language</title>
<link>https://arxiv.org/abs/2410.14812</link>
<guid>https://arxiv.org/abs/2410.14812</guid>
<content:encoded><![CDATA[
arXiv:2410.14812v2 Announce Type: replace 
Abstract: As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the isolated causal effect of some focal language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. Drawing on the principle of omitted variable bias, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions</title>
<link>https://arxiv.org/abs/2410.18966</link>
<guid>https://arxiv.org/abs/2410.18966</guid>
<content:encoded><![CDATA[
arXiv:2410.18966v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Creative Short Story Generation in Humans and Large Language Models</title>
<link>https://arxiv.org/abs/2411.02316</link>
<guid>https://arxiv.org/abs/2411.02316</guid>
<content:encoded><![CDATA[
arXiv:2411.02316v5 Announce Type: replace 
Abstract: Story-writing is a fundamental aspect of human imagination, relying heavily on creativity to produce narratives that are novel, effective, and surprising. While large language models (LLMs) have demonstrated the ability to generate high-quality stories, their creative story-writing capabilities remain under-explored. In this work, we conduct a systematic analysis of creativity in short story generation across 60 LLMs and 60 people using a five-sentence cue-word-based creative story-writing task. We use measures to automatically evaluate model- and human-generated stories across several dimensions of creativity, including novelty, surprise, diversity, and linguistic complexity. We also collect creativity ratings and Turing Test classifications from non-expert and expert human raters and LLMs. Automated metrics show that LLMs generate stylistically complex stories, but tend to fall short in terms of novelty, surprise and diversity when compared to average human writers. Expert ratings generally coincide with automated metrics. However, LLMs and non-experts rate LLM stories to be more creative than human-generated stories. We discuss why and how these differences in ratings occur, and their implications for both human and artificial creativity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models</title>
<link>https://arxiv.org/abs/2411.03700</link>
<guid>https://arxiv.org/abs/2411.03700</guid>
<content:encoded><![CDATA[
arXiv:2411.03700v2 Announce Type: replace 
Abstract: Natural-language assistants are designed to provide users with helpful responses while avoiding harmful outputs, largely achieved through alignment to human preferences. Yet there is limited understanding of whether alignment techniques may inadvertently perpetuate or even amplify harmful biases inherited from their pre-aligned base models. This issue is compounded by the choice of bias evaluation benchmarks in popular preference-finetuned models, which predominantly focus on dominant social categories, such as binary gender, thereby limiting insights into biases affecting underrepresented groups. Towards addressing this gap, we center transgender, nonbinary, and other gender-diverse identities to investigate how alignment procedures interact with pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a comprehensive survey of bias evaluation modalities across leading preference-finetuned LLMs, highlighting critical gaps in gender-diverse representation, 2) systematic evaluation of gender-diverse biases across 16 models spanning Direct Preference Optimization (DPO) stages, uncovering harms popular bias benchmarks fail to detect, and 3) a flexible framework for measuring harmful biases in implicit reward signals applicable to other social contexts. Our findings reveal that DPO-aligned models are particularly sensitive to supervised finetuning (SFT), and can amplify two forms of real-world gender-diverse harms from their base models: stigmatization and gender non-affirmative language. We conclude with recommendations tailored to DPO and broader alignment practices, advocating for the adoption of community-informed bias evaluation frameworks to more effectively identify and address underrepresented harms in LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity Helps Jailbreak Large Language Models</title>
<link>https://arxiv.org/abs/2411.04223</link>
<guid>https://arxiv.org/abs/2411.04223</guid>
<content:encoded><![CDATA[
arXiv:2411.04223v3 Announce Type: replace 
Abstract: We have uncovered a powerful jailbreak technique that leverages large language models' ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</title>
<link>https://arxiv.org/abs/2411.15100</link>
<guid>https://arxiv.org/abs/2411.15100</guid>
<content:encoded><![CDATA[
arXiv:2411.15100v3 Announce Type: replace 
Abstract: The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation</title>
<link>https://arxiv.org/abs/2412.05342</link>
<guid>https://arxiv.org/abs/2412.05342</guid>
<content:encoded><![CDATA[
arXiv:2412.05342v3 Announce Type: replace 
Abstract: Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning</title>
<link>https://arxiv.org/abs/2412.08587</link>
<guid>https://arxiv.org/abs/2412.08587</guid>
<content:encoded><![CDATA[
arXiv:2412.08587v2 Announce Type: replace 
Abstract: Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain</title>
<link>https://arxiv.org/abs/2412.20309</link>
<guid>https://arxiv.org/abs/2412.20309</guid>
<content:encoded><![CDATA[
arXiv:2412.20309v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) complements the knowledge of Large Language Models (LLMs) by leveraging external information to enhance response accuracy for queries. This approach is widely applied in several fields by taking its advantage of injecting the most up-to-date information, and researchers are focusing on understanding and improving this aspect to unlock the full potential of RAG in such high-stakes applications. However, despite the potential of RAG to address these needs, the mechanisms behind the confidence levels of its outputs remain underexplored, although the confidence of information is very critical in some domains, such as finance, healthcare, and medicine. Our study focuses the impact of RAG on confidence within the medical domain under various configurations and models. We evaluate confidence by treating the model's predicted probability as its output and calculating Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores based on the probabilities and accuracy. In addition, we analyze whether the order of retrieved documents within prompts calibrates the confidence. Our findings reveal large variation in confidence and accuracy depending on the model, settings, and the format of input prompts. These results underscore the necessity of optimizing configurations based on the specific model and conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments</title>
<link>https://arxiv.org/abs/2501.01652</link>
<guid>https://arxiv.org/abs/2501.01652</guid>
<content:encoded><![CDATA[
arXiv:2501.01652v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs' proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs' performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs' capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs' capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in \href{https://github.com/lime728/MIRAGE}{github}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective</title>
<link>https://arxiv.org/abs/2501.11110</link>
<guid>https://arxiv.org/abs/2501.11110</guid>
<content:encoded><![CDATA[
arXiv:2501.11110v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models</title>
<link>https://arxiv.org/abs/2501.13428</link>
<guid>https://arxiv.org/abs/2501.13428</guid>
<content:encoded><![CDATA[
arXiv:2501.13428v3 Announce Type: replace 
Abstract: Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by decomposing the Softmax operation into a non-linear transformation and the $l_1$-norm. We identify the latter as essential for maintaining model performance. By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths. To further improve the length extrapolation ability of the proposed attention mechanism, we introduce a novel re-weighting mechanism that amplifies significant attention weights while diminishing weaker ones, enabling the model to concentrate more effectively on relevant tokens. When combined with our proposed attention mechanism, this approach maintains nearly constant validation loss even at 16$\times$ the training token length, ensures numerical stability, and achieves superior results on downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Theory of Mind Enables the Invention of Proto-Writing</title>
<link>https://arxiv.org/abs/2502.01568</link>
<guid>https://arxiv.org/abs/2502.01568</guid>
<content:encoded><![CDATA[
arXiv:2502.01568v5 Announce Type: replace 
Abstract: Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues</title>
<link>https://arxiv.org/abs/2502.12084</link>
<guid>https://arxiv.org/abs/2502.12084</guid>
<content:encoded><![CDATA[
arXiv:2502.12084v3 Announce Type: replace 
Abstract: Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks</title>
<link>https://arxiv.org/abs/2502.12896</link>
<guid>https://arxiv.org/abs/2502.12896</guid>
<content:encoded><![CDATA[
arXiv:2502.12896v3 Announce Type: replace 
Abstract: In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking</title>
<link>https://arxiv.org/abs/2503.00955</link>
<guid>https://arxiv.org/abs/2503.00955</guid>
<content:encoded><![CDATA[
arXiv:2503.00955v2 Announce Type: replace 
Abstract: The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01 and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition</title>
<link>https://arxiv.org/abs/2503.01217</link>
<guid>https://arxiv.org/abs/2503.01217</guid>
<content:encoded><![CDATA[
arXiv:2503.01217v2 Announce Type: replace 
Abstract: Incorrect boundary division, complex semantic representation, and differences in pronunciation and meaning often lead to errors in Chinese Named Entity Recognition(CNER). To address these issues, this paper proposes HREB-CRF framework: Hierarchical Reduced-bias EMA with CRF. The proposed method amplifies word boundaries and pools long text gradients through exponentially fixed-bias weighted average of local and global hierarchical attention. Experimental results on the MSRA, Resume, and Weibo datasets show excellent in F1, outperforming the baseline model by 1.1\%, 1.6\%, and 9.8\%. The significant improvement in F1 shows evidences of strong effectiveness and robustness of approach in CNER tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can (A)I Change Your Mind?</title>
<link>https://arxiv.org/abs/2503.01844</link>
<guid>https://arxiv.org/abs/2503.01844</guid>
<content:encoded><![CDATA[
arXiv:2503.01844v2 Announce Type: replace 
Abstract: The increasing integration of large language model (LLM) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled, English-language settings. Addressing this, our preregistered study explored LLM's persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios, except in static LLM interactions. These findings demonstrate LLM-based agents' robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT</title>
<link>https://arxiv.org/abs/2503.01921</link>
<guid>https://arxiv.org/abs/2503.01921</guid>
<content:encoded><![CDATA[
arXiv:2503.01921v2 Announce Type: replace 
Abstract: SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in content generated by various large language models (LLMs) across multiple languages. This task involves not only identifying the presence of hallucinations but also pinpointing their specific occurrences. To tackle this challenge, this study introduces two methods: modified RefChecker and modified SelfCheckGPT. The modified RefChecker integrates prompt-based factual verification into References, structuring them as claim-based tests rather than single external knowledge sources. The modified SelfCheckGPT incorporates external knowledge to overcome its reliance on internal knowledge. In addition, both methods' original prompt designs are enhanced to identify hallucinated words within LLM-generated texts. Experimental results demonstrate the effectiveness of the approach, achieving a high ranking on the test dataset in detecting hallucinations across various languages, with an average IoU of 0.5310 and an average COR of 0.5669.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models</title>
<link>https://arxiv.org/abs/2503.03122</link>
<guid>https://arxiv.org/abs/2503.03122</guid>
<content:encoded><![CDATA[
arXiv:2503.03122v3 Announce Type: replace 
Abstract: Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization</title>
<link>https://arxiv.org/abs/2503.10354</link>
<guid>https://arxiv.org/abs/2503.10354</guid>
<content:encoded><![CDATA[
arXiv:2503.10354v2 Announce Type: replace 
Abstract: Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs</title>
<link>https://arxiv.org/abs/2504.13471</link>
<guid>https://arxiv.org/abs/2504.13471</guid>
<content:encoded><![CDATA[
arXiv:2504.13471v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) workflows, facilitating their integration into various systems. Many such NLP systems, including ours, directly incorporate LLMs. However, this approach either results in expensive costs or yields suboptimal performance after fine-tuning. In this paper, we introduce a three-stage cost-efficient end-to-end LLM deployment pipeline, comprising prototyping, knowledge transfer, and model compression, to effectively tackle the cost-performance dilemma in LLM-based frameworks. Its high cost-efficiency is manifested not only in simplifying system complexity and producing super-tiny online models with enhanced performance and reduced costs in the results, but also in addressing development cycle constraints, the lack of extensive high-quality data, and limited computational resources during the project development process. In the first stage, we construct an optimal performance prototype system by transforming complex tasks into a function call-based LLM-driven pipeline, which serves as a teacher model to generate high-quality data. In the second stage, we combine techniques like rejection sampling fine-tuning, reinforcement learning, and knowledge distillation to transfer knowledge to 0.5B student models, delivering effective performance at minimal cost. In the final stage, we further compress models to 0.4B via quantization and pruning, achieving ultra-low latency and cost. Extensive experimental results and the framework's modular design suggest cross-domain capabilities and potential applicability in other NLP areas.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Methods for Recognizing Nested Terms</title>
<link>https://arxiv.org/abs/2504.16007</link>
<guid>https://arxiv.org/abs/2504.16007</guid>
<content:encoded><![CDATA[
arXiv:2504.16007v2 Announce Type: replace 
Abstract: In this paper, we describe our participation in the RuTermEval competition devoted to extracting nested terms. We apply the Binder model, which was previously successfully applied to the recognition of nested named entities, to extract nested terms. We obtained the best results of term recognition in all three tracks of the RuTermEval competition. In addition, we study the new task of recognition of nested terms from flat training data annotated with terms without nestedness. We can conclude that several approaches we proposed in this work are viable enough to retrieve nested terms effectively without nested labeling of them.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.16394</link>
<guid>https://arxiv.org/abs/2504.16394</guid>
<content:encoded><![CDATA[
arXiv:2504.16394v2 Announce Type: replace 
Abstract: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic and Expressive Variation in Image Captions Across Languages</title>
<link>https://arxiv.org/abs/2310.14356</link>
<guid>https://arxiv.org/abs/2310.14356</guid>
<content:encoded><![CDATA[
arXiv:2310.14356v5 Announce Type: replace-cross 
Abstract: Computer vision often treats human perception as homogeneous: an implicit assumption that visual stimuli are perceived similarly by everyone. This assumption is reflected in the way researchers collect datasets and train vision models. By contrast, literature in cross-cultural psychology and linguistics has provided evidence that people from different cultural backgrounds observe vastly different concepts even when viewing the same visual stimuli. In this paper, we study how these differences manifest themselves in vision-language datasets and models, using language as a proxy for culture. By comparing textual descriptions generated across 7 languages for the same images, we find significant differences in the semantic content and linguistic expression. When datasets are multilingual as opposed to monolingual, descriptions have higher semantic coverage on average, where coverage is measured using scene graphs, model embeddings, and linguistic taxonomies. For example, multilingual descriptions have on average 29.9% more objects, 24.5% more relations, and 46.0% more attributes than a set of monolingual captions. When prompted to describe images in different languages, popular models (e.g. LLaVA) inherit this bias and describe different parts of the image. Moreover, finetuning models on captions from one language performs best on corresponding test data from that language, while finetuning on multilingual data performs consistently well across all test data compositions. Our work points towards the need to account for and embrace the diversity of human perception in the computer vision community.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems</title>
<link>https://arxiv.org/abs/2311.11796</link>
<guid>https://arxiv.org/abs/2311.11796</guid>
<content:encoded><![CDATA[
arXiv:2311.11796v2 Announce Type: replace-cross 
Abstract: As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIOS: LLM Agent Operating System</title>
<link>https://arxiv.org/abs/2403.16971</link>
<guid>https://arxiv.org/abs/2403.16971</guid>
<content:encoded><![CDATA[
arXiv:2403.16971v4 Announce Type: replace-cross 
Abstract: LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. As the diversity and complexity of agents continue to grow, addressing these resource management issues becomes increasingly critical to LLM-based agent systems. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) and efficient management of resources (e.g., LLM and external tools) for runtime agents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization</title>
<link>https://arxiv.org/abs/2405.15189</link>
<guid>https://arxiv.org/abs/2405.15189</guid>
<content:encoded><![CDATA[
arXiv:2405.15189v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable progress in code generation, but their generated code often suffers from inefficiency, resulting in longer execution times and higher memory consumption. To address this issue, we propose \textbf{EffiLearner}, a self-optimization framework that utilizes execution overhead profiles to improve the efficiency of LLM-generated code. EffiLearner first generates code using an LLM, then executes it locally to capture execution time and memory usage profiles. These profiles are fed back to the LLM, which then revises the code to reduce overhead. To evaluate the effectiveness of EffiLearner, we conduct extensive experiments on the EffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models. Our evaluation results demonstrate that through iterative self-optimization, EffiLearner significantly enhances the efficiency of LLM-generated code. For example, the execution time (ET) of StarCoder2-15B for the EffiBench decreases from 0.93 (s) to 0.12 (s) which reduces 87.1% the execution time requirement compared with the initial code. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% of total memory consumption during the execution process. The source code of EffiLearner was released in https://github.com/huangd1999/EffiLearner
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning</title>
<link>https://arxiv.org/abs/2406.06620</link>
<guid>https://arxiv.org/abs/2406.06620</guid>
<content:encoded><![CDATA[
arXiv:2406.06620v3 Announce Type: replace-cross 
Abstract: The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTime's transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. https://github.com/start2020/MedualTime
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2408.14419</link>
<guid>https://arxiv.org/abs/2408.14419</guid>
<content:encoded><![CDATA[
arXiv:2408.14419v2 Announce Type: replace-cross 
Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance. We benchmark leading LLMs as of late 2024 - including GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset and found that our benchmark was challenging to all of them, suggesting room for future large language models to improve.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Data Distillation for Recovering Quality in Pruned Large Language Models</title>
<link>https://arxiv.org/abs/2410.09982</link>
<guid>https://arxiv.org/abs/2410.09982</guid>
<content:encoded><![CDATA[
arXiv:2410.09982v4 Announce Type: replace-cross 
Abstract: Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning. To recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it can lead to catastrophic forgetting by shifting the model's learned data distribution. Therefore, addressing the degradation from both pruning and SFT is essential to preserve the original model's quality. In this work, we utilize self-data distilled fine-tuning to address these challenges. Our approach leverages the original, unpruned model to generate a distilled dataset that preserves semantic richness and mitigates catastrophic forgetting by maintaining alignment with the base model's knowledge. Empirically, we demonstrate that self-data distillation consistently outperforms standard SFT, improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct (i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our method retains 91.2% of the original model's accuracy compared to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore, combining self-data distilled models through model merging yields enhanced quality retention. Additionally, leveraging these pruned models in speculative decoding increases token acceptance rates, thereby improving inference efficiency in applied settings.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.13439</link>
<guid>https://arxiv.org/abs/2410.13439</guid>
<content:encoded><![CDATA[
arXiv:2410.13439v4 Announce Type: replace-cross 
Abstract: Supervised contrastive learning has achieved remarkable success by leveraging label information; however, determining positive samples in multi-label scenarios remains a critical challenge. In multi-label supervised contrastive learning (MSCL), multi-label relations are not yet fully defined, leading to ambiguity in identifying positive samples and formulating contrastive loss functions to construct the representation space. To address these challenges, we: (i) first define five distinct multi-label relations in MSCL to systematically identify positive samples, (ii) introduce a novel Similarity-Dissimilarity Loss that dynamically re-weights samples through computing the similarity and dissimilarity factors between positive samples and given anchors based on multi-label relations, and (iii) further provide theoretical grounded proofs for our method through rigorous mathematical analysis that supports the formulation and effectiveness of the proposed loss function. We conduct the experiments across both image and text modalities, and extend the evaluation to medical domain. The results demonstrate that our method consistently outperforms baselines in a comprehensive evaluation, confirming its effectiveness and robustness. Code is available at: https://github.com/guangminghuang/similarity-dissimilarity-loss.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface</title>
<link>https://arxiv.org/abs/2501.09798</link>
<guid>https://arxiv.org/abs/2501.09798</guid>
<content:encoded><![CDATA[
arXiv:2501.09798v2 Announce Type: replace-cross 
Abstract: We surface a new threat to closed-weight Large Language Models (LLMs) that enables an attacker to compute optimization-based prompt injections. Specifically, we characterize how an attacker can leverage the loss-like information returned from the remote fine-tuning interface to guide the search for adversarial prompts. The fine-tuning interface is hosted by an LLM vendor and allows developers to fine-tune LLMs for their tasks, thus providing utility, but also exposes enough information for an attacker to compute adversarial prompts. Through an experimental analysis, we characterize the loss-like values returned by the Gemini fine-tuning API and demonstrate that they provide a useful signal for discrete optimization of adversarial prompts using a greedy search algorithm. Using the PurpleLlama prompt injection benchmark, we demonstrate attack success rates between 65% and 82% on Google's Gemini family of LLMs. These attacks exploit the classic utility-security tradeoff - the fine-tuning interface provides a useful feature for developers but also exposes the LLMs to powerful attacks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training and Evaluating with Human Label Variation: An Empirical Study</title>
<link>https://arxiv.org/abs/2502.01891</link>
<guid>https://arxiv.org/abs/2502.01891</guid>
<content:encoded><![CDATA[
arXiv:2502.01891v3 Announce Type: replace-cross 
Abstract: Human label variation (HLV) challenges the standard assumption that a labelled instance has a single ground truth, instead embracing the natural variation in human annotation to train and evaluate models. While various training methods and metrics for HLV have been proposed, it is still unclear which methods and metrics perform best in what settings. We propose new evaluation metrics for HLV leveraging fuzzy set theory. Since these new proposed metrics are differentiable, we then in turn experiment with employing these metrics as training objectives. We conduct an extensive study over 6 HLV datasets testing 14 training methods and 6 evaluation metrics. We find that training on either disaggregated annotations or soft labels performs best across metrics, outperforming training using the proposed training objectives with differentiable metrics. We also show that our proposed soft metric is more interpretable and correlates best with human preference.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Expert Knowledge into Logical Programs via LLMs</title>
<link>https://arxiv.org/abs/2502.12275</link>
<guid>https://arxiv.org/abs/2502.12275</guid>
<content:encoded><![CDATA[
arXiv:2502.12275v2 Announce Type: replace-cross 
Abstract: This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models' capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal that most models generate nearly perfect syntactically correct code and exhibit strong performance in translating expert knowledge into correct code. At the same time, while most LLMs produce nearly flawless syntactic output, their ability to correctly implement logical rules varies, as does their capacity for self-improvement. Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Case Against Empirical Human-AI Alignment</title>
<link>https://arxiv.org/abs/2502.14581</link>
<guid>https://arxiv.org/abs/2502.14581</guid>
<content:encoded><![CDATA[
arXiv:2502.14581v2 Announce Type: replace-cross 
Abstract: Empirical human-AI alignment aims to make AI systems act in line with observed human behavior. While noble in its goals, we argue that empirical alignment can inadvertently introduce statistical biases that warrant caution. This position paper thus advocates against naive empirical alignment, offering prescriptive alignment and a posteriori empirical alignment as alternatives. We substantiate our principled argument by tangible examples like human-centric decoding of language models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.14908</link>
<guid>https://arxiv.org/abs/2502.14908</guid>
<content:encoded><![CDATA[
arXiv:2502.14908v2 Announce Type: replace-cross 
Abstract: Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lacks systematic investigation of cross-modal knowledge conflicts. This research introduces \segsub, a framework for applying targeted image perturbations to investigate VLM resilience against knowledge conflicts. Our analysis reveals distinct vulnerability patterns: while VLMs are robust to parametric conflicts (20% adherence rates), they exhibit significant weaknesses in identifying counterfactual conditions (<30% accuracy) and resolving source conflicts (<1% accuracy). Correlations between contextual richness and hallucination rate (r = -0.368, p = 0.003) reveal the kinds of images that are likely to cause hallucinations. Through targeted fine-tuning on our benchmark dataset, we demonstrate improvements in VLM knowledge conflict detection, establishing a foundation for developing hallucination-resilient multimodal systems in information-sensitive environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?</title>
<link>https://arxiv.org/abs/2503.08980</link>
<guid>https://arxiv.org/abs/2503.08980</guid>
<content:encoded><![CDATA[
arXiv:2503.08980v3 Announce Type: replace-cross 
Abstract: The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human-interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result, i.e., the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts given input context, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also provide a unified prospective for understanding of the linear representation hypothesis. Taking this a step further, our finding motivates a reliable evaluation of sparse autoencoders by treating the performance of supervised concept extractors as an upper bound. Pushing this idea even further, it inspires a structural variant that enforces dependence among latent concepts in addition to promoting sparsity. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families, and demonstrate the effectiveness of our structured sparse autoencoder.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Illusion of Progress? Assessing the Current State of Web Agents</title>
<link>https://arxiv.org/abs/2504.01382</link>
<guid>https://arxiv.org/abs/2504.01382</guid>
<content:encoded><![CDATA[
arXiv:2504.01382v2 Announce Type: replace-cross 
Abstract: As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines</title>
<link>https://arxiv.org/abs/2504.07840</link>
<guid>https://arxiv.org/abs/2504.07840</guid>
<content:encoded><![CDATA[
arXiv:2504.07840v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification</title>
<link>https://arxiv.org/abs/2505.05583</link>
<guid>https://arxiv.org/abs/2505.05583</guid>
<content:encoded><![CDATA[
<div> Keywords: Hierarchical Text Classification, Knowledge Graphs, Large Language Models, Zero-shot, Semantic Context

Summary:
Hierarchical Text Classification (HTC) faces challenges in real-world scenarios due to lack of annotated data, large label spaces, and long-tail distributions. To address these issues, Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC) integrates knowledge graphs with Large Language Models (LLMs). The KG-HTC method retrieves relevant subgraphs from knowledge graphs related to input text using a Retrieval-Augmented Generation approach. By enhancing LLMs to understand label semantics at various hierarchy levels, KG-HTC significantly outperforms baselines in the strict zero-shot setting, showing improvements at deeper hierarchy levels. Evaluation on WoS, DBpedia, and Amazon datasets demonstrates the effectiveness of incorporating structured knowledge into LLMs for HTC challenges in large label spaces and long-tailed label distributions. The code for KG-HTC is available on GitHub at https://github.com/QianboZang/KG-HTC.

<br /><br />Summary: <div>
arXiv:2505.05583v1 Announce Type: new 
Abstract: Hierarchical Text Classification (HTC) involves assigning documents to labels organized within a taxonomy. Most previous research on HTC has focused on supervised methods. However, in real-world scenarios, employing supervised HTC can be challenging due to a lack of annotated data. Moreover, HTC often faces issues with large label spaces and long-tail distributions. In this work, we present Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC), which aims to address these challenges of HTC in applications by integrating knowledge graphs with Large Language Models (LLMs) to provide structured semantic context during classification. Our method retrieves relevant subgraphs from knowledge graphs related to the input text using a Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to understand label semantics at various hierarchy levels. We evaluate KG-HTC on three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental results show that KG-HTC significantly outperforms three baselines in the strict zero-shot setting, particularly achieving substantial improvements at deeper levels of the hierarchy. This evaluation demonstrates the effectiveness of incorporating structured knowledge into LLMs to address HTC's challenges in large label spaces and long-tailed label distributions. Our code is available at: https://github.com/QianboZang/KG-HTC.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation</title>
<link>https://arxiv.org/abs/2505.05648</link>
<guid>https://arxiv.org/abs/2505.05648</guid>
<content:encoded><![CDATA[
<div> transformer, differential privacy, language modeling, SwiftKey, GPT2

Summary:<br /><br />
This paper explores the use of a transformer trained with differential privacy for language modeling in SwiftKey. The study involves running experiments to find the optimal balance between model size, run-time speed, and accuracy. The results indicate small yet consistent improvements in next-word prediction and overall accuracy, achieved through scaling down a GPT2 architecture to meet size requirements and a two-stage training process. The approach involves first building a seed model on general data and then fine-tuning it with differential privacy on typing data. By integrating the transformer using ONNX, the researchers were able to maintain flexibility and efficiency. Overall, this study demonstrates the potential of utilizing differential privacy in language modeling tasks, yielding improved performance without sacrificing memory or speed. <div>
arXiv:2505.05648v1 Announce Type: new 
Abstract: In this paper we train a transformer using differential privacy (DP) for language modeling in SwiftKey. We run multiple experiments to balance the trade-off between the model size, run-time speed and accuracy. We show that we get small and consistent gains in the next-word-prediction and accuracy with graceful increase in memory and speed compared to the production GRU. This is obtained by scaling down a GPT2 architecture to fit the required size and a two stage training process that builds a seed model on general data and DP finetunes it on typing data. The transformer is integrated using ONNX offering both flexibility and efficiency.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration of COVID-19 Discourse on Twitter: American Politician Edition</title>
<link>https://arxiv.org/abs/2505.05687</link>
<guid>https://arxiv.org/abs/2505.05687</guid>
<content:encoded><![CDATA[
<div> keywords: COVID-19, political, polarization, Democrats, Republicans

Summary:
Democrats and Republicans exhibit contrasting approaches to the COVID-19 pandemic, as evidenced by an analysis of tweets from leading American political figures. Democrats prioritize the public health aspect, emphasizing casualties, medical precautions, and recommendations. In contrast, Republicans focus on political responsibilities such as media updates and monitoring virus progress. The study utilizes bag-of-words, bigram, and TF-IDF models to identify keywords, topics, and sentiments, revealing distinct partisan attitudes towards handling the crisis. By employing classification algorithms on different language models, the research aims to predict and differentiate a tweet's political leaning based on its COVID-19 related terms. This systematic approach sheds light on the partisan divide in response to the international crisis and highlights the different priorities and perspectives of Democrats and Republicans. <div>
arXiv:2505.05687v1 Announce Type: new 
Abstract: The advent of the COVID-19 pandemic has undoubtedly affected the political scene worldwide and the introduction of new terminology and public opinions regarding the virus has further polarized partisan stances. Using a collection of tweets gathered from leading American political figures online (Republican and Democratic), we explored the partisan differences in approach, response, and attitude towards handling the international crisis. Implementation of the bag-of-words, bigram, and TF-IDF models was used to identify and analyze keywords, topics, and overall sentiments from each party. Results suggest that Democrats are more concerned with the casualties of the pandemic, and give more medical precautions and recommendations to the public whereas Republicans are more invested in political responsibilities such as keeping the public updated through media and carefully watching the progress of the virus. We propose a systematic approach to predict and distinguish a tweet's political stance (left or right leaning) based on its COVID-19 related terms using different classification algorithms on different language models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Robustness to Spurious Correlations in Post-Training Language Models</title>
<link>https://arxiv.org/abs/2505.05704</link>
<guid>https://arxiv.org/abs/2505.05704</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, fine-tuning techniques, spurious correlations, post-training algorithms, synthetic tasks 

Summary: 
The study evaluates three post-training algorithms (SFT, DPO, KTO) on various synthetic tasks to address spurious correlations in large language models (LLMs). These correlations, often arising from biases or dataset artifacts, can impact model performance and generalization. Tasks included mathematical reasoning, instruction-following, and question answering, with varying levels of spuriousness and types of artifacts. Results show models may degrade under higher spuriousness, with preference-based methods (DPO/KTO) showing relative robustness in some tasks. SFT performs better in more complex tasks requiring contextual understanding. The study emphasizes the lack of a universal best post-training strategy, highlighting the importance of task type and spurious correlation nature in selecting the most effective approach.<br /><br />Summary: <div>
arXiv:2505.05704v1 Announce Type: new 
Abstract: Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations -- arising from biases, dataset artifacts, or other "shortcut" features -- that can compromise a model's performance or generalization. In this paper, we systematically evaluate three post-training algorithms -- Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO (Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and spuriousness conditions. Our tasks span mathematical reasoning, constrained instruction-following, and document-grounded question answering. We vary the degree of spurious correlation (10% vs. 90%) and investigate two forms of artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results show that the models often but not always degrade under higher spuriousness. The preference-based methods (DPO/KTO) can demonstrate relative robustness in mathematical reasoning tasks. By contrast, SFT maintains stronger performance in complex, context-intensive tasks. These findings highlight that no single post-training strategy universally outperforms in all scenarios; the best choice depends on the type of target task and the nature of spurious correlations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries</title>
<link>https://arxiv.org/abs/2505.05714</link>
<guid>https://arxiv.org/abs/2505.05714</guid>
<content:encoded><![CDATA[
<div> Dataset, Multimodal Machine Translation, Documentaries, TopicVD, Video-supported<br />
<br />
Summary: <br />
The study introduces TopicVD, a topic-based dataset for video-supported multimodal machine translation (MMT) of documentaries, with video-subtitle pairs categorized into eight topics. An MMT model with a cross-modal bidirectional attention module is proposed to capture shared semantics between text and video. Experimental results show that visual information consistently enhances NMT model performance in documentary translation. However, performance declines in out-of-domain scenarios, emphasizing the need for effective domain adaptation methods. Global context is shown to effectively improve translation performance as well. The dataset and implementations are available at the provided GitHub link. <div>
arXiv:2505.05714v1 Announce Type: new 
Abstract: Most existing multimodal machine translation (MMT) datasets are predominantly composed of static images or short video clips, lacking extensive video data across diverse domains and topics. As a result, they fail to meet the demands of real-world MMT tasks, such as documentary translation. In this study, we developed TopicVD, a topic-based dataset for video-supported multimodal machine translation of documentaries, aiming to advance research in this field. We collected video-subtitle pairs from documentaries and categorized them into eight topics, such as economy and nature, to facilitate research on domain adaptation in video-guided MMT. Additionally, we preserved their contextual information to support research on leveraging the global context of documentaries in video-guided MMT. To better capture the shared semantics between text and video, we propose an MMT model based on a cross-modal bidirectional attention module. Extensive experiments on the TopicVD dataset demonstrate that visual information consistently improves the performance of the NMT model in documentary translation. However, the MMT model's performance significantly declines in out-of-domain scenarios, highlighting the need for effective domain adaptation methods. Additionally, experiments demonstrate that global context can effectively improve translation performance. % Dataset and our implementations are available at https://github.com/JinzeLv/TopicVD
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions</title>
<link>https://arxiv.org/abs/2505.05755</link>
<guid>https://arxiv.org/abs/2505.05755</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive Models, Masked Diffusion Models, Insertion Language Models, Sequence Generation, Token Insertion

Summary: 
Autoregressive Models (ARMs) are effective at sequence generation tasks but struggle with complex constraints and dependencies. Masked Diffusion Models (MDMs) address some limitations but can introduce incoherences when unmasking multiple tokens. Insertion Language Models (ILMs) are introduced to insert tokens at arbitrary positions, allowing for strong dependencies between tokens and generating sequences in arbitrary order. ILMs outperform ARMs and MDMs in planning tasks and perform on par with ARMs in unconditional text generation. They offer greater flexibility than MDMs in text infilling tasks of arbitrary length. To train ILMs, a tailored network parameterization and a denoising objective are used. Empirical evaluation shows the effectiveness of ILMs in various tasks, showcasing their potential for handling complex sequence generation challenges.<br /><br />Summary: <div>
arXiv:2505.05755v1 Announce Type: new 
Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one ``from left to right,'' have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that require satisfying sophisticated constraints or whose sequential dependencies are better addressed by out-of-order generation. Masked Diffusion Models (MDMs) address some of these limitations, but the process of unmasking multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs cannot handle arbitrary infilling constraints when the number of tokens to be filled in is not known in advance. In this work, we introduce Insertion Language Models (ILMs), which learn to insert tokens at arbitrary positions in a sequence -- that is, they select jointly both the position and the vocabulary element to be inserted. By inserting tokens one at a time, ILMs can represent strong dependencies between tokens, and their ability to generate sequences in arbitrary order allows them to accurately model sequences where token dependencies do not follow a left-to-right sequential structure. To train ILMs, we propose a tailored network parameterization and use a simple denoising objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs and MDMs on common planning tasks. Furthermore, we show that ILMs outperform MDMs and perform on par with ARMs in an unconditional text generation task while offering greater flexibility than MDMs in arbitrary-length text infilling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM</title>
<link>https://arxiv.org/abs/2505.05772</link>
<guid>https://arxiv.org/abs/2505.05772</guid>
<content:encoded><![CDATA[
<div> scheme, LLM decoding, PIM architectures, sparsity optimization, STARC

Summary:
STARC is a sparsity-optimized data mapping scheme designed for efficient large language model (LLM) decoding on Processing-in-Memory (PIM) architectures. It clusters key-value (KV) pairs based on semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. Queries can retrieve relevant tokens at cluster granularity by matching against precomputed centroids, allowing for selective attention and parallel processing without the need for frequent reclustering or data movement. Experiments on the HBM-PIM system showed that STARC reduces attention-layer latency and energy consumption significantly compared to common token-wise sparsity methods. Under a KV cache budget of 1024, STARC achieved substantial reductions in latency and energy consumption while maintaining model accuracy comparable to state-of-the-art sparse attention methods. STARC proves to be effective in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures. 

<br /><br />Summary: <div>
arXiv:2505.05772v1 Announce Type: new 
Abstract: Transformer-based models are the foundation of modern machine learning, but their execution, particularly during autoregressive decoding in large language models (LLMs), places significant pressure on memory systems due to frequent memory accesses and growing key-value (KV) caches. This creates a bottleneck in memory bandwidth, especially as context lengths increase. Processing-in-memory (PIM) architectures are a promising solution, offering high internal bandwidth and compute parallelism near memory. However, current PIM designs are primarily optimized for dense attention and struggle with the dynamic, irregular access patterns introduced by modern KV cache sparsity techniques. Consequently, they suffer from workload imbalance, reducing throughput and resource utilization. In this work, we propose STARC, a novel sparsity-optimized data mapping scheme tailored specifically for efficient LLM decoding on PIM architectures. STARC clusters KV pairs by semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. During decoding, queries retrieve relevant tokens at cluster granularity by matching against precomputed centroids, enabling selective attention and parallel processing without frequent reclustering or data movement overhead. Experiments on the HBM-PIM system show that, compared to common token-wise sparsity methods, STARC reduces attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a KV cache budget of 1024, it achieves up to 54%--74% latency reduction and 45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC maintains model accuracy comparable to state-of-the-art sparse attention methods, demonstrating its effectiveness in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted</title>
<link>https://arxiv.org/abs/2505.05815</link>
<guid>https://arxiv.org/abs/2505.05815</guid>
<content:encoded><![CDATA[
<div> Keywords: AnaQuest, multiple-choice questions, language model, Item Response Theory, assessment <br />
Summary:<br />
The study introduces AnaQuest, an innovative prompting technique utilizing a large language model to generate multiple-choice questions. It integrates formative and summative assessments by having students answer open-ended questions initially and then using AI to create MCQs with sentence-level assertions. Expert instructors rated MCQs from AnaQuest and another AI model, ChatGPT, as valid as human-crafted questions. However, Item Response Theory analysis showed that AnaQuest's MCQs, especially those with incorrect assertions, were more comparable to human-crafted questions in terms of difficulty and discrimination than ChatGPT-generated questions. This suggests that AnaQuest is effective in generating valid and challenging MCQs for assessments. <br /> <div>
arXiv:2505.05815v1 Announce Type: new 
Abstract: The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI</title>
<link>https://arxiv.org/abs/2505.05864</link>
<guid>https://arxiv.org/abs/2505.05864</guid>
<content:encoded><![CDATA[
<div> Keywords: experimental datasets, natural language processing, entity recognition, structured data, text-mining

Summary: 
- The article discusses the importance of constructing experimental datasets for data-driven scientific discovery.
- Recent advances in natural language processing have enabled the automatic extraction of structured data from unstructured scientific literature.
- A novel hybrid text-mining framework is proposed to convert unstructured scientific text into structured data by integrating multi-step and direct methods.
- The approach includes transforming raw text into entity-recognized text and then into a structured form, improving entity recognition performance with an entity marker technique.
- The entity marker-based hybrid approach outperforms previous entity recognition methods across benchmark datasets and significantly enhances the quality of final structured data. <div>
arXiv:2505.05864v1 Announce Type: new 
Abstract: The construction of experimental datasets is essential for expanding the scope of data-driven scientific discovery. Recent advances in natural language processing (NLP) have facilitated automatic extraction of structured data from unstructured scientific literature. While existing approaches-multi-step and direct methods-offer valuable capabilities, they also come with limitations when applied independently. Here, we propose a novel hybrid text-mining framework that integrates the advantages of both methods to convert unstructured scientific text into structured data. Our approach first transforms raw text into entity-recognized text, and subsequently into structured form. Furthermore, beyond the overall data structuring framework, we also enhance entity recognition performance by introducing an entity marker-a simple yet effective technique that uses symbolic annotations to highlight target entities. Specifically, our entity marker-based hybrid approach not only consistently outperforms previous entity recognition approaches across three benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the quality of final structured data-yielding up to a 58% improvement in entity-level F1 score and up to 83% improvement in relation-level F1 score compared to direct approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2</title>
<link>https://arxiv.org/abs/2505.05946</link>
<guid>https://arxiv.org/abs/2505.05946</guid>
<content:encoded><![CDATA[
<div> experiment, autoregressive pre-training, Gemma2, large language model, Lithuanian language, continual learning, elastic weight consolidation, benchmarks, catastrophic forgetting effects

Summary:<br /><br />This technical report discusses an experiment on autoregressive pre-training of the Gemma2 2 billion parameter large language model (LLM) with a focus on Lithuanian language learning using CulturaX data. The study applies elastic weight consolidation (EWC) to all model parameters to address catastrophic forgetting and enhance learning of new tasks. Various language understanding benchmarks in English and Lithuanian, including Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande sets, as well as perplexity benchmarks, are evaluated. The empirical results demonstrate the effectiveness of EWC regularization in reducing catastrophic forgetting and improving the LLM's ability to learn new tasks. <div>
arXiv:2505.05946v1 Announce Type: new 
Abstract: This technical report describes an experiment on autoregressive pre-training of Gemma2 2 billion parameter large language model (LLM) with 10\% on the Lithuanian language component of CulturaX from the point of view of continual learning. We apply elastic weight consolidation (EWC) to the full set of the model's parameters and investigate language understanding benchmarks, consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande sets (both in English and Lithuanian versions), and perplexity benchmarks. We empirically demonstrate that EWC regularisation allows us not only to mitigate catastrophic forgetting effects but also that it is potentially beneficial for learning of the new task with LLMs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Summarisation of German Judgments in conjunction with a Class-based Evaluation</title>
<link>https://arxiv.org/abs/2505.05947</link>
<guid>https://arxiv.org/abs/2505.05947</guid>
<content:encoded><![CDATA[
<div> legal documents, automated summarisation, German judgments, language model, legal entities <br />
<br />
Summary: The study focuses on automating the summarisation of lengthy legal documents, specifically German judgments, by fine-tuning a decoder-based language model. The inclusion of information about legal entities before training improves the model's ability to identify relevant content. However, the quality of the generated summaries falls short of practical use, despite enhancements from legal entities. An evaluation framework measures language quality, relevance, completeness, and accuracy of the summaries. This research highlights the potential benefits of leveraging language models for legal document summarisation but underscores the need for further refinement to achieve practical applicability. <br /> <div>
arXiv:2505.05947v1 Announce Type: new 
Abstract: The automated summarisation of long legal documents can be a great aid for legal experts in their daily work. We automatically create summaries (guiding principles) of German judgments by fine-tuning a decoder-based large language model. We enrich the judgments with information about legal entities before the training. For the evaluation of the created summaries, we define a set of evaluation classes which allows us to measure their language, pertinence, completeness and correctness. Our results show that employing legal entities helps the generative model to find the relevant content, but the quality of the created summaries is not yet sufficient for a use in practice.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeoQA: Evidence-based Question Answering with Generated News Events</title>
<link>https://arxiv.org/abs/2505.05949</link>
<guid>https://arxiv.org/abs/2505.05949</guid>
<content:encoded><![CDATA[
<div> benchmark, retrieval-augmented generation, large language models, evidence-based reasoning, NeoQA

Summary:
The article introduces NeoQA, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs). NeoQA is designed to prevent LLMs from relying on pretraining knowledge by generating fictional news events and entities with corresponding news articles and Q&amp;A pairs. This ensures that LLMs must generate responses exclusively from retrieved evidence. The dataset allows for evaluation across various evidence scenarios, including cases with missing or misleading details. Results show that LLMs struggle to differentiate between questions and evidence, and exhibit short-cut reasoning when key information is missing from the evidence. This highlights limitations in evidence-based reasoning and underscores the importance of developing robust methods for handling incomplete or inaccurate information. 

Summary: <div>
arXiv:2505.05949v1 Announce Type: new 
Abstract: Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q\&amp;A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models</title>
<link>https://arxiv.org/abs/2505.05970</link>
<guid>https://arxiv.org/abs/2505.05970</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, interactive training, child language acquisition, reinforcement learning, communicative success

Summary: 
In this study, the authors propose a novel method for training language models in an interactive setting inspired by child language acquisition. They utilize a single-turn dialogue where a speaker communicates with a listener to achieve communicative success and receives rewards. The success is measured in a language-only question-answering context. Through experiments using reinforcement learning, they aim to fine-tune language models. The feasibility study demonstrated that the reward signal indirectly indicates grammaticality. The results showed that imposing cognitive constraints on communication led to interpretable changes in speaker behavior, but did not show improvements in linguistic evaluations. The authors suggest potential modifications to improve task design and training configuration for future studies to observe the benefits of interaction on language learning in computational models. 

<br /><br />Summary: <div>
arXiv:2505.05970v1 Announce Type: new 
Abstract: We propose a method for training language models in an interactive setting inspired by child language acquisition. In our setting, a speaker attempts to communicate some information to a listener in a single-turn dialogue and receives a reward if communicative success is achieved. Unlike earlier related work using image--caption data for interactive reference games, we operationalize communicative success in a more abstract language-only question--answering setting. First, we present a feasibility study demonstrating that our reward provides an indirect signal about grammaticality. Second, we conduct experiments using reinforcement learning to fine-tune language models. We observe that cognitively plausible constraints on the communication channel lead to interpretable changes in speaker behavior. However, we do not yet see improvements on linguistic evaluations from our training regime. We outline potential modifications to the task design and training configuration that could better position future work to use our methodology to observe the benefits of interaction on language learning in computational cognitive models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition</title>
<link>https://arxiv.org/abs/2505.05973</link>
<guid>https://arxiv.org/abs/2505.05973</guid>
<content:encoded><![CDATA[
<div> clustering analysis, word embeddings, shift vectors, Generalized Additive Mixed Models, semantic transparency <br />
Summary: 
This study explores the impact of semantic transparency on word recognition through embedding-based measures. The geometry of complex Malay prefixed words in semantic space was analyzed, revealing distinct clusters based on prefix class. Five measures were derived to predict lexical decision latencies, including word embeddings and shift vectors. Generalized Additive Mixed Models were used to evaluate the predictive power of these measures, with the model incorporating the correlation between each word and their centroid proving to be the most effective. The study highlights the importance of semantic transparency in morphological processing and offers insights into the computational operationalization of this concept. <div>
arXiv:2505.05973v1 Announce Type: new 
Abstract: Studies of morphological processing have shown that semantic transparency is crucial for word recognition. Its computational operationalization is still under discussion. Our primary objectives are to explore embedding-based measures of semantic transparency, and assess their impact on reading. First, we explored the geometry of complex words in semantic space. To do so, we conducted a t-distributed Stochastic Neighbor Embedding clustering analysis on 4,226 Malay prefixed words. Several clusters were observed for complex words varied by their prefix class. Then, we derived five simple measures, and investigated whether they were significant predictors of lexical decision latencies. Two sets of Linear Discriminant Analyses were run in which the prefix of a word is predicted from either word embeddings or shift vectors (i.e., a vector subtraction of the base word from the derived word). The accuracy with which the model predicts the prefix of a word indicates the degree of transparency of the prefix. Three further measures were obtained by comparing embeddings between each word and all other words containing the same prefix (i.e., centroid), between each word and the shift from their base word, and between each word and the predicted word of the Functional Representations of Affixes in Compositional Semantic Space model. In a series of Generalized Additive Mixed Models, all measures predicted decision latencies after accounting for word frequency, word length, and morphological family size. The model that included the correlation between each word and their centroid as a predictor provided the best fit to the data.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models</title>
<link>https://arxiv.org/abs/2505.06004</link>
<guid>https://arxiv.org/abs/2505.06004</guid>
<content:encoded><![CDATA[
<div> language models, grammatical error correction, multilingual, Gemma 9B, English, German, Italian, Swedish
Summary:
- The study examines the performance of 17 popular language models in correcting grammatical errors in texts written in English, German, Italian, and Swedish using a single model for all languages.
- Analysis focuses on minimizing grammatical errors while making minimal changes to the texts.
- Six models are identified as effective in enhancing grammatical correctness across all four languages.
- Gemma 9B is highlighted as the top-performing model among those considered.
- The findings offer insights into the challenges faced by language models in multilingual grammatical error correction tasks and recommend Gemma 9B for such tasks.
<br /><br />Summary: <div>
arXiv:2505.06004v1 Announce Type: new 
Abstract: Recent language models can successfully solve various language-related tasks, and many understand inputs stated in different languages. In this paper, we explore the performance of 17 popular models used to correct grammatical issues in texts stated in English, German, Italian, and Swedish when using a single model to correct texts in all those languages. We analyze the outputs generated by these models, focusing on decreasing the number of grammatical errors while keeping the changes small. The conclusions drawn help us understand what problems occur among those models and which models can be recommended for multilingual grammatical error correction tasks. We list six models that improve grammatical correctness in all four languages and show that Gemma 9B is currently the best performing one for the languages considered.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective</title>
<link>https://arxiv.org/abs/2505.06010</link>
<guid>https://arxiv.org/abs/2505.06010</guid>
<content:encoded><![CDATA[
<div> URL addresses, IBAN numbers, emails, NMT models, entities

Summary:
The paper explores the abilities of popular NMT models, including those from the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities such as URL addresses, IBAN numbers, or emails during translation between English, German, Polish, and Ukrainian. The study investigates the accuracy of these models and discusses the errors they make, focusing on specific challenges like emojis. A new multilingual synthetic dataset of 36,000 sentences is proposed to evaluate entity transfer quality across nine categories in the four languages. Overall, the analysis sheds light on the performance of NMT models in entity preservation and identifies areas for improvement in handling entities during translation.<br /><br />Summary: <div>
arXiv:2505.06010v1 Announce Type: new 
Abstract: Current machine translation models provide us with high-quality outputs in most scenarios. However, they still face some specific problems, such as detecting which entities should not be changed during translation. In this paper, we explore the abilities of popular NMT models, including models from the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities such as URL addresses, IBAN numbers, or emails when producing translations between four languages: English, German, Polish, and Ukrainian. We investigate the quality of popular NMT models in terms of accuracy, discuss errors made by the models, and examine the reasons for errors. Our analysis highlights specific categories, such as emojis, that pose significant challenges for many models considered. In addition to the analysis, we propose a new multilingual synthetic dataset of 36,000 sentences that can help assess the quality of entity transfer across nine categories and four aforementioned languages.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation</title>
<link>https://arxiv.org/abs/2505.06027</link>
<guid>https://arxiv.org/abs/2505.06027</guid>
<content:encoded><![CDATA[
<div> Keywords: Unilogit, machine unlearning, Large Language Models, data privacy regulations, self-distillation

Summary: 
Unilogit introduces a new self-distillation method for Large Language Models, specifically designed for machine unlearning tasks in compliance with data privacy regulations like GDPR. It dynamically adjusts target logits to selectively forget specific information while maintaining overall model utility. By leveraging the current model's outputs for more accurate targets, Unilogit eliminates the need for additional hyperparameters and enhances the model's ability to approximate golden targets. Extensive experiments on public benchmarks and an e-commerce dataset show Unilogit outperforms existing methods such as NPO and UnDIAL in balancing forget and retain objectives. The analysis demonstrates Unilogit's robustness across various scenarios, showcasing its practical applicability and effectiveness in achieving successful machine unlearning tasks.<br /><br />Summary: <div>
arXiv:2505.06027v1 Announce Type: new 
Abstract: This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information</title>
<link>https://arxiv.org/abs/2505.06046</link>
<guid>https://arxiv.org/abs/2505.06046</guid>
<content:encoded><![CDATA[
<div> benchmark, PubHealthBench, Large Language Models, UK Government, public health

Summary:<br /><br />Large Language Models (LLMs) are increasingly accessible, raising the need to assess their knowledge in specific domains like public health. A new benchmark called PubHealthBench with over 8000 questions was introduced to evaluate LLMs' knowledge of UK Government public health information. The benchmark includes Multiple Choice Question Answering (MCQA) and free form response tasks. Evaluation of 24 LLMs showed that the latest models like GPT-4.5 and GPT-4.1 performed well in MCQA, exceeding human performance with basic search engine assistance. However, in free form responses, no model scored above 75%, indicating the need for additional safeguards when providing open-ended public health information. While SOTA LLMs show promise as accurate sources of public health information, there is still room for improvement in providing comprehensive responses in this critical domain. <div>
arXiv:2505.06046v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax</title>
<link>https://arxiv.org/abs/2505.06062</link>
<guid>https://arxiv.org/abs/2505.06062</guid>
<content:encoded><![CDATA[
<div> MWEs, fine-tuned encoder-only models, BERT-based models, idioms, MSUs

Summary:
- The study analyzes attention patterns of fine-tuned BERT-based models towards idioms and MSUs.
- Idioms pose challenges due to semantic non-compositionality, while MSUs display unconventional syntactic behavior.
- The effects of fine-tuning on attention to MWEs are investigated, with a focus on semantic and syntactic tasks.
- Attention scores to MWEs in pre-trained and fine-tuned models across six Indo-European languages are examined.
- Results indicate that fine-tuning impacts how models allocate attention to MWEs, with semantic tasks leading to more even attention to idiomatic expressions and syntactic tasks increasing attention to MSUs in lower layers.

<br /><br />Summary: <div>
arXiv:2505.06062v1 Announce Type: new 
Abstract: This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages - English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models</title>
<link>https://arxiv.org/abs/2505.06110</link>
<guid>https://arxiv.org/abs/2505.06110</guid>
<content:encoded><![CDATA[
<div> dataset, sentiment analysis, transformer-based models, early fusion, multimodal learning <br />
Summary: <br />
This project focuses on multimodal sentiment analysis using the CMU-MOSEI dataset and transformer-based models with early fusion. By integrating text, audio, and visual modalities using BERT-based encoders and early fusion, the model achieves strong performance, with high accuracy and F1-score on the test set. The use of transformer architectures proves superior in capturing cross-modal interactions for sentiment analysis. Training strategies such as Adam optimization, dropout, and early stopping ensure generalization and robustness, leading to precise sentiment intensity prediction with low MAE. Future work may involve comparing fusion strategies or enhancing interpretability. This approach effectively combines linguistic, acoustic, and visual cues for sentiment analysis through multimodal learning. <br /> <div>
arXiv:2505.06110v1 Announce Type: new 
Abstract: This project performs multimodal sentiment analysis using the CMU-MOSEI dataset, using transformer-based models with early fusion to integrate text, audio, and visual modalities. We employ BERT-based encoders for each modality, extracting embeddings that are concatenated before classification. The model achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682 F1-score on the test set, demonstrating the effectiveness of early fusion in capturing cross-modal interactions. The training utilized Adam optimization (lr=1e-4), dropout (0.3), and early stopping to ensure generalization and robustness. Results highlight the superiority of transformer architectures in modeling multimodal sentiment, with a low MAE (0.1060) indicating precise sentiment intensity prediction. Future work may compare fusion strategies or enhance interpretability. This approach utilizes multimodal learning by effectively combining linguistic, acoustic, and visual cues for sentiment analysis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Get Lost In Multi-Turn Conversation</title>
<link>https://arxiv.org/abs/2505.06120</link>
<guid>https://arxiv.org/abs/2505.06120</guid>
<content:encoded><![CDATA[
<div> conversation logs, underspecification, LLM performance, multi-turn setting, simulation experiments

Summary:
Large Language Models (LLMs) serve as conversational interfaces and can assist users in tasks through multi-turn conversations. However, analysis shows that LLM performance drops by 39% in multi-turn settings compared to single-turn interactions. The decrease in performance is attributed to a minor loss in aptitude and a significant increase in unreliability. LLMs tend to make assumptions early in conversations and rely too heavily on these assumptions when generating solutions. Ultimately, when LLMs take a wrong turn in a conversation, they struggle to recover, leading to lower performance in multi-turn interactions. This study highlights the importance of evaluating LLMs in different conversation settings to improve overall performance and reliability.  

<br /><br />Summary: <div>
arXiv:2505.06120v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies</title>
<link>https://arxiv.org/abs/2505.06145</link>
<guid>https://arxiv.org/abs/2505.06145</guid>
<content:encoded><![CDATA[
<div> Few-shot text classification, adaptive fine-tuning, contrastive learning, regularization optimization, Transformer-based models <br />
<br />
Summary: 
This paper introduces a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to enhance the performance of Transformer-based models in few-shot text classification tasks. Experimental results on the FewRel 2.0 dataset demonstrate the effectiveness of T5-small, DeBERTa-v3, and RoBERTa-base models, especially in the 5-shot setting. The study identifies varying levels of classification difficulty across different relationship categories, with some categories posing challenges due to ambiguous boundaries and complex feature distributions. By incorporating contrastive and regularization losses, the model's generalization ability is strengthened, addressing overfitting issues in low-resource scenarios. Furthermore, leveraging Transformer models or generative architectures with robust self-attention mechanisms enhances the stability and accuracy of few-shot classification tasks. <div>
arXiv:2505.06145v1 Announce Type: new 
Abstract: Few-shot text classification has important application value in low-resource environments. This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study</title>
<link>https://arxiv.org/abs/2505.06149</link>
<guid>https://arxiv.org/abs/2505.06149</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, multilingual language models, prompting techniques, zero-shot, few-shot

Summary:
Multilingual language models like LLaMA, Aya, Qwen, and BloomZ show promise in automated hate speech detection across languages. This study evaluates the effectiveness of prompting-based detection using these models in eight non-English languages. While zero-shot and few-shot prompting techniques may not perform as well as fine-tuned encoder models on real-world evaluation sets, they show better generalization on functional hate speech detection tests. The study underscores the importance of customized prompt design for each language to maximize performance. The results suggest that while prompting techniques may lag behind in some aspects, they offer potential for improved hate speech detection across linguistically diverse online content. This research highlights the need for further exploration and refinement of multilingual language models for detecting hate speech effectively. 

<br /><br />Summary: Multilingual language models show promise in hate speech detection but require customized prompting techniques for optimal performance. Zero-shot and few-shot prompting methods outperform fine-tuned encoder models in generalization for hate speech detection tests across multiple languages. <div>
arXiv:2505.06149v1 Announce Type: new 
Abstract: Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets</title>
<link>https://arxiv.org/abs/2505.06150</link>
<guid>https://arxiv.org/abs/2505.06150</guid>
<content:encoded><![CDATA[
<div> scaling law, language models, compute budgets, dataset volume, token efficiency
<br />
Summary:
This study introduces a novel scaling law for fine-tuning large language models (LLMs) within fixed compute budgets, taking into account the data composition aspect. The conventional approach of measuring training data based solely on total tokens is deemed insufficient, as the number of examples and their average token length, or dataset volume, strongly influences model performance. Through experiments on the BRICC and MMLU datasets, it was demonstrated that the composition of data has a significant impact on token efficiency. These findings emphasize the need for more refined scaling laws for effective LLM fine-tuning in scenarios where resources are constrained. <div>
arXiv:2505.06150v1 Announce Type: new 
Abstract: We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework</title>
<link>https://arxiv.org/abs/2505.06151</link>
<guid>https://arxiv.org/abs/2505.06151</guid>
<content:encoded><![CDATA[
<div> Keywords: engagement, natural language processing, counseling sessions, therapeutic success, conversational dynamics

Summary:
- A multi-dimensional natural language processing framework is proposed to objectively classify engagement quality in counseling sessions based on textual transcripts.
- 42 features across four domains are extracted: conversational dynamics, semantic similarity, sentiment classification, and question detection.
- Classifiers like Random Forest, Cat-Boost, and Support Vector Machines are trained using a stratified 5-fold cross-validation and evaluated on a holdout test set.
- Performance significantly improves after SMOTE-Tomek augmentation, with Random Forest achieving up to 88.9% accuracy and SVM reaching 81.1% accuracy.
- Feature contribution analysis shows conversational dynamics and semantic similarity are top contributors, led by words uttered by the client.
- The framework is robust across original and augmented datasets, showing consistent improvements in F1 scores and recall, with potential for future multimodal extensions for holistic assessments.
- This scalable, data-driven method provides real-time feedback to enhance the quality of therapy sessions, both virtual and in-person. 

<br /><br />Summary: 
A natural language processing framework is proposed to classify engagement quality in counseling sessions based on textual transcripts, showing improved performance with data augmentation and robustness across datasets. Feature analysis highlights the importance of conversational dynamics and semantic similarity. The framework offers real-time feedback for enhancing the quality of therapy sessions, supporting future multimodal extensions. <div>
arXiv:2505.06151v1 Announce Type: new 
Abstract: Engagement between client and therapist is a critical determinant of therapeutic success. We propose a multi-dimensional natural language processing (NLP) framework that objectively classifies engagement quality in counseling sessions based on textual transcripts. Using 253 motivational interviewing transcripts (150 high-quality, 103 low-quality), we extracted 42 features across four domains: conversational dynamics, semantic similarity as topic alignment, sentiment classification, and question detection. Classifiers, including Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM), were hyperparameter tuned and trained using a stratified 5-fold cross-validation and evaluated on a holdout test set. On balanced (non-augmented) data, RF achieved the highest classification accuracy (76.7%), and SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation, performance improved significantly: RF achieved up to 88.9% accuracy, 90.0% F1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and 93.6% AUC. The augmented data results reflect the potential of the framework in future larger-scale applications. Feature contribution revealed conversational dynamics and semantic similarity between clients and therapists were among the top contributors, led by words uttered by the client (mean and standard deviation). The framework was robust across the original and augmented datasets and demonstrated consistent improvements in F1 scores and recall. While currently text-based, the framework supports future multimodal extensions (e.g., vocal tone, facial affect) for more holistic assessments. This work introduces a scalable, data-driven method for evaluating engagement quality of the therapy session, offering clinicians real-time feedback to enhance the quality of both virtual and in-person therapeutic interactions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies</title>
<link>https://arxiv.org/abs/2505.06186</link>
<guid>https://arxiv.org/abs/2505.06186</guid>
<content:encoded><![CDATA[
<div> CochraneForest, scientific evidence extraction, biomedical studies, URCA, retrieval-augmented generation

Summary:
The paper introduces CochraneForest, a dataset for document-level scientific evidence extraction in clinical research questions with conflicting evidence. It includes annotated forest plots, associated questions, full texts of studies, and conclusions. The URCA framework is proposed to address challenges in evidence extraction, outperforming existing methods by up to 10.3% in F1 score. The experiments highlight the complexity of CochraneForest, emphasizing its significance as a testing ground for automated evidence synthesis systems. <div>
arXiv:2505.06186v1 Announce Type: new 
Abstract: Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
<div> vulnerability, adversarial perturbations, Universal Adversarial Perturbation (UAP), super transferability, surrogate scaling

Summary:
This paper introduces X-Transfer, a new attack method that exploits a universal vulnerability in CLIP models, particularly focusing on their susceptibility to adversarial perturbations. X-Transfer generates a Universal Adversarial Perturbation (UAP) that can deceive various CLIP encoders and downstream VLMs across different samples, tasks, and domains. The key innovation of X-Transfer is surrogate scaling, which dynamically selects suitable surrogate models to efficiently achieve adversarial transferability. Extensive evaluations show that X-Transfer outperforms existing UAP methods, setting a new benchmark for cross-data, cross-domain, cross-model, and cross-task adversarial transferability in CLIP models. The code for X-Transfer is publicly available on their GitHub repository. <br /><br />Summary: <div>
arXiv:2505.05528v1 Announce Type: cross 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Stress Testing Black-Box LLM Planners</title>
<link>https://arxiv.org/abs/2505.05665</link>
<guid>https://arxiv.org/abs/2505.05665</guid>
<content:encoded><![CDATA[
<div> hallucination, language models, safety-critical scenarios, perturbations, Adaptive Stress Testing<br />
Summary: <br />
The article discusses the challenges of large language models (LLMs) hallucinating unsafe outputs in decision-making tasks, posing risks in safety-critical scenarios. Existing methods detect hallucinations by introducing prompt perturbations to test model stability. The study shows that various perturbations trigger hallucinations in LLMs in a driving environment. To efficiently search prompt perturbations, the authors propose Adaptive Stress Testing (AST) with Monte-Carlo Tree Search (MCTS). The AST formulation discovers scenarios causing model uncertainty, enabling the generation of prompts to influence model behavior. By constructing prompt perturbation trees across diverse scenarios, offline analyses can inform real-time trust assessments of LLMs. This method aims to enhance LLM safety and reliability in decision-making tasks. <br /> <div>
arXiv:2505.05665v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated success in generalizing across decision-making tasks including planning, control and prediction, but their tendency to hallucinate unsafe and undesired outputs poses risks. We argue that detecting such failures is necessary, especially in safety-critical scenarios. Existing black-box methods often detect hallucinations by identifying inconsistencies across multiple samples. Many of these approaches typically introduce prompt perturbations like randomizing detail order or generating adversarial inputs, with the intuition that a confident model should produce stable outputs. We first perform a manual case study showing that other forms of perturbations (e.g., adding noise, removing sensor details) cause LLMs to hallucinate in a driving environment. We then propose a novel method for efficiently searching the space of prompt perturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search (MCTS). Our AST formulation enables discovery of scenarios and prompts that cause language models to act with high uncertainty. By generating MCTS prompt perturbation trees across diverse scenarios, we show that offline analyses can be used at runtime to automatically generate prompts that influence model uncertainty, and to inform real-time trust assessments of an LLM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompted Meta-Learning for Few-shot Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
<div> keywords: few-shot, knowledge graph completion, meta-semantics, prompt, meta-learning <br />
Summary: 
The article introduces a novel framework, PromptMeta, for few-shot knowledge graph completion (KGC) that incorporates both meta-semantics and relational information. The framework includes a meta-semantic prompt pool to capture high-level meta-semantics and a learnable fusion prompt to combine meta-semantic information with task-specific relational information. These components are optimized within a meta-learning framework to enhance knowledge transfer and adaptation to rare and emerging relations in KGs. Experimental results on benchmark datasets demonstrate the effectiveness of PromptMeta in improving few-shot KGC tasks by leveraging rich semantics in knowledge graphs. <div>
arXiv:2505.05684v1 Announce Type: cross 
Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention due to its practical applications in real-world scenarios, where new knowledge often emerges with limited available data. While most existing methods for few-shot KGC have predominantly focused on leveraging relational information, rich semantics inherent in KGs have been largely overlooked. To address this gap, we propose a novel prompted meta-learning (PromptMeta) framework that seamlessly integrates meta-semantics with relational information for few-shot KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that captures and consolidates high-level meta-semantics, enabling effective knowledge transfer and adaptation to rare and newly emerging relations. (2) a learnable fusion prompt that dynamically combines meta-semantic information with task-specific relational information tailored to different few-shot tasks. Both components are optimized together with model parameters within a meta-learning framework. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
<link>https://arxiv.org/abs/2505.05736</link>
<guid>https://arxiv.org/abs/2505.05736</guid>
<content:encoded><![CDATA[
<div> framework, multimodal biomedical data, Large Language Models, preference optimization, knowledge transfer
Summary:
- The MINT framework aligns unimodal Large Language Models (LLMs) with domain-specific decision patterns from multimodal biomedical data through preference optimization.
- It supports different optimization techniques but primarily uses the Odds Ratio Preference Optimization (ORPO) framework.
- MINT leverages an upstream multimodal machine learning (MML) model to transfer domain-specific insights to downstream text-only or image-only LLMs.
- Demonstrated effectiveness in rare genetic disease prediction from texts, outperforming existing models and even larger LLMs.
- Improved tissue type classification using cell nucleus images by aligning downstream image-only models with multimodal knowledge.
<br /><br />Summary: <div>
arXiv:2505.05736v1 Announce Type: cross 
Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification</title>
<link>https://arxiv.org/abs/2505.05744</link>
<guid>https://arxiv.org/abs/2505.05744</guid>
<content:encoded><![CDATA[
<div> Explanation Generation, Surrogate Language Model, Tabular Prediction, Interpretability, Demonstration Selection 
Summary:
The article proposes a novel in-context learning framework for tabular prediction using Large Language Models (LLMs) to generate explanations guiding a Surrogate Language Model (SLM). The framework consists of three stages: Explanation Generation, where LLMs provide insights into reasoning; Explanation-Guided Demonstration Selection, using LLM explanations to select demonstrations; and Explanation-Guided Interpretable SLM Prediction, merging explanations with demonstrations to improve SLM performance and interpretability. Experimental results demonstrate a 5.31% average accuracy improvement across diverse tabular datasets. <div>
arXiv:2505.05744v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable ability in solving complex tasks, making them a promising tool for enhancing tabular learning. However, existing LLM-based methods suffer from high resource requirements, suboptimal demonstration selection, and limited interpretability, which largely hinder their prediction performance and application in the real world. To overcome these problems, we propose a novel in-context learning framework for tabular prediction. The core idea is to leverage the explanations generated by LLMs to guide a smaller, locally deployable Surrogate Language Model (SLM) to make interpretable tabular predictions. Specifically, our framework mainly involves three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to generate explanations for question-answer pairs in candidate demonstrations, providing insights into the reasoning behind the answer. (ii) Post Hoc Explanation-Guided Demonstrations Selection, which utilizes explanations generated by LLMs to guide the process of demonstration selection from candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM Prediction, which utilizes the demonstrations obtained in step (ii) as in-context and merges corresponding explanations as rationales to improve the performance of SLM and guide the model to generate interpretable outputs. Experimental results highlight the framework's effectiveness, with an average accuracy improvement of 5.31% across various tabular datasets in diverse domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection</title>
<link>https://arxiv.org/abs/2505.05763</link>
<guid>https://arxiv.org/abs/2505.05763</guid>
<content:encoded><![CDATA[
<div> Keywords: academic misconduct, biomedical research, deep learning, multimodal fusion, dataset

Summary:
BMMDetect is a novel deep learning framework designed to detect academic misconduct in biomedical research by integrating various data sources and features. It utilizes journal metadata, semantic embeddings, and textual attributes to provide a comprehensive evaluation of manuscripts. The framework incorporates domain-specific features to reduce bias and identifies important predictors such as journal authority metrics and textual anomalies. The BioMCD dataset, consisting of retracted articles and control samples, serves as a benchmark for evaluation. BMMDetect achieves a high AUC of 74.33%, outperforming single-modality baselines by 8.6%. This approach shows promise for detecting misconduct across different biomedical subfields and contributes to the development of scalable and interpretable tools for research integrity. 

Summary: <div>
arXiv:2505.05763v1 Announce Type: cross 
Abstract: Academic misconduct detection in biomedical research remains challenging due to algorithmic narrowness in existing methods and fragmented analytical pipelines. We present BMMDetect, a multimodal deep learning framework that integrates journal metadata (SJR, institutional data), semantic embeddings (PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics, data anomalies) for holistic manuscript evaluation. Key innovations include: (1) multimodal fusion of domain-specific features to reduce detection bias; (2) quantitative evaluation of feature importance, identifying journal authority metrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as dominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with 13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC, outperforming single-modality baselines by 8.6%, and demonstrates transferability across biomedical subfields. This work advances scalable, interpretable tools for safeguarding research integrity.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers</title>
<link>https://arxiv.org/abs/2505.05828</link>
<guid>https://arxiv.org/abs/2505.05828</guid>
<content:encoded><![CDATA[
<div> Keywords: chatbot, mental disorders, self-disclosure technique, teenagers, empathy

Summary:
This paper introduces a chatbot-based system designed to engage young Spanish individuals in understanding various mental disorders through a self-disclosure approach. The system targets teenagers aged 12 to 18, employing a mix of closed and open conversations with controlled messages focusing on specific disorders. By tailoring conversations based on users' sensitivity to a particular disorder, the system aims to establish empathetic communication. Following initial trial questions, the chatbot transitions to open conversations using the GPT-3 language model for more expressive communication. The study indicates that such systems hold promise in capturing the interest of young individuals and raising awareness about mental disorders.<br /><br />Summary: <div>
arXiv:2505.05828v1 Announce Type: cross 
Abstract: This paper presents a chatbot-based system to engage young Spanish people in the awareness of certain mental disorders through a self-disclosure technique. The study was carried out in a population of teenagers aged between 12 and 18 years. The dialogue engine mixes closed and open conversations, so certain controlled messages are sent to focus the chat on a specific disorder, which will change over time. Once a set of trial questions is answered, the system can initiate the conversation on the disorder under the focus according to the user's sensibility to that disorder, in an attempt to establish a more empathetic communication. Then, an open conversation based on the GPT-3 language model is initiated, allowing the user to express themselves with more freedom. The results show that these systems are of interest to young people and could help them become aware of certain mental disorders.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary ecology of words</title>
<link>https://arxiv.org/abs/2505.05863</link>
<guid>https://arxiv.org/abs/2505.05863</guid>
<content:encoded><![CDATA[
<div> evolutionary ecology, words, language models, agent-based models, interactions <br />
Summary: 
The article proposes a model for the evolutionary ecology of words using Large Language Models (LLMs). In this model, agents possess short words generated by LLMs and interact in a spatial environment, with word replacements and mutations based on LLM outputs. Experiments were conducted with a focus on the survival of "strong animal species," resulting in the emergence of diverse populations with species adapted to various extreme habitats. Both gradual and punctuated equilibrium evolution patterns were observed, with dominant species types varying across trials. In a long-term experiment, a large population demonstrated the coexistence of diverse species. This model showcases the potential for LLMs to simulate and explore the evolution of word interactions and the emergence of diverse ecological populations. <br /> <div>
arXiv:2505.05863v1 Announce Type: cross 
Abstract: We propose a model for the evolutionary ecology of words as one attempt to extend evolutionary game theory and agent-based models by utilizing the rich linguistic expressions of Large Language Models (LLMs). Our model enables the emergence and evolution of diverse and infinite options for interactions among agents. Within the population, each agent possesses a short word (or phrase) generated by an LLM and moves within a spatial environment. When agents become adjacent, the outcome of their interaction is determined by the LLM based on the relationship between their words, with the loser's word being replaced by the winner's. Word mutations, also based on LLM outputs, may occur. We conducted preliminary experiments assuming that ``strong animal species" would survive. The results showed that from an initial population consisting of well-known species, many species emerged both gradually and in a punctuated equilibrium manner. Each trial demonstrated the unique evolution of diverse populations, with one type of large species becoming dominant, such as terrestrial animals, marine life, or extinct species, which were ecologically specialized and adapted ones across diverse extreme habitats. We also conducted a long-term experiment with a large population, demonstrating the emergence and coexistence of diverse species.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification</title>
<link>https://arxiv.org/abs/2505.06032</link>
<guid>https://arxiv.org/abs/2505.06032</guid>
<content:encoded><![CDATA[
<div> actor names, shortcuts, language models, attention heads, Head-based Token Attribution

Summary:
The study investigates how language models rely on shortcuts, identified as spurious correlations, in decision-making processes. By utilizing actor names in movie reviews as controllable shortcuts, the researchers identify specific attention heads within the model that focus on these shortcuts. These attention heads lead the model to make premature decisions before fully processing all contextual information. The researchers introduce Head-based Token Attribution (HTA) as a method to trace intermediate decisions back to input tokens and detect shortcuts in language models effectively. By selectively deactivating attention heads related to shortcuts, HTA allows for targeted mitigation of shortcut reliance within language models.<br /><br />Summary: <div>
arXiv:2505.06032v1 Announce Type: cross 
Abstract: Reliance on spurious correlations (shortcuts) has been shown to underlie many of the successes of language models. Previous work focused on identifying the input elements that impact prediction. We investigate how shortcuts are actually processed within the model's decision-making mechanism. We use actor names in movie reviews as controllable shortcuts with known impact on the outcome. We use mechanistic interpretability methods and identify specific attention heads that focus on shortcuts. These heads gear the model towards a label before processing the complete input, effectively making premature decisions that bypass contextual analysis. Based on these findings, we introduce Head-based Token Attribution (HTA), which traces intermediate decisions back to input tokens. We show that HTA is effective in detecting shortcuts in LLMs and enables targeted mitigation by selectively deactivating shortcut-related attention heads.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models</title>
<link>https://arxiv.org/abs/2505.06107</link>
<guid>https://arxiv.org/abs/2505.06107</guid>
<content:encoded><![CDATA[
<div> detect nationality, migration research, digital trace data, machine learning model, return migration<br />
Summary:<br />
The lack of nationality information in web and digital trace data poses challenges for migration research. To address this issue, a method to detect nationality using full names was proposed. Using a character-based machine learning model trained on Wikipedia data, the study achieved high accuracy in categorizing nationalities. Applying this method to academic data revealed discrepancies in the estimation of return migration when using country of academic origin versus country of name origin. For instance, a significant proportion of emigrants from the USA were actually return migrants when considering their names. The study also found that scholars' names can provide valuable insights into migration patterns, such as the high proportion of scholars with Chinese names transitioning from the USA to China. Overall, the proposed methods offer a solution to left-censoring issues in migration research using digital trace data.<br /> 
Summary: <div>
arXiv:2505.06107v1 Announce Type: cross 
Abstract: Most web and digital trace data do not include information about an individual's nationality due to privacy concerns. The lack of data on nationality can create challenges for migration research. It can lead to a left-censoring issue since we are uncertain about the migrant's country of origin. Once we observe an emigration event, if we know the nationality, we can differentiate it from return migration. We propose methods to detect the nationality with the least available data, i.e., full names. We use the detected nationality in comparison with the country of academic origin, which is a common approach in studying the migration of researchers. We gathered 2.6 million unique name-nationality pairs from Wikipedia and categorized them into families of nationalities with three granularity levels to use as our training data. Using a character-based machine learning model, we achieved a weighted F1 score of 84% for the broadest and 67% for the most granular, country-level categorization. In our empirical study, we used the trained and tested model to assign nationality to 8+ million scholars' full names in Scopus data. Our results show that using the country of first publication as a proxy for nationality underestimates the size of return flows, especially for countries with a more diverse academic workforce, such as the USA, Australia, and Canada. We found that around 48% of emigration from the USA was return migration once we used the country of name origin, in contrast to 33% based on academic origin. In the most recent period, 79% of scholars whose affiliation has consistently changed from the USA to China, and are considered emigrants, have Chinese names in contrast to 41% with a Chinese academic origin. Our proposed methods for addressing left-censoring issues are beneficial for other research that uses digital trace data to study migration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
<link>https://arxiv.org/abs/2505.06184</link>
<guid>https://arxiv.org/abs/2505.06184</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media profiling, Large language model, User behavior modeling, Misinformation detection, Twitter dataset <br />
Summary: Our novel approach leverages large language models to create interpretable user profiles through domain-defining statements. The method consists of a two-stage process involving semi-supervised filtering with a knowledge base and generating abstractive and extractive user profiles. By harnessing the knowledge of LLMs with minimal human validation, our approach reduces the need for large labeled datasets while producing flexible and adaptable profiles. We introduce a Persian political Twitter (X) dataset and an evaluation framework for LLM-based profiling. Experimental results demonstrate a 9.8% improvement over state-of-the-art methods, showcasing the effectiveness of our approach in creating interpretable and adaptable user profiles. <br /><br />Summary: Our approach utilizes large language models to create interpretable user profiles for social media tasks. Through domain-defining statements and a two-stage process, the method effectively condenses user data into flexible profiles for downstream tasks. Results show a significant improvement over existing methods, underscoring the adaptability and effectiveness of our approach. <div>
arXiv:2505.06184v1 Announce Type: cross 
Abstract: Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Concepts</title>
<link>https://arxiv.org/abs/2505.06191</link>
<guid>https://arxiv.org/abs/2505.06191</guid>
<content:encoded><![CDATA[
<div> Keywords: concept-centric paradigm, neuro-symbolic concepts, continual learning, compositional generalization, zero-shot transfer

Summary:
The article introduces a concept-centric paradigm for developing agents capable of continuous learning and flexible reasoning. These agents operate by utilizing a specialized vocabulary of neuro-symbolic concepts, grounded in sensory inputs and actions, and allowing for the creation of new concepts through compositional combination. Concept types are represented using a hybrid approach of symbolic programs and neural networks, enabling efficient learning and problem-solving across diverse domains like images, videos, 3D scenes, and robotics. The concept-centric framework provides benefits such as data efficiency, broad generalization, continual learning, and the ability to transfer knowledge to new tasks without prior training. <div>
arXiv:2505.06191v1 Announce Type: cross 
Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PART: Pre-trained Authorship Representation Transformer</title>
<link>https://arxiv.org/abs/2209.15373</link>
<guid>https://arxiv.org/abs/2209.15373</guid>
<content:encoded><![CDATA[
<div> Keywords: authorship embeddings, stylometric representations, contrastive training, text classification, data visualization

Summary:
In this paper, the authors introduce PART, a model trained to learn authorship embeddings rather than focusing on semantics. By training on a diverse dataset of texts from literature authors, blog posters, and corporate email accounts, PART shows competitive performance on various authorship challenges. When evaluated on test datasets, PART achieves a remarkable zero-shot accuracy of 72.39% with 250 authors, outperforming RoBERTa embeddings by 54% and 56%. The model's representations are qualitatively analyzed through data visualizations, revealing insights into the author's gender, age, and occupation based on writing styles. This approach demonstrates the effectiveness of utilizing stylometric representations for authorship identification tasks, highlighting the significance of contrastive training in improving model performance across diverse author profiles.<br /><br />Summary: <div>
arXiv:2209.15373v2 Announce Type: replace 
Abstract: Authors writing documents imprint identifying information within their texts: vocabulary, registry, punctuation, misspellings, or even emoji usage. Previous works use hand-crafted features or classification tasks to train their authorship models, leading to poor performance on out-of-domain authors. Using stylometric representations is more suitable, but this by itself is an open research challenge. In this paper, we propose PART, a contrastively trained model fit to learn \textbf{authorship embeddings} instead of semantics. We train our model on ~1.5M texts belonging to 1162 literature authors, 17287 blog posters and 135 corporate email accounts; a heterogeneous set with identifiable writing styles. We evaluate the model on current challenges, achieving competitive performance. We also evaluate our model on test splits of the datasets achieving zero-shot 72.39\% accuracy when bounded to 250 authors, a 54\% and 56\% higher than RoBERTa embeddings. We qualitatively assess the representations with different data visualizations on the available datasets, observing features such as gender, age, or occupation of the author.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking Heads: Understanding Inter-layer Communication in Transformer Language Models</title>
<link>https://arxiv.org/abs/2406.09519</link>
<guid>https://arxiv.org/abs/2406.09519</guid>
<content:encoded><![CDATA[
<div> low-rank subspaces, communication channels, transformer language models, context retrieval, Singular Value Decomposition <br />
Summary:<br />
The study explores how transformer language models (LMs) use low-rank subspaces to represent and route information between layers. A mechanism that selectively inhibits items in a context is analyzed, revealing the formation of low-rank communication channels between layers. By decomposing attention heads using Singular Value Decomposition (SVD), interactions between heads in different layers can be predicted based on weight matrices alone. The study shows how manipulating internal model representations and editing model weights based on the discovered mechanism can significantly improve performance on a synthetic Laundry List task. The analysis uncovers an intricate interpretable structure learned during LM pretraining and sheds light on the reasons behind the occasional failures of sophisticated LMs in simpler domains, providing insights for future studies on more complex behaviors. <br /> <div>
arXiv:2406.09519v4 Announce Type: replace 
Abstract: Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank communication channels (Elhage et al., 2021) between layers. A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. That is, the model has trouble copying the correct information from context when many items ``crowd" this limited space. By decomposing attention heads with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices alone. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?</title>
<link>https://arxiv.org/abs/2407.11963</link>
<guid>https://arxiv.org/abs/2407.11963</guid>
<content:encoded><![CDATA[
<div> framework, bilingual, long-context tasks, retrieval, reasoning

Summary:
NeedleBench is a synthetic framework designed to assess the retrieval and reasoning performance of large language models in bilingual long-context tasks with adaptive context lengths. It categorizes tasks into information-sparse and information-dense scenarios to simulate simple retrieval and complex reasoning tasks, respectively. The framework embeds key data points at varying depths to rigorously test model capabilities. Findings from experiments show that models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning but struggle with continuous retrieval and reasoning in information-dense scenarios. The study also identifies a phenomenon dubbed 'under-thinking,' where models prematurely conclude reasoning despite available information. NeedleBench provides critical insights and tools for evaluating and enhancing the long-context capabilities of large language models. Resources are available on the OpenCompass GitHub repository at https://github.com/open-compass/opencompass. 

Summary: <br /><br /> <div>
arXiv:2407.11963v2 Announce Type: replace 
Abstract: The capability of large language models to handle long-context information is crucial across various real-world applications. Existing evaluation methods often rely either on real-world long texts, making it difficult to exclude the influence of models' inherent knowledge, or introduce irrelevant filler content to artificially achieve target lengths, reducing assessment effectiveness. To address these limitations, we introduce NeedleBench, a synthetic framework for assessing retrieval and reasoning performance in bilingual long-context tasks with adaptive context lengths. NeedleBench systematically embeds key data points at varying depths to rigorously test model capabilities. Tasks are categorized into two scenarios: information-sparse, featuring minimal relevant details within extensive irrelevant text to simulate simple retrieval tasks; and information-dense (the Ancestral Trace Challenge), where relevant information is continuously distributed throughout the context to simulate complex reasoning tasks. Our experiments reveal that although recent reasoning models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they struggle with continuous retrieval and reasoning in information-dense scenarios, even at shorter context lengths. We also characterize a phenomenon termed 'under-thinking', where models prematurely conclude reasoning despite available information. NeedleBench thus provides critical insights and targeted tools essential for evaluating and improving LLMs' long-context capabilities. All resources are available at OpenCompass: https://github.com/open-compass/opencompass.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget</title>
<link>https://arxiv.org/abs/2408.00103</link>
<guid>https://arxiv.org/abs/2408.00103</guid>
<content:encoded><![CDATA[
<div> Retriever-Reader architecture, Entity Linking, Relation Extraction, ReLiK, input representation<br />
Summary:<br />
In this paper, a novel Retriever-Reader architecture named ReLiK is proposed for Entity Linking (EL) and Relation Extraction (RE) tasks in Natural Language Processing. The Retriever module identifies candidate entities or relations in the input text, while the Reader module determines the relevant entities or relations and aligns them with the text. An innovative input representation incorporating candidate entities or relations with the text enables linking entities and extracting relations in a single pass, leveraging pre-trained language models. The approach achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks with academic budget training and up to 40x faster inference speed. Additionally, the architecture seamlessly applies to Information Extraction (cIE) by sharing a Reader for entity and relation extraction, further setting a new state-of-the-art. <div>
arXiv:2408.00103v3 Announce Type: replace 
Abstract: Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits</title>
<link>https://arxiv.org/abs/2410.18234</link>
<guid>https://arxiv.org/abs/2410.18234</guid>
<content:encoded><![CDATA[
<div> draft models, token-level draft selection scheme, linear program, importance sampling, speculative sampling <br />
Summary: <br />
The article discusses multi-draft speculative sampling, focusing on token-level draft selection schemes. It introduces an optimal scheme derived from a linear program solution and decomposes it into a two-step process involving importance sampling and speculative sampling. For identical draft models, conditions for achieving a maximum acceptance probability are established, along with an explicit expression for optimal acceptance probability. Additionally, the study proposes a new class of selection schemes based on weighted importance sampling. Experimental results show significant enhancements in block efficiency and token rates compared to baseline schemes across various scenarios. <div>
arXiv:2410.18234v2 Announce Type: replace 
Abstract: We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation</title>
<link>https://arxiv.org/abs/2411.11053</link>
<guid>https://arxiv.org/abs/2411.11053</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning-augmented data generation, SRA-MCTS, self-improvement, complex tasks<br />
Summary:<br />
The article discusses the challenges faced by large language models in tackling complex tasks and proposes a novel solution called SRA-MCTS. This method enhances the model's reasoning and problem decomposition capabilities by autonomously generating high-quality reasoning paths. Through a feedback loop, the model continuously improves its performance without additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy in solving complex tasks. Experimental results show performance improvements across different model scales, highlighting the significant potential of self-improvement in smaller models. The method remains robust even when traditional approaches exhibit performance degradation, with notable enhancements seen in diversity metrics. The study emphasizes the importance of exploring reasoning processes within training data to enhance language models' ability to address complex problems.<br /><br />Summary: <div>
arXiv:2411.11053v5 Announce Type: replace 
Abstract: Large language models demonstrate exceptional performance in simple code generation tasks but still face challenges in tackling complex problems. These challenges may stem from insufficient reasoning and problem decomposition capabilities. To address this issue, we propose a reasoning-augmented data generation process, SRA-MCTS, which guides the model to autonomously generate high-quality intermediate reasoning paths. This creates a positive feedback loop, enabling continuous improvement. Our method operates entirely through the model itself without requiring additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy and enhances the success rate in solving complex tasks. Experimental results show that, even without additional supervisory signals, our method achieves performance improvements across different model scales, demonstrating the significant potential of self-improvement in small models. Furthermore, the method remains robust when traditional Chain-of-Thought (CoT) approaches exhibit performance degradation, with notable improvements observed in diversity metrics such as pass@10. We encourage further exploration of reasoning processes within training data to enhance the ability of language models to address complex problems. Our code and data are public at https://github.com/DIRECT-BIT/SRA-MCTS.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes</title>
<link>https://arxiv.org/abs/2501.12106</link>
<guid>https://arxiv.org/abs/2501.12106</guid>
<content:encoded><![CDATA[
<div> Keywords: tumor documentation, large language models, medical NLP, dataset, German-language

Summary:
Large language models (LLMs) were evaluated for tumor documentation tasks in Germany, showing potential to automate the process efficiently and reliably. Eleven open source LLMs were tested on tasks such as identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis using a dataset of annotated text snippets from urology. Models with 7-12 billion parameters performed well, indicating an optimal balance between performance and resource efficiency. Few-shot prompting with examples from different medical domains improved outcomes, demonstrating the LLMs' versatility. The study highlights the potential of LLMs in clinical documentation and offers valuable resources, including code for evaluation and the released dataset for German-language medical NLP benchmarking.<br /><br />Summary: <div>
arXiv:2501.12106v3 Announce Type: replace 
Abstract: Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors' notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2501.14851</link>
<guid>https://arxiv.org/abs/2501.14851</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, deductive reasoning, benchmark, JustLogic, error analysis 

Summary: 
JustLogic is a new deductive reasoning benchmark designed to evaluate Large Language Models (LLMs). Unlike existing benchmarks, JustLogic is highly complex, independent of prior knowledge, and allows for in-depth error analysis on reasoning depth and argument form. Experimental results show that state-of-the-art reasoning LLMs perform comparably to the human average but fall short of the human ceiling. Non-reasoning models still lag behind the human average. JustLogic provides a challenging evaluation platform for assessing LLMs' deductive reasoning capabilities, addressing deficiencies in current benchmarks such as task complexity and confounding factors. The benchmark is publicly available, enabling further research and development. 

<br /><br />Summary: <div>
arXiv:2501.14851v2 Announce Type: replace 
Abstract: Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average. All code and data are available at https://github.com/michaelchen-lab/JustLogic
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought</title>
<link>https://arxiv.org/abs/2501.16154</link>
<guid>https://arxiv.org/abs/2501.16154</guid>
<content:encoded><![CDATA[
<div> multilingual models, pretraining, AdaCoT, factual reasoning, low-resource languages <br />
<br />
Summary: 
The article introduces AdaCoT, a framework designed to improve multilingual factual reasoning by dynamically routing thought processes through intermediary "thinking languages" before generating responses in the target language. AdaCoT utilizes a language-agnostic core and incorporates an adaptive, reward-based mechanism to select optimal reasoning pathways without the need for additional pretraining. Through comprehensive evaluations on multiple benchmarks, AdaCoT demonstrates significant enhancements in both factual reasoning quality and cross-lingual consistency, particularly benefiting low-resource languages. The results indicate that adaptive reasoning paths can effectively narrow the performance gap between high and low-resource languages while preserving cultural and linguistic nuances. <div>
arXiv:2501.16154v2 Announce Type: replace 
Abstract: Large language models have shown impressive multilingual capabilities through pretraining on diverse corpora. While these models show strong reasoning abilities, their performance varies significantly across languages due to imbalanced training data distribution. Existing approaches using sample-level translation for extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages. In this paper, we introduce AdaCoT (Adaptive Chain-of-Thought), a framework that enhances multilingual factual reasoning by dynamically routing thought processes in intermediary ``thinking languages'' before generating target-language responses. AdaCoT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phonetic accommodation and inhibition in a dynamic neural field model</title>
<link>https://arxiv.org/abs/2502.01210</link>
<guid>https://arxiv.org/abs/2502.01210</guid>
<content:encoded><![CDATA[
<div> Keywords: short-term phonetic accommodation, accent change, speech planning representations, dynamic neural field equations, memory dynamics <br />
Summary: 
The study investigates how real-time input from another speaker's voice influences the speech planning representations of an interlocutor during short-term phonetic accommodation. A computational model grounded in dynamic neural field equations for movement planning and memory dynamics is proposed. The model suggests that convergence to a model talker on one trial can lead to divergence on subsequent trials due to delayed inhibitory effects in the memory field. Empirical patterns from an experimental pilot study are compared with the model's predictions, indicating that variations in inhibitory memory dynamics may reflect resistance to accommodation influenced by phonological and sociolinguistic pressures. The findings shed light on the relationship between short-term phonetic accommodation and accent change, highlighting the complex interplay between memory dynamics and societal influences on speech adaptation.<br /><br />Summary: <div>
arXiv:2502.01210v2 Announce Type: replace 
Abstract: Short-term phonetic accommodation is a fundamental driver behind accent change, but how does real-time input from another speaker's voice shape the speech planning representations of an interlocutor? We advance a computational model of change in speech planning representations during phonetic accommodation, grounded in dynamic neural field equations for movement planning and memory dynamics. A dual-layer planning/memory field predicts that convergence to a model talker on one trial can trigger divergence on subsequent trials, due to a delayed inhibitory effect in the more slowly evolving memory field. The model's predictions are compared with empirical patterns of accommodation from an experimental pilot study. We show that observed empirical phenomena may correspond to variation in the magnitude of inhibitory memory dynamics, which could reflect resistance to accommodation due to phonological and/or sociolinguistic pressures. We discuss the implications of these results for the relations between short-term phonetic accommodation and sound change.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs</title>
<link>https://arxiv.org/abs/2502.04134</link>
<guid>https://arxiv.org/abs/2502.04134</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, order sensitivity, reliability, input arrangement, output accuracy

Summary:
Large language models (LLMs) are crucial in various applications, but the issue of order sensitivity remains a challenge. This study explores the impact of input order on LLM performance across tasks like paraphrasing and relevance judgment. Results reveal that shuffled inputs significantly decrease output accuracy, highlighting the persistent risks associated with order sensitivity in high-stakes applications. While few-shot prompting shows some effectiveness in mitigating the issue, it does not completely solve the problem. The study underscores the need for more robust LLMs or improved input-handling techniques in future development to address the reliability concerns posed by order sensitivity in hidden LLM components. 

Summary: <div>
arXiv:2502.04134v2 Announce Type: replace 
Abstract: As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in the input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in LLMs whose internal components are hidden from users (such as closed-source models or those accessed via API calls). We conduct experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation; however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids</title>
<link>https://arxiv.org/abs/2502.09667</link>
<guid>https://arxiv.org/abs/2502.09667</guid>
<content:encoded><![CDATA[
<div> Keywords: k-LLMmeans, text clustering, LLM-generated summaries, semantic interpretability, mini-batch variant

Summary: 
The study introduces k-LLMmeans, a modified version of the k-means algorithm for text clustering that uses LLM-generated summaries as cluster centroids to capture semantic nuances. This approach enhances semantic interpretability while maintaining the optimization properties of k-means and addressing scalability and instability issues associated with LLM-based clustering. The method does not rely on increased LLM usage with dataset size and provides transparent intermediate outputs. Additionally, a mini-batch variant is introduced for efficient real-time clustering of streaming text. Experimental results across various datasets, embeddings, and LLMs demonstrate that k-LLMmeans consistently outperforms traditional baselines and achieves comparable results to state-of-the-art LLM-based clustering with fewer LLM calls. A case study on sequential text streams is presented, along with a new benchmark dataset from StackExchange for evaluating text-stream clustering methods. 

<br /><br />Summary: <div>
arXiv:2502.09667v2 Announce Type: replace 
Abstract: We introduce k-LLMmeans, a novel modification of the k-means algorithm for text clustering that leverages LLM-generated summaries as cluster centroids, capturing semantic nuances often missed by purely numerical averages. This design preserves the core optimization properties of k-means while enhancing semantic interpretability and avoiding the scalability and instability issues typical of modern LLM-based clustering. Unlike existing methods, our approach does not increase LLM usage with dataset size and produces transparent intermediate outputs. We further extend it with a mini-batch variant for efficient, real-time clustering of streaming text. Extensive experiments across multiple datasets, embeddings, and LLMs show that k-LLMmeans consistently outperforms k-means and other traditional baselines and achieves results comparable to state-of-the-art LLM-based clustering, with a fraction of the LLM calls. Finally, we present a case study on sequential text streams and introduce a new benchmark dataset constructed from StackExchange to evaluate text-stream clustering methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization</title>
<link>https://arxiv.org/abs/2502.20364</link>
<guid>https://arxiv.org/abs/2502.20364</guid>
<content:encoded><![CDATA[
arXiv:2502.20364v2 Announce Type: replace 
Abstract: Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2503.17460</link>
<guid>https://arxiv.org/abs/2503.17460</guid>
<content:encoded><![CDATA[
arXiv:2503.17460v2 Announce Type: replace 
Abstract: In this paper, we present ConvoGen: an innovative framework for generating synthetic conversational data using multi-agent systems. Our method leverages few-shot learning and introduces iterative sampling from a dynamically updated few-shot hub to create diverse and realistic conversational scenarios. The generated data has numerous applications, including training and evaluating conversational AI models, and augmenting existing datasets for tasks like conversational intent classification or conversation summarization. Our experiments demonstrate the effectiveness of this method in producing high-quality diverse synthetic conversational data, highlighting its potential to enhance the development and evaluation of conversational AI systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Urban Science: Scaling Causal Inference with Large Language Models</title>
<link>https://arxiv.org/abs/2504.12345</link>
<guid>https://arxiv.org/abs/2504.12345</guid>
<content:encoded><![CDATA[
arXiv:2504.12345v2 Announce Type: replace 
Abstract: Urban causal research is essential for understanding the complex dynamics of cities and informing evidence-based policies. However, it is challenged by the inefficiency and bias of hypothesis generation, barriers to multimodal data complexity, and the methodological fragility of causal experimentation. Recent advances in large language models (LLMs) present an opportunity to rethink how urban causal analysis is conducted. This Perspective examines current urban causal research by analyzing taxonomies that categorize research topics, data sources, and methodological approaches to identify structural gaps. We then introduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy recommendations. We propose evaluation criteria for rigor and transparency and reflect on implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces AI-augmented workflows not as replacements for human expertise but as tools to broaden participation, improve reproducibility, and unlock more inclusive forms of urban causal reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
<link>https://arxiv.org/abs/2406.09831</link>
<guid>https://arxiv.org/abs/2406.09831</guid>
<content:encoded><![CDATA[
arXiv:2406.09831v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.24289</link>
<guid>https://arxiv.org/abs/2503.24289</guid>
<content:encoded><![CDATA[
arXiv:2503.24289v2 Announce Type: replace-cross 
Abstract: We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2404.04545</link>
<guid>https://arxiv.org/abs/2404.04545</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, Cross-Attention Network, Text-oriented, Gated Control Mechanism, Joint Learning  

<br /><br />Summary:  
Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic signals to determine human sentiment. Previous MSA techniques demonstrated strong performance but often treated different modalities equally, leading to a lack of attention to the variation in semantic richness among them. Recognizing that certain modalities may carry more weight, the authors propose the Text-oriented Cross-Attention Network (TCAN), which prioritizes the text modality. The framework begins by organizing unimodal features into visual-text and acoustic-text pairs, followed by implementing self-attention on the text modality. The method includes text-queried cross-attention to refine features from the visual and acoustic streams. A gated control mechanism is used to control noise and redundant features, enhancing the systems focus on significant data. Furthermore, unimodal joint learning is employed to better understand emotional dynamics across different modalities via backpropagation. Extensive experiments validate the effectiveness of the TCAN model, showing consistent improvements over state-of-the-art MSA methods on two datasets, CMU-MOSI and CMU-MOSEI. This approach innovatively addresses the challenge of multimodal heterogeneities in sentiment analysis, marking a significant advancement in the field. <div>
arXiv:2404.04545v3 Announce Type: replace-cross 
Abstract: Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the variation in semantic richness among different modalities, treating each modality uniformly. This approach may lead to underestimating the significance of strong modalities while overemphasizing the importance of weak ones. Motivated by these insights, we introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the predominant role of the text modality in MSA. Specifically, for each multimodal sample, by taking unaligned sequences of the three modalities as inputs, we initially allocate the extracted unimodal features into a visual-text and an acoustic-text pair. Subsequently, we implement self-attention on the text modality and apply text-queried cross-attention to the visual and acoustic modalities. To mitigate the influence of noise signals and redundant features, we incorporate a gated control mechanism into the framework. Additionally, we introduce unimodal joint learning to gain a deeper understanding of homogeneous emotional tendencies across diverse modalities through backpropagation. Experimental results demonstrate that TCAN consistently outperforms state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks</title>
<link>https://arxiv.org/abs/2505.04628</link>
<guid>https://arxiv.org/abs/2505.04628</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social interaction, benchmark, communication, efficiency

<br /><br />Summary: The article discusses the need for large language models (LLMs) to extend their roles beyond being single-user assistants to effectively engage in multi-user, multi-turn social agent tasks within complex environments. Currently, there is no systematic approach to measure this capability. To tackle this issue, the authors introduce an agent task leveling framework based on sociological principles and a new benchmark called How Social Is It (HSII). HSII is aimed at evaluating the social capabilities of LLMs through tasks that encompass format parsing, target selection, target switching conversation, and stable conversation, all derived from a dataset called HSII-Dataset. They also conduct an ablation study involving clustering to enhance the dataset's effectiveness. Additionally, the impact of the chain of thought (COT) method on LLMs' social performance is examined. The authors propose a new metric, COT-complexity, to measure the efficiency of LLMs using COT for social tasks, aiming to balance correctness and efficiency. Experimental results indicate that HSII serves as a suitable benchmark for assessing the social skills of LLMs. <div>
arXiv:2505.04628v1 Announce Type: new 
Abstract: Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.04637</link>
<guid>https://arxiv.org/abs/2505.04637</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, human cognition, tokenization, dynamic representation, Visual Question Answering<br /><br />Summary: This research explores the discrepancies between human cognitive processes and the methodologies employed by multimodal large language models (MLLMs) for integrating diverse data types. It systematically investigates how human cross-modal chunking mechanisms align with token representation in MLLMs. Through empirical studies, the authors compare human performance with model behaviors across visual-linguistic tasks, revealing that traditional static tokenization significantly limits models from simulating the dynamic nature of human information processing. To address these limitations, the study proposes a new framework for dynamic cross-modal tokenization which includes adaptive boundaries, hierarchical representations, and alignment mechanisms based on cognitive science principles. Quantitative evaluations indicate that this novel approach surpasses existing state-of-the-art models, achieving improvements of +7.8% on Visual Question Answering and +5.3% on Complex Scene Description tasks. Additionally, it demonstrates more human-like error patterns and attention distributions. The findings contribute to a deeper theoretical understanding of the relationship between human cognition and AI, providing valuable empirical evidence for creating AI systems that are more aligned with cognitive processes. <div>
arXiv:2505.04637v1 Announce Type: new 
Abstract: Recent advancements in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing diverse data types, yet significant disparities persist between human cognitive processes and computational approaches to multimodal information integration. This research presents a systematic investigation into the parallels between human cross-modal chunking mechanisms and token representation methodologies in MLLMs. Through empirical studies comparing human performance patterns with model behaviors across visual-linguistic tasks, we demonstrate that conventional static tokenization schemes fundamentally constrain current models' capacity to simulate the dynamic, context-sensitive nature of human information processing. We propose a novel framework for dynamic cross-modal tokenization that incorporates adaptive boundaries, hierarchical representations, and alignment mechanisms grounded in cognitive science principles. Quantitative evaluations demonstrate that our approach yields statistically significant improvements over state-of-the-art models on benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene Description) while exhibiting more human-aligned error patterns and attention distributions. These findings contribute to the theoretical understanding of the relationship between human cognition and artificial intelligence, while providing empirical evidence for developing more cognitively plausible AI systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language translation, and change of accent for speech-to-speech task using diffusion model</title>
<link>https://arxiv.org/abs/2505.04639</link>
<guid>https://arxiv.org/abs/2505.04639</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech-to-speech translation, accent adaptation, conditional generation, diffusion models, Mel spectrograms

<br /><br />Summary: The paper presents a unified approach for simultaneous speech-to-speech translation (S2ST) and accent adaptation. Traditional methods have largely focused on either translating spoken content or adjusting accents, but effective cross-cultural communication requires addressing both aspects concurrently. The authors reformulate S2ST as a conditional generation task where the target speech is generated based on phonemes while integrating features that reflect the desired accent of the target language. To achieve this, they leverage diffusion models, which are recognized for their high-fidelity generative capabilities, adapting strategies commonly used in text-to-image diffusion. Their model conditions on source speech transcriptions to generate Mel spectrogramsacoustic representations of speechreflecting both linguistic content and accentual characteristics. This integrated framework allows for joint optimization of translation and accent adaptation, providing a more parameter-efficient and effective solution compared to conventional S2ST pipelines. By exploring this underexplored area in the literature, the authors aim to enhance cross-cultural communication through more natural-sounding and contextually appropriate speech output in the target language. <div>
arXiv:2505.04639v1 Announce Type: new 
Abstract: Speech-to-speech translation (S2ST) aims to convert spoken input in one language to spoken output in another, typically focusing on either language translation or accent adaptation. However, effective cross-cultural communication requires handling both aspects simultaneously - translating content while adapting the speaker's accent to match the target language context. In this work, we propose a unified approach for simultaneous speech translation and change of accent, a task that remains underexplored in current literature. Our method reformulates the problem as a conditional generation task, where target speech is generated based on phonemes and guided by target speech features. Leveraging the power of diffusion models, known for high-fidelity generative capabilities, we adapt text-to-image diffusion strategies by conditioning on source speech transcriptions and generating Mel spectrograms representing the target speech with desired linguistic and accentual attributes. This integrated framework enables joint optimization of translation and accent adaptation, offering a more parameter-efficient and effective model compared to traditional pipelines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)</title>
<link>https://arxiv.org/abs/2505.04640</link>
<guid>https://arxiv.org/abs/2505.04640</guid>
<content:encoded><![CDATA[
<div> Keywords: Typica.ai, Moroccan Darija, toxicity detection, LLM-based moderation, cultural adaptation  

<br /><br />Summary: This paper introduces a benchmark assessing the effectiveness of Typica.ai's Moroccan Darija toxicity detection model in comparison to prominent LLM-based moderation APIs such as OpenAI, Mistral, and Anthropic Claude. The focus is on culturally nuanced toxic content that includes implicit insults, sarcasm, and specific aggression that general models may overlook. A balanced test set was derived from the OMCD_Typica.ai_Mix dataset to evaluate the performance of each model. Key metrics used for assessment included precision, recall, F1-score, and accuracy. The findings of the study indicate that Typica.ai outperforms other models, thereby emphasizing the significance of creating culturally adapted moderation systems. The research highlights both the challenges and opportunities present in moderating content in underrepresented languages, illustrating the necessity for tailored solutions to ensure reliable and relevant content moderation. Overall, the results advocate for the development of specialized models that can address the complexities of cultural context in toxicity detection, ultimately contributing to more effective content management. <div>
arXiv:2505.04640v1 Announce Type: new 
Abstract: This paper presents a comparative benchmark evaluating the performance of Typica.ai's custom Moroccan Darija toxicity detection model against major LLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral (mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We focus on culturally grounded toxic content, including implicit insults, sarcasm, and culturally specific aggression often overlooked by general-purpose systems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset, we report precision, recall, F1-score, and accuracy, offering insights into challenges and opportunities for moderation in underrepresented languages. Our results highlight Typica.ai's superior performance, underlining the importance of culturally adapted models for reliable content moderation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture</title>
<link>https://arxiv.org/abs/2505.04642</link>
<guid>https://arxiv.org/abs/2505.04642</guid>
<content:encoded><![CDATA[
<div> Multimodal, sentiment analysis, affective computing, deep learning, emotion classification  

<br /><br />Summary:  
This article addresses multimodal sentiment analysis, a significant aspect of affective computing, which aims to understand human emotions by integrating textual, audio, and visual cues. The authors highlight that many recent models employ complex attention mechanisms and hierarchical architectures; however, they introduce a lightweight deep learning model specifically designed for utterance-level emotion classification. Their research utilizes the IEMOCAP dataset, which comprises aligned text, audio-derived numeric features, and visual descriptors. A modality-specific encoder is developed using fully connected layers with dropout regularization to enhance performance. The representations derived from individual modalities are fused through simple concatenation and subsequently processed by a dense fusion layer to effectively capture cross-modal interactions. Notably, this architecture minimizes computational overhead while achieving impressive performance, recording a classification accuracy of 92% across six emotional categories. The findings suggest that with judicious feature engineering and a modular design, simpler fusion techniques can either outperform or match the effectiveness of more complex models, particularly in environments where resources are limited. <div>
arXiv:2505.04642v1 Announce Type: new 
Abstract: Multimodal sentiment analysis, a pivotal task in affective computing, seeks to understand human emotions by integrating cues from language, audio, and visual signals. While many recent approaches leverage complex attention mechanisms and hierarchical architectures, we propose a lightweight, yet effective fusion-based deep learning model tailored for utterance-level emotion classification. Using the benchmark IEMOCAP dataset, which includes aligned text, audio-derived numeric features, and visual descriptors, we design a modality-specific encoder using fully connected layers followed by dropout regularization. The modality-specific representations are then fused using simple concatenation and passed through a dense fusion layer to capture cross-modal interactions. This streamlined architecture avoids computational overhead while preserving performance, achieving a classification accuracy of 92% across six emotion categories. Our approach demonstrates that with careful feature engineering and modular design, simpler fusion strategies can outperform or match more complex models, particularly in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation</title>
<link>https://arxiv.org/abs/2505.04643</link>
<guid>https://arxiv.org/abs/2505.04643</guid>
<content:encoded><![CDATA[
<div> Keywords: population parameters, transformer encoder, survey sampling, hate crime statistics, manual annotation

<br /><br />Summary: The article addresses the challenge of estimating population parameters in finite populations of text documents, particularly when obtaining target variable labels demands manual annotation. To tackle this issue, the authors propose a method that combines predictions from a transformer encoder neural network with traditional survey sampling estimators, using the model predictions as auxiliary variables. This approach is applied to analyze hate crime statistics in Sweden, specifically drawing from Swedish police reports. The researchers derive estimates for the yearly incidence of hate crimes as well as the extent of under-reporting by the police. They utilize several statistical methods, including the Hansen-Hurwitz estimator, difference estimation, and stratified random sampling estimation to strengthen their findings. The results suggest that when labeled training data exists, their proposed method can yield efficient estimates, significantly reducing the reliance on time-consuming manual annotation processes. Overall, this innovative method shows promise in enhancing the accuracy and efficiency of population parameter estimation in the context of social issues like hate crimes. <div>
arXiv:2505.04643v1 Announce Type: new 
Abstract: Estimating population parameters in finite populations of text documents can be challenging when obtaining the labels for the target variable requires manual annotation. To address this problem, we combine predictions from a transformer encoder neural network with well-established survey sampling estimators using the model predictions as an auxiliary variable. The applicability is demonstrated in Swedish hate crime statistics based on Swedish police reports. Estimates of the yearly number of hate crimes and the police's under-reporting are derived using the Hansen-Hurwitz estimator, difference estimation, and stratified random sampling estimation. We conclude that if labeled training data is available, the proposed method can provide very efficient estimates with reduced time spent on manual annotation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT for automated grading of short answer questions in mechanical ventilation</title>
<link>https://arxiv.org/abs/2505.04645</link>
<guid>https://arxiv.org/abs/2505.04645</guid>
<content:encoded><![CDATA[
<div> Keywords: short answer questions, automated grading, ChatGPT, postgraduate education, grading rubric  

<br /><br />Summary: The study evaluates the use of ChatGPT 4o for grading short answer questions (SAQs) in postgraduate medical education, utilizing data from 215 students' responses in an online mechanical ventilation course. ChatGPT was tasked with grading three case-based scenarios using a standardized prompt and rubric. The analysis employed complex statistical methods, including mixed-effects modeling and intraclass correlation coefficients (ICCs). Results showed that ChatGPT assigned systematically lower grades than human graders, with an average bias of -1.34 on a 10-point scale. The agreement between ChatGPT and human graders was poor, evidenced by low ICC values (ICC1 = 0.086) and negative Cohen's kappa (-0.0786), indicating a lack of meaningful agreement. Variance component analysis found minimal variability across five ChatGPT grading sessions, suggesting internal consistency but also divergence from human assessment. The study highlighted that the lowest agreement occurred in evaluative and analytic items, whereas checklist items demonstrated less disagreement. Ultimately, over 60% of grades assigned by ChatGPT were beyond acceptable discrepancies when compared to human grades, prompting caution regarding the use of large language models in high-stakes postgraduate coursework grading. <div>
arXiv:2505.04645v1 Announce Type: new 
Abstract: Standardised tests using short answer questions (SAQs) are common in postgraduate education. Large language models (LLMs) simulate conversational language and interpret unstructured free-text responses in ways aligning with applying SAQ grading rubrics, making them attractive for automated grading. We evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data from 215 students (557 short-answer responses) enrolled in an online course on mechanical ventilation (2020--2024). Deidentified responses to three case-based scenarios were presented to ChatGPT with a standardised grading prompt and rubric. Outputs were analysed using mixed-effects modelling, variance component analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks than human graders with a mean difference (bias) of -1.34 on a 10-point scale. ICC values indicated poor individual-level agreement (ICC1 = 0.086), and Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component analysis showed minimal variability among the five ChatGPT sessions (G-value = 0.87), indicating internal consistency but divergence from the human grader. The poorest agreement was observed for evaluative and analytic items, whereas checklist and prescriptive rubric items had less disagreement. We caution against the use of LLMs in grading postgraduate coursework. Over 60% of ChatGPT-assigned grades differed from human grades by more than acceptable boundaries for high-stakes assessments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
<link>https://arxiv.org/abs/2505.04649</link>
<guid>https://arxiv.org/abs/2505.04649</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, FRAME, medical paper generation, iterative refinement, evaluation framework  

<br /><br />Summary: The paper introduces the Feedback-Refined Agent Methodology (FRAME), a new framework aimed at improving the automation of medical paper generation utilizing large language models (LLMs). FRAME addresses challenges in knowledge synthesis and quality assurance through three key innovations. First, it presents a structured dataset construction method that breaks down 4,287 medical papers into essential research components via iterative refinement. Second, the tripartite architecture of FRAME consists of Generator, Evaluator, and Reflector agents working in conjunction to enhance the quality of generated content through metric-driven feedback. Lastly, a comprehensive evaluation framework combines statistical metrics with human-grounded benchmarks to assess the quality of outputs. Experimental results reveal that FRAME significantly outperforms conventional methods, achieving an average gain of 9.91% with DeepSeek V3 and showing comparable improvements with GPT-4o Mini. Human evaluations indicate that the papers generated by FRAME meet quality standards comparable to those authored by humans, particularly excelling in synthesizing future research directions. Overall, FRAME establishes a solid foundation for automating medical research paper generation while upholding rigorous academic standards. <div>
arXiv:2505.04649v1 Announce Type: new 
Abstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions</title>
<link>https://arxiv.org/abs/2505.04651</link>
<guid>https://arxiv.org/abs/2505.04651</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hypothesis generation, validation, symbolic frameworks, human-AI collaboration  

<br /><br />Summary: The article discusses the transformative role of Large Language Models (LLMs) in scientific hypothesis generation and validation. It provides a structured overview of various LLM-driven approaches, such as symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. The survey highlights techniques like retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, addressing trade-offs among interpretability, novelty, and domain alignment. Additionally, it contrasts early symbolic discovery systems (e.g., BACON, KEKADA) with current LLM pipelines that utilize in-context learning and domain adaptation methods. For the validation of hypotheses, it reviews methods including simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing the importance of iterative assessment in open-world scenarios. The article maps datasets across various fields, including biomedicine, materials science, environmental science, and social science, while introducing new resources like AHTech and CSKG-600. Finally, it outlines a roadmap for future research focusing on novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, advocating for LLMs as agents of principled and scalable scientific discovery. <div>
arXiv:2505.04651v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming scientific hypothesis generation and validation by enabling information synthesis, latent relationship discovery, and reasoning augmentation. This survey provides a structured overview of LLM-driven approaches, including symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. We examine techniques such as retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, highlighting trade-offs in interpretability, novelty, and domain alignment. We contrast early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM pipelines that leverage in-context learning and domain adaptation via fine-tuning, retrieval, and symbolic grounding. For validation, we review simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing iterative assessment in open-world contexts. The survey maps datasets across biomedicine, materials science, environmental science, and social science, introducing new resources like AHTech and CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, positioning LLMs as agents for principled, scalable scientific discovery.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Conversational Diagnostic AI with Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.04653</link>
<guid>https://arxiv.org/abs/2505.04653</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AMIE, multimodal data, diagnostic accuracy, clinical consultation

<br /><br />Summary: Large Language Models (LLMs) have shown promise in facilitating diagnostic conversations, but evaluations so far have primarily focused on text-based interactions, neglecting the real-world demands of remote medical care. To address this, the Articulate Medical Intelligence Explorer (AMIE) has been enhanced to process and interpret multimodal medical data during consultations. Utilizing Gemini 2.0 Flash, AMIE employs a dynamic dialogue framework that adapts based on patient states and evolving diagnoses. The system strategically poses follow-up questions, emulating the structured history-taking of experienced clinicians. A randomized, blinded, OSCE-style study comparing AMIE to primary care physicians (PCPs) was conducted with 105 scenarios that included various medical artifacts like skin photos and ECGs. The evaluation assessed multiple axes, including multimodal capabilities, history-taking, diagnostic accuracy, and empathy. Results indicated that AMIE outperformed PCPs in 7 out of 9 multimodal and 29 out of 32 non-multimodal categories, including diagnostic accuracy. The findings suggest significant advancements in multimodal conversational diagnostic AI, although further research is needed for real-world application. <div>
arXiv:2505.04653v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient</title>
<link>https://arxiv.org/abs/2505.04654</link>
<guid>https://arxiv.org/abs/2505.04654</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, ethical analysis, Relative Danger Coefficient, human oversight  

<br /><br />Summary: This article discusses the rapid evolution of Artificial Intelligence (AI) and Large Language Models (LLMs) in natural language processing, highlighting their impressive capabilities. However, it also addresses significant ethical issues such as safety concerns, potential for misuse, discrimination, and broader societal impacts. The study provides a comparative analysis of the ethical performance of multiple AI models, including the latest DeepSeek-V3 (R1 with and without reasoning), various versions of GPT (4o, 3.5 Turbo, 4 Turbo, o1/o3 mini), and Gemini (1.5 flash, 2.0 flash, and 2.0 flash exp). It underscores the importance of maintaining robust human oversight, particularly in high-stakes scenarios where the ramifications of AI decisions can be profound. Furthermore, the paper introduces a novel metric for assessing harm in LLMs, termed the Relative Danger Coefficient (RDC), which aims to quantify the potential risks associated with different models. The findings advocate for a thoughtful approach to AI development and deployment, ensuring ethical considerations are a fundamental aspect of AI technologies. <div>
arXiv:2505.04654v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly evolved in recent years, showcasing remarkable capabilities in natural language understanding and generation. However, these advancements also raise critical ethical questions regarding safety, potential misuse, discrimination and overall societal impact. This article provides a comparative analysis of the ethical performance of various AI models, including the brand new DeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5 Turbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp) and highlights the need for robust human oversight, especially in situations with high stakes. Furthermore, we present a new metric for calculating harm in LLMs called Relative Danger Coefficient (RDC).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction</title>
<link>https://arxiv.org/abs/2505.04655</link>
<guid>https://arxiv.org/abs/2505.04655</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Determinants of Health, deep learning, Large Language Models, classification, healthcare

<br /><br />Summary: This study focuses on the extraction of Social Determinants of Health (SDoH), which are pivotal factors that influence individuals' health statuses. Recognizing the correlation between SDoH and wellness outcomes, the authors utilized both traditional deep learning methods and Large Language Models (LLMs) to assess their effectiveness in classifying SDoHs from clinical texts. Their approach resulted in a 10-point improvement over previous benchmarks in multilabel SDoH classification. Importantly, the researchers introduced a novel method that significantly speeds up the classification processachieving a 12-fold reduction in execution timeby minimizing the reliance on expensive LLM processing. This innovative method combines the high precision capabilities of LLMs with the efficiency of traditional deep learning techniques. Moreover, the study demonstrates that traditional deep learning models can outperform LLMs, especially when trained on a dataset augmented with synthetic data. Ultimately, the findings present a more effective strategy for the automatic prediction of SDoHs, which is crucial for identifying and assisting at-risk patients. <div>
arXiv:2505.04655v1 Announce Type: new 
Abstract: Social Determinants of Health (SDoH) are economic, social and personal circumstances that affect or influence an individual's health status. SDoHs have shown to be correlated to wellness outcomes, and therefore, are useful to physicians in diagnosing diseases and in decision-making. In this work, we automatically extract SDoHs from clinical text using traditional deep learning and Large Language Models (LLMs) to find the advantages and disadvantages of each on an existing publicly available dataset. Our models outperform a previous reference point on a multilabel SDoH classification by 10 points, and we present a method and model to drastically speed up classification (12X execution time) by eliminating expensive LLM processing. The method we present combines a more nimble and efficient solution that leverages the power of the LLM for precision and traditional deep learning methods for efficiency. We also show highly performant results on a dataset supplemented with synthetic data and several traditional deep learning models that outperform LLMs. Our models and methods offer the next iteration of automatic prediction of SDoHs that impact at-risk patients.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection</title>
<link>https://arxiv.org/abs/2505.04660</link>
<guid>https://arxiv.org/abs/2505.04660</guid>
<content:encoded><![CDATA[
<div> Keywords: fall detection, synthetic data, Large Language Models, motion simulation, LSTM

<br /><br />Summary: The article addresses the challenge of training fall detection systems due to limited real-world fall data among the elderly. It investigates the use of Large Language Models (LLMs) to generate synthetic fall data. Various models, including text-to-motion (T2M, SATO, ParCo) and text-to-text (GPT4o, GPT4, Gemini), were evaluated for simulating realistic fall scenarios. Synthetic datasets created were then integrated with four real-world baseline datasets to analyze their impact on the performance of a Long Short-Term Memory (LSTM) model. The study also compares LLM-generated synthetic data with a diffusion-based method, assessing the alignment of each with actual accelerometer data distributions. Results reveal that dataset characteristics are crucial in determining the effectiveness of synthetic data, with LLM-generated datasets notably performing well in low-frequency settings (20Hz) but exhibiting instability at higher frequencies (200Hz). Furthermore, text-to-motion models tend to produce more realistic biomechanical data compared to text-to-text models, although their influence on fall detection varies. The diffusion-based synthetic data aligns closely with real data but does not consistently improve model performance. An ablation study underscores that sensor placement and fall representation are essential factors in the effectiveness of synthetic data. <div>
arXiv:2505.04660v1 Announce Type: new 
Abstract: Training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. To address this, we explore the potential of Large Language Models (LLMs) for generating synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall scenarios. We generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a Long Short-Term Memory (LSTM) model. Additionally, we compare LLM-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. Results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with LLM-generated data performing best in low-frequency settings (e.g., 20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. Diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. An ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. These findings provide insights into optimizing synthetic data generation for fall detection models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising</title>
<link>https://arxiv.org/abs/2505.04665</link>
<guid>https://arxiv.org/abs/2505.04665</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, personalized advertising, user privacy protection, BERT, data security

<br /><br />Summary: This paper explores the intersection of personalized advertising recommendations and user privacy in the context of large language models (LLMs). It begins by outlining the LLM principles, particularly the self-attention mechanism derived from the Transformer architecture, which enhances natural language understanding and generation. The study then integrates BERT (Bidirectional Encoder Representations from Transformers) with the attention mechanism to create an algorithmic model focused on personalized advertising recommendations while safeguarding user privacy. The research details the specific steps involved: data collection and preprocessing, feature construction, semantic embedding using LLMs, and ad recommendations tailored to user profiles. To address privacy concerns, it emphasizes local model training and data encryption methods that aim to prevent personal data leakage. The paper culminates in an experimental validation using real user data, demonstrating that BERT-based advertising strategies significantly enhance the click-through and conversion rates. Furthermore, the incorporated privacy protection mechanisms indicate a reduction in the risks associated with user data exposure, thereby aligning efficient advertising with privacy assurance in digital marketing. <div>
arXiv:2505.04665v1 Announce Type: new 
Abstract: Although large language models have demonstrated the potential for personalized advertising recommendations in experimental environments, in actual operations, how advertising recommendation systems can be combined with measures such as user privacy protection and data security is still an area worthy of in-depth discussion. To this end, this paper studies the personalized risks and regulatory strategies of large language models in digital advertising. This study first outlines the principles of Large Language Model (LLM), especially the self-attention mechanism based on the Transformer architecture, and how to enable the model to understand and generate natural language text. Then, the BERT (Bidirectional Encoder Representations from Transformers) model and the attention mechanism are combined to construct an algorithmic model for personalized advertising recommendations and user factor risk protection. The specific steps include: data collection and preprocessing, feature selection and construction, using large language models such as BERT for advertising semantic embedding, and ad recommendations based on user portraits. Then, local model training and data encryption are used to ensure the security of user privacy and avoid the leakage of personal data. This paper designs an experiment for personalized advertising recommendation based on a large language model of BERT and verifies it with real user data. The experimental results show that BERT-based advertising push can effectively improve the click-through rate and conversion rate of advertisements. At the same time, through local model training and privacy protection mechanisms, the risk of user privacy leakage can be reduced to a certain extent.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes</title>
<link>https://arxiv.org/abs/2505.04666</link>
<guid>https://arxiv.org/abs/2505.04666</guid>
<content:encoded><![CDATA[
<div> Keywords: building codes, Question-Answering, Retrieval-Augmented Generation, fine-tuning, Elasticsearch  

<br /><br />Summary: Building codes serve as essential regulations to ensure the design, construction, and safety of buildings, but they are often complex and challenging to navigate. This study proposes a solution through a Question-Answering (QA) system that utilizes a Retrieval-Augmented Generation (RAG) approach, comprising a retriever and a language model. The primary focus is on identifying an effective retriever method tailored for building codes while enhancing the generational capability of the language model via fine-tuning techniques. The research involved a thorough evaluation of various retrieval methods, particularly using the National Building Code of Canada (NBCC), and assessed the impact of fine-tuning on different language models with datasets derived from the NBCC. The findings revealed that Elasticsearch emerged as the most effective retriever, demonstrating robust performance. Furthermore, the study highlighted that fine-tuning language models on NBCC-specific data significantly improved their capacity to produce contextually relevant responses. By integrating a powerful retriever like Elasticsearch with fine-tuned language models, the study contributes to optimizing RAG systems, effectively addressing the complexities associated with navigating the NBCC. <div>
arXiv:2505.04666v1 Announce Type: new 
Abstract: Building codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards</title>
<link>https://arxiv.org/abs/2505.04671</link>
<guid>https://arxiv.org/abs/2505.04671</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Text-to-SQL, process reward models, reasoning chains, BIRD benchmark  

<br /><br />Summary: Recent advancements in large language models (LLMs) have notably enhanced performance on the Text-to-SQL task by utilizing their reasoning capabilities. To improve the accuracy of reasoning, external Process Reward Models (PRMs) can be integrated, although improper use may lead to incorrect SQL generation. This study proposes Reward-SQL, a systematic framework for effectively incorporating PRMs into Text-to-SQL reasoning. It adopts a "cold start, then PRM supervision" approach, starting with training the model to decompose SQL queries into structured reasoning chains using common table expressions (Chain-of-CTEs). This establishes a strong reasoning baseline. The study also examines four strategies for PRM integration, determining that combining PRMs as an online training signal with PRM-guided inference yields optimal results. Empirical results on the BIRD benchmark indicate that models leveraging a 7B PRM through Reward-SQL achieve a 13.1% performance improvement. Furthermore, the GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct reaches an accuracy of 68.9% on the BIRD development set, surpassing all baseline models of comparable size. This underscores the potential of reward-based supervision in enhancing Text-to-SQL reasoning. The code is made publicly available. <div>
arXiv:2505.04671v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation.To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a "cold start, then PRM supervision" paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning. Our code is publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM</title>
<link>https://arxiv.org/abs/2505.04673</link>
<guid>https://arxiv.org/abs/2505.04673</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Large Language Models, REVEAL Framework, harm assessment, multi-turn interactions, misinformation

<br /><br />Summary: Vision Large Language Models (VLLMs) represent a breakthrough in AI by merging image processing with text understanding, thereby improving user interaction and broadening applications. However, their complexity brings new safety and ethical issues, especially in multi-modal, multi-turn dialogues, for which existing evaluation frameworks are ineffective. To tackle this, the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework is introduced, offering a scalable and automated approach for assessing image-related harms in VLLMs. The framework encompasses automated image mining, synthetic adversarial data generation, and multi-turn conversational enhancements using crescendo attack strategies, alongside harm evaluations via metrics such as the Safety-Usability Index (SUI). Five leading VLLMsGPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtralwere assessed across three harm categories: sexual harm, violence, and misinformation. Results indicated that multi-turn interactions significantly raised defect rates compared to single-turn settings, exposing vulnerabilities. GPT-4o showed the best overall performance, with misinformation identified as a critical concern needing stronger defenses. Llama-3.2 had the highest defect rate at 16.55%, while Qwen2-VL displayed the highest refusal rate at 19.1%. <div>
arXiv:2505.04673v1 Announce Type: new 
Abstract: Vision Large Language Models (VLLMs) represent a significant advancement in artificial intelligence by integrating image-processing capabilities with textual understanding, thereby enhancing user interactions and expanding application domains. However, their increased complexity introduces novel safety and ethical challenges, particularly in multi-modal and multi-turn conversations. Traditional safety evaluation frameworks, designed for text-based, single-turn interactions, are inadequate for addressing these complexities. To bridge this gap, we introduce the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated image mining, synthetic adversarial data generation, multi-turn conversational expansion using crescendo attack strategies, and comprehensive harm assessment through evaluators like GPT-4o.
  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual harm, violence, and misinformation. Our findings reveal that multi-turn interactions result in significantly higher defect rates compared to single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably, GPT-4o demonstrated the most balanced performance as measured by our Safety-Usability Index (SUI) followed closely by Pixtral. Additionally, misinformation emerged as a critical area requiring enhanced contextual defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \%$) while Qwen2-VL showed the highest MT refusal rate ($19.1 \%$).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols</title>
<link>https://arxiv.org/abs/2505.04678</link>
<guid>https://arxiv.org/abs/2505.04678</guid>
<content:encoded><![CDATA[
<div> Keywords: cuneiform, deep learning, Akkadian, translation, archaeology  

<br /><br />Summary: This paper introduces an automated method for identifying and interpreting cuneiform characters through advanced deep-learning algorithms. Five distinct models were trained on a comprehensive dataset of cuneiform symbols and evaluated on performance metrics such as accuracy and precision. Among these, two models exhibited exceptional performance levels. These models were tested using symbols from the Hammurabi law acquisition, specifically Hammurabi Law 1, effectively recognizing the corresponding Akkadian meanings and providing accurate English translations. The authors intend to explore ensemble and stacking methods in future work, aiming to enhance detection accuracy and reliability via hybrid architectures. Additionally, this research emphasizes the linguistic connections between Akkadian, an ancient Mesopotamian language, and Arabic, highlighting their historical and cultural links. Overall, the study showcases the potential of deep learning in deciphering ancient scripts, integrating computational linguistics with archaeological insights, thereby offering significant contributions to the understanding and preservation of human history. <div>
arXiv:2505.04678v1 Announce Type: new 
Abstract: This paper presents a thoroughly automated method for identifying and interpreting cuneiform characters via advanced deep-learning algorithms. Five distinct deep-learning models were trained on a comprehensive dataset of cuneiform characters and evaluated according to critical performance metrics, including accuracy and precision. Two models demonstrated outstanding performance and were subsequently assessed using cuneiform symbols from the Hammurabi law acquisition, notably Hammurabi Law 1. Each model effectively recognized the relevant Akkadian meanings of the symbols and delivered precise English translations. Future work will investigate ensemble and stacking approaches to optimize performance, utilizing hybrid architectures to improve detection accuracy and reliability. This research explores the linguistic relationships between Akkadian, an ancient Mesopotamian language, and Arabic, emphasizing their historical and cultural linkages. This study demonstrates the capability of deep learning to decipher ancient scripts by merging computational linguistics with archaeology, therefore providing significant insights for the comprehension and conservation of human history.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.04723</link>
<guid>https://arxiv.org/abs/2505.04723</guid>
<content:encoded><![CDATA[
<div> Keywords: domain-specific, large language models, Chinese SOAEs, continual pre-training, inference acceleration

<br /><br />Summary: This study addresses significant challenges in developing domain-specific large language models (LLMs) for Chinese state-owned assets and enterprises (SOAEs). The limitations identified include model capacity constraints, excessive reliance on domain-specific supervised fine-tuning (SFT) data, and inefficient inference acceleration. The authors propose the SOAEsV2-7B/72B model series, developed through a three-phase framework: Firstly, continual pre-training integrates domain knowledge while keeping base capabilities intact. Secondly, domain-progressive SFT employs a curriculum-based learning strategy, transitioning from weakly relevant conversational data to expert-annotated SOAE datasets. Lastly, distillation-enhanced speculative decoding accelerates inference via logit distillation between 72B target and 7B draft models, achieving a speedup of 1.39-1.52 times without compromising quality. Experimental results affirm that the domain-specific pre-training retains 99.8% of original capabilities while enhancing domain performanceshowcasing a 1.08 times improvement in Rouge-1 scores and a 1.17 times enhancement in BLEU-4 scores. Further ablation studies revealed that domain-progressive SFT surpasses single-stage training, yielding improvements in both Rouge-1 and BLEU-4 scores. This work presents a comprehensive approach for optimizing SOAEs LLMs, effectively bridging general language capabilities with domain-specific expertise. <div>
arXiv:2505.04723v1 Announce Type: new 
Abstract: This study addresses key challenges in developing domain-specific large language models (LLMs) for Chinese state-owned assets and enterprises (SOAEs), where current approaches face three limitations: 1) constrained model capacity that limits knowledge integration and cross-task adaptability; 2) excessive reliance on domain-specific supervised fine-tuning (SFT) data, which neglects the broader applicability of general language patterns; and 3) inefficient inference acceleration for large models processing long contexts. In this work, we propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase framework: 1) continual pre-training integrates domain knowledge while retaining base capabilities; 2) domain-progressive SFT employs curriculum-based learning strategy, transitioning from weakly relevant conversational data to expert-annotated SOAEs datasets to optimize domain-specific tasks; 3) distillation-enhanced speculative decoding accelerates inference via logit distillation between 72B target and 7B draft models, achieving 1.39-1.52$\times$ speedup without quality loss. Experimental results demonstrate that our domain-specific pre-training phase maintains 99.8% of original general language capabilities while significantly improving domain performance, resulting in a 1.08$\times$ improvement in Rouge-1 score and a 1.17$\times$ enhancement in BLEU-4 score. Ablation studies further show that domain-progressive SFT outperforms single-stage training, achieving 1.02$\times$ improvement in Rouge-1 and 1.06$\times$ in BLEU-4. Our work introduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs, bridging the gap between general language capabilities and domain-specific expertise.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence</title>
<link>https://arxiv.org/abs/2505.04785</link>
<guid>https://arxiv.org/abs/2505.04785</guid>
<content:encoded><![CDATA[
<div> Keywords: Tang dynasty, Song dynasty, floral motifs, sentiment analysis, artistic representation

<br /><br />Summary: This study explores the intricate relationship between literary emotions and visual culture during the Tang (618-907) and Song (960-1279) dynasties, periods renowned for their rich cultural expression. It addresses a gap in scholarship that has largely viewed literary and artistic domains in isolation. By utilizing BERT-based sentiment analysis, the research quantifies emotional patterns related to floral imagery in classical poetry, particularly focusing on peony and plum blossom motifs. The study detects significant shifts in emotional connotations between the two dynasties. To validate these findings, the emotional patterns identified in poetry are cross-referenced with visual representations found in textiles, ceramics, and other forms of material culture. The approach integrates computational methodologies with traditional sinological analysis, highlighting previously unrecognized connections between literary expression and artistic representation. This comprehensive examination not only enhances our understanding of cultural dynamics during these historical periods but also illustrates the importance of interdisciplinary methods in the study of humanities. Overall, the research reveals the dynamic interplay between poetry and visual arts, contributing to a deeper appreciation of Chinese cultural heritage. <div>
arXiv:2505.04785v1 Announce Type: new 
Abstract: The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an extraordinary flourishing of Chinese cultural expression, where floral motifs served as a dynamic medium for both poetic sentiment and artistic design. While previous scholarship has examined these domains independently, the systematic correlation between evolving literary emotions and visual culture remains underexplored. This study addresses that gap by employing BERT-based sentiment analysis to quantify emotional patterns in floral imagery across Tang Song poetry, then validating these patterns against contemporaneous developments in decorative arts.Our approach builds upon recent advances in computational humanities while remaining grounded in traditional sinological methods. By applying a fine tuned BERT model to analyze peony and plum blossom imagery in classical poetry, we detect measurable shifts in emotional connotations between the Tang and Song periods. These textual patterns are then cross berenced with visual evidence from textiles, ceramics, and other material culture, revealing previously unrecognized synergies between literary expression and artistic representation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Osiris: A Lightweight Open-Source Hallucination Detection System</title>
<link>https://arxiv.org/abs/2505.04844</link>
<guid>https://arxiv.org/abs/2505.04844</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, hallucinations, multi-hop QA dataset, fine-tuning, RAGTruth

<br /><br />Summary: Retrieval-Augmented Generation (RAG) systems are increasingly popular for improving the factual accuracy of outputs from Large Language Models (LLMs). However, the problem of hallucinationswhere the model generates responses that are not faithful to the provided contextposes challenges for deploying these systems in production. Current methods for detecting hallucinations rely on either human evaluation or closed-source models, which face scalability issues due to their high costs and slow inference speeds. This work introduces a perturbed multi-hop question-answering dataset that induces hallucinations to facilitate research in this area. The authors present a supervised fine-tuning approach using their dataset, achieving superior recall with a 7B parameter model compared to the performance of GPT-4o on the RAGTruth hallucination detection benchmark. Additionally, their model competes well in terms of precision and accuracy while using significantly fewer parameters. This advancement demonstrates a promising solution to the challenges of hallucination detection in RAG systems, making it more feasible for practical applications. The code related to this research has been made publicly available in their repository. <div>
arXiv:2505.04844v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems have gained widespread adoption by application builders because they leverage sources of truth to enable Large Language Models (LLMs) to generate more factually sound responses. However, hallucinations, instances of LLM responses that are unfaithful to the provided context, often prevent these systems from being deployed in production environments. Current hallucination detection methods typically involve human evaluation or the use of closed-source models to review RAG system outputs for hallucinations. Both human evaluators and closed-source models suffer from scaling issues due to their high costs and slow inference speeds. In this work, we introduce a perturbed multi-hop QA dataset with induced hallucinations. Via supervised fine-tuning on our dataset, we achieve better recall with a 7B model than GPT-4o on the RAGTruth hallucination detection benchmark and offer competitive performance on precision and accuracy, all while using a fraction of the parameters. Code is released at our repository.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards</title>
<link>https://arxiv.org/abs/2505.04847</link>
<guid>https://arxiv.org/abs/2505.04847</guid>
<content:encoded><![CDATA[
<div> Keywords: hallucinations, LLMs, summarization, FaithJudge, evaluation

<br /><br />Summary: Hallucinations remain a significant issue for Large Language Models (LLMs), particularly in tasks involving summarization. While Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations by providing contextual grounding, LLMs still often produce unsupported information or contradictions. This paper focuses on measuring the prevalence of hallucinations in various LLMs during summarization tasks, analyzing their performance through the existing Hughes Hallucination Evaluation Model (HHEM) and Vectara's Hallucination Leaderboard. Despite generating interest, the authors identify challenges with HHEM and current methods of hallucination detection, evaluating their effectiveness with existing datasets. To address these limitations, the authors propose FaithJudge, an innovative LLM-as-a-judge framework that uses few-shot human annotations to significantly enhance automated hallucination evaluation. The introduction of FaithJudge leads to the development of an improved hallucination leaderboard, alongside the existing Vectara leaderboard, facilitating more dependable benchmarking of LLMs with respect to hallucinations in RAG settings. This research contributes to the ongoing discourse surrounding LLM performance and the persistent challenge of hallucinations in AI-generated responses, presenting new frameworks for better assessment and understanding. <div>
arXiv:2505.04847v1 Announce Type: new 
Abstract: Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce hallucinations by grounding responses in contexts. However, even when provided context, LLMs still frequently introduce unsupported information or contradictions. This paper presents our efforts to measure LLM hallucinations with a focus on summarization tasks, assessing how often various LLMs introduce hallucinations when summarizing documents. We discuss Vectara's existing LLM hallucination leaderboard, based on the Hughes Hallucination Evaluation Model (HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great research interest, we examine challenges faced by HHEM and current hallucination detection methods by analyzing the effectiveness of these methods on existing hallucination datasets. To address these limitations, we propose FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination annotations, which substantially improves automated LLM hallucination evaluation over current methods. We introduce an enhanced hallucination leaderboard centered on FaithJudge, alongside our current hallucination leaderboard, enabling more reliable benchmarking of LLMs for hallucinations in RAG.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>